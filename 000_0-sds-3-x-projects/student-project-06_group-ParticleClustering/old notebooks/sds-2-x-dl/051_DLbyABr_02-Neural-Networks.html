
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SDS-2.x, Scalable Data Engineering Science &#8212; Scalable Data Science &amp; Distributed Machine Learning</title>
    
  <link rel="stylesheet" href="../../../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../../index.html">
  
  <img src="../../../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Scalable Data Science & Distributed Machine Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/000_ScaDaMaLe.html">
   Introduction
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction to Apache Spark Core
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/001_whySpark.html">
   Why Apache Spark?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/002_00_loginToDatabricks.html">
   databricks community edition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/002_00_loginToDatabricks.html#essentials-of-databricks-cloud-dbc-in-a-big-hurry">
   Essentials of Databricks Cloud (DBC) in a Big Hurry
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/002_00_loginToDatabricks.html#dbc-essentials-what-is-databricks-cloud">
   DBC Essentials: What is Databricks Cloud?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/002_00_loginToDatabricks.html#dbc-essentials-shard-cluster-notebook-and-dashboard">
   DBC Essentials: Shard, Cluster, Notebook and Dashboard
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/002_00_loginToDatabricks.html#dbc-essentials-team-state-collaboration-elastic-resources">
   DBC Essentials: Team, State, Collaboration, Elastic Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/002_00_loginToDatabricks.html#you-should-all-have-databricks-community-edition-account-by-now">
   You Should All Have databricks community edition account by now!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/002_00_loginToDatabricks.html#import-course-content-now">
   Import Course Content Now!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/002_00_loginToDatabricks.html#cloud-free-computing-environment-optional-but-recommended">
   Cloud-free Computing Environment (Optional but recommended)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/002_01_multiLingualNotebooks.html">
   Notebooks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/002_01_multiLingualNotebooks.html#further-reference-homework-recurrrent-points-of-reference">
   Further Reference / Homework / Recurrrent Points of Reference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_00_scalaCrashCourse.html">
   Introduction to Scala through Scala Notebook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_00_scalaCrashCourse.html#scala-in-your-own-computer">
   Scala in your own computer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_00_scalaCrashCourse.html#scala-resources">
   Scala Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_00_scalaCrashCourse.html#introduction-to-scala">
   Introduction to Scala
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_00_scalaCrashCourse.html#let-s-get-our-hands-dirty-in-scala">
   Let’s get our hands dirty in Scala
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_00_scalaCrashCourse.html#scala-types">
   Scala Types
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_00_scalaCrashCourse.html#expressions">
   Expressions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_00_scalaCrashCourse.html#blocks">
   Blocks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_00_scalaCrashCourse.html#functions">
   Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_00_scalaCrashCourse.html#methods">
   Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_00_scalaCrashCourse.html#classes">
   Classes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_00_scalaCrashCourse.html#case-classes">
   Case Classes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_00_scalaCrashCourse.html#objects">
   Objects
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_00_scalaCrashCourse.html#traits">
   Traits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_00_scalaCrashCourse.html#main-method">
   Main Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_00_scalaCrashCourse.html#what-i-try-not-do-while-learning-a-new-language">
   What I try not do while learning a new language?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_01_scalaCrashCourse.html">
   Scala Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_01_scalaCrashCourse.html#let-s-continue-to-get-our-hands-dirty-in-scala">
   Let’s continue to get our hands dirty in Scala
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_01_scalaCrashCourse.html#scala-type-hierarchy">
   Scala Type Hierarchy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_01_scalaCrashCourse.html#scala-collections">
   Scala Collections
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_01_scalaCrashCourse.html#exercise-in-functional-programming">
   Exercise in Functional Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_01_scalaCrashCourse.html#lazy-evaluation">
   Lazy Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/003_01_scalaCrashCourse.html#recursions">
   Recursions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/004_RDDsTransformationsActions.html">
   Introduction to Spark
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/004_RDDsTransformationsActions.html#spark-cluster-overview">
   Spark Cluster Overview:
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/005_RDDsTransformationsActionsHOMEWORK.html">
   HOMEWORK notebook - RDDs Transformations and Actions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/006a_PipedRDD.html">
   Piped RDDs and Bayesian AB Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/006a_PipedRDD.html#from-a-local-collection">
   From a Local Collection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/006a_PipedRDD.html#glom">
   glom
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/006a_PipedRDD.html#checkpointing">
   Checkpointing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/006a_PipedRDD.html#pipe-rdds-to-system-commands">
   Pipe RDDs to System Commands
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/006a_PipedRDD.html#mappartitions">
   mapPartitions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/006a_PipedRDD.html#foreachpartition">
   foreachPartition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/006a_PipedRDD.html#numerically-rigorous-bayesian-ab-testing">
   Numerically Rigorous Bayesian AB Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/006a_PipedRDD.html#usage-instructions-for-isit1or2coins">
   Usage instructions for IsIt1or2Coins
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/006_WordCount.html">
   Word Count on US State of the Union (SoU) Addresses
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction to Apache Spark SQL
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007_SparkSQLIntroBasics.html">
   Introduction to Spark SQL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007_SparkSQLIntroBasics.html#overview">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007_SparkSQLIntroBasics.html#datasets-and-dataframes">
   Datasets and DataFrames
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007_SparkSQLIntroBasics.html#getting-started-in-spark-2-x">
   Getting Started in Spark 2.x
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007a_SparkSQLProgGuide_HW.html">
   Spark Sql Programming Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007b_SparkSQLProgGuide_HW.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007b_SparkSQLProgGuide_HW.html#id1">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007c_SparkSQLProgGuide_HW.html">
   Getting Started - Exercise
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007d_SparkSQLProgGuide_HW.html">
   Data Sources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007d_SparkSQLProgGuide_HW.html#id1">
   Data Sources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007e_SparkSQLProgGuide_HW.html">
   Performance Tuning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007f_SparkSQLProgGuide_HW.html">
   Distributed SQL Engine
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007f_SparkSQLProgGuide_HW.html#id1">
   Distributed SQL Engine
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007g_PivotInSQL.html">
   ScaDaMaLe, Scalable Data Science and Distributed Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007g_PivotInSQL.html#sql-pivoting-since-spark-2-4">
   SQL Pivoting since Spark 2.4
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007g_PivotInSQL.html#load-data">
   Load Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007g_PivotInSQL.html#pivoting-in-sql">
   Pivoting in SQL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007g_PivotInSQL.html#pivoting-with-multiple-aggregate-expressions">
   Pivoting with Multiple Aggregate Expressions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007g_PivotInSQL.html#pivoting-with-multiple-grouping-columns">
   Pivoting with Multiple Grouping Columns
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/007g_PivotInSQL.html#pivoting-with-multiple-pivot-columns">
   Pivoting with Multiple Pivot Columns
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/008_DiamondsPipeline_01ETLEDA.html">
   Diamonds ML Pipeline Workflow - DataFrame ETL and EDA Part
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/008_DiamondsPipeline_01ETLEDA.html#step-1-load-data-as-dataframe">
   Step 1. Load data as DataFrame
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/008_DiamondsPipeline_01ETLEDA.html#step-2-understand-the-data">
   Step 2. Understand the data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/008_DiamondsPipeline_01ETLEDA.html#let-us-run-through-some-basic-inteactive-sql-queries-next">
   Let us run through some basic inteactive SQL queries next
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/008_DiamondsPipeline_01ETLEDA.html#why-do-we-need-to-know-these-interactive-sql-queries">
   Why do we need to know these interactive SQL queries?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/008_DiamondsPipeline_01ETLEDA.html#we-will-continue-later-with-ml-pipelines-to-do-prediction-with-a-fitted-model-from-this-dataset">
   We will continue later with ML pipelines to do prediction with a fitted model from this dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/009_PowerPlantPipeline_01ETLEDA.html">
   Power Plant ML Pipeline Application - DataFrame Part
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/009_PowerPlantPipeline_01ETLEDA.html#step-1-business-understanding">
   Step 1: Business Understanding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/009_PowerPlantPipeline_01ETLEDA.html#step-2-load-your-data">
   Step 2: Load Your Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/009_PowerPlantPipeline_01ETLEDA.html#use-this-for-other-smallish-datasets-you-want-to-import-to-your-ce">
   USE THIS FOR OTHER SMALLish DataSets you want to import to your CE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/009_PowerPlantPipeline_01ETLEDA.html#step-3-explore-your-data">
   Step 3: Explore Your Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/009_PowerPlantPipeline_01ETLEDA.html#step-4-visualize-your-data">
   Step 4: Visualize Your Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/010_wikipediaClickStream_01ETLEDA.html">
   Wiki Clickstream Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/010_wikipediaClickStream_01ETLEDA.html#wikipedia-logo">
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/033_OBO_LoadExtract.html">
   Old Bailey Online Data Analysis in Apache Spark
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/033_OBO_LoadExtract.html#analysing-the-full-old-bailey-online-sessions-papers-dataset">
   Analysing the Full Old Bailey Online Sessions Papers Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html">
   Piped RDDs and Bayesian AB Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#from-a-local-collection">
   From a Local Collection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#glom">
   glom
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#checkpointing">
   Checkpointing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#pipe-rdds-to-system-commands">
   Pipe RDDs to System Commands
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#mappartitions">
   mapPartitions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#foreachpartition">
   foreachPartition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#numerically-rigorous-bayesian-ab-testing">
   Numerically Rigorous Bayesian AB Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#usage-instructions-for-isit1or2coins">
   Usage instructions for IsIt1or2Coins
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#parsing-the-output-from-isit1or2coins">
   Parsing the output from
   <code class="docutils literal notranslate">
    <span class="pre">
     IsIt1or2Coins
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html#providing-case-classes-for-input-and-output-for-easy-spark-communication">
   Providing case classes for input and output for easy spark communication
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/035_LDA_CornellMovieDialogs.html">
   Topic Modeling of Movie Dialogs with Latent Dirichlet Allocation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Distribute Deep Learning with Horovod
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_7-sds-3-x-ddl/00_DDL_Introduction.html">
   Introduction to Distributed Deep Learning (DDL) with Horovod over Tensorflow/keras and Pytorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_7-sds-3-x-ddl/0x_mnist-tensorflow-keras.html">
   Distributed deep learning training using TensorFlow and Keras with HorovodRunner
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_7-sds-3-x-ddl/0y_mnist-pytorch.html">
   Distributed deep learning training using PyTorch with HorovodRunner for MNIST
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  WASP AI-Track Student Projects
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/035_LDA_CornellMovieDialogs.html">
   Topic Modeling of Movie Dialogs with Latent Dirichlet Allocation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Voluntary Student Projects
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/030_Spark_GDELT_project.html">
   Plugging into GDELT Streams - TODO - IN PROGRESS
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/030_Spark_GDELT_project.html#this-is-just-dipping-our-pinky-toe-in-this-ocean-of-information">
   This is just dipping our pinky toe in this ocean of information!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../000_1-sds-3-x/030_Spark_GDELT_project.html#download-from-gdelt-project">
   Download from gdelt-project
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../../_sources/000_0-sds-3-x-projects/student-project-06_group-ParticleClustering/old notebooks/sds-2-x-dl/051_DLbyABr_02-Neural-Networks.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://lamastex.github.io/ScaDaMaLe/index.html"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://lamastex.github.io/ScaDaMaLe/index.html/issues/new?title=Issue%20on%20page%20%2F000_0-sds-3-x-projects/student-project-06_group-ParticleClustering/old notebooks/sds-2-x-dl/051_DLbyABr_02-Neural-Networks.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   SDS-2.x, Scalable Data Engineering Science
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#artificial-neural-network-perceptron">
   Artificial Neural Network - Perceptron
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-perceptron">
     Linear Perceptron
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-network-with-keras">
     Neural Network with Keras
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#keras-is-a-high-level-api-for-neural-networks-and-deep-learning">
       Keras is a High-Level API for Neural Networks and Deep Learning
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#being-able-to-go-from-idea-to-result-with-the-least-possible-delay-is-key-to-doing-good-research">
         “
         <em>
          Being able to go from idea to result with the least possible delay is key to doing good research.
         </em>
         ”
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#keras-has-wide-adoption-in-industry">
     Keras has wide adoption in industry
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#we-ll-build-a-dense-feed-forward-shallow-network">
       We’ll build a “Dense Feed-Forward Shallow” Network:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-gradient-descent">
       Training: Gradient Descent
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#some-ideas-to-help-build-your-intuition">
         Some ideas to help build your intuition
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-backpropagation">
       Training: Backpropagation
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ok-so-we-ve-come-up-with-a-very-slow-way-to-perform-a-linear-regression">
     Ok so we’ve come up with a very slow way to perform a linear regression.
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#welcome-to-neural-networks-in-the-1960s">
       <em>
        Welcome to Neural Networks in the 1960s!
       </em>
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#watch-closely-now-because-this-is-where-the-magic-happens">
       Watch closely now because this is where the magic happens…
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#non-linearity-perceptron-universal-approximation">
   Non-Linearity + Perceptron = Universal Approximation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#where-does-the-non-linearity-fit-in">
     Where does the non-linearity fit in?
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#now-the-network-can-learn-non-linear-functions">
     Now the network can “learn” non-linear functions
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#where-does-the-signal-go-from-these-neurons">
     Where does the signal “go” from these neurons?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#what-is-different-here">
       What is different here?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rectified-linear-unit-relu">
     Rectified Linear Unit (ReLU)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#go-change-your-hidden-layer-activation-from-sigmoid-to-relu">
     Go change your hidden-layer activation from ‘sigmoid’ to ‘relu’
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multilayer-networks">
   Multilayer Networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#so-instead-of-just-feature-weighting-and-combining-we-have-new-feature-learning">
     So instead of just feature weighting and combining, we have new feature learning!
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#let-s-try-it">
     Let’s try it!
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#our-network-has-gone-meta">
     Our network has “gone meta”
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#congrats-you-have-built-your-first-deep-learning-model">
     Congrats! You have built your first deep-learning model!
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="sds-2-x-scalable-data-engineering-science">
<h1><a class="reference external" href="https://lamastex.github.io/scalable-data-science/sds/2/x/">SDS-2.x, Scalable Data Engineering Science</a><a class="headerlink" href="#sds-2-x-scalable-data-engineering-science" title="Permalink to this headline">¶</a></h1>
<p>This is a 2019 augmentation and update of <a class="reference external" href="https://www.linkedin.com/in/adbreind">Adam
Breindel</a>’s initial notebooks.</p>
</div>
<div class="section" id="artificial-neural-network-perceptron">
<h1>Artificial Neural Network - Perceptron<a class="headerlink" href="#artificial-neural-network-perceptron" title="Permalink to this headline">¶</a></h1>
<p>The field of artificial neural networks started out with an
electromechanical binary unit called a perceptron.</p>
<p>The perceptron took a weighted set of input signals and chose an ouput
state (on/off or high/low) based on a threshold.</p>
<p>&lt;img src=”http://i.imgur.com/c4pBaaU.jpg”&gt;</p>
<p>(raaz) Thus, the perceptron is defined by:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split} f(1, x*1,x*2,\\ldots , x*n \\, ; \\, w*0,w*1,w*2,\\ldots , w*n) =
\\begin{cases} 1 &amp; \\text{if} \\quad \\sum*{i=0}^n w*i x*i &amp;gt; 0 \\ 0 &amp;
\\text{otherwise} \\end{cases} $$ and implementable with the following
arithmetical and logical unit (ALU) operations in a machine:\end{split}\\\begin{split}-   n inputs from one $n$-dimensional data point: $x*1,x*2,\\ldots x\_n
    \\, \\in \\, \\mathbb{R}^n$
-   arithmetic operations
    -   n+1 multiplications
    -   n additions
-   boolean operations
    -   one if-then on an inequality
-   one output $o \\in {0,1}$, i.e., $o$ belongs to the set containing
    $0$ and $1$
-   n+1 parameters of interest\end{split}\\\begin{split}This is just a hyperplane given by a dot product of $n+1$ known inputs
and $n+1$ unknown parameters that can be estimated. This hyperplane can
be used to define a hyperplane that partitions $\\mathbb{R}^{n+1}$, the
real Euclidean space, into two parts labelled by the outputs $0$ and
$1$.\end{split}\\\begin{split}The problem of finding estimates of the parameters,
$(\\hat{w}*0,\\hat{w}*1,\\hat{w}*2,\\ldots \\hat{w}*n) \\in
\\mathbb{R}^{(n+1)}$, in some statistically meaningful manner for a
predicting task by using the training data given by, say $k$ *labelled
points*, where you know both the input and output: $$ \\left( ( \\, 1,
x*1^{(1)},x*2^{(1)}, \\ldots x*n^{(1)}), (o^{(1)}) \\, ), \\, ( \\, 1,
x*1^{(2)},x*2^{(2)}, \\ldots x*n^{(2)}), (o^{(2)}) \\, ), \\, \\ldots
\\, , ( \\, 1, x*1^{(k)},x*2^{(k)}, \\ldots x\_n^{(k)}), (o^{(k)}) \\, )
\\right) \\, \\in \\, (\\mathbb{R}^{n+1} \\times { 0,1 } )^k $$ is the
machine learning problem here.\end{split}\\\begin{split}Succinctly, we are after a random mapping, denoted below by
$\\mapsto*{\\rightsquigarrow}$, called the *estimator*: \end{split}\end{aligned}\end{align} \]</div>
<p>(\mathbb{R}^{n+1} \times {0,1})^k \mapsto*{\rightsquigarrow} \,
\left( \, \mathtt{model}( (1,x<em>1,x</em>2,\ldots,x*n) \,;\,
(\hat{w}*0,\hat{w}*1,\hat{w}*2,\ldots \hat{w}_n)) :
\mathbb{R}^{n+1} \to {0,1} \, \right) $<span class="math notranslate nohighlight">\( which takes *random*
labelled dataset (to understand random here think of two scientists
doing independent experiments to get their own training datasets) of
size \)</span>k$ and returns a <em>model</em>. These mathematical notions correspond
exactly to the <code class="docutils literal notranslate"><span class="pre">estimator</span></code> and <code class="docutils literal notranslate"><span class="pre">model</span></code> (which is a <code class="docutils literal notranslate"><span class="pre">transformer</span></code>) in the
language of Apache Spark’s Machine Learning Pipleines we have seen
before.</p>
<p>We can use this <code class="docutils literal notranslate"><span class="pre">transformer</span></code> for <em>prediction</em> of <em>unlabelled data</em>
where we only observe the input and what to know the output under some
reasonable assumptions.</p>
<p>Of course we want to be able to generalize so we don’t overfit to the
training data using some <em>empirical risk minisation rule</em> such as
cross-validation. Again, we have seen these in Apache Spark for other ML
methods like linear regression and decision trees.</p>
<p>If the output isn’t right, we can adjust the weights, threshold, or bias
(<span class="math notranslate nohighlight">\(x\_0\)</span> above)</p>
<p>The model was inspired by discoveries about the neurons of animals, so
hopes were quite high that it could lead to a sophisticated machine.
This model can be extended by adding multiple neurons in parallel. And
we can use linear output instead of a threshold if we like for the
output.</p>
<p>If we were to do so, the output would look like <span class="math notranslate nohighlight">\({x \\cdot w} + w\_0\)</span>
(this is where the vector multiplication and, eventually, matrix
multiplication, comes in)</p>
<p>When we look at the math this way, we see that despite this being an
interesting model, it’s really just a fancy linear calculation.</p>
<p>And, in fact, the proof that this model – being linear – could not
solve any problems whose solution was nonlinear … led to the first of
several “AI / neural net winters” when the excitement was quickly
replaced by disappointment, and most research was abandoned.</p>
<div class="section" id="linear-perceptron">
<h2>Linear Perceptron<a class="headerlink" href="#linear-perceptron" title="Permalink to this headline">¶</a></h2>
<p>We’ll get to the non-linear part, but the linear perceptron model is a
great way to warm up and bridge the gap from traditional linear
regression to the neural-net flavor.</p>
<p>Let’s look at a problem – the diamonds dataset from R – and analyze it
using two traditional methods in Scikit-Learn, and then we’ll start
attacking it with neural networks!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="n">input_file</span> <span class="o">=</span> <span class="s2">&quot;/dbfs/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv&quot;</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">input_file</span><span class="p">,</span> <span class="n">header</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">IPython.display</span> <span class="k">as</span> <span class="nn">disp</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.width&#39;</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>   Unnamed: 0  carat        cut color clarity  depth  table  price     x     y     z
0           1   0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43
1           2   0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31
2           3   0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31
3           4   0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63
4           5   0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75
5           6   0.24  Very Good     J    VVS2   62.8   57.0    336  3.94  3.96  2.48
6           7   0.24  Very Good     I    VVS1   62.3   57.0    336  3.95  3.98  2.47
7           8   0.26  Very Good     H     SI1   61.9   55.0    337  4.07  4.11  2.53
8           9   0.22       Fair     E     VS2   65.1   61.0    337  3.87  3.78  2.49
9          10   0.23  Very Good     H     VS1   59.4   61.0    338  4.00  4.05  2.39
</pre></div>
</div>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">disp</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">df2</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>   carat      cut color clarity  depth  table  price     x     y     z
0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43
1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31
2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31
</pre></div>
</div>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">df3</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span> <span class="c1"># this gives a one-hot encoding of categorial variables</span>

<span class="n">disp</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">df3</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">18</span><span class="p">)][:</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>   cut_Fair  cut_Good  cut_Ideal  cut_Premium  cut_Very Good  color_D  color_E  color_F  color_G  color_H  color_I
0         0         0          1            0              0        0        1        0        0        0        0
1         0         0          0            1              0        0        1        0        0        0        0
2         0         1          0            0              0        0        1        0        0        0        0
</pre></div>
</div>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># pre-process to get y</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df3</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">3</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">y</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="c1"># preprocess and reshape X as a matrix</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df3</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df3</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span>
<span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># break the dataset into training and test set with a 75% and 25% split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Define a decisoin tree model with max depth 10</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># fit the decision tree to the training data to get a fitted model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># predict the features or X values of the test data using the fitted model</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># print the MSE performance measure of the fit by comparing the predicted versus the observed values of y </span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RMSE </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span> <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>RMSE 726.921870
</pre></div>
</div>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>

<span class="c1"># Do the same with linear regression and not a worse MSE</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">linear_model</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RMSE </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span> <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>RMSE 1124.086095
</pre></div>
</div>
</div></blockquote>
<p>Now that we have a baseline, let’s build a neural network – linear at
first – and go further.</p>
</div>
<div class="section" id="neural-network-with-keras">
<h2>Neural Network with Keras<a class="headerlink" href="#neural-network-with-keras" title="Permalink to this headline">¶</a></h2>
<div class="section" id="keras-is-a-high-level-api-for-neural-networks-and-deep-learning">
<h3><a class="reference external" href="https://keras.io/">Keras</a> is a High-Level API for Neural Networks and Deep Learning<a class="headerlink" href="#keras-is-a-high-level-api-for-neural-networks-and-deep-learning" title="Permalink to this headline">¶</a></h3>
<p>&lt;img
src=”https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png”
width=600&gt;</p>
<div class="section" id="being-able-to-go-from-idea-to-result-with-the-least-possible-delay-is-key-to-doing-good-research">
<h4>“<em>Being able to go from idea to result with the least possible delay is key to doing good research.</em>”<a class="headerlink" href="#being-able-to-go-from-idea-to-result-with-the-least-possible-delay-is-key-to-doing-good-research" title="Permalink to this headline">¶</a></h4>
<p>Maintained by Francois Chollet at Google, it provides</p>
<ul class="simple">
<li><p>High level APIs</p></li>
<li><p>Pluggable backends for Theano, TensorFlow, CNTK, MXNet</p></li>
<li><p>CPU/GPU support</p></li>
<li><p>The now-officially-endorsed high-level wrapper for TensorFlow; a
version ships in TF</p></li>
<li><p>Model persistence and other niceties</p></li>
<li><p>JavaScript, iOS, etc. deployment</p></li>
<li><p>Interop with further frameworks, like DeepLearning4J, Spark DL
Pipelines …</p></li>
</ul>
<p>Well, with all this, why would you ever <em>not</em> use Keras?</p>
<p>As an API/Facade, Keras doesn’t directly expose all of the internals you
might need for something custom and low-level … so you might need to
implement at a lower level first, and then perhaps wrap it to make it
easily usable in Keras.</p>
<p>Mr. Chollet compiles stats (roughly quarterly) on “[t]he state of the
deep learning landscape: GitHub activity of major libraries over the
past quarter (tickets, forks, and contributors).”</p>
<p>(October 2017: https://twitter.com/fchollet/status/915366704401719296;
https://twitter.com/fchollet/status/915626952408436736)
&lt;table&gt;&lt;tr&gt;&lt;td&gt;<strong>GitHub</strong>&lt;br&gt; &lt;img
src=”https://i.imgur.com/Dru8N9K.jpg” width=600&gt;
&lt;/td&gt;&lt;td&gt;<strong>Research</strong>&lt;br&gt; &lt;img
src=”https://i.imgur.com/i23TAwf.png”
width=600&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</p>
</div>
</div>
</div>
<div class="section" id="keras-has-wide-adoption-in-industry">
<h2>Keras has wide adoption in industry<a class="headerlink" href="#keras-has-wide-adoption-in-industry" title="Permalink to this headline">¶</a></h2>
<p>&lt;img
src=”https://s3.amazonaws.com/keras.io/img/dl<em>frameworks</em>power_scores.png”
width=600&gt;</p>
<div class="section" id="we-ll-build-a-dense-feed-forward-shallow-network">
<h3>We’ll build a “Dense Feed-Forward Shallow” Network:<a class="headerlink" href="#we-ll-build-a-dense-feed-forward-shallow-network" title="Permalink to this headline">¶</a></h3>
<p>(the number of units in the following diagram does not exactly match
ours) &lt;img src=”https://i.imgur.com/84fxFKa.png”&gt;</p>
<p>Grab a Keras API cheat sheet from
https://s3.amazonaws.com/assets.datacamp.com/blog<em>assets/Keras</em>Cheat<em>Sheet</em>Python.pdf</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>

<span class="c1"># we are going to add layers sequentially one after the other (feed-forward) to our neural network model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="c1"># the first layer has 30 nodes (or neurons) with input dimension 26 for our diamonds data</span>
<span class="c1"># we will use Nomal or Guassian kernel to initialise the weights we want to estimate</span>
<span class="c1"># our activation function is linear (to mimic linear regression)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">26</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">))</span>
<span class="c1"># the next layer is for the response y and has only one node</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">))</span>
<span class="c1"># compile the model with other specifications for loss and type of gradient descent optimisation routine</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mean_squared_error&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mean_squared_error&#39;</span><span class="p">])</span>
<span class="c1"># fit the model to the training data using stochastic gradient descent with a batch-size of 200 and 10% of data held out for validation</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;test set RMSE: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Train on 36409 samples, validate on 4046 samples
Epoch 1/10
  200/36409 [..............................] - ETA: 6s - loss: 31224094.0000 - mean_squared_error: 31224094.0000 9400/36409 [======&gt;.......................] - ETA: 0s - loss: 30944916.5957 - mean_squared_error: 30944916.595718800/36409 [==============&gt;...............] - ETA: 0s - loss: 30824087.2340 - mean_squared_error: 30824087.234028400/36409 [======================&gt;.......] - ETA: 0s - loss: 30608667.0986 - mean_squared_error: 30608667.098636409/36409 [==============================] - 0s - loss: 30675831.5550 - mean_squared_error: 30675831.5550 - val_loss: 30065817.1715 - val_mean_squared_error: 30065817.1715
Epoch 2/10
  200/36409 [..............................] - ETA: 0s - loss: 30545272.0000 - mean_squared_error: 30545272.0000 9600/36409 [======&gt;.......................] - ETA: 0s - loss: 28146635.1042 - mean_squared_error: 28146635.104219200/36409 [==============&gt;...............] - ETA: 0s - loss: 27564036.7396 - mean_squared_error: 27564036.739628800/36409 [======================&gt;.......] - ETA: 0s - loss: 26721811.2708 - mean_squared_error: 26721811.270836409/36409 [==============================] - 0s - loss: 26152942.8957 - mean_squared_error: 26152942.8957 - val_loss: 23769236.3549 - val_mean_squared_error: 23769236.3549
Epoch 3/10
  200/36409 [..............................] - ETA: 0s - loss: 20422714.0000 - mean_squared_error: 20422714.0000 9800/36409 [=======&gt;......................] - ETA: 0s - loss: 22692571.2857 - mean_squared_error: 22692571.285719200/36409 [==============&gt;...............] - ETA: 0s - loss: 21344668.1875 - mean_squared_error: 21344668.187528800/36409 [======================&gt;.......] - ETA: 0s - loss: 20461590.8403 - mean_squared_error: 20461590.840336409/36409 [==============================] - 0s - loss: 20010043.9827 - mean_squared_error: 20010043.9827 - val_loss: 18357240.2195 - val_mean_squared_error: 18357240.2195
Epoch 4/10
  200/36409 [..............................] - ETA: 0s - loss: 14938532.0000 - mean_squared_error: 14938532.0000 9800/36409 [=======&gt;......................] - ETA: 0s - loss: 16965881.4286 - mean_squared_error: 16965881.428619000/36409 [==============&gt;...............] - ETA: 0s - loss: 16703069.7895 - mean_squared_error: 16703069.789528600/36409 [======================&gt;.......] - ETA: 0s - loss: 16424291.5734 - mean_squared_error: 16424291.573436409/36409 [==============================] - 0s - loss: 16346794.1603 - mean_squared_error: 16346794.1603 - val_loss: 16184870.8927 - val_mean_squared_error: 16184870.8927
Epoch 5/10
  200/36409 [..............................] - ETA: 0s - loss: 15538066.0000 - mean_squared_error: 15538066.0000 7600/36409 [=====&gt;........................] - ETA: 0s - loss: 15490160.3947 - mean_squared_error: 15490160.394717200/36409 [=============&gt;................] - ETA: 0s - loss: 15445834.4767 - mean_squared_error: 15445834.476726600/36409 [====================&gt;.........] - ETA: 0s - loss: 15349497.5639 - mean_squared_error: 15349497.563936000/36409 [============================&gt;.] - ETA: 0s - loss: 15268502.4000 - mean_squared_error: 15268502.400036409/36409 [==============================] - 0s - loss: 15265320.6092 - mean_squared_error: 15265320.6092 - val_loss: 15741750.1518 - val_mean_squared_error: 15741750.1518
Epoch 6/10
  200/36409 [..............................] - ETA: 0s - loss: 14412805.0000 - mean_squared_error: 14412805.0000 9800/36409 [=======&gt;......................] - ETA: 0s - loss: 15223196.7551 - mean_squared_error: 15223196.755119400/36409 [==============&gt;...............] - ETA: 0s - loss: 15234776.4639 - mean_squared_error: 15234776.463929000/36409 [======================&gt;.......] - ETA: 0s - loss: 15098120.2828 - mean_squared_error: 15098120.282836409/36409 [==============================] - 0s - loss: 15076170.1684 - mean_squared_error: 15076170.1684 - val_loss: 15631488.4825 - val_mean_squared_error: 15631488.4825
Epoch 7/10
  200/36409 [..............................] - ETA: 0s - loss: 18830928.0000 - mean_squared_error: 18830928.0000 9400/36409 [======&gt;.......................] - ETA: 0s - loss: 15545285.4681 - mean_squared_error: 15545285.468118600/36409 [==============&gt;...............] - ETA: 0s - loss: 15304064.8387 - mean_squared_error: 15304064.838728000/36409 [======================&gt;.......] - ETA: 0s - loss: 15159809.7929 - mean_squared_error: 15159809.792936409/36409 [==============================] - 0s - loss: 14990644.7577 - mean_squared_error: 14990644.7577 - val_loss: 15545448.1700 - val_mean_squared_error: 15545448.1700
Epoch 8/10
  200/36409 [..............................] - ETA: 0s - loss: 13167877.0000 - mean_squared_error: 13167877.0000 9600/36409 [======&gt;.......................] - ETA: 0s - loss: 14997702.5625 - mean_squared_error: 14997702.562519200/36409 [==============&gt;...............] - ETA: 0s - loss: 15045306.1667 - mean_squared_error: 15045306.166728800/36409 [======================&gt;.......] - ETA: 0s - loss: 14895919.7847 - mean_squared_error: 14895919.784736409/36409 [==============================] - 0s - loss: 14905258.8482 - mean_squared_error: 14905258.8482 - val_loss: 15450581.2491 - val_mean_squared_error: 15450581.2491
Epoch 9/10
  200/36409 [..............................] - ETA: 0s - loss: 13070927.0000 - mean_squared_error: 13070927.0000 9800/36409 [=======&gt;......................] - ETA: 0s - loss: 14927823.1633 - mean_squared_error: 14927823.163319400/36409 [==============&gt;...............] - ETA: 0s - loss: 14795454.3918 - mean_squared_error: 14795454.391828800/36409 [======================&gt;.......] - ETA: 0s - loss: 14833517.0347 - mean_squared_error: 14833517.034736409/36409 [==============================] - 0s - loss: 14810648.2165 - mean_squared_error: 14810648.2165 - val_loss: 15349156.2027 - val_mean_squared_error: 15349156.2027
Epoch 10/10
  200/36409 [..............................] - ETA: 0s - loss: 13465992.0000 - mean_squared_error: 13465992.0000 9600/36409 [======&gt;.......................] - ETA: 0s - loss: 14671580.1458 - mean_squared_error: 14671580.145819000/36409 [==============&gt;...............] - ETA: 0s - loss: 14810533.3053 - mean_squared_error: 14810533.305328600/36409 [======================&gt;.......] - ETA: 0s - loss: 14778363.0350 - mean_squared_error: 14778363.035036409/36409 [==============================] - 0s - loss: 14708575.5622 - mean_squared_error: 14708575.5622 - val_loss: 15239347.9204 - val_mean_squared_error: 15239347.9204
   32/13485 [..............................] - ETA: 0s 2944/13485 [=====&gt;........................] - ETA: 0s 5856/13485 [============&gt;.................] - ETA: 0s 8800/13485 [==================&gt;...........] - ETA: 0s11680/13485 [========================&gt;.....] - ETA: 0s()
test set RMSE: 3801.667197
</pre></div>
</div>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span> <span class="c1"># do you understand why the number of parameters in layer 1 is 810? 26*30+30=810</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_3 (Dense)              (None, 30)                810       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 31        
=================================================================
Total params: 841
Trainable params: 841
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div></blockquote>
<p>Notes:</p>
<ul class="simple">
<li><p>We didn’t have to explicitly write the “input” layer, courtesy of
the Keras API. We just said <code class="docutils literal notranslate"><span class="pre">input_dim=26</span></code> on the first (and only)
hidden layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kernel_initializer='normal'</span></code> is a simple (though not always
optimal) <em>weight initialization</em></p></li>
<li><p>Epoch: 1 pass over all of the training data</p></li>
<li><p>Batch: Records processes together in a single training pass</p></li>
</ul>
<p>How is our RMSE vs. the std dev of the response?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Out[12]: 3989.4027576288736
</pre></div>
</div>
</div></blockquote>
<p>Let’s look at the error …</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;model loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>

<span class="n">display</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s set up a “long-running” training. This will take a few minutes to
converge to the same performance we got more or less instantly with our
sklearn linear regression :)</p>
<p>While it’s running, we can talk about the training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">input_file</span> <span class="o">=</span> <span class="s2">&quot;/dbfs/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv&quot;</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">input_file</span><span class="p">,</span> <span class="n">header</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;cut_&#39;</span><span class="p">,</span> <span class="s1">&#39;color_&#39;</span><span class="p">,</span> <span class="s1">&#39;clarity_&#39;</span><span class="p">])</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">3</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">y</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span>
<span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">26</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mean_squared_error&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mean_squared_error&#39;</span><span class="p">])</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">root </span><span class="si">%s</span><span class="s2">: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">metrics_names</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Train on 36409 samples, validate on 4046 samples
Epoch 1/250
0s - loss: 28488681.4617 - mean_squared_error: 28488681.4617 - val_loss: 23874080.5418 - val_mean_squared_error: 23874080.5418
Epoch 2/250
0s - loss: 18229086.8179 - mean_squared_error: 18229086.8179 - val_loss: 16189728.0830 - val_mean_squared_error: 16189728.0830
Epoch 3/250
0s - loss: 15166499.5991 - mean_squared_error: 15166499.5991 - val_loss: 15623473.2966 - val_mean_squared_error: 15623473.2966
Epoch 4/250
0s - loss: 14946794.6502 - mean_squared_error: 14946794.6502 - val_loss: 15451932.3416 - val_mean_squared_error: 15451932.3416
Epoch 5/250
0s - loss: 14768661.4713 - mean_squared_error: 14768661.4713 - val_loss: 15255701.9491 - val_mean_squared_error: 15255701.9491
Epoch 6/250
0s - loss: 14561062.3305 - mean_squared_error: 14561062.3305 - val_loss: 15022190.2719 - val_mean_squared_error: 15022190.2719
Epoch 7/250
0s - loss: 14316009.8804 - mean_squared_error: 14316009.8804 - val_loss: 14745841.2986 - val_mean_squared_error: 14745841.2986
Epoch 8/250
0s - loss: 14022112.4315 - mean_squared_error: 14022112.4315 - val_loss: 14410444.0415 - val_mean_squared_error: 14410444.0415
Epoch 9/250
0s - loss: 13661721.1350 - mean_squared_error: 13661721.1350 - val_loss: 14000927.5487 - val_mean_squared_error: 14000927.5487
Epoch 10/250
0s - loss: 13215663.0982 - mean_squared_error: 13215663.0982 - val_loss: 13486251.6288 - val_mean_squared_error: 13486251.6288
Epoch 11/250
0s - loss: 12666634.4279 - mean_squared_error: 12666634.4279 - val_loss: 12865517.2798 - val_mean_squared_error: 12865517.2798
Epoch 12/250
0s - loss: 12006921.9485 - mean_squared_error: 12006921.9485 - val_loss: 12119694.9130 - val_mean_squared_error: 12119694.9130
Epoch 13/250
0s - loss: 11229263.1284 - mean_squared_error: 11229263.1284 - val_loss: 11260469.6698 - val_mean_squared_error: 11260469.6698
Epoch 14/250
0s - loss: 10353514.4225 - mean_squared_error: 10353514.4225 - val_loss: 10316194.0959 - val_mean_squared_error: 10316194.0959
Epoch 15/250
0s - loss: 9422868.6090 - mean_squared_error: 9422868.6090 - val_loss: 9340960.0435 - val_mean_squared_error: 9340960.0435
Epoch 16/250
0s - loss: 8487981.9583 - mean_squared_error: 8487981.9583 - val_loss: 8368759.9125 - val_mean_squared_error: 8368759.9125
Epoch 17/250
0s - loss: 7584754.6092 - mean_squared_error: 7584754.6092 - val_loss: 7457210.3542 - val_mean_squared_error: 7457210.3542
Epoch 18/250
0s - loss: 6746151.3189 - mean_squared_error: 6746151.3189 - val_loss: 6620543.4394 - val_mean_squared_error: 6620543.4394
Epoch 19/250
0s - loss: 6001624.5078 - mean_squared_error: 6001624.5078 - val_loss: 5876436.0129 - val_mean_squared_error: 5876436.0129
Epoch 20/250
0s - loss: 5358165.4339 - mean_squared_error: 5358165.4339 - val_loss: 5247298.6883 - val_mean_squared_error: 5247298.6883
Epoch 21/250
0s - loss: 4828392.9302 - mean_squared_error: 4828392.9302 - val_loss: 4730418.3690 - val_mean_squared_error: 4730418.3690
Epoch 22/250
0s - loss: 4399680.0461 - mean_squared_error: 4399680.0461 - val_loss: 4315441.9890 - val_mean_squared_error: 4315441.9890
Epoch 23/250
0s - loss: 4064137.3211 - mean_squared_error: 4064137.3211 - val_loss: 3987325.7475 - val_mean_squared_error: 3987325.7475
Epoch 24/250
0s - loss: 3804387.2306 - mean_squared_error: 3804387.2306 - val_loss: 3736058.7546 - val_mean_squared_error: 3736058.7546
Epoch 25/250
0s - loss: 3606198.7104 - mean_squared_error: 3606198.7104 - val_loss: 3537546.5429 - val_mean_squared_error: 3537546.5429
Epoch 26/250
0s - loss: 3452605.0296 - mean_squared_error: 3452605.0296 - val_loss: 3378940.3998 - val_mean_squared_error: 3378940.3998
Epoch 27/250
0s - loss: 3327355.0379 - mean_squared_error: 3327355.0379 - val_loss: 3250864.3769 - val_mean_squared_error: 3250864.3769
Epoch 28/250
0s - loss: 3227474.0017 - mean_squared_error: 3227474.0017 - val_loss: 3146721.7549 - val_mean_squared_error: 3146721.7549
Epoch 29/250
0s - loss: 3142451.5537 - mean_squared_error: 3142451.5537 - val_loss: 3057893.6639 - val_mean_squared_error: 3057893.6639
Epoch 30/250
0s - loss: 3069000.7258 - mean_squared_error: 3069000.7258 - val_loss: 2980783.2383 - val_mean_squared_error: 2980783.2383
Epoch 31/250
0s - loss: 3005016.5666 - mean_squared_error: 3005016.5666 - val_loss: 2912329.0091 - val_mean_squared_error: 2912329.0091
Epoch 32/250
0s - loss: 2945598.7436 - mean_squared_error: 2945598.7436 - val_loss: 2849878.2276 - val_mean_squared_error: 2849878.2276
Epoch 33/250
0s - loss: 2891473.2870 - mean_squared_error: 2891473.2870 - val_loss: 2793290.3015 - val_mean_squared_error: 2793290.3015
Epoch 34/250
0s - loss: 2843366.1829 - mean_squared_error: 2843366.1829 - val_loss: 2741582.8161 - val_mean_squared_error: 2741582.8161
Epoch 35/250
0s - loss: 2798384.1091 - mean_squared_error: 2798384.1091 - val_loss: 2698598.5163 - val_mean_squared_error: 2698598.5163
Epoch 36/250
0s - loss: 2754918.4916 - mean_squared_error: 2754918.4916 - val_loss: 2656071.9637 - val_mean_squared_error: 2656071.9637
Epoch 37/250
0s - loss: 2716886.0110 - mean_squared_error: 2716886.0110 - val_loss: 2612379.2228 - val_mean_squared_error: 2612379.2228
Epoch 38/250
0s - loss: 2680750.1550 - mean_squared_error: 2680750.1550 - val_loss: 2581572.0864 - val_mean_squared_error: 2581572.0864
Epoch 39/250
0s - loss: 2647750.9281 - mean_squared_error: 2647750.9281 - val_loss: 2547053.7806 - val_mean_squared_error: 2547053.7806
Epoch 40/250
0s - loss: 2614633.3723 - mean_squared_error: 2614633.3723 - val_loss: 2502690.5565 - val_mean_squared_error: 2502690.5565
Epoch 41/250
0s - loss: 2585038.8468 - mean_squared_error: 2585038.8468 - val_loss: 2474372.3826 - val_mean_squared_error: 2474372.3826
Epoch 42/250
0s - loss: 2559401.6082 - mean_squared_error: 2559401.6082 - val_loss: 2443888.3585 - val_mean_squared_error: 2443888.3585
Epoch 43/250
0s - loss: 2532813.3276 - mean_squared_error: 2532813.3276 - val_loss: 2418242.1923 - val_mean_squared_error: 2418242.1923
Epoch 44/250
0s - loss: 2510198.0725 - mean_squared_error: 2510198.0725 - val_loss: 2397035.2743 - val_mean_squared_error: 2397035.2743
Epoch 45/250
0s - loss: 2488252.0433 - mean_squared_error: 2488252.0433 - val_loss: 2374199.5779 - val_mean_squared_error: 2374199.5779
Epoch 46/250
0s - loss: 2467734.8528 - mean_squared_error: 2467734.8528 - val_loss: 2350500.9301 - val_mean_squared_error: 2350500.9301
Epoch 47/250
0s - loss: 2449345.1447 - mean_squared_error: 2449345.1447 - val_loss: 2331064.3030 - val_mean_squared_error: 2331064.3030
Epoch 48/250
0s - loss: 2432405.8400 - mean_squared_error: 2432405.8400 - val_loss: 2312961.1415 - val_mean_squared_error: 2312961.1415
Epoch 49/250
0s - loss: 2416510.3260 - mean_squared_error: 2416510.3260 - val_loss: 2297868.3544 - val_mean_squared_error: 2297868.3544
Epoch 50/250
0s - loss: 2400876.9122 - mean_squared_error: 2400876.9122 - val_loss: 2286882.4501 - val_mean_squared_error: 2286882.4501
Epoch 51/250
0s - loss: 2388083.7821 - mean_squared_error: 2388083.7821 - val_loss: 2267811.9818 - val_mean_squared_error: 2267811.9818
Epoch 52/250
0s - loss: 2374082.5599 - mean_squared_error: 2374082.5599 - val_loss: 2254034.9846 - val_mean_squared_error: 2254034.9846
Epoch 53/250
0s - loss: 2362793.5659 - mean_squared_error: 2362793.5659 - val_loss: 2243986.0586 - val_mean_squared_error: 2243986.0586
Epoch 54/250
0s - loss: 2350723.2258 - mean_squared_error: 2350723.2258 - val_loss: 2231136.9094 - val_mean_squared_error: 2231136.9094
Epoch 55/250
0s - loss: 2341111.2496 - mean_squared_error: 2341111.2496 - val_loss: 2222626.1373 - val_mean_squared_error: 2222626.1373
Epoch 56/250
0s - loss: 2330947.0380 - mean_squared_error: 2330947.0380 - val_loss: 2209991.0192 - val_mean_squared_error: 2209991.0192
Epoch 57/250
0s - loss: 2322436.9411 - mean_squared_error: 2322436.9411 - val_loss: 2200346.6637 - val_mean_squared_error: 2200346.6637
Epoch 58/250
0s - loss: 2312180.1612 - mean_squared_error: 2312180.1612 - val_loss: 2200348.3131 - val_mean_squared_error: 2200348.3131
Epoch 59/250
0s - loss: 2304027.6972 - mean_squared_error: 2304027.6972 - val_loss: 2185797.1092 - val_mean_squared_error: 2185797.1092
Epoch 60/250
0s - loss: 2295507.3035 - mean_squared_error: 2295507.3035 - val_loss: 2178808.8276 - val_mean_squared_error: 2178808.8276
Epoch 61/250
0s - loss: 2288186.5474 - mean_squared_error: 2288186.5474 - val_loss: 2172675.4569 - val_mean_squared_error: 2172675.4569
Epoch 62/250
0s - loss: 2280459.4387 - mean_squared_error: 2280459.4387 - val_loss: 2160138.4005 - val_mean_squared_error: 2160138.4005
Epoch 63/250
0s - loss: 2273555.1472 - mean_squared_error: 2273555.1472 - val_loss: 2154209.1761 - val_mean_squared_error: 2154209.1761
Epoch 64/250
0s - loss: 2267387.2401 - mean_squared_error: 2267387.2401 - val_loss: 2146368.0001 - val_mean_squared_error: 2146368.0001
Epoch 65/250
0s - loss: 2259540.6544 - mean_squared_error: 2259540.6544 - val_loss: 2139889.5100 - val_mean_squared_error: 2139889.5100
Epoch 66/250
0s - loss: 2252675.3133 - mean_squared_error: 2252675.3133 - val_loss: 2142529.6848 - val_mean_squared_error: 2142529.6848
Epoch 67/250
0s - loss: 2247124.1685 - mean_squared_error: 2247124.1685 - val_loss: 2133515.4996 - val_mean_squared_error: 2133515.4996
Epoch 68/250
0s - loss: 2239819.6204 - mean_squared_error: 2239819.6204 - val_loss: 2124088.7704 - val_mean_squared_error: 2124088.7704
Epoch 69/250
0s - loss: 2235481.4342 - mean_squared_error: 2235481.4342 - val_loss: 2118866.4429 - val_mean_squared_error: 2118866.4429
Epoch 70/250
0s - loss: 2227842.9432 - mean_squared_error: 2227842.9432 - val_loss: 2116489.2648 - val_mean_squared_error: 2116489.2648
Epoch 71/250
0s - loss: 2222389.5691 - mean_squared_error: 2222389.5691 - val_loss: 2109329.5252 - val_mean_squared_error: 2109329.5252
Epoch 72/250
0s - loss: 2217531.8429 - mean_squared_error: 2217531.8429 - val_loss: 2099498.1513 - val_mean_squared_error: 2099498.1513
Epoch 73/250
0s - loss: 2210847.0403 - mean_squared_error: 2210847.0403 - val_loss: 2093976.1300 - val_mean_squared_error: 2093976.1300
Epoch 74/250
0s - loss: 2205064.4717 - mean_squared_error: 2205064.4717 - val_loss: 2096019.6917 - val_mean_squared_error: 2096019.6917
Epoch 75/250
0s - loss: 2199245.1302 - mean_squared_error: 2199245.1302 - val_loss: 2085533.7033 - val_mean_squared_error: 2085533.7033
Epoch 76/250
0s - loss: 2193349.8692 - mean_squared_error: 2193349.8692 - val_loss: 2079436.7238 - val_mean_squared_error: 2079436.7238
Epoch 77/250
0s - loss: 2188263.9696 - mean_squared_error: 2188263.9696 - val_loss: 2078335.5267 - val_mean_squared_error: 2078335.5267
Epoch 78/250
0s - loss: 2182608.5343 - mean_squared_error: 2182608.5343 - val_loss: 2075093.3773 - val_mean_squared_error: 2075093.3773
Epoch 79/250
0s - loss: 2177324.1784 - mean_squared_error: 2177324.1784 - val_loss: 2065729.8246 - val_mean_squared_error: 2065729.8246
Epoch 80/250
0s - loss: 2172172.7067 - mean_squared_error: 2172172.7067 - val_loss: 2059672.8591 - val_mean_squared_error: 2059672.8591
Epoch 81/250
0s - loss: 2167139.5290 - mean_squared_error: 2167139.5290 - val_loss: 2058287.9642 - val_mean_squared_error: 2058287.9642
Epoch 82/250
0s - loss: 2161254.9477 - mean_squared_error: 2161254.9477 - val_loss: 2049552.9627 - val_mean_squared_error: 2049552.9627
Epoch 83/250
0s - loss: 2155459.7217 - mean_squared_error: 2155459.7217 - val_loss: 2047344.6170 - val_mean_squared_error: 2047344.6170
Epoch 84/250
0s - loss: 2151657.2357 - mean_squared_error: 2151657.2357 - val_loss: 2039571.8497 - val_mean_squared_error: 2039571.8497
Epoch 85/250
0s - loss: 2145618.5328 - mean_squared_error: 2145618.5328 - val_loss: 2035332.1645 - val_mean_squared_error: 2035332.1645
Epoch 86/250
0s - loss: 2140224.4930 - mean_squared_error: 2140224.4930 - val_loss: 2038711.6039 - val_mean_squared_error: 2038711.6039
Epoch 87/250
0s - loss: 2135980.4546 - mean_squared_error: 2135980.4546 - val_loss: 2025889.8646 - val_mean_squared_error: 2025889.8646
Epoch 88/250
0s - loss: 2130591.8489 - mean_squared_error: 2130591.8489 - val_loss: 2021677.8099 - val_mean_squared_error: 2021677.8099
Epoch 89/250
0s - loss: 2125141.5852 - mean_squared_error: 2125141.5852 - val_loss: 2019031.4621 - val_mean_squared_error: 2019031.4621
Epoch 90/250
0s - loss: 2120561.9612 - mean_squared_error: 2120561.9612 - val_loss: 2015348.2049 - val_mean_squared_error: 2015348.2049
Epoch 91/250
0s - loss: 2115254.3667 - mean_squared_error: 2115254.3667 - val_loss: 2009890.7876 - val_mean_squared_error: 2009890.7876
Epoch 92/250
0s - loss: 2111683.1785 - mean_squared_error: 2111683.1785 - val_loss: 2009529.4525 - val_mean_squared_error: 2009529.4525
Epoch 93/250
0s - loss: 2106094.0561 - mean_squared_error: 2106094.0561 - val_loss: 1999995.7113 - val_mean_squared_error: 1999995.7113
Epoch 94/250
0s - loss: 2101431.5341 - mean_squared_error: 2101431.5341 - val_loss: 1996198.3522 - val_mean_squared_error: 1996198.3522
Epoch 95/250
0s - loss: 2096510.5197 - mean_squared_error: 2096510.5197 - val_loss: 1994582.2173 - val_mean_squared_error: 1994582.2173
Epoch 96/250
0s - loss: 2091822.3899 - mean_squared_error: 2091822.3899 - val_loss: 1987896.7952 - val_mean_squared_error: 1987896.7952
Epoch 97/250
0s - loss: 2087355.8434 - mean_squared_error: 2087355.8434 - val_loss: 1983434.7677 - val_mean_squared_error: 1983434.7677
Epoch 98/250
0s - loss: 2083115.5390 - mean_squared_error: 2083115.5390 - val_loss: 1981595.0187 - val_mean_squared_error: 1981595.0187
Epoch 99/250
0s - loss: 2077102.0598 - mean_squared_error: 2077102.0598 - val_loss: 1980368.2116 - val_mean_squared_error: 1980368.2116
Epoch 100/250
0s - loss: 2073968.7934 - mean_squared_error: 2073968.7934 - val_loss: 1974796.1993 - val_mean_squared_error: 1974796.1993
Epoch 101/250
0s - loss: 2068667.0503 - mean_squared_error: 2068667.0503 - val_loss: 1969765.0780 - val_mean_squared_error: 1969765.0780
Epoch 102/250
0s - loss: 2064690.6578 - mean_squared_error: 2064690.6578 - val_loss: 1966384.0665 - val_mean_squared_error: 1966384.0665
Epoch 103/250
0s - loss: 2060186.1627 - mean_squared_error: 2060186.1627 - val_loss: 1965574.2973 - val_mean_squared_error: 1965574.2973
Epoch 104/250
0s - loss: 2054823.4142 - mean_squared_error: 2054823.4142 - val_loss: 1962934.2374 - val_mean_squared_error: 1962934.2374
Epoch 105/250
0s - loss: 2050762.5834 - mean_squared_error: 2050762.5834 - val_loss: 1963156.1925 - val_mean_squared_error: 1963156.1925
Epoch 106/250
0s - loss: 2045922.1364 - mean_squared_error: 2045922.1364 - val_loss: 1954789.0355 - val_mean_squared_error: 1954789.0355
Epoch 107/250
0s - loss: 2041155.5787 - mean_squared_error: 2041155.5787 - val_loss: 1947729.4626 - val_mean_squared_error: 1947729.4626
Epoch 108/250
0s - loss: 2037137.9055 - mean_squared_error: 2037137.9055 - val_loss: 1944769.3830 - val_mean_squared_error: 1944769.3830
Epoch 109/250
0s - loss: 2032794.9416 - mean_squared_error: 2032794.9416 - val_loss: 1941635.0489 - val_mean_squared_error: 1941635.0489
Epoch 110/250
0s - loss: 2028380.1420 - mean_squared_error: 2028380.1420 - val_loss: 1931580.4396 - val_mean_squared_error: 1931580.4396
Epoch 111/250
0s - loss: 2023606.8445 - mean_squared_error: 2023606.8445 - val_loss: 1928708.8721 - val_mean_squared_error: 1928708.8721
Epoch 112/250
0s - loss: 2019222.1547 - mean_squared_error: 2019222.1547 - val_loss: 1928838.3536 - val_mean_squared_error: 1928838.3536
Epoch 113/250
0s - loss: 2015556.0977 - mean_squared_error: 2015556.0977 - val_loss: 1922861.4312 - val_mean_squared_error: 1922861.4312
Epoch 114/250
0s - loss: 2010233.4348 - mean_squared_error: 2010233.4348 - val_loss: 1916408.8726 - val_mean_squared_error: 1916408.8726
Epoch 115/250
0s - loss: 2006104.0397 - mean_squared_error: 2006104.0397 - val_loss: 1922482.2547 - val_mean_squared_error: 1922482.2547
Epoch 116/250
0s - loss: 2001278.5629 - mean_squared_error: 2001278.5629 - val_loss: 1910170.1377 - val_mean_squared_error: 1910170.1377
Epoch 117/250
0s - loss: 1996985.5113 - mean_squared_error: 1996985.5113 - val_loss: 1905622.7374 - val_mean_squared_error: 1905622.7374
Epoch 118/250
0s - loss: 1993243.6008 - mean_squared_error: 1993243.6008 - val_loss: 1903218.0332 - val_mean_squared_error: 1903218.0332
Epoch 119/250
0s - loss: 1988322.9259 - mean_squared_error: 1988322.9259 - val_loss: 1897341.5341 - val_mean_squared_error: 1897341.5341
Epoch 120/250
0s - loss: 1984616.1014 - mean_squared_error: 1984616.1014 - val_loss: 1899326.0840 - val_mean_squared_error: 1899326.0840
Epoch 121/250
0s - loss: 1979608.8977 - mean_squared_error: 1979608.8977 - val_loss: 1897938.1212 - val_mean_squared_error: 1897938.1212
Epoch 122/250
0s - loss: 1976952.4138 - mean_squared_error: 1976952.4138 - val_loss: 1886597.4134 - val_mean_squared_error: 1886597.4134
Epoch 123/250
0s - loss: 1972243.2959 - mean_squared_error: 1972243.2959 - val_loss: 1886249.0314 - val_mean_squared_error: 1886249.0314
Epoch 124/250
0s - loss: 1967164.1162 - mean_squared_error: 1967164.1162 - val_loss: 1879247.9453 - val_mean_squared_error: 1879247.9453
Epoch 125/250
0s - loss: 1963242.1783 - mean_squared_error: 1963242.1783 - val_loss: 1876850.0489 - val_mean_squared_error: 1876850.0489
Epoch 126/250
0s - loss: 1958947.5973 - mean_squared_error: 1958947.5973 - val_loss: 1872630.0703 - val_mean_squared_error: 1872630.0703
Epoch 127/250
0s - loss: 1953799.0429 - mean_squared_error: 1953799.0429 - val_loss: 1873813.3243 - val_mean_squared_error: 1873813.3243
Epoch 128/250
0s - loss: 1950685.8404 - mean_squared_error: 1950685.8404 - val_loss: 1873086.9813 - val_mean_squared_error: 1873086.9813
Epoch 129/250
0s - loss: 1946082.8337 - mean_squared_error: 1946082.8337 - val_loss: 1865504.3232 - val_mean_squared_error: 1865504.3232
Epoch 130/250
0s - loss: 1941948.3297 - mean_squared_error: 1941948.3297 - val_loss: 1857014.4238 - val_mean_squared_error: 1857014.4238
Epoch 131/250
0s - loss: 1937903.8021 - mean_squared_error: 1937903.8021 - val_loss: 1889080.1796 - val_mean_squared_error: 1889080.1796
Epoch 132/250
0s - loss: 1933453.9736 - mean_squared_error: 1933453.9736 - val_loss: 1853606.2867 - val_mean_squared_error: 1853606.2867
Epoch 133/250
0s - loss: 1929543.1667 - mean_squared_error: 1929543.1667 - val_loss: 1845947.6530 - val_mean_squared_error: 1845947.6530
Epoch 134/250
0s - loss: 1924888.8127 - mean_squared_error: 1924888.8127 - val_loss: 1857894.5316 - val_mean_squared_error: 1857894.5316
Epoch 135/250
0s - loss: 1922789.7090 - mean_squared_error: 1922789.7090 - val_loss: 1840319.3262 - val_mean_squared_error: 1840319.3262
Epoch 136/250
0s - loss: 1916827.9438 - mean_squared_error: 1916827.9438 - val_loss: 1835055.8625 - val_mean_squared_error: 1835055.8625
Epoch 137/250
0s - loss: 1912334.4946 - mean_squared_error: 1912334.4946 - val_loss: 1832843.5136 - val_mean_squared_error: 1832843.5136
Epoch 138/250
0s - loss: 1908212.6282 - mean_squared_error: 1908212.6282 - val_loss: 1829784.9870 - val_mean_squared_error: 1829784.9870
Epoch 139/250
0s - loss: 1905601.5578 - mean_squared_error: 1905601.5578 - val_loss: 1824150.5903 - val_mean_squared_error: 1824150.5903
Epoch 140/250
0s - loss: 1901159.4617 - mean_squared_error: 1901159.4617 - val_loss: 1821092.4890 - val_mean_squared_error: 1821092.4890
Epoch 141/250
0s - loss: 1895819.1844 - mean_squared_error: 1895819.1844 - val_loss: 1816414.2478 - val_mean_squared_error: 1816414.2478
Epoch 142/250
0s - loss: 1892209.2715 - mean_squared_error: 1892209.2715 - val_loss: 1817700.9221 - val_mean_squared_error: 1817700.9221
Epoch 143/250
0s - loss: 1887862.7678 - mean_squared_error: 1887862.7678 - val_loss: 1809886.6367 - val_mean_squared_error: 1809886.6367
Epoch 144/250
0s - loss: 1883392.6189 - mean_squared_error: 1883392.6189 - val_loss: 1805897.7690 - val_mean_squared_error: 1805897.7690
Epoch 145/250
0s - loss: 1879688.2490 - mean_squared_error: 1879688.2490 - val_loss: 1808144.8508 - val_mean_squared_error: 1808144.8508
Epoch 146/250
0s - loss: 1874985.6462 - mean_squared_error: 1874985.6462 - val_loss: 1798752.8237 - val_mean_squared_error: 1798752.8237
Epoch 147/250
0s - loss: 1871220.9131 - mean_squared_error: 1871220.9131 - val_loss: 1797674.9889 - val_mean_squared_error: 1797674.9889
Epoch 148/250
0s - loss: 1868417.2108 - mean_squared_error: 1868417.2108 - val_loss: 1796638.2140 - val_mean_squared_error: 1796638.2140
Epoch 149/250
0s - loss: 1863316.5651 - mean_squared_error: 1863316.5651 - val_loss: 1788795.8538 - val_mean_squared_error: 1788795.8538
Epoch 150/250
0s - loss: 1858257.2962 - mean_squared_error: 1858257.2962 - val_loss: 1790632.6463 - val_mean_squared_error: 1790632.6463
Epoch 151/250
0s - loss: 1854766.9910 - mean_squared_error: 1854766.9910 - val_loss: 1789483.1170 - val_mean_squared_error: 1789483.1170
Epoch 152/250
0s - loss: 1849985.5356 - mean_squared_error: 1849985.5356 - val_loss: 1791846.7069 - val_mean_squared_error: 1791846.7069
Epoch 153/250
0s - loss: 1848028.1423 - mean_squared_error: 1848028.1423 - val_loss: 1773171.5716 - val_mean_squared_error: 1773171.5716
Epoch 154/250
0s - loss: 1843831.6297 - mean_squared_error: 1843831.6297 - val_loss: 1771350.8618 - val_mean_squared_error: 1771350.8618
Epoch 155/250
0s - loss: 1838917.1400 - mean_squared_error: 1838917.1400 - val_loss: 1767548.2048 - val_mean_squared_error: 1767548.2048
Epoch 156/250
0s - loss: 1833980.0406 - mean_squared_error: 1833980.0406 - val_loss: 1762363.3768 - val_mean_squared_error: 1762363.3768
Epoch 157/250
0s - loss: 1830403.1305 - mean_squared_error: 1830403.1305 - val_loss: 1763467.2515 - val_mean_squared_error: 1763467.2515
Epoch 158/250
0s - loss: 1826200.7238 - mean_squared_error: 1826200.7238 - val_loss: 1762496.3849 - val_mean_squared_error: 1762496.3849
Epoch 159/250
0s - loss: 1822537.7925 - mean_squared_error: 1822537.7925 - val_loss: 1755522.3327 - val_mean_squared_error: 1755522.3327
Epoch 160/250
0s - loss: 1818753.2099 - mean_squared_error: 1818753.2099 - val_loss: 1750551.0342 - val_mean_squared_error: 1750551.0342
Epoch 161/250
0s - loss: 1815868.0601 - mean_squared_error: 1815868.0601 - val_loss: 1744776.9519 - val_mean_squared_error: 1744776.9519
Epoch 162/250
0s - loss: 1810144.9616 - mean_squared_error: 1810144.9616 - val_loss: 1740964.0973 - val_mean_squared_error: 1740964.0973
Epoch 163/250
0s - loss: 1805573.3993 - mean_squared_error: 1805573.3993 - val_loss: 1739116.9242 - val_mean_squared_error: 1739116.9242
Epoch 164/250
0s - loss: 1801767.6498 - mean_squared_error: 1801767.6498 - val_loss: 1736558.6159 - val_mean_squared_error: 1736558.6159
Epoch 165/250
0s - loss: 1797356.5291 - mean_squared_error: 1797356.5291 - val_loss: 1731078.9323 - val_mean_squared_error: 1731078.9323
Epoch 166/250
0s - loss: 1793916.4087 - mean_squared_error: 1793916.4087 - val_loss: 1734475.7658 - val_mean_squared_error: 1734475.7658
Epoch 167/250
0s - loss: 1790518.4628 - mean_squared_error: 1790518.4628 - val_loss: 1730191.5828 - val_mean_squared_error: 1730191.5828
Epoch 168/250
0s - loss: 1785805.9124 - mean_squared_error: 1785805.9124 - val_loss: 1719618.1999 - val_mean_squared_error: 1719618.1999
Epoch 169/250
0s - loss: 1781726.6154 - mean_squared_error: 1781726.6154 - val_loss: 1726603.2285 - val_mean_squared_error: 1726603.2285
Epoch 170/250
0s - loss: 1778248.7883 - mean_squared_error: 1778248.7883 - val_loss: 1735455.0362 - val_mean_squared_error: 1735455.0362
Epoch 171/250
0s - loss: 1774466.9006 - mean_squared_error: 1774466.9006 - val_loss: 1721617.0734 - val_mean_squared_error: 1721617.0734
Epoch 172/250
0s - loss: 1770234.9116 - mean_squared_error: 1770234.9116 - val_loss: 1707746.7860 - val_mean_squared_error: 1707746.7860
Epoch 173/250
0s - loss: 1765879.3335 - mean_squared_error: 1765879.3335 - val_loss: 1701577.3759 - val_mean_squared_error: 1701577.3759
Epoch 174/250
0s - loss: 1761810.7733 - mean_squared_error: 1761810.7733 - val_loss: 1697890.8165 - val_mean_squared_error: 1697890.8165
Epoch 175/250
0s - loss: 1757112.3404 - mean_squared_error: 1757112.3404 - val_loss: 1695446.8824 - val_mean_squared_error: 1695446.8824
Epoch 176/250
0s - loss: 1754088.3442 - mean_squared_error: 1754088.3442 - val_loss: 1693044.2007 - val_mean_squared_error: 1693044.2007
Epoch 177/250
0s - loss: 1749726.2942 - mean_squared_error: 1749726.2942 - val_loss: 1687403.1916 - val_mean_squared_error: 1687403.1916
Epoch 178/250
0s - loss: 1745868.2238 - mean_squared_error: 1745868.2238 - val_loss: 1683885.3479 - val_mean_squared_error: 1683885.3479
Epoch 179/250
0s - loss: 1741785.8429 - mean_squared_error: 1741785.8429 - val_loss: 1687420.8096 - val_mean_squared_error: 1687420.8096
Epoch 180/250
0s - loss: 1738709.1977 - mean_squared_error: 1738709.1977 - val_loss: 1677222.8691 - val_mean_squared_error: 1677222.8691
Epoch 181/250
0s - loss: 1733345.2182 - mean_squared_error: 1733345.2182 - val_loss: 1680064.8729 - val_mean_squared_error: 1680064.8729
Epoch 182/250
0s - loss: 1730149.8998 - mean_squared_error: 1730149.8998 - val_loss: 1683411.6875 - val_mean_squared_error: 1683411.6875
Epoch 183/250
0s - loss: 1726408.1358 - mean_squared_error: 1726408.1358 - val_loss: 1682925.9373 - val_mean_squared_error: 1682925.9373
Epoch 184/250
0s - loss: 1722247.2033 - mean_squared_error: 1722247.2033 - val_loss: 1668686.2010 - val_mean_squared_error: 1668686.2010
Epoch 185/250
0s - loss: 1718507.5918 - mean_squared_error: 1718507.5918 - val_loss: 1666698.9611 - val_mean_squared_error: 1666698.9611
Epoch 186/250
0s - loss: 1713606.7246 - mean_squared_error: 1713606.7246 - val_loss: 1655468.5166 - val_mean_squared_error: 1655468.5166
Epoch 187/250
0s - loss: 1709152.5771 - mean_squared_error: 1709152.5771 - val_loss: 1652352.3302 - val_mean_squared_error: 1652352.3302
Epoch 188/250
0s - loss: 1705850.4658 - mean_squared_error: 1705850.4658 - val_loss: 1649543.0098 - val_mean_squared_error: 1649543.0098
Epoch 189/250
0s - loss: 1701942.4894 - mean_squared_error: 1701942.4894 - val_loss: 1644905.3089 - val_mean_squared_error: 1644905.3089
Epoch 190/250
0s - loss: 1697940.9734 - mean_squared_error: 1697940.9734 - val_loss: 1644603.7912 - val_mean_squared_error: 1644603.7912
Epoch 191/250
0s - loss: 1694759.5746 - mean_squared_error: 1694759.5746 - val_loss: 1645379.0333 - val_mean_squared_error: 1645379.0333
Epoch 192/250
0s - loss: 1689886.4290 - mean_squared_error: 1689886.4290 - val_loss: 1634460.9281 - val_mean_squared_error: 1634460.9281
Epoch 193/250
0s - loss: 1685948.3251 - mean_squared_error: 1685948.3251 - val_loss: 1641714.3667 - val_mean_squared_error: 1641714.3667
Epoch 194/250
0s - loss: 1683186.3515 - mean_squared_error: 1683186.3515 - val_loss: 1627595.1391 - val_mean_squared_error: 1627595.1391
Epoch 195/250
0s - loss: 1678192.9793 - mean_squared_error: 1678192.9793 - val_loss: 1625529.8073 - val_mean_squared_error: 1625529.8073
Epoch 196/250
0s - loss: 1674476.4529 - mean_squared_error: 1674476.4529 - val_loss: 1622107.9969 - val_mean_squared_error: 1622107.9969
Epoch 197/250
0s - loss: 1670724.4318 - mean_squared_error: 1670724.4318 - val_loss: 1616963.1277 - val_mean_squared_error: 1616963.1277
Epoch 198/250
0s - loss: 1667113.3325 - mean_squared_error: 1667113.3325 - val_loss: 1617531.1524 - val_mean_squared_error: 1617531.1524
Epoch 199/250
0s - loss: 1662485.1907 - mean_squared_error: 1662485.1907 - val_loss: 1611483.1937 - val_mean_squared_error: 1611483.1937
Epoch 200/250
0s - loss: 1659095.2644 - mean_squared_error: 1659095.2644 - val_loss: 1620385.3625 - val_mean_squared_error: 1620385.3625
Epoch 201/250
0s - loss: 1655968.4249 - mean_squared_error: 1655968.4249 - val_loss: 1609900.5784 - val_mean_squared_error: 1609900.5784
Epoch 202/250
0s - loss: 1651376.3636 - mean_squared_error: 1651376.3636 - val_loss: 1604755.1207 - val_mean_squared_error: 1604755.1207
Epoch 203/250
0s - loss: 1647407.6971 - mean_squared_error: 1647407.6971 - val_loss: 1596535.5395 - val_mean_squared_error: 1596535.5395
Epoch 204/250
0s - loss: 1643843.2520 - mean_squared_error: 1643843.2520 - val_loss: 1597280.3045 - val_mean_squared_error: 1597280.3045
Epoch 205/250
0s - loss: 1640501.1278 - mean_squared_error: 1640501.1278 - val_loss: 1589909.1643 - val_mean_squared_error: 1589909.1643
Epoch 206/250
0s - loss: 1637449.0752 - mean_squared_error: 1637449.0752 - val_loss: 1586960.2207 - val_mean_squared_error: 1586960.2207
Epoch 207/250
0s - loss: 1633022.4314 - mean_squared_error: 1633022.4314 - val_loss: 1589978.2194 - val_mean_squared_error: 1589978.2194
Epoch 208/250
0s - loss: 1629158.6126 - mean_squared_error: 1629158.6126 - val_loss: 1580997.8498 - val_mean_squared_error: 1580997.8498
Epoch 209/250
0s - loss: 1625007.0002 - mean_squared_error: 1625007.0002 - val_loss: 1581501.3130 - val_mean_squared_error: 1581501.3130
Epoch 210/250
0s - loss: 1621535.4290 - mean_squared_error: 1621535.4290 - val_loss: 1575202.3754 - val_mean_squared_error: 1575202.3754
Epoch 211/250
0s - loss: 1617952.2137 - mean_squared_error: 1617952.2137 - val_loss: 1569094.1986 - val_mean_squared_error: 1569094.1986
Epoch 212/250
0s - loss: 1613506.0875 - mean_squared_error: 1613506.0875 - val_loss: 1565731.9416 - val_mean_squared_error: 1565731.9416
Epoch 213/250
0s - loss: 1609531.5398 - mean_squared_error: 1609531.5398 - val_loss: 1562545.8748 - val_mean_squared_error: 1562545.8748
Epoch 214/250
0s - loss: 1606658.6721 - mean_squared_error: 1606658.6721 - val_loss: 1562381.3088 - val_mean_squared_error: 1562381.3088
Epoch 215/250
0s - loss: 1603549.7601 - mean_squared_error: 1603549.7601 - val_loss: 1557409.6149 - val_mean_squared_error: 1557409.6149
Epoch 216/250
0s - loss: 1598695.9285 - mean_squared_error: 1598695.9285 - val_loss: 1552052.4229 - val_mean_squared_error: 1552052.4229
Epoch 217/250
0s - loss: 1595564.0566 - mean_squared_error: 1595564.0566 - val_loss: 1550724.7394 - val_mean_squared_error: 1550724.7394
Epoch 218/250
0s - loss: 1591018.6489 - mean_squared_error: 1591018.6489 - val_loss: 1545397.2224 - val_mean_squared_error: 1545397.2224
Epoch 219/250
0s - loss: 1587767.1251 - mean_squared_error: 1587767.1251 - val_loss: 1546505.7617 - val_mean_squared_error: 1546505.7617
Epoch 220/250
0s - loss: 1584253.3639 - mean_squared_error: 1584253.3639 - val_loss: 1551424.2942 - val_mean_squared_error: 1551424.2942
Epoch 221/250
0s - loss: 1579802.2742 - mean_squared_error: 1579802.2742 - val_loss: 1535535.4573 - val_mean_squared_error: 1535535.4573
Epoch 222/250
0s - loss: 1576280.2625 - mean_squared_error: 1576280.2625 - val_loss: 1531791.3806 - val_mean_squared_error: 1531791.3806
Epoch 223/250
0s - loss: 1573347.9679 - mean_squared_error: 1573347.9679 - val_loss: 1537267.9145 - val_mean_squared_error: 1537267.9145
Epoch 224/250
0s - loss: 1569397.2130 - mean_squared_error: 1569397.2130 - val_loss: 1530229.1269 - val_mean_squared_error: 1530229.1269
Epoch 225/250
0s - loss: 1565550.6658 - mean_squared_error: 1565550.6658 - val_loss: 1522887.0238 - val_mean_squared_error: 1522887.0238
Epoch 226/250
0s - loss: 1561247.6334 - mean_squared_error: 1561247.6334 - val_loss: 1519037.9480 - val_mean_squared_error: 1519037.9480
Epoch 227/250
0s - loss: 1558774.2273 - mean_squared_error: 1558774.2273 - val_loss: 1515065.1012 - val_mean_squared_error: 1515065.1012
Epoch 228/250
0s - loss: 1554999.3584 - mean_squared_error: 1554999.3584 - val_loss: 1517494.8972 - val_mean_squared_error: 1517494.8972
Epoch 229/250
0s - loss: 1551283.4775 - mean_squared_error: 1551283.4775 - val_loss: 1514921.8493 - val_mean_squared_error: 1514921.8493
Epoch 230/250
0s - loss: 1548173.0661 - mean_squared_error: 1548173.0661 - val_loss: 1508863.5602 - val_mean_squared_error: 1508863.5602
Epoch 231/250
0s - loss: 1544465.2120 - mean_squared_error: 1544465.2120 - val_loss: 1504739.4705 - val_mean_squared_error: 1504739.4705
Epoch 232/250
0s - loss: 1540600.5711 - mean_squared_error: 1540600.5711 - val_loss: 1499019.3015 - val_mean_squared_error: 1499019.3015
Epoch 233/250
0s - loss: 1538577.3459 - mean_squared_error: 1538577.3459 - val_loss: 1506011.2035 - val_mean_squared_error: 1506011.2035
Epoch 234/250
0s - loss: 1533526.0004 - mean_squared_error: 1533526.0004 - val_loss: 1493005.9019 - val_mean_squared_error: 1493005.9019
Epoch 235/250
0s - loss: 1531700.7249 - mean_squared_error: 1531700.7249 - val_loss: 1489937.0910 - val_mean_squared_error: 1489937.0910
Epoch 236/250
0s - loss: 1527645.5102 - mean_squared_error: 1527645.5102 - val_loss: 1486564.5927 - val_mean_squared_error: 1486564.5927
Epoch 237/250
0s - loss: 1523949.8098 - mean_squared_error: 1523949.8098 - val_loss: 1483267.2962 - val_mean_squared_error: 1483267.2962
Epoch 238/250
0s - loss: 1521658.9500 - mean_squared_error: 1521658.9500 - val_loss: 1480662.7238 - val_mean_squared_error: 1480662.7238
Epoch 239/250
0s - loss: 1517222.4655 - mean_squared_error: 1517222.4655 - val_loss: 1478196.8099 - val_mean_squared_error: 1478196.8099
Epoch 240/250
0s - loss: 1514874.1922 - mean_squared_error: 1514874.1922 - val_loss: 1474388.9170 - val_mean_squared_error: 1474388.9170
Epoch 241/250
0s - loss: 1510850.0449 - mean_squared_error: 1510850.0449 - val_loss: 1471600.3893 - val_mean_squared_error: 1471600.3893
Epoch 242/250
0s - loss: 1507766.9016 - mean_squared_error: 1507766.9016 - val_loss: 1468082.8954 - val_mean_squared_error: 1468082.8954
Epoch 243/250
0s - loss: 1504710.6541 - mean_squared_error: 1504710.6541 - val_loss: 1467789.0764 - val_mean_squared_error: 1467789.0764
Epoch 244/250
0s - loss: 1501241.3013 - mean_squared_error: 1501241.3013 - val_loss: 1464039.5800 - val_mean_squared_error: 1464039.5800
Epoch 245/250
0s - loss: 1498228.6206 - mean_squared_error: 1498228.6206 - val_loss: 1459805.8586 - val_mean_squared_error: 1459805.8586
Epoch 246/250
0s - loss: 1494273.3504 - mean_squared_error: 1494273.3504 - val_loss: 1459658.6752 - val_mean_squared_error: 1459658.6752
Epoch 247/250
0s - loss: 1491648.6830 - mean_squared_error: 1491648.6830 - val_loss: 1465072.1720 - val_mean_squared_error: 1465072.1720
Epoch 248/250
0s - loss: 1489039.1954 - mean_squared_error: 1489039.1954 - val_loss: 1454388.2064 - val_mean_squared_error: 1454388.2064
Epoch 249/250
0s - loss: 1485836.2284 - mean_squared_error: 1485836.2284 - val_loss: 1447167.1258 - val_mean_squared_error: 1447167.1258
Epoch 250/250
0s - loss: 1482002.9176 - mean_squared_error: 1482002.9176 - val_loss: 1445375.0866 - val_mean_squared_error: 1445375.0866
   32/13485 [..............................] - ETA: 0s 2848/13485 [=====&gt;........................] - ETA: 0s 5728/13485 [===========&gt;..................] - ETA: 0s 8608/13485 [==================&gt;...........] - ETA: 0s11456/13485 [========================&gt;.....] - ETA: 0s
root mean_squared_error: 1207.738466
</pre></div>
</div>
</div></blockquote>
<p>After all this hard work we are closer to the MSE we got from linear
regression, but purely using a shallow feed-forward neural network.</p>
</div>
<div class="section" id="training-gradient-descent">
<h3>Training: Gradient Descent<a class="headerlink" href="#training-gradient-descent" title="Permalink to this headline">¶</a></h3>
<p>A family of numeric optimization techniques, where we solve a problem
with the following pattern:</p>
<ol class="simple">
<li><p>Describe the error in the model output: this is usually some
difference between the the true values and the model’s predicted
values, as a function of the model parameters (weights)</p></li>
<li><p>Compute the gradient, or directional derivative, of the error – the
“slope toward lower error”</p></li>
<li><p>Adjust the parameters of the model variables in the indicated
direction</p></li>
<li><p>Repeat</p></li>
</ol>
<p>&lt;img src=”https://i.imgur.com/HOYViqN.png” width=500&gt;</p>
<div class="section" id="some-ideas-to-help-build-your-intuition">
<h4>Some ideas to help build your intuition<a class="headerlink" href="#some-ideas-to-help-build-your-intuition" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>What happens if the variables (imagine just 2, to keep the mental
picture simple) are on wildly different scales … like one ranges
from -1 to 1 while another from -1e6 to +1e6?</p></li>
<li><p>What if some of the variables are correlated? I.e., a change in one
corresponds to, say, a linear change in another?</p></li>
<li><p>Other things being equal, an approximate solution with fewer
variables is easier to work with than one with more – how could we
get rid of some less valuable parameters? (e.g., L1 penalty)</p></li>
<li><p>How do we know how far to “adjust” our parameters with each step?</p></li>
</ul>
<p>&lt;img src=”http://i.imgur.com/AvM2TN6.png” width=600&gt;</p>
<p>What if we have billions of data points? Does it makes sense to use all
of them for each update? Is there a shortcut?</p>
<p>Yes: <em>Stochastic Gradient Descent</em></p>
<p>Stochastic gradient descent is an iterative learning algorithm that uses
a training dataset to update a model. - The batch size is a
hyperparameter of gradient descent that controls the number of training
samples to work through before the model’s internal parameters are
updated. - The number of epochs is a hyperparameter of gradient descent
that controls the number of complete passes through the training
dataset.</p>
<p>See
<a class="reference external" href="https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9">https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9</a>.</p>
<p>But SGD has some shortcomings, so we typically use a “smarter” version
of SGD, which has rules for adjusting the learning rate and even
direction in order to avoid common problems.</p>
<p>What about that “Adam” optimizer? Adam is short for “adaptive moment”
and is a variant of SGD that includes momentum calculations that change
over time. For more detail on optimizers, see the chapter “Training Deep
Neural Nets” in Aurélien Géron’s book: <em>Hands-On Machine Learning with
Scikit-Learn and TensorFlow</em>
(http://shop.oreilly.com/product/0636920052289.do)</p>
<p>See <a class="reference external" href="https://keras.io/optimizers/">https://keras.io/optimizers/</a> and references therein.</p>
</div>
</div>
<div class="section" id="training-backpropagation">
<h3>Training: Backpropagation<a class="headerlink" href="#training-backpropagation" title="Permalink to this headline">¶</a></h3>
<p>With a simple, flat model, we could use SGD or a related algorithm to
derive the weights, since the error depends directly on those weights.</p>
<p>With a deeper network, we have a couple of challenges:</p>
<ul class="simple">
<li><p>The error is computed from the final layer, so the gradient of the
error doesn’t tell us immediately about problems in other-layer
weights</p></li>
<li><p>Our tiny diamonds model has almost a thousand weights. Bigger models
can easily have millions of weights. Each of those weights may need
to move a little at a time, and we have to watch out for underflow
or undersignificance situations.</p></li>
</ul>
<p><strong>The insight is to iteratively calculate errors, one layer at a time,
starting at the output. This is called backpropagation. It is neither
magical nor surprising. The challenge is just doing it fast and not
losing information.</strong></p>
<p>&lt;img src=”http://i.imgur.com/bjlYwjM.jpg” width=800&gt;</p>
</div>
</div>
<div class="section" id="ok-so-we-ve-come-up-with-a-very-slow-way-to-perform-a-linear-regression">
<h2>Ok so we’ve come up with a very slow way to perform a linear regression.<a class="headerlink" href="#ok-so-we-ve-come-up-with-a-very-slow-way-to-perform-a-linear-regression" title="Permalink to this headline">¶</a></h2>
<div class="section" id="welcome-to-neural-networks-in-the-1960s">
<h3><em>Welcome to Neural Networks in the 1960s!</em><a class="headerlink" href="#welcome-to-neural-networks-in-the-1960s" title="Permalink to this headline">¶</a></h3>
</div>
<hr class="docutils" />
<div class="section" id="watch-closely-now-because-this-is-where-the-magic-happens">
<h3>Watch closely now because this is where the magic happens…<a class="headerlink" href="#watch-closely-now-because-this-is-where-the-magic-happens" title="Permalink to this headline">¶</a></h3>
<p>&lt;img src=”https://media.giphy.com/media/Hw5LkPYy9yfVS/giphy.gif”&gt;</p>
</div>
</div>
</div>
<div class="section" id="non-linearity-perceptron-universal-approximation">
<h1>Non-Linearity + Perceptron = Universal Approximation<a class="headerlink" href="#non-linearity-perceptron-universal-approximation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="where-does-the-non-linearity-fit-in">
<h2>Where does the non-linearity fit in?<a class="headerlink" href="#where-does-the-non-linearity-fit-in" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>We start with the inputs to a perceptron – these could be from
source data, for example.</p></li>
<li><p>We multiply each input by its respective weight, which gets us the
<span class="math notranslate nohighlight">\(x \\cdot w\)</span></p></li>
<li><p>Then add the “bias” – an extra learnable parameter, to get <span class="math notranslate nohighlight">\({x
\\cdot w} + b\)</span></p>
<ul>
<li><p>This value (so far) is sometimes called the “pre-activation”</p></li>
</ul>
</li>
<li><p>Now, apply a non-linear “activation function” to this value, such as
the logistic sigmoid</p></li>
</ul>
<p>&lt;img src=”https://i.imgur.com/MhokAmo.gif”&gt;</p>
</div>
<div class="section" id="now-the-network-can-learn-non-linear-functions">
<h2>Now the network can “learn” non-linear functions<a class="headerlink" href="#now-the-network-can-learn-non-linear-functions" title="Permalink to this headline">¶</a></h2>
<p>To gain some intuition, consider that where the sigmoid is close to 1,
we can think of that neuron as being “on” or activated, giving a
specific output. When close to zero, it is “off.”</p>
<p>So each neuron is a bit like a switch. If we have enough of them, we can
theoretically express arbitrarily many different signals.</p>
<p>In some ways this is like the original artificial neuron, with the
thresholding output – the main difference is that the sigmoid gives us
a smooth (arbitrarily differentiable) output that we can optimize over
using gradient descent to learn the weights.</p>
</div>
<div class="section" id="where-does-the-signal-go-from-these-neurons">
<h2>Where does the signal “go” from these neurons?<a class="headerlink" href="#where-does-the-signal-go-from-these-neurons" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>In a regression problem, like the diamonds dataset, the activations
from the hidden layer can feed into a single output neuron, with a
simple linear activation representing the final output of the
calculation.</p></li>
<li><p>Frequently we want a classification output instead – e.g., with
MNIST digits, where we need to choose from 10 classes. In that case,
we can feed the outputs from these hidden neurons forward into a
final layer of 10 neurons, and compare those final neurons’
activation levels.</p></li>
</ul>
<p>Ok, before we talk any more theory, let’s run it and see if we can do
better on our diamonds dataset adding this “sigmoid activation.”</p>
<p>While that’s running, let’s look at the code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">input_file</span> <span class="o">=</span> <span class="s2">&quot;/dbfs/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv&quot;</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">input_file</span><span class="p">,</span> <span class="n">header</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;cut_&#39;</span><span class="p">,</span> <span class="s1">&#39;color_&#39;</span><span class="p">,</span> <span class="s1">&#39;clarity_&#39;</span><span class="p">])</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">3</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">y</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span>
<span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">26</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span> <span class="c1"># &lt;- change to nonlinear activation</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">))</span> <span class="c1"># &lt;- activation is linear in output layer for this regression</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mean_squared_error&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mean_squared_error&#39;</span><span class="p">])</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">root </span><span class="si">%s</span><span class="s2">: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">metrics_names</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Train on 36409 samples, validate on 4046 samples
Epoch 1/2000
 - 1s - loss: 31380780.6047 - mean_squared_error: 31380780.6047 - val_loss: 32358332.3638 - val_mean_squared_error: 32358332.3638
Epoch 2/2000
 - 0s - loss: 31298065.4203 - mean_squared_error: 31298065.4203 - val_loss: 32276571.6461 - val_mean_squared_error: 32276571.6461
Epoch 3/2000
 - 0s - loss: 31216004.9920 - mean_squared_error: 31216004.9920 - val_loss: 32191762.6911 - val_mean_squared_error: 32191762.6911
Epoch 4/2000
 - 0s - loss: 31133911.1605 - mean_squared_error: 31133911.1605 - val_loss: 32109959.1181 - val_mean_squared_error: 32109959.1181
Epoch 5/2000
 - 0s - loss: 31053780.8829 - mean_squared_error: 31053780.8829 - val_loss: 32029314.2709 - val_mean_squared_error: 32029314.2709
Epoch 6/2000
 - 0s - loss: 30974261.1565 - mean_squared_error: 30974261.1565 - val_loss: 31949091.0203 - val_mean_squared_error: 31949091.0203
Epoch 7/2000
 - 0s - loss: 30895358.6525 - mean_squared_error: 30895358.6525 - val_loss: 31869483.1992 - val_mean_squared_error: 31869483.1992
Epoch 8/2000
 - 0s - loss: 30816933.3015 - mean_squared_error: 30816933.3015 - val_loss: 31790248.7850 - val_mean_squared_error: 31790248.7850
Epoch 9/2000
 - 0s - loss: 30735031.6336 - mean_squared_error: 30735031.6336 - val_loss: 31703530.1018 - val_mean_squared_error: 31703530.1018
Epoch 10/2000
 - 0s - loss: 30650979.2737 - mean_squared_error: 30650979.2737 - val_loss: 31620223.0746 - val_mean_squared_error: 31620223.0746
Epoch 11/2000
 - 0s - loss: 30564208.1914 - mean_squared_error: 30564208.1914 - val_loss: 31527235.8487 - val_mean_squared_error: 31527235.8487
Epoch 12/2000
 - 0s - loss: 30471671.6893 - mean_squared_error: 30471671.6893 - val_loss: 31434505.1438 - val_mean_squared_error: 31434505.1438
Epoch 13/2000
 - 0s - loss: 30382774.6087 - mean_squared_error: 30382774.6087 - val_loss: 31345867.3376 - val_mean_squared_error: 31345867.3376
Epoch 14/2000
 - 0s - loss: 30295878.0809 - mean_squared_error: 30295878.0809 - val_loss: 31258539.8992 - val_mean_squared_error: 31258539.8992
Epoch 15/2000
 - 0s - loss: 30210245.2780 - mean_squared_error: 30210245.2780 - val_loss: 31172163.3673 - val_mean_squared_error: 31172163.3673
Epoch 16/2000
 - 0s - loss: 30125167.6951 - mean_squared_error: 30125167.6951 - val_loss: 31086263.9644 - val_mean_squared_error: 31086263.9644
Epoch 17/2000
 - 0s - loss: 30040693.1683 - mean_squared_error: 30040693.1683 - val_loss: 31000947.0559 - val_mean_squared_error: 31000947.0559
Epoch 18/2000
 - 0s - loss: 29956545.2932 - mean_squared_error: 29956545.2932 - val_loss: 30915863.5709 - val_mean_squared_error: 30915863.5709
Epoch 19/2000
 - 0s - loss: 29872689.6088 - mean_squared_error: 29872689.6088 - val_loss: 30831129.4968 - val_mean_squared_error: 30831129.4968
Epoch 20/2000
 - 0s - loss: 29789302.8007 - mean_squared_error: 29789302.8007 - val_loss: 30746769.3732 - val_mean_squared_error: 30746769.3732
Epoch 21/2000
 - 0s - loss: 29706222.5647 - mean_squared_error: 29706222.5647 - val_loss: 30662787.0954 - val_mean_squared_error: 30662787.0954
Epoch 22/2000
 - 0s - loss: 29623479.2155 - mean_squared_error: 29623479.2155 - val_loss: 30579097.6026 - val_mean_squared_error: 30579097.6026
Epoch 23/2000
 - 0s - loss: 29540912.2036 - mean_squared_error: 29540912.2036 - val_loss: 30495587.1537 - val_mean_squared_error: 30495587.1537
Epoch 24/2000
 - 0s - loss: 29458598.1043 - mean_squared_error: 29458598.1043 - val_loss: 30412313.0509 - val_mean_squared_error: 30412313.0509
Epoch 25/2000
 - 0s - loss: 29376574.1866 - mean_squared_error: 29376574.1866 - val_loss: 30329317.4652 - val_mean_squared_error: 30329317.4652
Epoch 26/2000
 - 0s - loss: 29294745.7942 - mean_squared_error: 29294745.7942 - val_loss: 30246553.0677 - val_mean_squared_error: 30246553.0677
Epoch 27/2000
 - 0s - loss: 29213295.1197 - mean_squared_error: 29213295.1197 - val_loss: 30164191.6738 - val_mean_squared_error: 30164191.6738
Epoch 28/2000
 - 0s - loss: 29132082.6000 - mean_squared_error: 29132082.6000 - val_loss: 30082031.7647 - val_mean_squared_error: 30082031.7647
Epoch 29/2000
 - 0s - loss: 29051069.7344 - mean_squared_error: 29051069.7344 - val_loss: 30000055.4869 - val_mean_squared_error: 30000055.4869
Epoch 30/2000
 - 0s - loss: 28970459.9729 - mean_squared_error: 28970459.9729 - val_loss: 29918478.6387 - val_mean_squared_error: 29918478.6387
Epoch 31/2000
 - 0s - loss: 28889962.9339 - mean_squared_error: 28889962.9339 - val_loss: 29836927.4503 - val_mean_squared_error: 29836927.4503
Epoch 32/2000
 - 0s - loss: 28809790.9063 - mean_squared_error: 28809790.9063 - val_loss: 29755820.0830 - val_mean_squared_error: 29755820.0830
Epoch 33/2000
 - 0s - loss: 28729963.0352 - mean_squared_error: 28729963.0352 - val_loss: 29675137.9496 - val_mean_squared_error: 29675137.9496
Epoch 34/2000
 - 0s - loss: 28650360.5442 - mean_squared_error: 28650360.5442 - val_loss: 29594480.1928 - val_mean_squared_error: 29594480.1928
Epoch 35/2000
 - 0s - loss: 28570924.5415 - mean_squared_error: 28570924.5415 - val_loss: 29514072.8730 - val_mean_squared_error: 29514072.8730
Epoch 36/2000
 - 0s - loss: 28491709.2935 - mean_squared_error: 28491709.2935 - val_loss: 29433960.4943 - val_mean_squared_error: 29433960.4943
Epoch 37/2000
 - 0s - loss: 28412798.1367 - mean_squared_error: 28412798.1367 - val_loss: 29354021.7696 - val_mean_squared_error: 29354021.7696
Epoch 38/2000
 - 0s - loss: 28334104.4454 - mean_squared_error: 28334104.4454 - val_loss: 29274362.7504 - val_mean_squared_error: 29274362.7504
Epoch 39/2000
 - 0s - loss: 28255585.5671 - mean_squared_error: 28255585.5671 - val_loss: 29194935.7578 - val_mean_squared_error: 29194935.7578
Epoch 40/2000
 - 0s - loss: 28177311.3649 - mean_squared_error: 28177311.3649 - val_loss: 29115711.6283 - val_mean_squared_error: 29115711.6283
Epoch 41/2000
 - 0s - loss: 28099405.9583 - mean_squared_error: 28099405.9583 - val_loss: 29036816.6812 - val_mean_squared_error: 29036816.6812
Epoch 42/2000
 - 0s - loss: 28021745.1748 - mean_squared_error: 28021745.1748 - val_loss: 28958206.8423 - val_mean_squared_error: 28958206.8423
Epoch 43/2000
 - 0s - loss: 27944321.2553 - mean_squared_error: 27944321.2553 - val_loss: 28879830.6901 - val_mean_squared_error: 28879830.6901
Epoch 44/2000
 - 0s - loss: 27867160.2867 - mean_squared_error: 27867160.2867 - val_loss: 28801703.4691 - val_mean_squared_error: 28801703.4691
Epoch 45/2000
 - 0s - loss: 27790252.0029 - mean_squared_error: 27790252.0029 - val_loss: 28723817.4879 - val_mean_squared_error: 28723817.4879
Epoch 46/2000
 - 0s - loss: 27713554.5320 - mean_squared_error: 27713554.5320 - val_loss: 28646179.9268 - val_mean_squared_error: 28646179.9268
Epoch 47/2000
 - 0s - loss: 27637164.8281 - mean_squared_error: 27637164.8281 - val_loss: 28568974.5803 - val_mean_squared_error: 28568974.5803
Epoch 48/2000
 - 0s - loss: 27560923.5064 - mean_squared_error: 27560923.5064 - val_loss: 28491688.7721 - val_mean_squared_error: 28491688.7721
Epoch 49/2000
 - 0s - loss: 27485063.1059 - mean_squared_error: 27485063.1059 - val_loss: 28414863.0114 - val_mean_squared_error: 28414863.0114
Epoch 50/2000
 - 0s - loss: 27409439.2144 - mean_squared_error: 27409439.2144 - val_loss: 28338173.7677 - val_mean_squared_error: 28338173.7677
Epoch 51/2000
 - 0s - loss: 27334131.0234 - mean_squared_error: 27334131.0234 - val_loss: 28262001.8013 - val_mean_squared_error: 28262001.8013
Epoch 52/2000
 - 0s - loss: 27258972.5527 - mean_squared_error: 27258972.5527 - val_loss: 28185839.0064 - val_mean_squared_error: 28185839.0064
Epoch 53/2000
 - 0s - loss: 27184161.5334 - mean_squared_error: 27184161.5334 - val_loss: 28110077.3406 - val_mean_squared_error: 28110077.3406
Epoch 54/2000
 - 0s - loss: 27109518.9422 - mean_squared_error: 27109518.9422 - val_loss: 28034423.0133 - val_mean_squared_error: 28034423.0133
Epoch 55/2000
 - 0s - loss: 27035128.6361 - mean_squared_error: 27035128.6361 - val_loss: 27959151.0756 - val_mean_squared_error: 27959151.0756
Epoch 56/2000
 - 0s - loss: 26960971.6158 - mean_squared_error: 26960971.6158 - val_loss: 27884048.2521 - val_mean_squared_error: 27884048.2521
Epoch 57/2000
 - 0s - loss: 26887081.3519 - mean_squared_error: 26887081.3519 - val_loss: 27809200.8601 - val_mean_squared_error: 27809200.8601
Epoch 58/2000
 - 0s - loss: 26813503.5389 - mean_squared_error: 26813503.5389 - val_loss: 27734627.8863 - val_mean_squared_error: 27734627.8863
Epoch 59/2000
 - 0s - loss: 26740185.8467 - mean_squared_error: 26740185.8467 - val_loss: 27660315.3821 - val_mean_squared_error: 27660315.3821
Epoch 60/2000
 - 0s - loss: 26667075.5801 - mean_squared_error: 26667075.5801 - val_loss: 27586281.0489 - val_mean_squared_error: 27586281.0489
Epoch 61/2000
 - 0s - loss: 26594314.0441 - mean_squared_error: 26594314.0441 - val_loss: 27512567.3356 - val_mean_squared_error: 27512567.3356
Epoch 62/2000
 - 0s - loss: 26521670.2177 - mean_squared_error: 26521670.2177 - val_loss: 27438897.1527 - val_mean_squared_error: 27438897.1527
Epoch 63/2000
 - 0s - loss: 26449391.6573 - mean_squared_error: 26449391.6573 - val_loss: 27365727.1676 - val_mean_squared_error: 27365727.1676
Epoch 64/2000
 - 0s - loss: 26377206.7494 - mean_squared_error: 26377206.7494 - val_loss: 27292557.8497 - val_mean_squared_error: 27292557.8497
Epoch 65/2000
 - 0s - loss: 26305377.2261 - mean_squared_error: 26305377.2261 - val_loss: 27219727.1705 - val_mean_squared_error: 27219727.1705
Epoch 66/2000
 - 0s - loss: 26233774.5640 - mean_squared_error: 26233774.5640 - val_loss: 27147248.8393 - val_mean_squared_error: 27147248.8393
Epoch 67/2000
 - 0s - loss: 26162426.6621 - mean_squared_error: 26162426.6621 - val_loss: 27074894.4360 - val_mean_squared_error: 27074894.4360
Epoch 68/2000
 - 0s - loss: 26091317.9390 - mean_squared_error: 26091317.9390 - val_loss: 27002882.7682 - val_mean_squared_error: 27002882.7682
Epoch 69/2000
 - 0s - loss: 26020459.9902 - mean_squared_error: 26020459.9902 - val_loss: 26931139.3267 - val_mean_squared_error: 26931139.3267
Epoch 70/2000
 - 0s - loss: 25949811.0884 - mean_squared_error: 25949811.0884 - val_loss: 26859526.5754 - val_mean_squared_error: 26859526.5754
Epoch 71/2000
 - 0s - loss: 25879355.5384 - mean_squared_error: 25879355.5384 - val_loss: 26788026.7138 - val_mean_squared_error: 26788026.7138
Epoch 72/2000
 - 0s - loss: 25809385.0737 - mean_squared_error: 25809385.0737 - val_loss: 26717113.4187 - val_mean_squared_error: 26717113.4187
Epoch 73/2000
 - 0s - loss: 25739624.7902 - mean_squared_error: 25739624.7902 - val_loss: 26646419.4276 - val_mean_squared_error: 26646419.4276
Epoch 74/2000
 - 0s - loss: 25670110.1360 - mean_squared_error: 25670110.1360 - val_loss: 26576000.0336 - val_mean_squared_error: 26576000.0336
Epoch 75/2000
 - 0s - loss: 25600809.1711 - mean_squared_error: 25600809.1711 - val_loss: 26505692.0465 - val_mean_squared_error: 26505692.0465
Epoch 76/2000
 - 0s - loss: 25531733.8437 - mean_squared_error: 25531733.8437 - val_loss: 26435641.7113 - val_mean_squared_error: 26435641.7113
Epoch 77/2000
 - 0s - loss: 25462972.3005 - mean_squared_error: 25462972.3005 - val_loss: 26365985.2813 - val_mean_squared_error: 26365985.2813
Epoch 78/2000
 - 0s - loss: 25394526.8229 - mean_squared_error: 25394526.8229 - val_loss: 26296592.8700 - val_mean_squared_error: 26296592.8700
Epoch 79/2000
 - 0s - loss: 25326216.6557 - mean_squared_error: 25326216.6557 - val_loss: 26227286.5368 - val_mean_squared_error: 26227286.5368
Epoch 80/2000
 - 0s - loss: 25258160.7971 - mean_squared_error: 25258160.7971 - val_loss: 26158228.9204 - val_mean_squared_error: 26158228.9204
Epoch 81/2000
 - 0s - loss: 25190418.3716 - mean_squared_error: 25190418.3716 - val_loss: 26089568.0910 - val_mean_squared_error: 26089568.0910
Epoch 82/2000
 - 0s - loss: 25122887.6684 - mean_squared_error: 25122887.6684 - val_loss: 26021011.0905 - val_mean_squared_error: 26021011.0905
Epoch 83/2000
 - 0s - loss: 25055646.1286 - mean_squared_error: 25055646.1286 - val_loss: 25952877.7914 - val_mean_squared_error: 25952877.7914
Epoch 84/2000
 - 0s - loss: 24988565.3453 - mean_squared_error: 24988565.3453 - val_loss: 25884860.6960 - val_mean_squared_error: 25884860.6960
Epoch 85/2000
 - 0s - loss: 24921778.5702 - mean_squared_error: 24921778.5702 - val_loss: 25817128.1157 - val_mean_squared_error: 25817128.1157
Epoch 86/2000
 - 0s - loss: 24855278.4745 - mean_squared_error: 24855278.4745 - val_loss: 25749717.6303 - val_mean_squared_error: 25749717.6303
Epoch 87/2000
 - 0s - loss: 24789066.9677 - mean_squared_error: 24789066.9677 - val_loss: 25682459.5175 - val_mean_squared_error: 25682459.5175
Epoch 88/2000
 - 0s - loss: 24723090.3912 - mean_squared_error: 24723090.3912 - val_loss: 25615506.6901 - val_mean_squared_error: 25615506.6901
Epoch 89/2000
 - 0s - loss: 24657334.3180 - mean_squared_error: 24657334.3180 - val_loss: 25548882.4024 - val_mean_squared_error: 25548882.4024
Epoch 90/2000
 - 0s - loss: 24591838.8394 - mean_squared_error: 24591838.8394 - val_loss: 25482345.4513 - val_mean_squared_error: 25482345.4513
Epoch 91/2000
 - 0s - loss: 24526544.8664 - mean_squared_error: 24526544.8664 - val_loss: 25416164.0761 - val_mean_squared_error: 25416164.0761
Epoch 92/2000
 - 0s - loss: 24461648.8232 - mean_squared_error: 24461648.8232 - val_loss: 25350287.8626 - val_mean_squared_error: 25350287.8626
Epoch 93/2000
 - 0s - loss: 24396903.4064 - mean_squared_error: 24396903.4064 - val_loss: 25284579.1340 - val_mean_squared_error: 25284579.1340
Epoch 94/2000
 - 0s - loss: 24332436.7249 - mean_squared_error: 24332436.7249 - val_loss: 25219231.1409 - val_mean_squared_error: 25219231.1409
Epoch 95/2000
 - 0s - loss: 24268330.4633 - mean_squared_error: 24268330.4633 - val_loss: 25154138.4330 - val_mean_squared_error: 25154138.4330
Epoch 96/2000
 - 0s - loss: 24204372.0587 - mean_squared_error: 24204372.0587 - val_loss: 25089203.5255 - val_mean_squared_error: 25089203.5255
Epoch 97/2000
 - 0s - loss: 24140610.1780 - mean_squared_error: 24140610.1780 - val_loss: 25024465.2664 - val_mean_squared_error: 25024465.2664
Epoch 98/2000
 - 0s - loss: 24077252.2966 - mean_squared_error: 24077252.2966 - val_loss: 24960163.7430 - val_mean_squared_error: 24960163.7430
Epoch 99/2000
 - 0s - loss: 24013917.8359 - mean_squared_error: 24013917.8359 - val_loss: 24895891.1112 - val_mean_squared_error: 24895891.1112
Epoch 100/2000
 - 0s - loss: 23951020.9714 - mean_squared_error: 23951020.9714 - val_loss: 24831932.6505 - val_mean_squared_error: 24831932.6505
Epoch 101/2000
 - 0s - loss: 23888220.1202 - mean_squared_error: 23888220.1202 - val_loss: 24768160.1671 - val_mean_squared_error: 24768160.1671
Epoch 102/2000
 - 0s - loss: 23825687.9442 - mean_squared_error: 23825687.9442 - val_loss: 24704775.5087 - val_mean_squared_error: 24704775.5087
Epoch 103/2000
 - 0s - loss: 23763591.7315 - mean_squared_error: 23763591.7315 - val_loss: 24641768.4014 - val_mean_squared_error: 24641768.4014
Epoch 104/2000
 - 0s - loss: 23701595.4344 - mean_squared_error: 23701595.4344 - val_loss: 24578754.7355 - val_mean_squared_error: 24578754.7355
Epoch 105/2000
 - 0s - loss: 23639821.5487 - mean_squared_error: 23639821.5487 - val_loss: 24516050.8166 - val_mean_squared_error: 24516050.8166
Epoch 106/2000
 - 0s - loss: 23578443.7971 - mean_squared_error: 23578443.7971 - val_loss: 24453735.3900 - val_mean_squared_error: 24453735.3900
Epoch 107/2000
 - 0s - loss: 23517176.9031 - mean_squared_error: 23517176.9031 - val_loss: 24391556.2126 - val_mean_squared_error: 24391556.2126
Epoch 108/2000
 - 0s - loss: 23456172.7204 - mean_squared_error: 23456172.7204 - val_loss: 24329568.1493 - val_mean_squared_error: 24329568.1493
Epoch 109/2000
 - 0s - loss: 23395525.0800 - mean_squared_error: 23395525.0800 - val_loss: 24267975.9090 - val_mean_squared_error: 24267975.9090
Epoch 110/2000
 - 0s - loss: 23335126.7265 - mean_squared_error: 23335126.7265 - val_loss: 24206602.6041 - val_mean_squared_error: 24206602.6041
Epoch 111/2000
 - 0s - loss: 23274969.8814 - mean_squared_error: 23274969.8814 - val_loss: 24145492.8828 - val_mean_squared_error: 24145492.8828
Epoch 112/2000
 - 0s - loss: 23215052.1725 - mean_squared_error: 23215052.1725 - val_loss: 24084635.8517 - val_mean_squared_error: 24084635.8517
Epoch 113/2000
 - 0s - loss: 23155380.1623 - mean_squared_error: 23155380.1623 - val_loss: 24023868.4518 - val_mean_squared_error: 24023868.4518
Epoch 114/2000
 - 0s - loss: 23095990.8764 - mean_squared_error: 23095990.8764 - val_loss: 23963653.1221 - val_mean_squared_error: 23963653.1221
Epoch 115/2000
 - 0s - loss: 23036852.2296 - mean_squared_error: 23036852.2296 - val_loss: 23903552.8304 - val_mean_squared_error: 23903552.8304
Epoch 116/2000
 - 0s - loss: 22977854.2557 - mean_squared_error: 22977854.2557 - val_loss: 23843577.2842 - val_mean_squared_error: 23843577.2842
Epoch 117/2000
 - 0s - loss: 22919132.7115 - mean_squared_error: 22919132.7115 - val_loss: 23783941.1903 - val_mean_squared_error: 23783941.1903
Epoch 118/2000
 - 0s - loss: 22860675.2993 - mean_squared_error: 22860675.2993 - val_loss: 23724452.7316 - val_mean_squared_error: 23724452.7316
Epoch 119/2000
 - 0s - loss: 22802480.1820 - mean_squared_error: 22802480.1820 - val_loss: 23665374.7593 - val_mean_squared_error: 23665374.7593
Epoch 120/2000
 - 0s - loss: 22744486.7348 - mean_squared_error: 22744486.7348 - val_loss: 23606429.2111 - val_mean_squared_error: 23606429.2111
Epoch 121/2000
 - 0s - loss: 22686857.5650 - mean_squared_error: 22686857.5650 - val_loss: 23547899.4839 - val_mean_squared_error: 23547899.4839
Epoch 122/2000
 - 0s - loss: 22629439.9110 - mean_squared_error: 22629439.9110 - val_loss: 23489489.6391 - val_mean_squared_error: 23489489.6391
Epoch 123/2000
 - 0s - loss: 22572283.1380 - mean_squared_error: 22572283.1380 - val_loss: 23431363.3495 - val_mean_squared_error: 23431363.3495
Epoch 124/2000
 - 0s - loss: 22515383.5186 - mean_squared_error: 22515383.5186 - val_loss: 23373465.8695 - val_mean_squared_error: 23373465.8695
Epoch 125/2000
 - 0s - loss: 22458715.0694 - mean_squared_error: 22458715.0694 - val_loss: 23315892.6387 - val_mean_squared_error: 23315892.6387
Epoch 126/2000
 - 0s - loss: 22402235.9658 - mean_squared_error: 22402235.9658 - val_loss: 23258538.3223 - val_mean_squared_error: 23258538.3223
Epoch 127/2000
 - 0s - loss: 22346055.1642 - mean_squared_error: 22346055.1642 - val_loss: 23201325.3880 - val_mean_squared_error: 23201325.3880
Epoch 128/2000
 - 0s - loss: 22290144.3171 - mean_squared_error: 22290144.3171 - val_loss: 23144563.7865 - val_mean_squared_error: 23144563.7865
Epoch 129/2000
 - 0s - loss: 22234343.6437 - mean_squared_error: 22234343.6437 - val_loss: 23087770.5348 - val_mean_squared_error: 23087770.5348
Epoch 130/2000
 - 0s - loss: 22179189.1702 - mean_squared_error: 22179189.1702 - val_loss: 23031742.3094 - val_mean_squared_error: 23031742.3094
Epoch 131/2000
 - 0s - loss: 22124210.8962 - mean_squared_error: 22124210.8962 - val_loss: 22975737.6411 - val_mean_squared_error: 22975737.6411
Epoch 132/2000
 - 0s - loss: 22069474.0278 - mean_squared_error: 22069474.0278 - val_loss: 22920065.3208 - val_mean_squared_error: 22920065.3208
Epoch 133/2000
 - 0s - loss: 22014981.8085 - mean_squared_error: 22014981.8085 - val_loss: 22864551.8389 - val_mean_squared_error: 22864551.8389
Epoch 134/2000
 - 0s - loss: 21960659.8293 - mean_squared_error: 21960659.8293 - val_loss: 22809443.7350 - val_mean_squared_error: 22809443.7350
Epoch 135/2000
 - 0s - loss: 21906667.1327 - mean_squared_error: 21906667.1327 - val_loss: 22754352.2323 - val_mean_squared_error: 22754352.2323
Epoch 136/2000
 - 0s - loss: 21852773.1706 - mean_squared_error: 21852773.1706 - val_loss: 22699528.5279 - val_mean_squared_error: 22699528.5279
Epoch 137/2000
 - 0s - loss: 21799218.2557 - mean_squared_error: 21799218.2557 - val_loss: 22645053.6391 - val_mean_squared_error: 22645053.6391
Epoch 138/2000
 - 0s - loss: 21745988.5180 - mean_squared_error: 21745988.5180 - val_loss: 22590829.6362 - val_mean_squared_error: 22590829.6362
Epoch 139/2000
 - 0s - loss: 21692884.7601 - mean_squared_error: 21692884.7601 - val_loss: 22536811.3139 - val_mean_squared_error: 22536811.3139
Epoch 140/2000
 - 0s - loss: 21639922.0363 - mean_squared_error: 21639922.0363 - val_loss: 22482902.6268 - val_mean_squared_error: 22482902.6268
Epoch 141/2000
 - 0s - loss: 21587277.8681 - mean_squared_error: 21587277.8681 - val_loss: 22429230.5803 - val_mean_squared_error: 22429230.5803
Epoch 142/2000
 - 0s - loss: 21534855.1150 - mean_squared_error: 21534855.1150 - val_loss: 22375962.8720 - val_mean_squared_error: 22375962.8720
Epoch 143/2000
 - 0s - loss: 21482826.3633 - mean_squared_error: 21482826.3633 - val_loss: 22323030.2056 - val_mean_squared_error: 22323030.2056
Epoch 144/2000
 - 0s - loss: 21431066.9901 - mean_squared_error: 21431066.9901 - val_loss: 22270324.7177 - val_mean_squared_error: 22270324.7177
Epoch 145/2000
 - 0s - loss: 21379457.4848 - mean_squared_error: 21379457.4848 - val_loss: 22217833.6293 - val_mean_squared_error: 22217833.6293
Epoch 146/2000
 - 0s - loss: 21328267.0717 - mean_squared_error: 21328267.0717 - val_loss: 22165618.7365 - val_mean_squared_error: 22165618.7365
Epoch 147/2000
 - 0s - loss: 21277279.8207 - mean_squared_error: 21277279.8207 - val_loss: 22113582.5645 - val_mean_squared_error: 22113582.5645
Epoch 148/2000
 - 0s - loss: 21226358.6830 - mean_squared_error: 21226358.6830 - val_loss: 22061819.5957 - val_mean_squared_error: 22061819.5957
Epoch 149/2000
 - 0s - loss: 21175828.1236 - mean_squared_error: 21175828.1236 - val_loss: 22010352.4785 - val_mean_squared_error: 22010352.4785
Epoch 150/2000
 - 0s - loss: 21125455.0652 - mean_squared_error: 21125455.0652 - val_loss: 21958971.6164 - val_mean_squared_error: 21958971.6164
Epoch 151/2000
 - 0s - loss: 21075471.0393 - mean_squared_error: 21075471.0393 - val_loss: 21908054.8433 - val_mean_squared_error: 21908054.8433
Epoch 152/2000
 - 0s - loss: 21025684.3461 - mean_squared_error: 21025684.3461 - val_loss: 21857321.4810 - val_mean_squared_error: 21857321.4810
Epoch 153/2000
 - 0s - loss: 20976130.8501 - mean_squared_error: 20976130.8501 - val_loss: 21806772.0712 - val_mean_squared_error: 21806772.0712
Epoch 154/2000
 - 0s - loss: 20926800.1894 - mean_squared_error: 20926800.1894 - val_loss: 21756469.2773 - val_mean_squared_error: 21756469.2773
Epoch 155/2000
 - 0s - loss: 20877648.5055 - mean_squared_error: 20877648.5055 - val_loss: 21706429.8903 - val_mean_squared_error: 21706429.8903
Epoch 156/2000
 - 0s - loss: 20828803.0014 - mean_squared_error: 20828803.0014 - val_loss: 21656640.7276 - val_mean_squared_error: 21656640.7276
Epoch 157/2000
 - 0s - loss: 20780237.4358 - mean_squared_error: 20780237.4358 - val_loss: 21607164.1770 - val_mean_squared_error: 21607164.1770
Epoch 158/2000
 - 0s - loss: 20731964.5111 - mean_squared_error: 20731964.5111 - val_loss: 21557928.7899 - val_mean_squared_error: 21557928.7899
Epoch 159/2000
 - 0s - loss: 20683983.8124 - mean_squared_error: 20683983.8124 - val_loss: 21508942.7563 - val_mean_squared_error: 21508942.7563
Epoch 160/2000
 - 0s - loss: 20636215.6980 - mean_squared_error: 20636215.6980 - val_loss: 21460266.9946 - val_mean_squared_error: 21460266.9946
Epoch 161/2000
 - 0s - loss: 20588576.9063 - mean_squared_error: 20588576.9063 - val_loss: 21411659.6817 - val_mean_squared_error: 21411659.6817
Epoch 162/2000
 - 0s - loss: 20541391.1964 - mean_squared_error: 20541391.1964 - val_loss: 21363572.7583 - val_mean_squared_error: 21363572.7583
Epoch 163/2000
 - 0s - loss: 20494382.0617 - mean_squared_error: 20494382.0617 - val_loss: 21315584.2096 - val_mean_squared_error: 21315584.2096
Epoch 164/2000
 - 0s - loss: 20447582.2063 - mean_squared_error: 20447582.2063 - val_loss: 21267848.2007 - val_mean_squared_error: 21267848.2007
Epoch 165/2000
 - 0s - loss: 20401091.7874 - mean_squared_error: 20401091.7874 - val_loss: 21220484.2867 - val_mean_squared_error: 21220484.2867
Epoch 166/2000
 - 0s - loss: 20354782.9350 - mean_squared_error: 20354782.9350 - val_loss: 21173170.9224 - val_mean_squared_error: 21173170.9224
Epoch 167/2000
 - 0s - loss: 20308683.8885 - mean_squared_error: 20308683.8885 - val_loss: 21126166.0732 - val_mean_squared_error: 21126166.0732
Epoch 168/2000
 - 0s - loss: 20262880.0498 - mean_squared_error: 20262880.0498 - val_loss: 21079453.9427 - val_mean_squared_error: 21079453.9427
Epoch 169/2000
 - 0s - loss: 20217302.2948 - mean_squared_error: 20217302.2948 - val_loss: 21032954.3272 - val_mean_squared_error: 21032954.3272
Epoch 170/2000
 - 0s - loss: 20172039.3799 - mean_squared_error: 20172039.3799 - val_loss: 20986661.4800 - val_mean_squared_error: 20986661.4800
Epoch 171/2000
 - 0s - loss: 20127028.1829 - mean_squared_error: 20127028.1829 - val_loss: 20940699.0222 - val_mean_squared_error: 20940699.0222
Epoch 172/2000
 - 0s - loss: 20082127.6462 - mean_squared_error: 20082127.6462 - val_loss: 20894755.4909 - val_mean_squared_error: 20894755.4909
Epoch 173/2000
 - 0s - loss: 20037422.4802 - mean_squared_error: 20037422.4802 - val_loss: 20849197.8527 - val_mean_squared_error: 20849197.8527

*** WARNING: skipped 241242 bytes of output ***

Epoch 1832/2000
 - 0s - loss: 15919839.0633 - mean_squared_error: 15919839.0633 - val_loss: 16556644.7647 - val_mean_squared_error: 16556644.7647
Epoch 1833/2000
 - 0s - loss: 15919840.3255 - mean_squared_error: 15919840.3255 - val_loss: 16556635.6149 - val_mean_squared_error: 16556635.6149
Epoch 1834/2000
 - 0s - loss: 15919841.6200 - mean_squared_error: 15919841.6200 - val_loss: 16556647.3777 - val_mean_squared_error: 16556647.3777
Epoch 1835/2000
 - 0s - loss: 15919840.0938 - mean_squared_error: 15919840.0938 - val_loss: 16556649.3386 - val_mean_squared_error: 16556649.3386
Epoch 1836/2000
 - 0s - loss: 15919840.6451 - mean_squared_error: 15919840.6451 - val_loss: 16556650.5734 - val_mean_squared_error: 16556650.5734
Epoch 1837/2000
 - 0s - loss: 15919842.1420 - mean_squared_error: 15919842.1420 - val_loss: 16556663.7143 - val_mean_squared_error: 16556663.7143
Epoch 1838/2000
 - 0s - loss: 15919842.1427 - mean_squared_error: 15919842.1427 - val_loss: 16556685.6446 - val_mean_squared_error: 16556685.6446
Epoch 1839/2000
 - 0s - loss: 15919841.2475 - mean_squared_error: 15919841.2475 - val_loss: 16556689.8868 - val_mean_squared_error: 16556689.8868
Epoch 1840/2000
 - 0s - loss: 15919837.4733 - mean_squared_error: 15919837.4733 - val_loss: 16556697.9367 - val_mean_squared_error: 16556697.9367
Epoch 1841/2000
 - 0s - loss: 15919838.4133 - mean_squared_error: 15919838.4133 - val_loss: 16556688.7870 - val_mean_squared_error: 16556688.7870
Epoch 1842/2000
 - 0s - loss: 15919840.9300 - mean_squared_error: 15919840.9300 - val_loss: 16556710.4123 - val_mean_squared_error: 16556710.4123
Epoch 1843/2000
 - 0s - loss: 15919842.6381 - mean_squared_error: 15919842.6381 - val_loss: 16556705.8774 - val_mean_squared_error: 16556705.8774
Epoch 1844/2000
 - 0s - loss: 15919837.4598 - mean_squared_error: 15919837.4598 - val_loss: 16556709.6480 - val_mean_squared_error: 16556709.6480
Epoch 1845/2000
 - 0s - loss: 15919838.9912 - mean_squared_error: 15919838.9912 - val_loss: 16556699.7350 - val_mean_squared_error: 16556699.7350
Epoch 1846/2000
 - 0s - loss: 15919842.1577 - mean_squared_error: 15919842.1577 - val_loss: 16556698.5793 - val_mean_squared_error: 16556698.5793
Epoch 1847/2000
 - 0s - loss: 15919842.4977 - mean_squared_error: 15919842.4977 - val_loss: 16556697.8873 - val_mean_squared_error: 16556697.8873
Epoch 1848/2000
 - 0s - loss: 15919841.3402 - mean_squared_error: 15919841.3402 - val_loss: 16556698.8255 - val_mean_squared_error: 16556698.8255
Epoch 1849/2000
 - 0s - loss: 15919838.7708 - mean_squared_error: 15919838.7708 - val_loss: 16556703.6095 - val_mean_squared_error: 16556703.6095
Epoch 1850/2000
 - 0s - loss: 15919837.3867 - mean_squared_error: 15919837.3867 - val_loss: 16556710.7069 - val_mean_squared_error: 16556710.7069
Epoch 1851/2000
 - 0s - loss: 15919837.8497 - mean_squared_error: 15919837.8497 - val_loss: 16556712.7182 - val_mean_squared_error: 16556712.7182
Epoch 1852/2000
 - 0s - loss: 15919836.2385 - mean_squared_error: 15919836.2385 - val_loss: 16556729.8883 - val_mean_squared_error: 16556729.8883
Epoch 1853/2000
 - 0s - loss: 15919839.4165 - mean_squared_error: 15919839.4165 - val_loss: 16556707.3915 - val_mean_squared_error: 16556707.3915
Epoch 1854/2000
 - 0s - loss: 15919839.0620 - mean_squared_error: 15919839.0620 - val_loss: 16556693.9283 - val_mean_squared_error: 16556693.9283
Epoch 1855/2000
 - 0s - loss: 15919840.7434 - mean_squared_error: 15919840.7434 - val_loss: 16556700.7958 - val_mean_squared_error: 16556700.7958
Epoch 1856/2000
 - 0s - loss: 15919839.6519 - mean_squared_error: 15919839.6519 - val_loss: 16556705.2462 - val_mean_squared_error: 16556705.2462
Epoch 1857/2000
 - 0s - loss: 15919839.2747 - mean_squared_error: 15919839.2747 - val_loss: 16556678.6411 - val_mean_squared_error: 16556678.6411
Epoch 1858/2000
 - 0s - loss: 15919837.8284 - mean_squared_error: 15919837.8284 - val_loss: 16556697.2106 - val_mean_squared_error: 16556697.2106
Epoch 1859/2000
 - 0s - loss: 15919842.1135 - mean_squared_error: 15919842.1135 - val_loss: 16556700.1913 - val_mean_squared_error: 16556700.1913
Epoch 1860/2000
 - 0s - loss: 15919840.7381 - mean_squared_error: 15919840.7381 - val_loss: 16556703.0657 - val_mean_squared_error: 16556703.0657
Epoch 1861/2000
 - 0s - loss: 15919837.5396 - mean_squared_error: 15919837.5396 - val_loss: 16556699.3920 - val_mean_squared_error: 16556699.3920
Epoch 1862/2000
 - 0s - loss: 15919838.4874 - mean_squared_error: 15919838.4874 - val_loss: 16556716.9224 - val_mean_squared_error: 16556716.9224
Epoch 1863/2000
 - 0s - loss: 15919837.8046 - mean_squared_error: 15919837.8046 - val_loss: 16556725.6139 - val_mean_squared_error: 16556725.6139
Epoch 1864/2000
 - 0s - loss: 15919840.9139 - mean_squared_error: 15919840.9139 - val_loss: 16556720.3747 - val_mean_squared_error: 16556720.3747
Epoch 1865/2000
 - 0s - loss: 15919840.1795 - mean_squared_error: 15919840.1795 - val_loss: 16556722.3109 - val_mean_squared_error: 16556722.3109
Epoch 1866/2000
 - 0s - loss: 15919843.0618 - mean_squared_error: 15919843.0618 - val_loss: 16556721.0391 - val_mean_squared_error: 16556721.0391
Epoch 1867/2000
 - 0s - loss: 15919841.2889 - mean_squared_error: 15919841.2889 - val_loss: 16556744.9086 - val_mean_squared_error: 16556744.9086
Epoch 1868/2000
 - 0s - loss: 15919839.5626 - mean_squared_error: 15919839.5626 - val_loss: 16556722.7899 - val_mean_squared_error: 16556722.7899
Epoch 1869/2000
 - 0s - loss: 15919845.4279 - mean_squared_error: 15919845.4279 - val_loss: 16556720.3470 - val_mean_squared_error: 16556720.3470
Epoch 1870/2000
 - 0s - loss: 15919837.2327 - mean_squared_error: 15919837.2327 - val_loss: 16556730.4474 - val_mean_squared_error: 16556730.4474
Epoch 1871/2000
 - 0s - loss: 15919836.4915 - mean_squared_error: 15919836.4915 - val_loss: 16556728.8285 - val_mean_squared_error: 16556728.8285
Epoch 1872/2000
 - 0s - loss: 15919848.0744 - mean_squared_error: 15919848.0744 - val_loss: 16556742.1928 - val_mean_squared_error: 16556742.1928
Epoch 1873/2000
 - 0s - loss: 15919837.7825 - mean_squared_error: 15919837.7825 - val_loss: 16556739.9011 - val_mean_squared_error: 16556739.9011
Epoch 1874/2000
 - 0s - loss: 15919839.3969 - mean_squared_error: 15919839.3969 - val_loss: 16556747.4587 - val_mean_squared_error: 16556747.4587
Epoch 1875/2000
 - 0s - loss: 15919839.6557 - mean_squared_error: 15919839.6557 - val_loss: 16556750.2323 - val_mean_squared_error: 16556750.2323
Epoch 1876/2000
 - 0s - loss: 15919835.1444 - mean_squared_error: 15919835.1444 - val_loss: 16556749.6050 - val_mean_squared_error: 16556749.6050
Epoch 1877/2000
 - 0s - loss: 15919836.5018 - mean_squared_error: 15919836.5018 - val_loss: 16556753.1676 - val_mean_squared_error: 16556753.1676
Epoch 1878/2000
 - 0s - loss: 15919839.9839 - mean_squared_error: 15919839.9839 - val_loss: 16556745.1547 - val_mean_squared_error: 16556745.1547
Epoch 1879/2000
 - 0s - loss: 15919842.4734 - mean_squared_error: 15919842.4734 - val_loss: 16556743.0104 - val_mean_squared_error: 16556743.0104
Epoch 1880/2000
 - 0s - loss: 15919841.0024 - mean_squared_error: 15919841.0024 - val_loss: 16556740.5314 - val_mean_squared_error: 16556740.5314
Epoch 1881/2000
 - 0s - loss: 15919838.0398 - mean_squared_error: 15919838.0398 - val_loss: 16556748.0262 - val_mean_squared_error: 16556748.0262
Epoch 1882/2000
 - 0s - loss: 15919840.2566 - mean_squared_error: 15919840.2566 - val_loss: 16556742.1710 - val_mean_squared_error: 16556742.1710
Epoch 1883/2000
 - 0s - loss: 15919839.9691 - mean_squared_error: 15919839.9691 - val_loss: 16556754.4973 - val_mean_squared_error: 16556754.4973
Epoch 1884/2000
 - 0s - loss: 15919842.2178 - mean_squared_error: 15919842.2178 - val_loss: 16556768.8739 - val_mean_squared_error: 16556768.8739
Epoch 1885/2000
 - 0s - loss: 15919836.8102 - mean_squared_error: 15919836.8102 - val_loss: 16556762.7469 - val_mean_squared_error: 16556762.7469
Epoch 1886/2000
 - 0s - loss: 15919842.2946 - mean_squared_error: 15919842.2946 - val_loss: 16556747.3361 - val_mean_squared_error: 16556747.3361
Epoch 1887/2000
 - 0s - loss: 15919845.0852 - mean_squared_error: 15919845.0852 - val_loss: 16556757.1236 - val_mean_squared_error: 16556757.1236
Epoch 1888/2000
 - 0s - loss: 15919842.2618 - mean_squared_error: 15919842.2618 - val_loss: 16556758.4439 - val_mean_squared_error: 16556758.4439
Epoch 1889/2000
 - 0s - loss: 15919836.4589 - mean_squared_error: 15919836.4589 - val_loss: 16556775.5561 - val_mean_squared_error: 16556775.5561
Epoch 1890/2000
 - 0s - loss: 15919837.9622 - mean_squared_error: 15919837.9622 - val_loss: 16556770.6382 - val_mean_squared_error: 16556770.6382
Epoch 1891/2000
 - 0s - loss: 15919837.7282 - mean_squared_error: 15919837.7282 - val_loss: 16556775.9486 - val_mean_squared_error: 16556775.9486
Epoch 1892/2000
 - 0s - loss: 15919838.4250 - mean_squared_error: 15919838.4250 - val_loss: 16556779.2289 - val_mean_squared_error: 16556779.2289
Epoch 1893/2000
 - 0s - loss: 15919843.4984 - mean_squared_error: 15919843.4984 - val_loss: 16556777.0732 - val_mean_squared_error: 16556777.0732
Epoch 1894/2000
 - 0s - loss: 15919843.3910 - mean_squared_error: 15919843.3910 - val_loss: 16556771.6846 - val_mean_squared_error: 16556771.6846
Epoch 1895/2000
 - 0s - loss: 15919844.0152 - mean_squared_error: 15919844.0152 - val_loss: 16556793.1028 - val_mean_squared_error: 16556793.1028
Epoch 1896/2000
 - 0s - loss: 15919838.8479 - mean_squared_error: 15919838.8479 - val_loss: 16556779.3030 - val_mean_squared_error: 16556779.3030
Epoch 1897/2000
 - 0s - loss: 15919841.8588 - mean_squared_error: 15919841.8588 - val_loss: 16556779.0929 - val_mean_squared_error: 16556779.0929
Epoch 1898/2000
 - 0s - loss: 15919841.9264 - mean_squared_error: 15919841.9264 - val_loss: 16556775.5067 - val_mean_squared_error: 16556775.5067
Epoch 1899/2000
 - 0s - loss: 15919837.4295 - mean_squared_error: 15919837.4295 - val_loss: 16556756.5571 - val_mean_squared_error: 16556756.5571
Epoch 1900/2000
 - 0s - loss: 15919842.6663 - mean_squared_error: 15919842.6663 - val_loss: 16556770.0306 - val_mean_squared_error: 16556770.0306
Epoch 1901/2000
 - 0s - loss: 15919840.8584 - mean_squared_error: 15919840.8584 - val_loss: 16556772.1295 - val_mean_squared_error: 16556772.1295
Epoch 1902/2000
 - 0s - loss: 15919838.3398 - mean_squared_error: 15919838.3398 - val_loss: 16556772.9174 - val_mean_squared_error: 16556772.9174
Epoch 1903/2000
 - 0s - loss: 15919840.8919 - mean_squared_error: 15919840.8919 - val_loss: 16556779.0826 - val_mean_squared_error: 16556779.0826
Epoch 1904/2000
 - 0s - loss: 15919840.6112 - mean_squared_error: 15919840.6112 - val_loss: 16556777.8384 - val_mean_squared_error: 16556777.8384
Epoch 1905/2000
 - 0s - loss: 15919841.8981 - mean_squared_error: 15919841.8981 - val_loss: 16556751.7761 - val_mean_squared_error: 16556751.7761
Epoch 1906/2000
 - 0s - loss: 15919839.4216 - mean_squared_error: 15919839.4216 - val_loss: 16556758.8156 - val_mean_squared_error: 16556758.8156
Epoch 1907/2000
 - 0s - loss: 15919839.1625 - mean_squared_error: 15919839.1625 - val_loss: 16556743.3915 - val_mean_squared_error: 16556743.3915
Epoch 1908/2000
 - 0s - loss: 15919844.9876 - mean_squared_error: 15919844.9876 - val_loss: 16556745.4009 - val_mean_squared_error: 16556745.4009
Epoch 1909/2000
 - 0s - loss: 15919838.2083 - mean_squared_error: 15919838.2083 - val_loss: 16556738.0514 - val_mean_squared_error: 16556738.0514
Epoch 1910/2000
 - 0s - loss: 15919838.9869 - mean_squared_error: 15919838.9869 - val_loss: 16556746.2289 - val_mean_squared_error: 16556746.2289
Epoch 1911/2000
 - 0s - loss: 15919837.5660 - mean_squared_error: 15919837.5660 - val_loss: 16556758.8403 - val_mean_squared_error: 16556758.8403
Epoch 1912/2000
 - 0s - loss: 15919839.5910 - mean_squared_error: 15919839.5910 - val_loss: 16556757.6569 - val_mean_squared_error: 16556757.6569
Epoch 1913/2000
 - 0s - loss: 15919841.6379 - mean_squared_error: 15919841.6379 - val_loss: 16556757.2595 - val_mean_squared_error: 16556757.2595
Epoch 1914/2000
 - 0s - loss: 15919838.3454 - mean_squared_error: 15919838.3454 - val_loss: 16556737.4622 - val_mean_squared_error: 16556737.4622
Epoch 1915/2000
 - 0s - loss: 15919836.9584 - mean_squared_error: 15919836.9584 - val_loss: 16556750.6288 - val_mean_squared_error: 16556750.6288
Epoch 1916/2000
 - 0s - loss: 15919841.6337 - mean_squared_error: 15919841.6337 - val_loss: 16556742.0465 - val_mean_squared_error: 16556742.0465
Epoch 1917/2000
 - 0s - loss: 15919837.1959 - mean_squared_error: 15919837.1959 - val_loss: 16556732.6154 - val_mean_squared_error: 16556732.6154
Epoch 1918/2000
 - 0s - loss: 15919839.7069 - mean_squared_error: 15919839.7069 - val_loss: 16556722.4315 - val_mean_squared_error: 16556722.4315
Epoch 1919/2000
 - 0s - loss: 15919841.5924 - mean_squared_error: 15919841.5924 - val_loss: 16556710.8077 - val_mean_squared_error: 16556710.8077
Epoch 1920/2000
 - 0s - loss: 15919838.2981 - mean_squared_error: 15919838.2981 - val_loss: 16556726.9560 - val_mean_squared_error: 16556726.9560
Epoch 1921/2000
 - 0s - loss: 15919842.2630 - mean_squared_error: 15919842.2630 - val_loss: 16556718.1676 - val_mean_squared_error: 16556718.1676
Epoch 1922/2000
 - 0s - loss: 15919841.9537 - mean_squared_error: 15919841.9537 - val_loss: 16556696.9728 - val_mean_squared_error: 16556696.9728
Epoch 1923/2000
 - 0s - loss: 15919837.5424 - mean_squared_error: 15919837.5424 - val_loss: 16556700.0440 - val_mean_squared_error: 16556700.0440
Epoch 1924/2000
 - 0s - loss: 15919838.7335 - mean_squared_error: 15919838.7335 - val_loss: 16556691.4266 - val_mean_squared_error: 16556691.4266
Epoch 1925/2000
 - 0s - loss: 15919841.1641 - mean_squared_error: 15919841.1641 - val_loss: 16556709.9333 - val_mean_squared_error: 16556709.9333
Epoch 1926/2000
 - 0s - loss: 15919839.5108 - mean_squared_error: 15919839.5108 - val_loss: 16556718.7815 - val_mean_squared_error: 16556718.7815
Epoch 1927/2000
 - 0s - loss: 15919843.1280 - mean_squared_error: 15919843.1280 - val_loss: 16556725.6747 - val_mean_squared_error: 16556725.6747
Epoch 1928/2000
 - 0s - loss: 15919839.0706 - mean_squared_error: 15919839.0706 - val_loss: 16556712.9263 - val_mean_squared_error: 16556712.9263
Epoch 1929/2000
 - 0s - loss: 15919838.6063 - mean_squared_error: 15919838.6063 - val_loss: 16556722.3450 - val_mean_squared_error: 16556722.3450
Epoch 1930/2000
 - 0s - loss: 15919834.3925 - mean_squared_error: 15919834.3925 - val_loss: 16556733.3292 - val_mean_squared_error: 16556733.3292
Epoch 1931/2000
 - 0s - loss: 15919839.9176 - mean_squared_error: 15919839.9176 - val_loss: 16556740.5798 - val_mean_squared_error: 16556740.5798
Epoch 1932/2000
 - 0s - loss: 15919837.0179 - mean_squared_error: 15919837.0179 - val_loss: 16556745.3638 - val_mean_squared_error: 16556745.3638
Epoch 1933/2000
 - 0s - loss: 15919848.6947 - mean_squared_error: 15919848.6947 - val_loss: 16556746.5329 - val_mean_squared_error: 16556746.5329
Epoch 1934/2000
 - 0s - loss: 15919844.9725 - mean_squared_error: 15919844.9725 - val_loss: 16556730.0910 - val_mean_squared_error: 16556730.0910
Epoch 1935/2000
 - 0s - loss: 15919841.7440 - mean_squared_error: 15919841.7440 - val_loss: 16556734.9565 - val_mean_squared_error: 16556734.9565
Epoch 1936/2000
 - 0s - loss: 15919842.5312 - mean_squared_error: 15919842.5312 - val_loss: 16556726.4142 - val_mean_squared_error: 16556726.4142
Epoch 1937/2000
 - 0s - loss: 15919840.9410 - mean_squared_error: 15919840.9410 - val_loss: 16556731.3485 - val_mean_squared_error: 16556731.3485
Epoch 1938/2000
 - 0s - loss: 15919840.0616 - mean_squared_error: 15919840.0616 - val_loss: 16556749.6298 - val_mean_squared_error: 16556749.6298
Epoch 1939/2000
 - 0s - loss: 15919847.3593 - mean_squared_error: 15919847.3593 - val_loss: 16556732.9338 - val_mean_squared_error: 16556732.9338
Epoch 1940/2000
 - 0s - loss: 15919842.1849 - mean_squared_error: 15919842.1849 - val_loss: 16556737.9916 - val_mean_squared_error: 16556737.9916
Epoch 1941/2000
 - 0s - loss: 15919838.7040 - mean_squared_error: 15919838.7040 - val_loss: 16556734.5022 - val_mean_squared_error: 16556734.5022
Epoch 1942/2000
 - 0s - loss: 15919838.1714 - mean_squared_error: 15919838.1714 - val_loss: 16556738.9278 - val_mean_squared_error: 16556738.9278
Epoch 1943/2000
 - 0s - loss: 15919840.9966 - mean_squared_error: 15919840.9966 - val_loss: 16556731.4320 - val_mean_squared_error: 16556731.4320
Epoch 1944/2000
 - 0s - loss: 15919839.2703 - mean_squared_error: 15919839.2703 - val_loss: 16556749.2343 - val_mean_squared_error: 16556749.2343
Epoch 1945/2000
 - 0s - loss: 15919835.8730 - mean_squared_error: 15919835.8730 - val_loss: 16556757.6569 - val_mean_squared_error: 16556757.6569
Epoch 1946/2000
 - 0s - loss: 15919843.3970 - mean_squared_error: 15919843.3970 - val_loss: 16556749.2126 - val_mean_squared_error: 16556749.2126
Epoch 1947/2000
 - 0s - loss: 15919836.3736 - mean_squared_error: 15919836.3736 - val_loss: 16556777.0475 - val_mean_squared_error: 16556777.0475
Epoch 1948/2000
 - 0s - loss: 15919838.4017 - mean_squared_error: 15919838.4017 - val_loss: 16556770.5734 - val_mean_squared_error: 16556770.5734
Epoch 1949/2000
 - 0s - loss: 15919841.0001 - mean_squared_error: 15919841.0001 - val_loss: 16556777.0475 - val_mean_squared_error: 16556777.0475
Epoch 1950/2000
 - 0s - loss: 15919840.6887 - mean_squared_error: 15919840.6887 - val_loss: 16556797.0094 - val_mean_squared_error: 16556797.0094
Epoch 1951/2000
 - 0s - loss: 15919837.1316 - mean_squared_error: 15919837.1316 - val_loss: 16556769.6401 - val_mean_squared_error: 16556769.6401
Epoch 1952/2000
 - 0s - loss: 15919839.7709 - mean_squared_error: 15919839.7709 - val_loss: 16556784.5185 - val_mean_squared_error: 16556784.5185
Epoch 1953/2000
 - 0s - loss: 15919842.3838 - mean_squared_error: 15919842.3838 - val_loss: 16556778.0722 - val_mean_squared_error: 16556778.0722
Epoch 1954/2000
 - 0s - loss: 15919838.7597 - mean_squared_error: 15919838.7597 - val_loss: 16556758.4439 - val_mean_squared_error: 16556758.4439
Epoch 1955/2000
 - 0s - loss: 15919837.6232 - mean_squared_error: 15919837.6232 - val_loss: 16556769.7741 - val_mean_squared_error: 16556769.7741
Epoch 1956/2000
 - 0s - loss: 15919841.1736 - mean_squared_error: 15919841.1736 - val_loss: 16556783.8428 - val_mean_squared_error: 16556783.8428
Epoch 1957/2000
 - 0s - loss: 15919838.4189 - mean_squared_error: 15919838.4189 - val_loss: 16556789.1789 - val_mean_squared_error: 16556789.1789
Epoch 1958/2000
 - 0s - loss: 15919844.6613 - mean_squared_error: 15919844.6613 - val_loss: 16556774.1008 - val_mean_squared_error: 16556774.1008
Epoch 1959/2000
 - 0s - loss: 15919839.5645 - mean_squared_error: 15919839.5645 - val_loss: 16556772.2037 - val_mean_squared_error: 16556772.2037
Epoch 1960/2000
 - 0s - loss: 15919837.8679 - mean_squared_error: 15919837.8679 - val_loss: 16556784.0519 - val_mean_squared_error: 16556784.0519
Epoch 1961/2000
 - 0s - loss: 15919845.2541 - mean_squared_error: 15919845.2541 - val_loss: 16556773.5858 - val_mean_squared_error: 16556773.5858
Epoch 1962/2000
 - 0s - loss: 15919843.2494 - mean_squared_error: 15919843.2494 - val_loss: 16556760.4770 - val_mean_squared_error: 16556760.4770
Epoch 1963/2000
 - 0s - loss: 15919841.6358 - mean_squared_error: 15919841.6358 - val_loss: 16556767.1972 - val_mean_squared_error: 16556767.1972
Epoch 1964/2000
 - 0s - loss: 15919845.4326 - mean_squared_error: 15919845.4326 - val_loss: 16556776.0381 - val_mean_squared_error: 16556776.0381
Epoch 1965/2000
 - 0s - loss: 15919841.3805 - mean_squared_error: 15919841.3805 - val_loss: 16556780.6110 - val_mean_squared_error: 16556780.6110
Epoch 1966/2000
 - 0s - loss: 15919840.0666 - mean_squared_error: 15919840.0666 - val_loss: 16556780.5225 - val_mean_squared_error: 16556780.5225
Epoch 1967/2000
 - 0s - loss: 15919842.7959 - mean_squared_error: 15919842.7959 - val_loss: 16556774.6332 - val_mean_squared_error: 16556774.6332
Epoch 1968/2000
 - 0s - loss: 15919840.4225 - mean_squared_error: 15919840.4225 - val_loss: 16556779.5625 - val_mean_squared_error: 16556779.5625
Epoch 1969/2000
 - 0s - loss: 15919840.7085 - mean_squared_error: 15919840.7085 - val_loss: 16556764.1537 - val_mean_squared_error: 16556764.1537
Epoch 1970/2000
 - 0s - loss: 15919842.7566 - mean_squared_error: 15919842.7566 - val_loss: 16556765.7059 - val_mean_squared_error: 16556765.7059
Epoch 1971/2000
 - 0s - loss: 15919838.0659 - mean_squared_error: 15919838.0659 - val_loss: 16556773.7187 - val_mean_squared_error: 16556773.7187
Epoch 1972/2000
 - 0s - loss: 15919840.6388 - mean_squared_error: 15919840.6388 - val_loss: 16556791.3376 - val_mean_squared_error: 16556791.3376
Epoch 1973/2000
 - 0s - loss: 15919840.7910 - mean_squared_error: 15919840.7910 - val_loss: 16556788.7588 - val_mean_squared_error: 16556788.7588
Epoch 1974/2000
 - 0s - loss: 15919847.2463 - mean_squared_error: 15919847.2463 - val_loss: 16556776.7909 - val_mean_squared_error: 16556776.7909
Epoch 1975/2000
 - 0s - loss: 15919839.2122 - mean_squared_error: 15919839.2122 - val_loss: 16556773.5858 - val_mean_squared_error: 16556773.5858
Epoch 1976/2000
 - 0s - loss: 15919842.3183 - mean_squared_error: 15919842.3183 - val_loss: 16556770.4745 - val_mean_squared_error: 16556770.4745
Epoch 1977/2000
 - 0s - loss: 15919842.5022 - mean_squared_error: 15919842.5022 - val_loss: 16556780.1186 - val_mean_squared_error: 16556780.1186
Epoch 1978/2000
 - 0s - loss: 15919842.7939 - mean_squared_error: 15919842.7939 - val_loss: 16556785.3589 - val_mean_squared_error: 16556785.3589
Epoch 1979/2000
 - 0s - loss: 15919842.2156 - mean_squared_error: 15919842.2156 - val_loss: 16556791.3109 - val_mean_squared_error: 16556791.3109
Epoch 1980/2000
 - 0s - loss: 15919837.1162 - mean_squared_error: 15919837.1162 - val_loss: 16556786.1241 - val_mean_squared_error: 16556786.1241
Epoch 1981/2000
 - 0s - loss: 15919843.6045 - mean_squared_error: 15919843.6045 - val_loss: 16556763.5976 - val_mean_squared_error: 16556763.5976
Epoch 1982/2000
 - 0s - loss: 15919840.3795 - mean_squared_error: 15919840.3795 - val_loss: 16556779.0465 - val_mean_squared_error: 16556779.0465
Epoch 1983/2000
 - 0s - loss: 15919840.6743 - mean_squared_error: 15919840.6743 - val_loss: 16556755.6426 - val_mean_squared_error: 16556755.6426
Epoch 1984/2000
 - 0s - loss: 15919839.0763 - mean_squared_error: 15919839.0763 - val_loss: 16556746.8809 - val_mean_squared_error: 16556746.8809
Epoch 1985/2000
 - 0s - loss: 15919843.7900 - mean_squared_error: 15919843.7900 - val_loss: 16556765.8789 - val_mean_squared_error: 16556765.8789
Epoch 1986/2000
 - 0s - loss: 15919839.8720 - mean_squared_error: 15919839.8720 - val_loss: 16556766.1518 - val_mean_squared_error: 16556766.1518
Epoch 1987/2000
 - 0s - loss: 15919841.0103 - mean_squared_error: 15919841.0103 - val_loss: 16556761.3658 - val_mean_squared_error: 16556761.3658
Epoch 1988/2000
 - 0s - loss: 15919844.0040 - mean_squared_error: 15919844.0040 - val_loss: 16556743.2175 - val_mean_squared_error: 16556743.2175
Epoch 1989/2000
 - 0s - loss: 15919840.8275 - mean_squared_error: 15919840.8275 - val_loss: 16556755.9051 - val_mean_squared_error: 16556755.9051
Epoch 1990/2000
 - 0s - loss: 15919839.0663 - mean_squared_error: 15919839.0663 - val_loss: 16556755.2264 - val_mean_squared_error: 16556755.2264
Epoch 1991/2000
 - 0s - loss: 15919841.7517 - mean_squared_error: 15919841.7517 - val_loss: 16556744.1176 - val_mean_squared_error: 16556744.1176
Epoch 1992/2000
 - 0s - loss: 15919839.2471 - mean_squared_error: 15919839.2471 - val_loss: 16556753.8700 - val_mean_squared_error: 16556753.8700
Epoch 1993/2000
 - 0s - loss: 15919839.7289 - mean_squared_error: 15919839.7289 - val_loss: 16556742.0484 - val_mean_squared_error: 16556742.0484
Epoch 1994/2000
 - 0s - loss: 15919842.0739 - mean_squared_error: 15919842.0739 - val_loss: 16556749.2343 - val_mean_squared_error: 16556749.2343
Epoch 1995/2000
 - 0s - loss: 15919844.3030 - mean_squared_error: 15919844.3030 - val_loss: 16556739.4686 - val_mean_squared_error: 16556739.4686
Epoch 1996/2000
 - 0s - loss: 15919837.5737 - mean_squared_error: 15919837.5737 - val_loss: 16556745.3144 - val_mean_squared_error: 16556745.3144
Epoch 1997/2000
 - 0s - loss: 15919836.2656 - mean_squared_error: 15919836.2656 - val_loss: 16556755.5467 - val_mean_squared_error: 16556755.5467
Epoch 1998/2000
 - 0s - loss: 15919835.8301 - mean_squared_error: 15919835.8301 - val_loss: 16556757.5057 - val_mean_squared_error: 16556757.5057
Epoch 1999/2000
 - 0s - loss: 15919842.5290 - mean_squared_error: 15919842.5290 - val_loss: 16556768.8245 - val_mean_squared_error: 16556768.8245
Epoch 2000/2000
 - 0s - loss: 15919839.9017 - mean_squared_error: 15919839.9017 - val_loss: 16556769.6495 - val_mean_squared_error: 16556769.6495

   32/13485 [..............................] - ETA: 0s
 4384/13485 [========&gt;.....................] - ETA: 0s
 8640/13485 [==================&gt;...........] - ETA: 0s
12960/13485 [===========================&gt;..] - ETA: 0s
13485/13485 [==============================] - 0s 12us/step

root mean_squared_error: 3963.688024
</pre></div>
</div>
</div></blockquote>
<div class="section" id="what-is-different-here">
<h3>What is different here?<a class="headerlink" href="#what-is-different-here" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We’ve changed the activation in the hidden layer to “sigmoid” per
our discussion.</p></li>
<li><p>Next, notice that we’re running 2000 training epochs!</p></li>
</ul>
<p>Even so, it takes a looooong time to converge. If you experiment a lot,
you’ll find that … it still takes a long time to converge. Around the
early part of the most recent deep learning renaissance, researchers
started experimenting with other non-linearities.</p>
<p>(Remember, we’re talking about non-linear activations in the hidden
layer. The output here is still using “linear” rather than “softmax”
because we’re performing regression, not classification.)</p>
<p>In theory, any non-linearity should allow learning, and maybe we can use
one that “works better”</p>
<p>By “works better” we mean</p>
<ul class="simple">
<li><p>Simpler gradient - faster to compute</p></li>
<li><p>Less prone to “saturation” – where the neuron ends up way off in
the 0 or 1 territory of the sigmoid and can’t easily learn anything</p></li>
<li><p>Keeps gradients “big” – avoiding the large, flat, near-zero
gradient areas of the sigmoid</p></li>
</ul>
<p>Turns out that a big breakthrough and popular solution is a very simple
hack:</p>
</div>
</div>
<div class="section" id="rectified-linear-unit-relu">
<h2>Rectified Linear Unit (ReLU)<a class="headerlink" href="#rectified-linear-unit-relu" title="Permalink to this headline">¶</a></h2>
<p>&lt;img src=”http://i.imgur.com/oAYh9DN.png” width=1000&gt;</p>
</div>
<div class="section" id="go-change-your-hidden-layer-activation-from-sigmoid-to-relu">
<h2>Go change your hidden-layer activation from ‘sigmoid’ to ‘relu’<a class="headerlink" href="#go-change-your-hidden-layer-activation-from-sigmoid-to-relu" title="Permalink to this headline">¶</a></h2>
<p>Start your script and watch the error for a bit!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">input_file</span> <span class="o">=</span> <span class="s2">&quot;/dbfs/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv&quot;</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">input_file</span><span class="p">,</span> <span class="n">header</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;cut_&#39;</span><span class="p">,</span> <span class="s1">&#39;color_&#39;</span><span class="p">,</span> <span class="s1">&#39;clarity_&#39;</span><span class="p">])</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">3</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">y</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span>
<span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">26</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span> <span class="c1"># &lt;--- CHANGE IS HERE</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mean_squared_error&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mean_squared_error&#39;</span><span class="p">])</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">root </span><span class="si">%s</span><span class="s2">: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">metrics_names</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Train on 36409 samples, validate on 4046 samples
Epoch 1/2000
 - 1s - loss: 29938863.4237 - mean_squared_error: 29938863.4237 - val_loss: 27832422.8473 - val_mean_squared_error: 27832422.8473
Epoch 2/2000
 - 0s - loss: 22388635.5017 - mean_squared_error: 22388635.5017 - val_loss: 19167540.5645 - val_mean_squared_error: 19167540.5645
Epoch 3/2000
 - 0s - loss: 16431872.0007 - mean_squared_error: 16431872.0007 - val_loss: 16017428.7776 - val_mean_squared_error: 16017428.7776
Epoch 4/2000
 - 0s - loss: 15169837.3108 - mean_squared_error: 15169837.3108 - val_loss: 15670100.1587 - val_mean_squared_error: 15670100.1587
Epoch 5/2000
 - 0s - loss: 15005770.5474 - mean_squared_error: 15005770.5474 - val_loss: 15541303.1453 - val_mean_squared_error: 15541303.1453
Epoch 6/2000
 - 0s - loss: 14886009.9103 - mean_squared_error: 14886009.9103 - val_loss: 15410182.4088 - val_mean_squared_error: 15410182.4088
Epoch 7/2000
 - 0s - loss: 14750829.7501 - mean_squared_error: 14750829.7501 - val_loss: 15262493.8393 - val_mean_squared_error: 15262493.8393
Epoch 8/2000
 - 0s - loss: 14598241.2764 - mean_squared_error: 14598241.2764 - val_loss: 15092539.8774 - val_mean_squared_error: 15092539.8774
Epoch 9/2000
 - 0s - loss: 14426493.4177 - mean_squared_error: 14426493.4177 - val_loss: 14905501.4780 - val_mean_squared_error: 14905501.4780
Epoch 10/2000
 - 0s - loss: 14228848.9103 - mean_squared_error: 14228848.9103 - val_loss: 14688015.9975 - val_mean_squared_error: 14688015.9975
Epoch 11/2000
 - 1s - loss: 14001212.6608 - mean_squared_error: 14001212.6608 - val_loss: 14434330.9461 - val_mean_squared_error: 14434330.9461
Epoch 12/2000
 - 0s - loss: 13736570.0820 - mean_squared_error: 13736570.0820 - val_loss: 14139074.0257 - val_mean_squared_error: 14139074.0257
Epoch 13/2000
 - 0s - loss: 13428367.8534 - mean_squared_error: 13428367.8534 - val_loss: 13795523.7548 - val_mean_squared_error: 13795523.7548
Epoch 14/2000
 - 0s - loss: 13070743.7831 - mean_squared_error: 13070743.7831 - val_loss: 13397306.3450 - val_mean_squared_error: 13397306.3450
Epoch 15/2000
 - 0s - loss: 12656768.5956 - mean_squared_error: 12656768.5956 - val_loss: 12945219.0559 - val_mean_squared_error: 12945219.0559
Epoch 16/2000
 - 0s - loss: 12185512.7982 - mean_squared_error: 12185512.7982 - val_loss: 12423358.2358 - val_mean_squared_error: 12423358.2358
Epoch 17/2000
 - 0s - loss: 11655525.9851 - mean_squared_error: 11655525.9851 - val_loss: 11847318.5447 - val_mean_squared_error: 11847318.5447
Epoch 18/2000
 - 0s - loss: 11071839.4945 - mean_squared_error: 11071839.4945 - val_loss: 11221099.8136 - val_mean_squared_error: 11221099.8136
Epoch 19/2000
 - 0s - loss: 10444863.5801 - mean_squared_error: 10444863.5801 - val_loss: 10549720.4142 - val_mean_squared_error: 10549720.4142
Epoch 20/2000
 - 0s - loss: 9788272.2497 - mean_squared_error: 9788272.2497 - val_loss: 9859712.9110 - val_mean_squared_error: 9859712.9110
Epoch 21/2000
 - 0s - loss: 9115201.0716 - mean_squared_error: 9115201.0716 - val_loss: 9161731.1725 - val_mean_squared_error: 9161731.1725
Epoch 22/2000
 - 0s - loss: 8446740.0727 - mean_squared_error: 8446740.0727 - val_loss: 8473166.5581 - val_mean_squared_error: 8473166.5581
Epoch 23/2000
 - 0s - loss: 7793792.6583 - mean_squared_error: 7793792.6583 - val_loss: 7801613.9909 - val_mean_squared_error: 7801613.9909
Epoch 24/2000
 - 0s - loss: 7166955.5462 - mean_squared_error: 7166955.5462 - val_loss: 7164038.4654 - val_mean_squared_error: 7164038.4654
Epoch 25/2000
 - 0s - loss: 6582194.8524 - mean_squared_error: 6582194.8524 - val_loss: 6574832.5309 - val_mean_squared_error: 6574832.5309
Epoch 26/2000
 - 0s - loss: 6047351.9016 - mean_squared_error: 6047351.9016 - val_loss: 6035904.0623 - val_mean_squared_error: 6035904.0623
Epoch 27/2000
 - 0s - loss: 5566922.6957 - mean_squared_error: 5566922.6957 - val_loss: 5558418.2491 - val_mean_squared_error: 5558418.2491
Epoch 28/2000
 - 0s - loss: 5143339.4477 - mean_squared_error: 5143339.4477 - val_loss: 5132038.7902 - val_mean_squared_error: 5132038.7902
Epoch 29/2000
 - 0s - loss: 4767790.8131 - mean_squared_error: 4767790.8131 - val_loss: 4757215.1453 - val_mean_squared_error: 4757215.1453
Epoch 30/2000
 - 0s - loss: 4439649.8817 - mean_squared_error: 4439649.8817 - val_loss: 4431028.5476 - val_mean_squared_error: 4431028.5476
Epoch 31/2000
 - 0s - loss: 4152965.2396 - mean_squared_error: 4152965.2396 - val_loss: 4137767.6985 - val_mean_squared_error: 4137767.6985
Epoch 32/2000
 - 0s - loss: 3901266.2654 - mean_squared_error: 3901266.2654 - val_loss: 3882356.7435 - val_mean_squared_error: 3882356.7435
Epoch 33/2000
 - 0s - loss: 3677236.9365 - mean_squared_error: 3677236.9365 - val_loss: 3646491.6930 - val_mean_squared_error: 3646491.6930
Epoch 34/2000
 - 0s - loss: 3477286.9178 - mean_squared_error: 3477286.9178 - val_loss: 3443949.0140 - val_mean_squared_error: 3443949.0140
Epoch 35/2000
 - 0s - loss: 3302099.7013 - mean_squared_error: 3302099.7013 - val_loss: 3258397.3565 - val_mean_squared_error: 3258397.3565
Epoch 36/2000
 - 0s - loss: 3144469.7298 - mean_squared_error: 3144469.7298 - val_loss: 3091707.3880 - val_mean_squared_error: 3091707.3880
Epoch 37/2000
 - 0s - loss: 3001370.1311 - mean_squared_error: 3001370.1311 - val_loss: 2935768.6719 - val_mean_squared_error: 2935768.6719
Epoch 38/2000
 - 0s - loss: 2869674.9387 - mean_squared_error: 2869674.9387 - val_loss: 2804373.1770 - val_mean_squared_error: 2804373.1770
Epoch 39/2000
 - 0s - loss: 2750813.5957 - mean_squared_error: 2750813.5957 - val_loss: 2669017.5266 - val_mean_squared_error: 2669017.5266
Epoch 40/2000
 - 0s - loss: 2639596.0706 - mean_squared_error: 2639596.0706 - val_loss: 2549130.5860 - val_mean_squared_error: 2549130.5860
Epoch 41/2000
 - 0s - loss: 2539679.8545 - mean_squared_error: 2539679.8545 - val_loss: 2439497.7667 - val_mean_squared_error: 2439497.7667
Epoch 42/2000
 - 0s - loss: 2446811.9564 - mean_squared_error: 2446811.9564 - val_loss: 2338626.5081 - val_mean_squared_error: 2338626.5081
Epoch 43/2000
 - 0s - loss: 2362560.7893 - mean_squared_error: 2362560.7893 - val_loss: 2244730.6283 - val_mean_squared_error: 2244730.6283
Epoch 44/2000
 - 0s - loss: 2283878.5586 - mean_squared_error: 2283878.5586 - val_loss: 2158127.9470 - val_mean_squared_error: 2158127.9470
Epoch 45/2000
 - 0s - loss: 2210463.4408 - mean_squared_error: 2210463.4408 - val_loss: 2076556.6046 - val_mean_squared_error: 2076556.6046
Epoch 46/2000
 - 0s - loss: 2141897.1993 - mean_squared_error: 2141897.1993 - val_loss: 2000310.3009 - val_mean_squared_error: 2000310.3009
Epoch 47/2000
 - 0s - loss: 2078454.0809 - mean_squared_error: 2078454.0809 - val_loss: 1931987.7154 - val_mean_squared_error: 1931987.7154
Epoch 48/2000
 - 0s - loss: 2021185.5238 - mean_squared_error: 2021185.5238 - val_loss: 1865060.8356 - val_mean_squared_error: 1865060.8356
Epoch 49/2000
 - 0s - loss: 1966803.1820 - mean_squared_error: 1966803.1820 - val_loss: 1808340.0919 - val_mean_squared_error: 1808340.0919
Epoch 50/2000
 - 0s - loss: 1915811.9821 - mean_squared_error: 1915811.9821 - val_loss: 1747630.3844 - val_mean_squared_error: 1747630.3844
Epoch 51/2000
 - 0s - loss: 1866521.3634 - mean_squared_error: 1866521.3634 - val_loss: 1693315.2384 - val_mean_squared_error: 1693315.2384
Epoch 52/2000
 - 0s - loss: 1821326.0610 - mean_squared_error: 1821326.0610 - val_loss: 1642927.1448 - val_mean_squared_error: 1642927.1448
Epoch 53/2000
 - 0s - loss: 1778010.7790 - mean_squared_error: 1778010.7790 - val_loss: 1593469.0986 - val_mean_squared_error: 1593469.0986
Epoch 54/2000
 - 0s - loss: 1737470.6897 - mean_squared_error: 1737470.6897 - val_loss: 1549439.4479 - val_mean_squared_error: 1549439.4479
Epoch 55/2000
 - 0s - loss: 1700140.8353 - mean_squared_error: 1700140.8353 - val_loss: 1512758.0407 - val_mean_squared_error: 1512758.0407
Epoch 56/2000
 - 0s - loss: 1664932.0321 - mean_squared_error: 1664932.0321 - val_loss: 1468464.1324 - val_mean_squared_error: 1468464.1324
Epoch 57/2000
 - 0s - loss: 1631250.8124 - mean_squared_error: 1631250.8124 - val_loss: 1430306.0396 - val_mean_squared_error: 1430306.0396
Epoch 58/2000
 - 0s - loss: 1599123.9408 - mean_squared_error: 1599123.9408 - val_loss: 1402570.5938 - val_mean_squared_error: 1402570.5938
Epoch 59/2000
 - 0s - loss: 1569169.3397 - mean_squared_error: 1569169.3397 - val_loss: 1361781.7992 - val_mean_squared_error: 1361781.7992
Epoch 60/2000
 - 0s - loss: 1540956.2998 - mean_squared_error: 1540956.2998 - val_loss: 1329689.5006 - val_mean_squared_error: 1329689.5006
Epoch 61/2000
 - 0s - loss: 1513870.4650 - mean_squared_error: 1513870.4650 - val_loss: 1299788.0126 - val_mean_squared_error: 1299788.0126
Epoch 62/2000
 - 0s - loss: 1488514.5449 - mean_squared_error: 1488514.5449 - val_loss: 1271596.4853 - val_mean_squared_error: 1271596.4853
Epoch 63/2000
 - 0s - loss: 1464872.0181 - mean_squared_error: 1464872.0181 - val_loss: 1245587.1696 - val_mean_squared_error: 1245587.1696
Epoch 64/2000
 - 0s - loss: 1441970.8339 - mean_squared_error: 1441970.8339 - val_loss: 1220000.9017 - val_mean_squared_error: 1220000.9017
Epoch 65/2000
 - 0s - loss: 1419747.2820 - mean_squared_error: 1419747.2820 - val_loss: 1200149.5500 - val_mean_squared_error: 1200149.5500
Epoch 66/2000
 - 0s - loss: 1399810.3150 - mean_squared_error: 1399810.3150 - val_loss: 1174179.1523 - val_mean_squared_error: 1174179.1523
Epoch 67/2000
 - 0s - loss: 1379762.8659 - mean_squared_error: 1379762.8659 - val_loss: 1155421.8361 - val_mean_squared_error: 1155421.8361
Epoch 68/2000
 - 0s - loss: 1361613.9023 - mean_squared_error: 1361613.9023 - val_loss: 1131801.8392 - val_mean_squared_error: 1131801.8392
Epoch 69/2000
 - 0s - loss: 1344540.7886 - mean_squared_error: 1344540.7886 - val_loss: 1112408.5979 - val_mean_squared_error: 1112408.5979
Epoch 70/2000
 - 0s - loss: 1327972.0930 - mean_squared_error: 1327972.0930 - val_loss: 1095669.5814 - val_mean_squared_error: 1095669.5814
Epoch 71/2000
 - 0s - loss: 1311907.4601 - mean_squared_error: 1311907.4601 - val_loss: 1080797.0534 - val_mean_squared_error: 1080797.0534
Epoch 72/2000
 - 0s - loss: 1297300.1515 - mean_squared_error: 1297300.1515 - val_loss: 1063274.4691 - val_mean_squared_error: 1063274.4691
Epoch 73/2000
 - 0s - loss: 1282702.7680 - mean_squared_error: 1282702.7680 - val_loss: 1047914.8447 - val_mean_squared_error: 1047914.8447
Epoch 74/2000
 - 0s - loss: 1269224.1888 - mean_squared_error: 1269224.1888 - val_loss: 1035664.7105 - val_mean_squared_error: 1035664.7105
Epoch 75/2000
 - 0s - loss: 1255846.9290 - mean_squared_error: 1255846.9290 - val_loss: 1022400.9963 - val_mean_squared_error: 1022400.9963
Epoch 76/2000
 - 0s - loss: 1243903.4871 - mean_squared_error: 1243903.4871 - val_loss: 1003828.2156 - val_mean_squared_error: 1003828.2156
Epoch 77/2000
 - 0s - loss: 1232750.9959 - mean_squared_error: 1232750.9959 - val_loss: 991562.2552 - val_mean_squared_error: 991562.2552
Epoch 78/2000
 - 0s - loss: 1221784.7819 - mean_squared_error: 1221784.7819 - val_loss: 988014.4801 - val_mean_squared_error: 988014.4801
Epoch 79/2000
 - 0s - loss: 1210924.4235 - mean_squared_error: 1210924.4235 - val_loss: 966999.6418 - val_mean_squared_error: 966999.6418
Epoch 80/2000
 - 0s - loss: 1201281.6524 - mean_squared_error: 1201281.6524 - val_loss: 957178.3798 - val_mean_squared_error: 957178.3798
Epoch 81/2000
 - 0s - loss: 1191342.2884 - mean_squared_error: 1191342.2884 - val_loss: 945778.2848 - val_mean_squared_error: 945778.2848
Epoch 82/2000
 - 0s - loss: 1182401.2386 - mean_squared_error: 1182401.2386 - val_loss: 941735.3018 - val_mean_squared_error: 941735.3018
Epoch 83/2000
 - 0s - loss: 1173785.2414 - mean_squared_error: 1173785.2414 - val_loss: 926331.6598 - val_mean_squared_error: 926331.6598
Epoch 84/2000
 - 0s - loss: 1165100.9749 - mean_squared_error: 1165100.9749 - val_loss: 922599.2534 - val_mean_squared_error: 922599.2534
Epoch 85/2000
 - 0s - loss: 1157599.6469 - mean_squared_error: 1157599.6469 - val_loss: 909060.7537 - val_mean_squared_error: 909060.7537
Epoch 86/2000
 - 0s - loss: 1149606.7023 - mean_squared_error: 1149606.7023 - val_loss: 905816.1376 - val_mean_squared_error: 905816.1376
Epoch 87/2000
 - 0s - loss: 1142876.2513 - mean_squared_error: 1142876.2513 - val_loss: 894544.3443 - val_mean_squared_error: 894544.3443
Epoch 88/2000
 - 0s - loss: 1135421.7432 - mean_squared_error: 1135421.7432 - val_loss: 885002.8672 - val_mean_squared_error: 885002.8672
Epoch 89/2000
 - 0s - loss: 1128510.0997 - mean_squared_error: 1128510.0997 - val_loss: 885705.4161 - val_mean_squared_error: 885705.4161
Epoch 90/2000
 - 0s - loss: 1122707.7963 - mean_squared_error: 1122707.7963 - val_loss: 869758.4723 - val_mean_squared_error: 869758.4723
Epoch 91/2000
 - 0s - loss: 1115116.7629 - mean_squared_error: 1115116.7629 - val_loss: 870980.7851 - val_mean_squared_error: 870980.7851
Epoch 92/2000
 - 0s - loss: 1108894.0348 - mean_squared_error: 1108894.0348 - val_loss: 855680.1749 - val_mean_squared_error: 855680.1749
Epoch 93/2000
 - 0s - loss: 1103963.9562 - mean_squared_error: 1103963.9562 - val_loss: 852032.6139 - val_mean_squared_error: 852032.6139
Epoch 94/2000
 - 0s - loss: 1098281.1880 - mean_squared_error: 1098281.1880 - val_loss: 845218.1591 - val_mean_squared_error: 845218.1591
Epoch 95/2000
 - 0s - loss: 1092883.6761 - mean_squared_error: 1092883.6761 - val_loss: 836951.9199 - val_mean_squared_error: 836951.9199
Epoch 96/2000
 - 0s - loss: 1087001.8999 - mean_squared_error: 1087001.8999 - val_loss: 831424.8668 - val_mean_squared_error: 831424.8668
Epoch 97/2000
 - 0s - loss: 1081821.6931 - mean_squared_error: 1081821.6931 - val_loss: 826467.4629 - val_mean_squared_error: 826467.4629
Epoch 98/2000
 - 0s - loss: 1076217.9142 - mean_squared_error: 1076217.9142 - val_loss: 819017.7581 - val_mean_squared_error: 819017.7581
Epoch 99/2000
 - 0s - loss: 1072431.6157 - mean_squared_error: 1072431.6157 - val_loss: 817342.3566 - val_mean_squared_error: 817342.3566
Epoch 100/2000
 - 0s - loss: 1066940.7825 - mean_squared_error: 1066940.7825 - val_loss: 808668.5299 - val_mean_squared_error: 808668.5299
Epoch 101/2000
 - 0s - loss: 1062883.5691 - mean_squared_error: 1062883.5691 - val_loss: 804621.1654 - val_mean_squared_error: 804621.1654
Epoch 102/2000
 - 0s - loss: 1058172.6794 - mean_squared_error: 1058172.6794 - val_loss: 798353.0681 - val_mean_squared_error: 798353.0681
Epoch 103/2000
 - 0s - loss: 1053867.6362 - mean_squared_error: 1053867.6362 - val_loss: 804793.9548 - val_mean_squared_error: 804793.9548
Epoch 104/2000
 - 0s - loss: 1049076.8368 - mean_squared_error: 1049076.8368 - val_loss: 789712.6155 - val_mean_squared_error: 789712.6155
Epoch 105/2000
 - 0s - loss: 1045631.3630 - mean_squared_error: 1045631.3630 - val_loss: 785457.5816 - val_mean_squared_error: 785457.5816
Epoch 106/2000
 - 0s - loss: 1040585.5933 - mean_squared_error: 1040585.5933 - val_loss: 780374.5012 - val_mean_squared_error: 780374.5012
Epoch 107/2000
 - 0s - loss: 1037087.4549 - mean_squared_error: 1037087.4549 - val_loss: 783961.0112 - val_mean_squared_error: 783961.0112
Epoch 108/2000
 - 0s - loss: 1033408.3128 - mean_squared_error: 1033408.3128 - val_loss: 773750.7693 - val_mean_squared_error: 773750.7693
Epoch 109/2000
 - 0s - loss: 1029350.9100 - mean_squared_error: 1029350.9100 - val_loss: 771727.1558 - val_mean_squared_error: 771727.1558
Epoch 110/2000
 - 0s - loss: 1025883.1703 - mean_squared_error: 1025883.1703 - val_loss: 768502.3288 - val_mean_squared_error: 768502.3288
Epoch 111/2000
 - 0s - loss: 1021833.5502 - mean_squared_error: 1021833.5502 - val_loss: 760970.9669 - val_mean_squared_error: 760970.9669
Epoch 112/2000
 - 0s - loss: 1018940.0217 - mean_squared_error: 1018940.0217 - val_loss: 760039.7225 - val_mean_squared_error: 760039.7225
Epoch 113/2000
 - 0s - loss: 1014862.8041 - mean_squared_error: 1014862.8041 - val_loss: 764375.0141 - val_mean_squared_error: 764375.0141
Epoch 114/2000
 - 0s - loss: 1011251.5824 - mean_squared_error: 1011251.5824 - val_loss: 754512.3164 - val_mean_squared_error: 754512.3164
Epoch 115/2000
 - 0s - loss: 1008575.3374 - mean_squared_error: 1008575.3374 - val_loss: 748945.7377 - val_mean_squared_error: 748945.7377
Epoch 116/2000
 - 0s - loss: 1005124.7456 - mean_squared_error: 1005124.7456 - val_loss: 749031.7273 - val_mean_squared_error: 749031.7273
Epoch 117/2000
 - 0s - loss: 1001222.4575 - mean_squared_error: 1001222.4575 - val_loss: 739509.4394 - val_mean_squared_error: 739509.4394
Epoch 118/2000
 - 0s - loss: 998861.1399 - mean_squared_error: 998861.1399 - val_loss: 745729.3119 - val_mean_squared_error: 745729.3119
Epoch 119/2000
 - 0s - loss: 996182.4343 - mean_squared_error: 996182.4343 - val_loss: 757251.6217 - val_mean_squared_error: 757251.6217
Epoch 120/2000
 - 0s - loss: 992934.4757 - mean_squared_error: 992934.4757 - val_loss: 733761.8970 - val_mean_squared_error: 733761.8970
Epoch 121/2000
 - 0s - loss: 989464.3495 - mean_squared_error: 989464.3495 - val_loss: 728592.8324 - val_mean_squared_error: 728592.8324
Epoch 122/2000
 - 0s - loss: 986923.2727 - mean_squared_error: 986923.2727 - val_loss: 731263.4120 - val_mean_squared_error: 731263.4120
Epoch 123/2000
 - 0s - loss: 984284.1567 - mean_squared_error: 984284.1567 - val_loss: 734552.3397 - val_mean_squared_error: 734552.3397
Epoch 124/2000
 - 0s - loss: 981964.4697 - mean_squared_error: 981964.4697 - val_loss: 721918.1604 - val_mean_squared_error: 721918.1604
Epoch 125/2000
 - 0s - loss: 978881.0860 - mean_squared_error: 978881.0860 - val_loss: 718240.2593 - val_mean_squared_error: 718240.2593
Epoch 126/2000
 - 0s - loss: 976116.4305 - mean_squared_error: 976116.4305 - val_loss: 716768.1848 - val_mean_squared_error: 716768.1848
Epoch 127/2000
 - 0s - loss: 973458.0178 - mean_squared_error: 973458.0178 - val_loss: 715422.3851 - val_mean_squared_error: 715422.3851
Epoch 128/2000
 - 0s - loss: 970484.2357 - mean_squared_error: 970484.2357 - val_loss: 709885.0327 - val_mean_squared_error: 709885.0327
Epoch 129/2000
 - 0s - loss: 967710.4944 - mean_squared_error: 967710.4944 - val_loss: 706953.8875 - val_mean_squared_error: 706953.8875
Epoch 130/2000
 - 0s - loss: 964926.7113 - mean_squared_error: 964926.7113 - val_loss: 704451.7275 - val_mean_squared_error: 704451.7275
Epoch 131/2000
 - 0s - loss: 962205.7342 - mean_squared_error: 962205.7342 - val_loss: 703047.8709 - val_mean_squared_error: 703047.8709
Epoch 132/2000
 - 0s - loss: 959855.5042 - mean_squared_error: 959855.5042 - val_loss: 699065.4193 - val_mean_squared_error: 699065.4193
Epoch 133/2000
 - 0s - loss: 957806.8261 - mean_squared_error: 957806.8261 - val_loss: 709171.6166 - val_mean_squared_error: 709171.6166
Epoch 134/2000
 - 0s - loss: 954564.9092 - mean_squared_error: 954564.9092 - val_loss: 696548.4695 - val_mean_squared_error: 696548.4695
Epoch 135/2000
 - 0s - loss: 952602.2452 - mean_squared_error: 952602.2452 - val_loss: 697575.4079 - val_mean_squared_error: 697575.4079
Epoch 136/2000
 - 0s - loss: 950352.9306 - mean_squared_error: 950352.9306 - val_loss: 689600.8201 - val_mean_squared_error: 689600.8201
Epoch 137/2000
 - 0s - loss: 947697.6631 - mean_squared_error: 947697.6631 - val_loss: 689256.6092 - val_mean_squared_error: 689256.6092
Epoch 138/2000
 - 0s - loss: 944707.3380 - mean_squared_error: 944707.3380 - val_loss: 697876.2123 - val_mean_squared_error: 697876.2123
Epoch 139/2000
 - 0s - loss: 942931.2569 - mean_squared_error: 942931.2569 - val_loss: 688838.1186 - val_mean_squared_error: 688838.1186
Epoch 140/2000
 - 0s - loss: 940727.0246 - mean_squared_error: 940727.0246 - val_loss: 683651.2944 - val_mean_squared_error: 683651.2944
Epoch 141/2000
 - 0s - loss: 937611.2467 - mean_squared_error: 937611.2467 - val_loss: 679610.9674 - val_mean_squared_error: 679610.9674
Epoch 142/2000
 - 0s - loss: 936145.2758 - mean_squared_error: 936145.2758 - val_loss: 679409.5040 - val_mean_squared_error: 679409.5040
Epoch 143/2000
 - 0s - loss: 933791.8269 - mean_squared_error: 933791.8269 - val_loss: 674969.9004 - val_mean_squared_error: 674969.9004
Epoch 144/2000
 - 0s - loss: 931934.5522 - mean_squared_error: 931934.5522 - val_loss: 673677.1087 - val_mean_squared_error: 673677.1087
Epoch 145/2000
 - 0s - loss: 929419.1329 - mean_squared_error: 929419.1329 - val_loss: 672486.2596 - val_mean_squared_error: 672486.2596
Epoch 146/2000
 - 0s - loss: 928259.8743 - mean_squared_error: 928259.8743 - val_loss: 679053.0771 - val_mean_squared_error: 679053.0771
Epoch 147/2000
 - 0s - loss: 925840.3659 - mean_squared_error: 925840.3659 - val_loss: 670425.6315 - val_mean_squared_error: 670425.6315
Epoch 148/2000
 - 0s - loss: 923053.3375 - mean_squared_error: 923053.3375 - val_loss: 666447.0378 - val_mean_squared_error: 666447.0378
Epoch 149/2000
 - 0s - loss: 921570.8007 - mean_squared_error: 921570.8007 - val_loss: 671207.0466 - val_mean_squared_error: 671207.0466
Epoch 150/2000
 - 0s - loss: 919778.3405 - mean_squared_error: 919778.3405 - val_loss: 663078.1736 - val_mean_squared_error: 663078.1736
Epoch 151/2000
 - 0s - loss: 916954.4063 - mean_squared_error: 916954.4063 - val_loss: 661748.4101 - val_mean_squared_error: 661748.4101
Epoch 152/2000
 - 0s - loss: 915275.2257 - mean_squared_error: 915275.2257 - val_loss: 660109.1333 - val_mean_squared_error: 660109.1333
Epoch 153/2000
 - 0s - loss: 913935.4979 - mean_squared_error: 913935.4979 - val_loss: 660320.5198 - val_mean_squared_error: 660320.5198
Epoch 154/2000
 - 0s - loss: 912403.0531 - mean_squared_error: 912403.0531 - val_loss: 658724.5849 - val_mean_squared_error: 658724.5849
Epoch 155/2000
 - 0s - loss: 909915.3601 - mean_squared_error: 909915.3601 - val_loss: 655034.8768 - val_mean_squared_error: 655034.8768
Epoch 156/2000
 - 0s - loss: 908297.1211 - mean_squared_error: 908297.1211 - val_loss: 654181.0143 - val_mean_squared_error: 654181.0143
Epoch 157/2000
 - 0s - loss: 905624.7004 - mean_squared_error: 905624.7004 - val_loss: 656986.1485 - val_mean_squared_error: 656986.1485
Epoch 158/2000
 - 0s - loss: 905111.6589 - mean_squared_error: 905111.6589 - val_loss: 655903.3115 - val_mean_squared_error: 655903.3115
Epoch 159/2000
 - 0s - loss: 902944.8210 - mean_squared_error: 902944.8210 - val_loss: 654524.7966 - val_mean_squared_error: 654524.7966
Epoch 160/2000
 - 0s - loss: 901144.1101 - mean_squared_error: 901144.1101 - val_loss: 651841.0205 - val_mean_squared_error: 651841.0205
Epoch 161/2000
 - 0s - loss: 899314.5646 - mean_squared_error: 899314.5646 - val_loss: 646358.7536 - val_mean_squared_error: 646358.7536
Epoch 162/2000
 - 0s - loss: 897382.2375 - mean_squared_error: 897382.2375 - val_loss: 646032.7151 - val_mean_squared_error: 646032.7151
Epoch 163/2000
 - 0s - loss: 895482.0745 - mean_squared_error: 895482.0745 - val_loss: 647151.8484 - val_mean_squared_error: 647151.8484
Epoch 164/2000
 - 0s - loss: 894577.8852 - mean_squared_error: 894577.8852 - val_loss: 643899.3744 - val_mean_squared_error: 643899.3744
Epoch 165/2000
 - 0s - loss: 892187.8241 - mean_squared_error: 892187.8241 - val_loss: 641453.4793 - val_mean_squared_error: 641453.4793
Epoch 166/2000
 - 0s - loss: 890396.7710 - mean_squared_error: 890396.7710 - val_loss: 639900.2524 - val_mean_squared_error: 639900.2524
Epoch 167/2000
 - 0s - loss: 889097.9396 - mean_squared_error: 889097.9396 - val_loss: 640526.9566 - val_mean_squared_error: 640526.9566
Epoch 168/2000
 - 0s - loss: 887303.5403 - mean_squared_error: 887303.5403 - val_loss: 637232.9263 - val_mean_squared_error: 637232.9263
Epoch 169/2000
 - 0s - loss: 885714.2098 - mean_squared_error: 885714.2098 - val_loss: 638746.7358 - val_mean_squared_error: 638746.7358
Epoch 170/2000
 - 0s - loss: 884497.2334 - mean_squared_error: 884497.2334 - val_loss: 634734.1812 - val_mean_squared_error: 634734.1812
Epoch 171/2000
 - 0s - loss: 882821.3801 - mean_squared_error: 882821.3801 - val_loss: 634757.9038 - val_mean_squared_error: 634757.9038
Epoch 172/2000
 - 0s - loss: 880668.8433 - mean_squared_error: 880668.8433 - val_loss: 631159.9473 - val_mean_squared_error: 631159.9473
Epoch 173/2000
 - 0s - loss: 879786.0717 - mean_squared_error: 879786.0717 - val_loss: 632434.0280 - val_mean_squared_error: 632434.0280
Epoch 174/2000
 - 0s - loss: 877921.8627 - mean_squared_error: 877921.8627 - val_loss: 629854.0848 - val_mean_squared_error: 629854.0848
Epoch 175/2000
 - 0s - loss: 876936.1627 - mean_squared_error: 876936.1627 - val_loss: 639762.5728 - val_mean_squared_error: 639762.5728
Epoch 176/2000
 - 0s - loss: 874990.3023 - mean_squared_error: 874990.3023 - val_loss: 627614.5739 - val_mean_squared_error: 627614.5739
Epoch 177/2000
 - 0s - loss: 873407.4634 - mean_squared_error: 873407.4634 - val_loss: 625926.6324 - val_mean_squared_error: 625926.6324
Epoch 178/2000
 - 0s - loss: 871298.3328 - mean_squared_error: 871298.3328 - val_loss: 654647.0849 - val_mean_squared_error: 654647.0849
Epoch 179/2000
 - 0s - loss: 871422.8988 - mean_squared_error: 871422.8988 - val_loss: 624784.2562 - val_mean_squared_error: 624784.2562
Epoch 180/2000
 - 0s - loss: 868830.5286 - mean_squared_error: 868830.5286 - val_loss: 623332.0319 - val_mean_squared_error: 623332.0319

*** WARNING: skipped 225655 bytes of output ***

Epoch 1823/2000
 - 0s - loss: 397390.2962 - mean_squared_error: 397390.2962 - val_loss: 353209.6269 - val_mean_squared_error: 353209.6269
Epoch 1824/2000
 - 0s - loss: 397583.4764 - mean_squared_error: 397583.4764 - val_loss: 353313.4139 - val_mean_squared_error: 353313.4139
Epoch 1825/2000
 - 0s - loss: 396889.4295 - mean_squared_error: 396889.4295 - val_loss: 353050.2891 - val_mean_squared_error: 353050.2891
Epoch 1826/2000
 - 0s - loss: 396688.1741 - mean_squared_error: 396688.1741 - val_loss: 352968.9833 - val_mean_squared_error: 352968.9833
Epoch 1827/2000
 - 0s - loss: 396879.9892 - mean_squared_error: 396879.9892 - val_loss: 353596.4543 - val_mean_squared_error: 353596.4543
Epoch 1828/2000
 - 0s - loss: 397163.0979 - mean_squared_error: 397163.0979 - val_loss: 355002.6447 - val_mean_squared_error: 355002.6447
Epoch 1829/2000
 - 0s - loss: 397371.9557 - mean_squared_error: 397371.9557 - val_loss: 360991.2768 - val_mean_squared_error: 360991.2768
Epoch 1830/2000
 - 0s - loss: 397330.9307 - mean_squared_error: 397330.9307 - val_loss: 352858.5714 - val_mean_squared_error: 352858.5714
Epoch 1831/2000
 - 0s - loss: 397069.0619 - mean_squared_error: 397069.0619 - val_loss: 356316.7071 - val_mean_squared_error: 356316.7071
Epoch 1832/2000
 - 0s - loss: 397248.6073 - mean_squared_error: 397248.6073 - val_loss: 352828.9623 - val_mean_squared_error: 352828.9623
Epoch 1833/2000
 - 0s - loss: 396082.9898 - mean_squared_error: 396082.9898 - val_loss: 352698.9154 - val_mean_squared_error: 352698.9154
Epoch 1834/2000
 - 0s - loss: 397319.9301 - mean_squared_error: 397319.9301 - val_loss: 352547.3161 - val_mean_squared_error: 352547.3161
Epoch 1835/2000
 - 0s - loss: 396171.3512 - mean_squared_error: 396171.3512 - val_loss: 352890.7365 - val_mean_squared_error: 352890.7365
Epoch 1836/2000
 - 0s - loss: 397558.3849 - mean_squared_error: 397558.3849 - val_loss: 352797.1587 - val_mean_squared_error: 352797.1587
Epoch 1837/2000
 - 0s - loss: 396650.8045 - mean_squared_error: 396650.8045 - val_loss: 352551.9260 - val_mean_squared_error: 352551.9260
Epoch 1838/2000
 - 0s - loss: 396322.6335 - mean_squared_error: 396322.6335 - val_loss: 352522.2167 - val_mean_squared_error: 352522.2167
Epoch 1839/2000
 - 0s - loss: 396564.7714 - mean_squared_error: 396564.7714 - val_loss: 356043.7555 - val_mean_squared_error: 356043.7555
Epoch 1840/2000
 - 0s - loss: 396659.8503 - mean_squared_error: 396659.8503 - val_loss: 353330.3860 - val_mean_squared_error: 353330.3860
Epoch 1841/2000
 - 0s - loss: 396947.4321 - mean_squared_error: 396947.4321 - val_loss: 352941.9958 - val_mean_squared_error: 352941.9958
Epoch 1842/2000
 - 0s - loss: 396686.2913 - mean_squared_error: 396686.2913 - val_loss: 353136.8284 - val_mean_squared_error: 353136.8284
Epoch 1843/2000
 - 0s - loss: 397013.0222 - mean_squared_error: 397013.0222 - val_loss: 357842.3895 - val_mean_squared_error: 357842.3895
Epoch 1844/2000
 - 0s - loss: 396760.1421 - mean_squared_error: 396760.1421 - val_loss: 353699.3172 - val_mean_squared_error: 353699.3172
Epoch 1845/2000
 - 0s - loss: 396413.3329 - mean_squared_error: 396413.3329 - val_loss: 353588.4062 - val_mean_squared_error: 353588.4062
Epoch 1846/2000
 - 0s - loss: 396626.7565 - mean_squared_error: 396626.7565 - val_loss: 352466.3480 - val_mean_squared_error: 352466.3480
Epoch 1847/2000
 - 0s - loss: 396830.8680 - mean_squared_error: 396830.8680 - val_loss: 353063.0210 - val_mean_squared_error: 353063.0210
Epoch 1848/2000
 - 0s - loss: 396382.9396 - mean_squared_error: 396382.9396 - val_loss: 352376.7488 - val_mean_squared_error: 352376.7488
Epoch 1849/2000
 - 0s - loss: 396371.6686 - mean_squared_error: 396371.6686 - val_loss: 353424.5889 - val_mean_squared_error: 353424.5889
Epoch 1850/2000
 - 0s - loss: 395971.4266 - mean_squared_error: 395971.4266 - val_loss: 353351.6123 - val_mean_squared_error: 353351.6123
Epoch 1851/2000
 - 0s - loss: 396122.5219 - mean_squared_error: 396122.5219 - val_loss: 352835.1150 - val_mean_squared_error: 352835.1150
Epoch 1852/2000
 - 0s - loss: 395927.3674 - mean_squared_error: 395927.3674 - val_loss: 358838.3104 - val_mean_squared_error: 358838.3104
Epoch 1853/2000
 - 0s - loss: 396239.7226 - mean_squared_error: 396239.7226 - val_loss: 353089.9473 - val_mean_squared_error: 353089.9473
Epoch 1854/2000
 - 0s - loss: 396131.3741 - mean_squared_error: 396131.3741 - val_loss: 352670.2379 - val_mean_squared_error: 352670.2379
Epoch 1855/2000
 - 0s - loss: 396213.9961 - mean_squared_error: 396213.9961 - val_loss: 355889.2487 - val_mean_squared_error: 355889.2487
Epoch 1856/2000
 - 0s - loss: 395907.4523 - mean_squared_error: 395907.4523 - val_loss: 353334.4587 - val_mean_squared_error: 353334.4587
Epoch 1857/2000
 - 0s - loss: 396093.9572 - mean_squared_error: 396093.9572 - val_loss: 355890.8466 - val_mean_squared_error: 355890.8466
Epoch 1858/2000
 - 0s - loss: 396148.8290 - mean_squared_error: 396148.8290 - val_loss: 352772.3574 - val_mean_squared_error: 352772.3574
Epoch 1859/2000
 - 0s - loss: 396374.1615 - mean_squared_error: 396374.1615 - val_loss: 352408.7239 - val_mean_squared_error: 352408.7239
Epoch 1860/2000
 - 0s - loss: 396224.3341 - mean_squared_error: 396224.3341 - val_loss: 352462.1340 - val_mean_squared_error: 352462.1340
Epoch 1861/2000
 - 0s - loss: 395775.2315 - mean_squared_error: 395775.2315 - val_loss: 354116.2042 - val_mean_squared_error: 354116.2042
Epoch 1862/2000
 - 0s - loss: 396024.3581 - mean_squared_error: 396024.3581 - val_loss: 352205.7111 - val_mean_squared_error: 352205.7111
Epoch 1863/2000
 - 0s - loss: 395613.7328 - mean_squared_error: 395613.7328 - val_loss: 352385.5322 - val_mean_squared_error: 352385.5322
Epoch 1864/2000
 - 0s - loss: 396227.5467 - mean_squared_error: 396227.5467 - val_loss: 352645.4930 - val_mean_squared_error: 352645.4930
Epoch 1865/2000
 - 0s - loss: 396004.5046 - mean_squared_error: 396004.5046 - val_loss: 353968.2366 - val_mean_squared_error: 353968.2366
Epoch 1866/2000
 - 0s - loss: 396020.5718 - mean_squared_error: 396020.5718 - val_loss: 354020.5083 - val_mean_squared_error: 354020.5083
Epoch 1867/2000
 - 0s - loss: 395672.2817 - mean_squared_error: 395672.2817 - val_loss: 353314.8828 - val_mean_squared_error: 353314.8828
Epoch 1868/2000
 - 0s - loss: 395795.3118 - mean_squared_error: 395795.3118 - val_loss: 352232.4819 - val_mean_squared_error: 352232.4819
Epoch 1869/2000
 - 0s - loss: 395839.6442 - mean_squared_error: 395839.6442 - val_loss: 352895.0211 - val_mean_squared_error: 352895.0211
Epoch 1870/2000
 - 0s - loss: 396112.3882 - mean_squared_error: 396112.3882 - val_loss: 353443.6297 - val_mean_squared_error: 353443.6297
Epoch 1871/2000
 - 0s - loss: 395928.2209 - mean_squared_error: 395928.2209 - val_loss: 352084.1819 - val_mean_squared_error: 352084.1819
Epoch 1872/2000
 - 0s - loss: 395532.7622 - mean_squared_error: 395532.7622 - val_loss: 351922.8059 - val_mean_squared_error: 351922.8059
Epoch 1873/2000
 - 0s - loss: 394991.3888 - mean_squared_error: 394991.3888 - val_loss: 352354.4650 - val_mean_squared_error: 352354.4650
Epoch 1874/2000
 - 0s - loss: 395868.8979 - mean_squared_error: 395868.8979 - val_loss: 353307.8581 - val_mean_squared_error: 353307.8581
Epoch 1875/2000
 - 0s - loss: 395731.7098 - mean_squared_error: 395731.7098 - val_loss: 351873.0492 - val_mean_squared_error: 351873.0492
Epoch 1876/2000
 - 0s - loss: 395132.8965 - mean_squared_error: 395132.8965 - val_loss: 351852.0628 - val_mean_squared_error: 351852.0628
Epoch 1877/2000
 - 0s - loss: 395518.6568 - mean_squared_error: 395518.6568 - val_loss: 351841.3919 - val_mean_squared_error: 351841.3919
Epoch 1878/2000
 - 0s - loss: 395575.1640 - mean_squared_error: 395575.1640 - val_loss: 351899.8707 - val_mean_squared_error: 351899.8707
Epoch 1879/2000
 - 0s - loss: 395166.7193 - mean_squared_error: 395166.7193 - val_loss: 352113.9540 - val_mean_squared_error: 352113.9540
Epoch 1880/2000
 - 0s - loss: 394905.8654 - mean_squared_error: 394905.8654 - val_loss: 354147.1539 - val_mean_squared_error: 354147.1539
Epoch 1881/2000
 - 0s - loss: 395029.2658 - mean_squared_error: 395029.2658 - val_loss: 354190.9711 - val_mean_squared_error: 354190.9711
Epoch 1882/2000
 - 0s - loss: 395449.4634 - mean_squared_error: 395449.4634 - val_loss: 352587.5978 - val_mean_squared_error: 352587.5978
Epoch 1883/2000
 - 0s - loss: 395276.2936 - mean_squared_error: 395276.2936 - val_loss: 352548.3871 - val_mean_squared_error: 352548.3871
Epoch 1884/2000
 - 0s - loss: 394877.4458 - mean_squared_error: 394877.4458 - val_loss: 352523.8919 - val_mean_squared_error: 352523.8919
Epoch 1885/2000
 - 0s - loss: 395268.0316 - mean_squared_error: 395268.0316 - val_loss: 353429.1411 - val_mean_squared_error: 353429.1411
Epoch 1886/2000
 - 0s - loss: 395224.4547 - mean_squared_error: 395224.4547 - val_loss: 351589.5054 - val_mean_squared_error: 351589.5054
Epoch 1887/2000
 - 0s - loss: 394930.0051 - mean_squared_error: 394930.0051 - val_loss: 351562.0556 - val_mean_squared_error: 351562.0556
Epoch 1888/2000
 - 0s - loss: 395516.0361 - mean_squared_error: 395516.0361 - val_loss: 355380.2538 - val_mean_squared_error: 355380.2538
Epoch 1889/2000
 - 0s - loss: 395076.7389 - mean_squared_error: 395076.7389 - val_loss: 351812.1470 - val_mean_squared_error: 351812.1470
Epoch 1890/2000
 - 0s - loss: 394752.7494 - mean_squared_error: 394752.7494 - val_loss: 352221.3691 - val_mean_squared_error: 352221.3691
Epoch 1891/2000
 - 0s - loss: 395093.2258 - mean_squared_error: 395093.2258 - val_loss: 351600.1716 - val_mean_squared_error: 351600.1716
Epoch 1892/2000
 - 0s - loss: 395046.2364 - mean_squared_error: 395046.2364 - val_loss: 352985.1909 - val_mean_squared_error: 352985.1909
Epoch 1893/2000
 - 0s - loss: 394981.4052 - mean_squared_error: 394981.4052 - val_loss: 355643.5620 - val_mean_squared_error: 355643.5620
Epoch 1894/2000
 - 0s - loss: 394896.0592 - mean_squared_error: 394896.0592 - val_loss: 352217.3145 - val_mean_squared_error: 352217.3145
Epoch 1895/2000
 - 0s - loss: 394552.8456 - mean_squared_error: 394552.8456 - val_loss: 354756.0459 - val_mean_squared_error: 354756.0459
Epoch 1896/2000
 - 0s - loss: 395051.9754 - mean_squared_error: 395051.9754 - val_loss: 353615.7922 - val_mean_squared_error: 353615.7922
Epoch 1897/2000
 - 0s - loss: 394813.7603 - mean_squared_error: 394813.7603 - val_loss: 351627.1015 - val_mean_squared_error: 351627.1015
Epoch 1898/2000
 - 0s - loss: 394762.4370 - mean_squared_error: 394762.4370 - val_loss: 351538.6374 - val_mean_squared_error: 351538.6374
Epoch 1899/2000
 - 0s - loss: 394675.4384 - mean_squared_error: 394675.4384 - val_loss: 354470.4357 - val_mean_squared_error: 354470.4357
Epoch 1900/2000
 - 0s - loss: 395040.6355 - mean_squared_error: 395040.6355 - val_loss: 351296.9343 - val_mean_squared_error: 351296.9343
Epoch 1901/2000
 - 0s - loss: 394279.4941 - mean_squared_error: 394279.4941 - val_loss: 351612.3258 - val_mean_squared_error: 351612.3258
Epoch 1902/2000
 - 0s - loss: 394407.4887 - mean_squared_error: 394407.4887 - val_loss: 353701.2704 - val_mean_squared_error: 353701.2704
Epoch 1903/2000
 - 0s - loss: 394554.4996 - mean_squared_error: 394554.4996 - val_loss: 357333.1414 - val_mean_squared_error: 357333.1414
Epoch 1904/2000
 - 0s - loss: 394564.6796 - mean_squared_error: 394564.6796 - val_loss: 352524.9584 - val_mean_squared_error: 352524.9584
Epoch 1905/2000
 - 0s - loss: 394802.8757 - mean_squared_error: 394802.8757 - val_loss: 351848.0285 - val_mean_squared_error: 351848.0285
Epoch 1906/2000
 - 0s - loss: 394504.0665 - mean_squared_error: 394504.0665 - val_loss: 352623.8804 - val_mean_squared_error: 352623.8804
Epoch 1907/2000
 - 0s - loss: 394084.1085 - mean_squared_error: 394084.1085 - val_loss: 352172.3271 - val_mean_squared_error: 352172.3271
Epoch 1908/2000
 - 0s - loss: 394395.0822 - mean_squared_error: 394395.0822 - val_loss: 353779.5877 - val_mean_squared_error: 353779.5877
Epoch 1909/2000
 - 0s - loss: 394750.7526 - mean_squared_error: 394750.7526 - val_loss: 351798.7503 - val_mean_squared_error: 351798.7503
Epoch 1910/2000
 - 0s - loss: 394402.4697 - mean_squared_error: 394402.4697 - val_loss: 353139.7217 - val_mean_squared_error: 353139.7217
Epoch 1911/2000
 - 0s - loss: 395000.8836 - mean_squared_error: 395000.8836 - val_loss: 351335.7235 - val_mean_squared_error: 351335.7235
Epoch 1912/2000
 - 0s - loss: 394422.7909 - mean_squared_error: 394422.7909 - val_loss: 351607.3362 - val_mean_squared_error: 351607.3362
Epoch 1913/2000
 - 0s - loss: 394186.5859 - mean_squared_error: 394186.5859 - val_loss: 351026.2007 - val_mean_squared_error: 351026.2007
Epoch 1914/2000
 - 0s - loss: 394105.2153 - mean_squared_error: 394105.2153 - val_loss: 351883.1954 - val_mean_squared_error: 351883.1954
Epoch 1915/2000
 - 0s - loss: 394142.5565 - mean_squared_error: 394142.5565 - val_loss: 351992.0573 - val_mean_squared_error: 351992.0573
Epoch 1916/2000
 - 0s - loss: 394209.5957 - mean_squared_error: 394209.5957 - val_loss: 352018.4708 - val_mean_squared_error: 352018.4708
Epoch 1917/2000
 - 0s - loss: 393973.0199 - mean_squared_error: 393973.0199 - val_loss: 352887.3921 - val_mean_squared_error: 352887.3921
Epoch 1918/2000
 - 0s - loss: 394573.7631 - mean_squared_error: 394573.7631 - val_loss: 351478.7113 - val_mean_squared_error: 351478.7113
Epoch 1919/2000
 - 0s - loss: 394233.9014 - mean_squared_error: 394233.9014 - val_loss: 351782.2848 - val_mean_squared_error: 351782.2848
Epoch 1920/2000
 - 0s - loss: 393708.4802 - mean_squared_error: 393708.4802 - val_loss: 350981.9167 - val_mean_squared_error: 350981.9167
Epoch 1921/2000
 - 0s - loss: 394551.6854 - mean_squared_error: 394551.6854 - val_loss: 351259.1100 - val_mean_squared_error: 351259.1100
Epoch 1922/2000
 - 0s - loss: 394300.5748 - mean_squared_error: 394300.5748 - val_loss: 352688.1585 - val_mean_squared_error: 352688.1585
Epoch 1923/2000
 - 0s - loss: 394623.9563 - mean_squared_error: 394623.9563 - val_loss: 354501.3561 - val_mean_squared_error: 354501.3561
Epoch 1924/2000
 - 0s - loss: 394134.7472 - mean_squared_error: 394134.7472 - val_loss: 351192.2701 - val_mean_squared_error: 351192.2701
Epoch 1925/2000
 - 0s - loss: 394189.8784 - mean_squared_error: 394189.8784 - val_loss: 351355.5768 - val_mean_squared_error: 351355.5768
Epoch 1926/2000
 - 0s - loss: 393910.6115 - mean_squared_error: 393910.6115 - val_loss: 350905.4286 - val_mean_squared_error: 350905.4286
Epoch 1927/2000
 - 0s - loss: 394152.6381 - mean_squared_error: 394152.6381 - val_loss: 351523.0727 - val_mean_squared_error: 351523.0727
Epoch 1928/2000
 - 0s - loss: 394294.2439 - mean_squared_error: 394294.2439 - val_loss: 355596.1872 - val_mean_squared_error: 355596.1872
Epoch 1929/2000
 - 0s - loss: 394115.4967 - mean_squared_error: 394115.4967 - val_loss: 355812.5067 - val_mean_squared_error: 355812.5067
Epoch 1930/2000
 - 0s - loss: 393797.4838 - mean_squared_error: 393797.4838 - val_loss: 351328.4472 - val_mean_squared_error: 351328.4472
Epoch 1931/2000
 - 0s - loss: 394132.0738 - mean_squared_error: 394132.0738 - val_loss: 351409.6900 - val_mean_squared_error: 351409.6900
Epoch 1932/2000
 - 0s - loss: 393748.0739 - mean_squared_error: 393748.0739 - val_loss: 351585.4802 - val_mean_squared_error: 351585.4802
Epoch 1933/2000
 - 0s - loss: 393903.1115 - mean_squared_error: 393903.1115 - val_loss: 353536.1962 - val_mean_squared_error: 353536.1962
Epoch 1934/2000
 - 0s - loss: 393744.9373 - mean_squared_error: 393744.9373 - val_loss: 351350.9460 - val_mean_squared_error: 351350.9460
Epoch 1935/2000
 - 0s - loss: 393876.7852 - mean_squared_error: 393876.7852 - val_loss: 351608.3271 - val_mean_squared_error: 351608.3271
Epoch 1936/2000
 - 0s - loss: 394187.8004 - mean_squared_error: 394187.8004 - val_loss: 351772.0344 - val_mean_squared_error: 351772.0344
Epoch 1937/2000
 - 0s - loss: 393540.9881 - mean_squared_error: 393540.9881 - val_loss: 353472.4013 - val_mean_squared_error: 353472.4013
Epoch 1938/2000
 - 0s - loss: 393782.9584 - mean_squared_error: 393782.9584 - val_loss: 355712.6185 - val_mean_squared_error: 355712.6185
Epoch 1939/2000
 - 0s - loss: 393449.4543 - mean_squared_error: 393449.4543 - val_loss: 352500.5792 - val_mean_squared_error: 352500.5792
Epoch 1940/2000
 - 0s - loss: 393696.1645 - mean_squared_error: 393696.1645 - val_loss: 350963.5166 - val_mean_squared_error: 350963.5166
Epoch 1941/2000
 - 0s - loss: 393587.2240 - mean_squared_error: 393587.2240 - val_loss: 351407.7796 - val_mean_squared_error: 351407.7796
Epoch 1942/2000
 - 0s - loss: 394481.9304 - mean_squared_error: 394481.9304 - val_loss: 351683.8144 - val_mean_squared_error: 351683.8144
Epoch 1943/2000
 - 0s - loss: 393125.3010 - mean_squared_error: 393125.3010 - val_loss: 351086.1665 - val_mean_squared_error: 351086.1665
Epoch 1944/2000
 - 0s - loss: 393339.4577 - mean_squared_error: 393339.4577 - val_loss: 351448.7661 - val_mean_squared_error: 351448.7661
Epoch 1945/2000
 - 0s - loss: 393831.5202 - mean_squared_error: 393831.5202 - val_loss: 350619.9688 - val_mean_squared_error: 350619.9688
Epoch 1946/2000
 - 0s - loss: 393500.0341 - mean_squared_error: 393500.0341 - val_loss: 350897.6872 - val_mean_squared_error: 350897.6872
Epoch 1947/2000
 - 0s - loss: 393520.8761 - mean_squared_error: 393520.8761 - val_loss: 350720.5475 - val_mean_squared_error: 350720.5475
Epoch 1948/2000
 - 0s - loss: 393573.5040 - mean_squared_error: 393573.5040 - val_loss: 350986.0485 - val_mean_squared_error: 350986.0485
Epoch 1949/2000
 - 0s - loss: 393692.4018 - mean_squared_error: 393692.4018 - val_loss: 351082.4832 - val_mean_squared_error: 351082.4832
Epoch 1950/2000
 - 0s - loss: 393331.1218 - mean_squared_error: 393331.1218 - val_loss: 351067.0518 - val_mean_squared_error: 351067.0518
Epoch 1951/2000
 - 0s - loss: 394190.3323 - mean_squared_error: 394190.3323 - val_loss: 351539.0065 - val_mean_squared_error: 351539.0065
Epoch 1952/2000
 - 0s - loss: 393509.6535 - mean_squared_error: 393509.6535 - val_loss: 350568.9273 - val_mean_squared_error: 350568.9273
Epoch 1953/2000
 - 0s - loss: 393606.4257 - mean_squared_error: 393606.4257 - val_loss: 350894.8561 - val_mean_squared_error: 350894.8561
Epoch 1954/2000
 - 0s - loss: 392888.9072 - mean_squared_error: 392888.9072 - val_loss: 354381.4223 - val_mean_squared_error: 354381.4223
Epoch 1955/2000
 - 0s - loss: 393230.7114 - mean_squared_error: 393230.7114 - val_loss: 351739.0441 - val_mean_squared_error: 351739.0441
Epoch 1956/2000
 - 0s - loss: 393856.4126 - mean_squared_error: 393856.4126 - val_loss: 350882.9445 - val_mean_squared_error: 350882.9445
Epoch 1957/2000
 - 0s - loss: 393106.8604 - mean_squared_error: 393106.8604 - val_loss: 350710.3429 - val_mean_squared_error: 350710.3429
Epoch 1958/2000
 - 0s - loss: 393181.0624 - mean_squared_error: 393181.0624 - val_loss: 352552.5458 - val_mean_squared_error: 352552.5458
Epoch 1959/2000
 - 0s - loss: 393266.2823 - mean_squared_error: 393266.2823 - val_loss: 351487.5482 - val_mean_squared_error: 351487.5482
Epoch 1960/2000
 - 0s - loss: 393295.5574 - mean_squared_error: 393295.5574 - val_loss: 350987.4455 - val_mean_squared_error: 350987.4455
Epoch 1961/2000
 - 0s - loss: 393692.0431 - mean_squared_error: 393692.0431 - val_loss: 350439.3957 - val_mean_squared_error: 350439.3957
Epoch 1962/2000
 - 0s - loss: 393166.8479 - mean_squared_error: 393166.8479 - val_loss: 350663.6741 - val_mean_squared_error: 350663.6741
Epoch 1963/2000
 - 0s - loss: 393099.1775 - mean_squared_error: 393099.1775 - val_loss: 350883.2008 - val_mean_squared_error: 350883.2008
Epoch 1964/2000
 - 0s - loss: 393037.7546 - mean_squared_error: 393037.7546 - val_loss: 353033.8329 - val_mean_squared_error: 353033.8329
Epoch 1965/2000
 - 0s - loss: 393300.0515 - mean_squared_error: 393300.0515 - val_loss: 350703.8575 - val_mean_squared_error: 350703.8575
Epoch 1966/2000
 - 0s - loss: 392879.9891 - mean_squared_error: 392879.9891 - val_loss: 350489.7307 - val_mean_squared_error: 350489.7307
Epoch 1967/2000
 - 0s - loss: 393060.9444 - mean_squared_error: 393060.9444 - val_loss: 350217.0181 - val_mean_squared_error: 350217.0181
Epoch 1968/2000
 - 0s - loss: 392965.4809 - mean_squared_error: 392965.4809 - val_loss: 351338.5765 - val_mean_squared_error: 351338.5765
Epoch 1969/2000
 - 0s - loss: 393158.6968 - mean_squared_error: 393158.6968 - val_loss: 350496.7281 - val_mean_squared_error: 350496.7281
Epoch 1970/2000
 - 0s - loss: 393017.0785 - mean_squared_error: 393017.0785 - val_loss: 351737.2013 - val_mean_squared_error: 351737.2013
Epoch 1971/2000
 - 0s - loss: 392864.5516 - mean_squared_error: 392864.5516 - val_loss: 351419.1868 - val_mean_squared_error: 351419.1868
Epoch 1972/2000
 - 0s - loss: 392802.1848 - mean_squared_error: 392802.1848 - val_loss: 350596.9279 - val_mean_squared_error: 350596.9279
Epoch 1973/2000
 - 0s - loss: 393229.4714 - mean_squared_error: 393229.4714 - val_loss: 352161.0665 - val_mean_squared_error: 352161.0665
Epoch 1974/2000
 - 0s - loss: 393905.6125 - mean_squared_error: 393905.6125 - val_loss: 351650.0413 - val_mean_squared_error: 351650.0413
Epoch 1975/2000
 - 0s - loss: 393232.5136 - mean_squared_error: 393232.5136 - val_loss: 350512.8810 - val_mean_squared_error: 350512.8810
Epoch 1976/2000
 - 0s - loss: 392996.3353 - mean_squared_error: 392996.3353 - val_loss: 350257.6995 - val_mean_squared_error: 350257.6995
Epoch 1977/2000
 - 0s - loss: 392572.7001 - mean_squared_error: 392572.7001 - val_loss: 351083.8819 - val_mean_squared_error: 351083.8819
Epoch 1978/2000
 - 0s - loss: 393093.1763 - mean_squared_error: 393093.1763 - val_loss: 350253.7080 - val_mean_squared_error: 350253.7080
Epoch 1979/2000
 - 0s - loss: 392868.2256 - mean_squared_error: 392868.2256 - val_loss: 353122.8400 - val_mean_squared_error: 353122.8400
Epoch 1980/2000
 - 0s - loss: 392905.3495 - mean_squared_error: 392905.3495 - val_loss: 350134.9386 - val_mean_squared_error: 350134.9386
Epoch 1981/2000
 - 0s - loss: 392815.0487 - mean_squared_error: 392815.0487 - val_loss: 350536.9625 - val_mean_squared_error: 350536.9625
Epoch 1982/2000
 - 0s - loss: 392264.1909 - mean_squared_error: 392264.1909 - val_loss: 351010.5383 - val_mean_squared_error: 351010.5383
Epoch 1983/2000
 - 0s - loss: 392464.8971 - mean_squared_error: 392464.8971 - val_loss: 352511.0336 - val_mean_squared_error: 352511.0336
Epoch 1984/2000
 - 0s - loss: 392728.0114 - mean_squared_error: 392728.0114 - val_loss: 350361.5722 - val_mean_squared_error: 350361.5722
Epoch 1985/2000
 - 0s - loss: 392867.1127 - mean_squared_error: 392867.1127 - val_loss: 350956.7653 - val_mean_squared_error: 350956.7653
Epoch 1986/2000
 - 0s - loss: 393184.4572 - mean_squared_error: 393184.4572 - val_loss: 350048.0675 - val_mean_squared_error: 350048.0675
Epoch 1987/2000
 - 0s - loss: 392299.4790 - mean_squared_error: 392299.4790 - val_loss: 350371.2309 - val_mean_squared_error: 350371.2309
Epoch 1988/2000
 - 0s - loss: 392543.2559 - mean_squared_error: 392543.2559 - val_loss: 349955.0816 - val_mean_squared_error: 349955.0816
Epoch 1989/2000
 - 0s - loss: 392207.9921 - mean_squared_error: 392207.9921 - val_loss: 350564.9174 - val_mean_squared_error: 350564.9174
Epoch 1990/2000
 - 0s - loss: 392585.1471 - mean_squared_error: 392585.1471 - val_loss: 351939.7183 - val_mean_squared_error: 351939.7183
Epoch 1991/2000
 - 0s - loss: 392636.6855 - mean_squared_error: 392636.6855 - val_loss: 350742.1343 - val_mean_squared_error: 350742.1343
Epoch 1992/2000
 - 0s - loss: 392685.5213 - mean_squared_error: 392685.5213 - val_loss: 350790.6480 - val_mean_squared_error: 350790.6480
Epoch 1993/2000
 - 0s - loss: 392365.0846 - mean_squared_error: 392365.0846 - val_loss: 350923.8519 - val_mean_squared_error: 350923.8519
Epoch 1994/2000
 - 0s - loss: 392777.8883 - mean_squared_error: 392777.8883 - val_loss: 352346.1241 - val_mean_squared_error: 352346.1241
Epoch 1995/2000
 - 0s - loss: 392558.7267 - mean_squared_error: 392558.7267 - val_loss: 350589.9146 - val_mean_squared_error: 350589.9146
Epoch 1996/2000
 - 0s - loss: 392280.0797 - mean_squared_error: 392280.0797 - val_loss: 350947.3711 - val_mean_squared_error: 350947.3711
Epoch 1997/2000
 - 0s - loss: 392389.0843 - mean_squared_error: 392389.0843 - val_loss: 351116.2743 - val_mean_squared_error: 351116.2743
Epoch 1998/2000
 - 0s - loss: 392736.0409 - mean_squared_error: 392736.0409 - val_loss: 350774.5216 - val_mean_squared_error: 350774.5216
Epoch 1999/2000
 - 0s - loss: 392251.0854 - mean_squared_error: 392251.0854 - val_loss: 349920.3269 - val_mean_squared_error: 349920.3269
Epoch 2000/2000
 - 0s - loss: 392027.3671 - mean_squared_error: 392027.3671 - val_loss: 349922.0056 - val_mean_squared_error: 349922.0056

   32/13485 [..............................] - ETA: 0s
 4544/13485 [=========&gt;....................] - ETA: 0s
 8928/13485 [==================&gt;...........] - ETA: 0s
13472/13485 [============================&gt;.] - ETA: 0s
13485/13485 [==============================] - 0s 11us/step

root mean_squared_error: 619.680206
</pre></div>
</div>
</div></blockquote>
<p>Would you look at that?!</p>
<ul class="simple">
<li><p>We break $1000 RMSE around epoch 112</p></li>
<li><p>$900 around epoch 220</p></li>
<li><p>$800 around epoch 450</p></li>
<li><p>By around epoch 2000, my RMSE is &lt; $600</p></li>
</ul>
<p>…</p>
<p><strong>Same theory; different activation function. Huge difference</strong></p>
</div>
</div>
<div class="section" id="multilayer-networks">
<h1>Multilayer Networks<a class="headerlink" href="#multilayer-networks" title="Permalink to this headline">¶</a></h1>
<p>If a single-layer perceptron network learns the importance of different
combinations of features in the data…</p>
<p>What would another network learn if it had a second (hidden) layer of
neurons?</p>
<p>It depends on how we train the network. We’ll talk in the next section
about how this training works, but the general idea is that we still
work backward from the error gradient.</p>
<p>That is, the last layer learns from error in the output; the
second-to-last layer learns from error transmitted through that last
layer, etc. It’s a touch hand-wavy for now, but we’ll make it more
concrete later.</p>
<p>Given this approach, we can say that:</p>
<ol class="simple">
<li><p>The second (hidden) layer is learning features composed of
activations in the first (hidden) layer</p></li>
<li><p>The first (hidden) layer is learning feature weights that enable the
second layer to perform best</p>
<ul class="simple">
<li><p>Why? Earlier, the first hidden layer just learned feature
weights because that’s how it was judged</p></li>
<li><p>Now, the first hidden layer is judged on the error in the second
layer, so it learns to contribute to that second layer</p></li>
</ul>
</li>
<li><p>The second layer is learning new features that aren’t explicit in
the data, and is teaching the first layer to supply it with the
necessary information to compose these new features</p></li>
</ol>
<div class="section" id="so-instead-of-just-feature-weighting-and-combining-we-have-new-feature-learning">
<h2>So instead of just feature weighting and combining, we have new feature learning!<a class="headerlink" href="#so-instead-of-just-feature-weighting-and-combining-we-have-new-feature-learning" title="Permalink to this headline">¶</a></h2>
<p>This concept is the foundation of the “Deep Feed-Forward Network”</p>
<p>&lt;img src=”http://i.imgur.com/fHGrs4X.png”&gt;</p>
</div>
<hr class="docutils" />
<div class="section" id="let-s-try-it">
<h2>Let’s try it!<a class="headerlink" href="#let-s-try-it" title="Permalink to this headline">¶</a></h2>
<p><strong>Add a layer to your Keras network, perhaps another 20 neurons, and see
how the training goes.</strong></p>
<p>if you get stuck, there is a solution in the Keras-DFFN notebook</p>
<hr class="docutils" />
<p>I’m getting RMSE &lt; $1000 by epoch 35 or so</p>
<p>&lt; $800 by epoch 90</p>
<p>In this configuration, mine makes progress to around 700 epochs or so
and then stalls with RMSE around $560</p>
</div>
<div class="section" id="our-network-has-gone-meta">
<h2>Our network has “gone meta”<a class="headerlink" href="#our-network-has-gone-meta" title="Permalink to this headline">¶</a></h2>
<p>It’s now able to exceed where a simple decision tree can go, because it
can create new features and then split on those</p>
</div>
<div class="section" id="congrats-you-have-built-your-first-deep-learning-model">
<h2>Congrats! You have built your first deep-learning model!<a class="headerlink" href="#congrats-you-have-built-your-first-deep-learning-model" title="Permalink to this headline">¶</a></h2>
<p>So does that mean we can just keep adding more layers and solve
anything?</p>
<p>Well, theoretically maybe … try reconfiguring your network, watch the
training, and see what happens.</p>
<p>&lt;img src=”http://i.imgur.com/BumsXgL.jpg” width=500&gt;</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./000_0-sds-3-x-projects/student-project-06_group-ParticleClustering/old notebooks/sds-2-x-dl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By ScaDaMaLe Team<br/>
        
            &copy; Copyright 2020 Creative Commons Zero v1.0 Universal.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>