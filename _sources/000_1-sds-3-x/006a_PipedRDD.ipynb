{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ScaDaMaLe Course\n",
    "[site](https://lamastex.github.io/scalable-data-science/sds/3/x/) and\n",
    "[book](https://lamastex.github.io/ScaDaMaLe/index.html)\n",
    "\n",
    "Piped RDDs and Bayesian AB Testing\n",
    "==================================\n",
    "\n",
    "Here we will first take excerpts with minor modifications from the end\n",
    "of **Chapter 12. Resilient Distributed Datasets (RDDs)** of *Spark: The\n",
    "Definitive Guide*:\n",
    "\n",
    "-   https://learning.oreilly.com/library/view/spark-the-definitive/9781491912201/ch12.html\n",
    "\n",
    "Next, we will do Bayesian AB Testing using PipedRDDs.\n",
    "\n",
    "First, we create the toy RDDs as in *The Definitive Guide*:\n",
    "\n",
    "> From a Local Collection\n",
    "> =======================\n",
    "\n",
    "To create an RDD from a collection, you will need to use the parallelize\n",
    "method on a SparkContext (within a SparkSession). This turns a single\n",
    "node collection into a parallel collection. When creating this parallel\n",
    "collection, you can also explicitly state the number of partitions into\n",
    "which you would like to distribute this array. In this case, we are\n",
    "creating two partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// in Scala\n",
    "val myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\"  .split(\" \")\n",
    "val words = spark.sparkContext.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\"\\\n",
    "  .split(\" \")\n",
    "words = spark.sparkContext.parallelize(myCollection, 2)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "> glom\n",
    "> ====\n",
    "\n",
    "> `glom` is an interesting function that takes every partition in your\n",
    "> dataset and converts them to arrays. This can be useful if you’re\n",
    "> going to collect the data to the driver and want to have an array for\n",
    "> each partition. However, this can cause serious stability issues\n",
    "> because if you have large partitions or a large number of partitions,\n",
    "> it’s simple to crash the driver.\n",
    "\n",
    "Let's use `glom` to see how our `words` are distributed among the two\n",
    "partitions we used explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.glom.collect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "> Checkpointing\n",
    "> =============\n",
    ">\n",
    "> One feature not available in the DataFrame API is the concept of\n",
    "> checkpointing. Checkpointing is the act of saving an RDD to disk so\n",
    "> that future references to this RDD point to those intermediate\n",
    "> partitions on disk rather than recomputing the RDD from its original\n",
    "> source. This is similar to caching except that it’s not stored in\n",
    "> memory, only disk. This can be helpful when performing iterative\n",
    "> computation, similar to the use cases for caching:\n",
    "\n",
    "Let's create a directory in `dbfs:///` for checkpointing of RDDs in the\n",
    "sequel. The following `%fs mkdirs /path_to_dir` is a shortcut to create\n",
    "a directory in `dbfs:///`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdirs /datasets/ScaDaMaLe/checkpointing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setCheckpointDir(\"dbfs:///datasets/ScaDaMaLe/checkpointing\")\n",
    "words.checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "> Now, when we reference this RDD, it will derive from the checkpoint\n",
    "> instead of the source data. This can be a helpful optimization.\n",
    "\n",
    "YouTry\n",
    "------\n",
    "\n",
    "Just some more words in `haha_words` with `\\n`, the End-Of-Line (EOL)\n",
    "characters, in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val haha_words = sc.parallelize(Seq(\"ha\\nha\", \"he\\nhe\\nhe\", \"ho\\nho\\nho\\nho\"),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Let's use `glom` to see how our `haha_words` are distributed among the\n",
    "partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha_words.glom.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "> Pipe RDDs to System Commands\n",
    "> ============================\n",
    "\n",
    "> The pipe method is probably one of Spark’s more interesting methods.\n",
    "> With pipe, you can return an RDD created by piping elements to a\n",
    "> forked external process. The resulting RDD is computed by executing\n",
    "> the given process once per partition. All elements of each input\n",
    "> partition are written to a process’s stdin as lines of input separated\n",
    "> by a newline. The resulting partition consists of the process’s stdout\n",
    "> output, with each line of stdout resulting in one element of the\n",
    "> output partition. A process is invoked even for empty partitions.\n",
    "\n",
    "> The print behavior can be customized by providing two functions.\n",
    "\n",
    "We can use a simple example and pipe each partition to the command wc.\n",
    "Each row will be passed in as a new line, so if we perform a line count,\n",
    "we will get the number of lines, one per partition:\n",
    "\n",
    "The following produces a `PipedRDD`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wc_l_PipedRDD = words.pipe(\"wc -l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_l_PipedRDD = words.pipe(\"wc -l\")\n",
    "wc_l_PipedRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Now, we take an action via `collect` to bring the results to the Driver.\n",
    "\n",
    "NOTE: Be careful what you collect! You can always write the output to\n",
    "parquet of binary files in `dbfs:///` if the returned output is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_l_PipedRDD.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_l_PipedRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "In this case, we got the number of lines returned by `wc -l` per\n",
    "partition.\n",
    "\n",
    "YouTry\n",
    "------\n",
    "\n",
    "Try to make sense of the next few cells where we do NOT specifiy the\n",
    "number of partitions explicitly and let Spark decide on the number of\n",
    "partitions automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val haha_words = sc.parallelize(Seq(\"ha\\nha\", \"he\\nhe\\nhe\", \"ho\\nho\\nho\\nho\"),3)\n",
    "haha_words.glom.collect\n",
    "val wc_l_PipedRDD_haha_words = haha_words.pipe(\"wc -l\")\n",
    "wc_l_PipedRDD_haha_words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Do you understand why the above `collect` statement returns what it\n",
    "does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val haha_words_again = sc.parallelize(Seq(\"ha\\nha\", \"he\\nhe\\nhe\", \"ho\\nho\\nho\\nho\"))\n",
    "haha_words_again.glom.collect\n",
    "val wc_l_PipedRDD_haha_words_again = haha_words_again.pipe(\"wc -l\")\n",
    "wc_l_PipedRDD_haha_words_again.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Did you understand why some of the results are `0` in the last `collect`\n",
    "statement?\n",
    "\n",
    "> mapPartitions\n",
    "> =============\n",
    "\n",
    "> The previous command revealed that Spark operates on a per-partition\n",
    "> basis when it comes to actually executing code. You also might have\n",
    "> noticed earlier that the return signature of a map function on an RDD\n",
    "> is actually `MapPartitionsRDD`.\n",
    "\n",
    "Or `ParallelCollectionRDD` in our case.\n",
    "\n",
    "> This is because map is just a row-wise alias for `mapPartitions`,\n",
    "> which makes it possible for you to map an individual partition\n",
    "> (represented as an iterator). That’s because physically on the cluster\n",
    "> we operate on each partition individually (and not a specific row). A\n",
    "> simple example creates the value “1” for every partition in our data,\n",
    "> and the sum of the following expression will count the number of\n",
    "> partitions we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// in Scala\n",
    "words.mapPartitions(part => Iterator[Int](1)).sum() // 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "words.mapPartitions(lambda part: [1]).sum() # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "> Naturally, this means that we operate on a per-partition basis and\n",
    "> therefore it allows us to perform an operation on that *entire*\n",
    "> partition. This is valuable for performing something on an entire\n",
    "> subdataset of your RDD. You can gather all values of a partition class\n",
    "> or group into one partition and then operate on that entire group\n",
    "> using arbitrary functions and controls. An example use case of this\n",
    "> would be that you could pipe this through some custom machine learning\n",
    "> algorithm and train an individual model for that company’s portion of\n",
    "> the dataset. A Facebook engineer has an interesting demonstration of\n",
    "> their particular implementation of the pipe operator with a similar\n",
    "> use case demonstrated at [Spark Summit East\n",
    "> 2017](https://spark-summit.org/east-2017/events/experiences-with-sparks-rdd-apis-for-complex-custom-applications/).\n",
    "\n",
    "> Other functions similar to `mapPartitions` include\n",
    "> `mapPartitionsWithIndex`. With this you specify a function that\n",
    "> accepts an index (within the partition) and an iterator that goes\n",
    "> through all items within the partition. The partition index is the\n",
    "> partition number in your RDD, which identifies where each record in\n",
    "> our dataset sits (and potentially allows you to debug). You might use\n",
    "> this to test whether your map functions are behaving correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// in Scala\n",
    "def indexedFunc(partitionIndex:Int, withinPartIterator: Iterator[String]) = {  withinPartIterator.toList.map(    \n",
    "  value => s\"Partition: $partitionIndex => $value\").iterator\n",
    "                                                                            }\n",
    "words.mapPartitionsWithIndex(indexedFunc).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "def indexedFunc(partitionIndex, withinPartIterator):  \n",
    "  return [\"partition: {} => {}\".format(partitionIndex,    x) for x in withinPartIterator]\n",
    "words.mapPartitionsWithIndex(indexedFunc).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "> foreachPartition\n",
    "> ================\n",
    "\n",
    "> Although `mapPartitions` needs a return value to work properly, this\n",
    "> next function does not. `foreachPartition` simply iterates over all\n",
    "> the partitions of the data. The difference is that the function has no\n",
    "> return value. This makes it great for doing something with each\n",
    "> partition like writing it out to a database. In fact, this is how many\n",
    "> data source connectors are written. You can create\n",
    "\n",
    "your\n",
    "\n",
    "> own text file source if you want by specifying outputs to the temp\n",
    "> directory with a random ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.foreachPartition { iter =>  \n",
    "  import java.io._  \n",
    "  import scala.util.Random  \n",
    "  val randomFileName = new Random().nextInt()  \n",
    "  val pw = new PrintWriter(new File(s\"/tmp/random-file-${randomFileName}.txt\"))  \n",
    "  while (iter.hasNext) {\n",
    "    pw.write(iter.next())  \n",
    "  }  \n",
    "  pw.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "> You’ll find these two files if you scan your /tmp directory.\n",
    "\n",
    "You need to scan for the file across all the nodes. As the file may not\n",
    "be in the Driver node's `/tmp/` directory but in those of the executors\n",
    "that hosted the partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /tmp/random-file-*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Numerically Rigorous Bayesian AB Testing\n",
    "========================================\n",
    "\n",
    "This is an example of Bayesian AB Testing with computer-aided proofs for\n",
    "the posterior samples.\n",
    "\n",
    "The main learning goal for you is to use pipedRDDs to distribute, in an\n",
    "embarassingly paralle way, across all the worker nodes in the Spark\n",
    "cluster an executible `IsIt1or2Coins`.\n",
    "\n",
    "### What does `IsIt1or2Coins` do?\n",
    "\n",
    "At a very high-level, to understand what `IsIt1or2Coins` does, imagine\n",
    "the following simple experiment.\n",
    "\n",
    "We are given\n",
    "\n",
    "-   the number of heads that result from a first sequence of independent\n",
    "    and identical tosses of a coin and then\n",
    "-   we are given the number of heads that result from a second sequence\n",
    "    of independent and identical tosses of a coin\n",
    "\n",
    "Our decision problem is to do help shed light on whether both sequence\n",
    "of tosses came from the same coin or not (whatever the bias may be).\n",
    "\n",
    "`IsIt1or2Coins` tries to help us decide if the two sequence of\n",
    "coin-tosses are based on one coin with an unknown bias or two coins with\n",
    "different biases.\n",
    "\n",
    "If you are curious about details feel free to see:\n",
    "\n",
    "-   Exact Bayesian A/B testing using distributed fault-tolerant Moore\n",
    "    rejection sampler, Benny Avelin and Raazesh Sainudiin, Extended\n",
    "    Abstract, 2 pages, 2018 [(PDF\n",
    "    104KB)](http://lamastex.org/preprints/20180507_ABTestingViaDistributedMRS.pdf).\n",
    "-   which builds on: An auto-validating, trans-dimensional, universal\n",
    "    rejection sampler for locally Lipschitz arithmetical expressions,\n",
    "    Raazesh Sainudiin and Thomas York, [Reliable Computing, vol.18,\n",
    "    pp.15-54,\n",
    "    2013](http://interval.louisiana.edu/reliable-computing-journal/volume-18/reliable-computing-18-pp-015-054.pdf)\n",
    "    ([preprint: PDF\n",
    "    2612KB](http://lamastex.org/preprints/avs_rc_2013.pdf))\n",
    "\n",
    "**See first about `PipedRDDs` excerpt from *Spark The Definitive Guide*\n",
    "earlier.**\n",
    "\n",
    "### Getting the executible `IsIt1or2Coins` into our Spark Cluster\n",
    "\n",
    "**This has already been done in the project-shard. You need not do it\n",
    "again for this executible!**\n",
    "\n",
    "You need to upload the C++ executible `IsIt1or2Coins` from: -\n",
    "https://github.com/lamastex/mrs2\n",
    "\n",
    "Here, suppose you have an executible for linux x86 64 bit processor with\n",
    "all dependencies pre-compiled into one executibe.\n",
    "\n",
    "Say this executible is `IsIt10r2Coins`.\n",
    "\n",
    "This executible comes from the following dockerised build:\n",
    "\n",
    "-   https://github.com/lamastex/mrs2/tree/master/docker\n",
    "-   by statically compiling inside the docerised environment for mrs2:\n",
    "    -   https://github.com/lamastex/mrs2/tree/master/mrs-2.0/examples/MooreRejSam/IsIt1or2Coins\n",
    "\n",
    "You can replace the executible with any other executible with\n",
    "appropriate I/O to it.\n",
    "\n",
    "Then you upload the executible to databricks' `FileStore`.\n",
    "\n",
    "Just note the path to the file and DO NOT click `Create Table` or other\n",
    "buttons!\n",
    "\n",
    "![creenShotOfUploadingStaticExecutibleIsIt1or2CoinsViaFileStore](https://raw.githubusercontent.com/lamastex/scalable-data-science/master/images/2020/ScaDaMaLe/screenShotOfUploadingStaticExecutibleIsIt1or2CoinsViaFileStore.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls \"/FileStore/tables/IsIt1or2Coins\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Now copy the file from `dbfs://FileStore` that you just uploaded into\n",
    "the local file system of the Driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"dbfs:/FileStore/tables/IsIt1or2Coins\", \"file:/tmp/IsIt1or2Coins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -al /tmp/IsIt1or2Coins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Note it is a big static executible with all dependencies inbuilt (it\n",
    "uses GNU Scientific Library and a specialized C++ Library called C-XSC\n",
    "or C Extended for Scientific Computing to do hard-ware optimized\n",
    "rigorous numerical proofs using Interval-Extended Hessian\n",
    "Differentiation Arithmetics over Rounding-Controlled Hardware-Specified\n",
    "Machine Intervals).\n",
    "\n",
    "Just note it is over 6.5MB. Also we need to change the permissions so it\n",
    "is indeed executible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chmod +x /tmp/IsIt1or2Coins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Usage instructions for IsIt1or2Coins\n",
    "====================================\n",
    "\n",
    "`./IsIt1or2Coins numboxes numiter seed numtosses1 heads1 numtosses2 heads2 logScale`\n",
    "- numboxes = Number of boxes for Moore Rejection Sampling (Rigorous von\n",
    "Neumann Rejection Sampler) - numiter = Number of samples drawn from\n",
    "posterior distribution to estimate the model probabilities - seed = a\n",
    "random number seed - numtosses1 = number of tosses for the first coin -\n",
    "heads1 = number of heads shown up on the first coin - numtosses2 =\n",
    "number of tosses for the second coin - heads2 = number of heads shown up\n",
    "on the second coin - logscale = True/False as Int\n",
    "\n",
    "Don't worry about the details of what the executible `IsIt1or2Coins` is\n",
    "doing for now. Just realise that this executible takes some input on\n",
    "command-line and gives some output.\n",
    "\n",
    "Let's make sure the executible takes input and returns output string on\n",
    "the Driver node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/tmp/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also do it like this\n",
    "\n",
    "/dbfs/FileStore/tables/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Moving the executables to the worker nodes\n",
    "------------------------------------------\n",
    "\n",
    "To copy the executible from `dbfs` to the local drive of each executor\n",
    "you can use the following helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.sys.process._\n",
    "import scala.concurrent.duration._\n",
    "// from Ivan Sadikov\n",
    "\n",
    "def copyFile(): Unit = {\n",
    "  \"mkdir -p /tmp/executor/bin\".!!\n",
    "  \"cp /dbfs/FileStore/tables/IsIt1or2Coins /tmp/executor/bin/\".!!\n",
    "}\n",
    "\n",
    "sc.runOnEachExecutor(copyFile, new FiniteDuration(1, HOURS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Now, let us use piped RDDs via `bash` to execute the given command in\n",
    "each partition as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val input = Seq(\"/tmp/executor/bin/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1\", \"/tmp/executor/bin/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1\")\n",
    "\n",
    "val output = sc\n",
    "  .parallelize(input)\n",
    "  .repartition(2)\n",
    "  .pipe(\"bash\")\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "In fact, you can just use `DBFS FUSE` to run the commands without any\n",
    "file copy in databricks-provisioned Spark clusters we are on here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val isIt1or2StaticExecutible = \"/dbfs/FileStore/tables/IsIt1or2Coins\"\n",
    "val same_input = Seq(s\"$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1\", \n",
    "                     s\"$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1\")\n",
    "\n",
    "val same_output = sc\n",
    "  .parallelize(same_input)\n",
    "  .repartition(2)\n",
    "  .pipe(\"bash\")\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Thus by mixing several different executibles that are statically\n",
    "compiled for linux 64 bit machine, we can mix and match multiple\n",
    "executibles with appropriate inputs.\n",
    "\n",
    "The resulting outputs can themselves be re-processed in Spark to feed\n",
    "into toher pipedRDDs or normal RDDs or DataFrames and DataSets.\n",
    "\n",
    "Finally, we can have more than one command per partition and then use\n",
    "`mapPartitions` to send all the executible commands within the input\n",
    "partition that is to be run by the executor in which that partition\n",
    "resides as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val isIt1or2StaticExecutible = \"/dbfs/FileStore/tables/IsIt1or2Coins\"\n",
    "\n",
    "// let us make 2 commands in each of the 2 input partitions\n",
    "val same_input_mp = Seq(s\"$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1\", \n",
    "                        s\"$isIt1or2StaticExecutible 1000 100 123456789 1000 500 1200 600 1\",\n",
    "                        s\"$isIt1or2StaticExecutible 1000 100 123456789 1000 500 1200 600 1\",\n",
    "                        s\"$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1\")\n",
    "\n",
    "val same_output_mp = sc\n",
    "  .parallelize(same_input)\n",
    "  .repartition(2)\n",
    "  .pipe(\"bash\")\n",
    "  .mapPartitions(x => Seq(x.mkString(\"\\n\")).iterator)\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "allCatch is a useful tool to use as a filtering function when testing if\n",
    "a command will work without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.control.Exception.allCatch\n",
    "(allCatch opt \" 12 \".trim.toLong).isDefined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
