{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ScaDaMaLe, Scalable Data Science and Distributed Machine Learning](https://lamastex.github.io/scalable-data-science/sds/3/x/)\n",
    "==============================================================================================================================\n",
    "\n",
    "This is an elaboration of the\n",
    "<http://spark.apache.org/docs/latest/sql-programming-guide.html> by Ivan\n",
    "Sadikov and Raazesh Sainudiin.\n",
    "\n",
    "Data Sources\n",
    "============\n",
    "\n",
    "Spark Sql Programming Guide\n",
    "---------------------------\n",
    "\n",
    "-   Data Sources\n",
    "    -   Generic Load/Save Functions\n",
    "        -   Manually Specifying Options\n",
    "        -   Run SQL on files directly\n",
    "        -   Save Modes\n",
    "        -   Saving to Persistent Tables\n",
    "    -   Parquet Files\n",
    "        -   Loading Data Programmatically\n",
    "        -   Partition Discovery\n",
    "        -   Schema Merging\n",
    "        -   Hive metastore Parquet table conversion\n",
    "            -   Hive/Parquet Schema Reconciliation\n",
    "            -   Metadata Refreshing\n",
    "        -   Configuration\n",
    "    -   JSON Datasets\n",
    "    -   Hive Tables\n",
    "        -   Interacting with Different Versions of Hive Metastore\n",
    "    -   JDBC To Other Databases\n",
    "    -   Troubleshooting\n",
    "\n",
    "Data Sources\n",
    "============\n",
    "\n",
    "Spark SQL supports operating on a variety of data sources through the\n",
    "`DataFrame` or `DataFrame` interfaces. A Dataset can be operated on as\n",
    "normal RDDs and can also be registered as a temporary table. Registering\n",
    "a Dataset as a table allows you to run SQL queries over its data. But\n",
    "from time to time you would need to either load or save Dataset. Spark\n",
    "SQL provides built-in data sources as well as Data Source API to define\n",
    "your own data source and use it read / write data into Spark.\n",
    "\n",
    "Overview\n",
    "--------\n",
    "\n",
    "Spark provides some built-in datasources that you can use straight out\n",
    "of the box, such as [Parquet](https://parquet.apache.org/),\n",
    "[JSON](http://www.json.org/),\n",
    "[JDBC](https://en.wikipedia.org/wiki/Java_Database_Connectivity),\n",
    "[ORC](https://orc.apache.org/) (available with enabled Hive Support, but\n",
    "this is changing, and ORC will not require Hive support and will work\n",
    "with default Spark session starting from next release), and Text (since\n",
    "Spark 1.6) and CSV (since Spark 2.0, before that it is accessible as a\n",
    "package).\n",
    "\n",
    "Third-party datasource packages\n",
    "-------------------------------\n",
    "\n",
    "Community also have built quite a few datasource packages to provide\n",
    "easy access to the data from other formats. You can find list of those\n",
    "packages on http://spark-packages.org/, e.g.\n",
    "[Avro](http://spark-packages.org/package/databricks/spark-avro),\n",
    "[CSV](http://spark-packages.org/package/databricks/spark-csv), [Amazon\n",
    "Redshit](http://spark-packages.org/package/databricks/spark-redshift)\n",
    "(for Spark &lt; 2.0),\n",
    "[XML](http://spark-packages.org/package/HyukjinKwon/spark-xml),\n",
    "[NetFlow](http://spark-packages.org/package/sadikovi/spark-netflow) and\n",
    "many others.\n",
    "\n",
    "Generic Load/Save functions\n",
    "---------------------------\n",
    "\n",
    "In order to load or save DataFrame you have to call either `read` or\n",
    "`write`. This will return\n",
    "[DataFrameReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader)\n",
    "or\n",
    "[DataFrameWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter)\n",
    "depending on what you are trying to achieve. Essentially these classes\n",
    "are entry points to the reading / writing actions. They allow you to\n",
    "specify writing mode or provide additional options to read data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This will return DataFrameReader to read data source\n",
    "println(spark.read)\n",
    "\n",
    "val df = spark.range(0, 10)\n",
    "\n",
    "// This will return DataFrameWriter to save DataFrame\n",
    "println(df.write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Saving Parquet table in Scala\n",
    "// DataFrames and tables can be saved as Parquet files, maintaining the schema information\n",
    "val df_save = spark.table(\"social_media_usage\").select(\"platform\", \"visits\") // assuming you made the social_media_usage table permanent in previous notebook\n",
    "df_save.write.mode(\"overwrite\").parquet(\"/tmp/platforms.parquet\")\n",
    "\n",
    "// Read in the parquet file created above\n",
    "// Parquet files are self-describing so the schema is preserved\n",
    "// The result of loading a Parquet file is also a DataFrame\n",
    "val df = spark.read.parquet(\"/tmp/platforms.parquet\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// in databricks '/tmp/...' is the same as 'dbfs:///tmp/...'\n",
    "display(dbutils.fs.ls(\"/tmp/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/tmp/platforms.parquet/\")) // note this is a directory with many files in it... files beginning with part have content in possibly many partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Parquet table in Python\n",
    "dfPy = spark.read.parquet(\"/tmp/platforms.parquet\")\n",
    "dfPy.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Saving JSON dataset in Scala\n",
    "val df_save = spark.table(\"social_media_usage\").select(\"platform\", \"visits\")\n",
    "df_save.write.mode(\"overwrite\").json(\"/tmp/platforms.json\")\n",
    "\n",
    "// Loading JSON dataset in Scala\n",
    "val df = spark.read.json(\"/tmp/platforms.json\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading JSON dataset in Python\n",
    "dfPy = spark.read.json(\"/tmp/platforms.json\")\n",
    "dfPy.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Manually Specifying Options\n",
    "\n",
    "You can also manually specify the data source that will be used along\n",
    "with any extra options that you would like to pass to the data source.\n",
    "Data sources are specified by their fully qualified name (i.e.,\n",
    "`org.apache.spark.sql.parquet`), but for built-in sources you can also\n",
    "use their short names (`json`, `parquet`, `jdbc`). DataFrames of any\n",
    "type can be converted into other types using this syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val json = sqlContext.read.format(\"json\").load(\"/tmp/platforms.json\")\n",
    "json.select(\"platform\").show(10)\n",
    "\n",
    "val parquet = sqlContext.read.format(\"parquet\").load(\"/tmp/platforms.parquet\")\n",
    "parquet.select(\"platform\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Run SQL on files directly\n",
    "\n",
    "Instead of using read API to load a file into DataFrame and query it,\n",
    "you can also query that file directly with SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = sqlContext.sql(\"SELECT * FROM parquet.`/tmp/platforms.parquet`\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Save Modes\n",
    "\n",
    "Save operations can optionally take a `SaveMode`, that specifies how to\n",
    "handle existing data if present. It is important to realize that these\n",
    "save modes do not utilize any locking and are not atomic. Additionally,\n",
    "when performing a `Overwrite`, the data will be deleted before writing\n",
    "out the new data.\n",
    "\n",
    "| Scala/Java | Any language | Meaning | | --- | --- | --- | |\n",
    "`SaveMode.ErrorIfExists` (default) | `\"error\"` (default) | When saving a\n",
    "DataFrame to a data source, if data already exists, an exception is\n",
    "expected to be thrown. | | `SaveMode.Append` | `\"append\"` | When saving\n",
    "a DataFrame to a data source, if data/table already exists, contents of\n",
    "the DataFrame are expected to be appended to existing data. | |\n",
    "`SaveMode.Overwrite` | `\"overwrite\"` | Overwrite mode means that when\n",
    "saving a DataFrame to a data source, if data/table already exists,\n",
    "existing data is expected to be overwritten by the contents of the\n",
    "DataFrame. | | `SaveMode.Ignore` | `\"ignore\"` | Ignore mode means that\n",
    "when saving a DataFrame to a data source, if data already exists, the\n",
    "save operation is expected to not save the contents of the DataFrame and\n",
    "to not change the existing data. This is similar to a\n",
    "`CREATE TABLE IF NOT EXISTS` in SQL. |\n",
    "\n",
    "### Saving to Persistent Tables\n",
    "\n",
    "`DataFrame` and `Dataset` can also be saved as persistent tables using\n",
    "the `saveAsTable` command. Unlike the `createOrReplaceTempView` command,\n",
    "`saveAsTable` will materialize the contents of the dataframe and create\n",
    "a pointer to the data in the metastore. Persistent tables will still\n",
    "exist even after your Spark program has restarted, as long as you\n",
    "maintain your connection to the same metastore. A DataFrame for a\n",
    "persistent table can be created by calling the `table` method on a\n",
    "`SparkSession` with the name of the table.\n",
    "\n",
    "By default `saveAsTable` will create a “managed table”, meaning that the\n",
    "location of the data will be controlled by the metastore. Managed tables\n",
    "will also have their data deleted automatically when a table is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// First of all list tables to see that table we are about to create does not exist\n",
    "spark.catalog.listTables.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop table if exists simple_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.range(0, 100)\n",
    "df.write.saveAsTable(\"simple_range\")\n",
    "\n",
    "// Verify that table is saved and it is marked as persistent (\"isTemporary\" value should be \"false\")\n",
    "spark.catalog.listTables.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Parquet Files\n",
    "-------------\n",
    "\n",
    "[Parquet](http://parquet.io) is a columnar format that is supported by\n",
    "many other data processing systems. Spark SQL provides support for both\n",
    "reading and writing Parquet files that automatically preserves the\n",
    "schema of the original data. When writing Parquet files, all columns are\n",
    "automatically converted to be nullable for compatibility reasons.\n",
    "\n",
    "### More on Parquet\n",
    "\n",
    "[Apache Parquet](https://parquet.apache.org/) is a [columnar\n",
    "storage](http://en.wikipedia.org/wiki/Column-oriented_DBMS) format\n",
    "available to any project in the Hadoop ecosystem, regardless of the\n",
    "choice of data processing framework, data model or programming language.\n",
    "It is a more efficient way to store data frames.\n",
    "\n",
    "-   To understand the ideas read [Dremel: Interactive Analysis of\n",
    "    Web-Scale Datasets, Sergey Melnik, Andrey Gubarev, Jing Jing Long,\n",
    "    Geoffrey Romer, Shiva Shivakumar, Matt Tolton and Theo\n",
    "    Vassilakis,Proc. of the 36th Int'l Conf on Very Large Data Bases\n",
    "    (2010), pp. 330-339](http://research.google.com/pubs/pub36632.html),\n",
    "    whose Abstract is as follows:\n",
    "    -   Dremel is a scalable, interactive ad-hoc query system for\n",
    "        analysis of read-only nested data. By combining multi-level\n",
    "        execution trees and columnar data layouts it is **capable of\n",
    "        running aggregation queries over trillion-row tables in\n",
    "        seconds**. The system **scales to thousands of CPUs and\n",
    "        petabytes of data, and has thousands of users at Google**. In\n",
    "        this paper, we describe the architecture and implementation of\n",
    "        Dremel, and explain how it complements MapReduce-based\n",
    "        computing. We present a novel columnar storage representation\n",
    "        for nested records and discuss experiments on few-thousand node\n",
    "        instances of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//This allows easy embedding of publicly available information into any other notebook\n",
    "//when viewing in git-book just ignore this block - you may have to manually chase the URL in frameIt(\"URL\").\n",
    "//Example usage:\n",
    "// displayHTML(frameIt(\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA\",250))\n",
    "def frameIt( u:String, h:Int ) : String = {\n",
    "      \"\"\"<iframe \n",
    " src=\"\"\"\"+ u+\"\"\"\"\n",
    " width=\"95%\" height=\"\"\"\" + h + \"\"\"\"\n",
    " sandbox>\n",
    "  <p>\n",
    "    <a href=\"http://spark.apache.org/docs/latest/index.html\">\n",
    "      Fallback link for browsers that, unlikely, don't support frames\n",
    "    </a>\n",
    "  </p>\n",
    "</iframe>\"\"\"\n",
    "   }\n",
    "displayHTML(frameIt(\"https://parquet.apache.org/documentation/latest/\",500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Loading Data Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Read in the parquet file created above. Parquet files are self-describing so the schema is preserved.\n",
    "// The result of loading a Parquet file is also a DataFrame.\n",
    "val parquetFile = sqlContext.read.parquet(\"/tmp/platforms.parquet\")\n",
    "\n",
    "// Parquet files can also be registered as tables and then used in SQL statements.\n",
    "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "val platforms = sqlContext.sql(\"SELECT platform FROM parquetFile WHERE visits > 0\")\n",
    "platforms.distinct.map(t => \"Name: \" + t(0)).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Bucketing, Sorting and Partitioning\n",
    "-----------------------------------\n",
    "\n",
    "For file-based data source, it is also possible to bucket and sort or\n",
    "partition the output. Bucketing and sorting are applicable only to\n",
    "persistent tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val social_media_usage_DF = spark.table(\"social_media_usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Find full example code at -\n",
    "https://raw.githubusercontent.com/apache/spark/master/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
    "in the Spark repo.\n",
    "\n",
    "Note that partitioning can be used with both save and saveAsTable when\n",
    "using the Dataset APIs.\n",
    "\n",
    "`partitionBy` creates a directory structure as described in the\n",
    "Partition Discovery section. Thus, it has limited applicability to\n",
    "columns with high cardinality. In contrast `bucketBy` distributes data\n",
    "across a fixed number of buckets and can be used when the number of\n",
    "unique values is unbounded. One can use `partitionBy` by itself or along\n",
    "with \\`bucketBy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_media_usage_DF.write.mode(\"overwrite\").parquet(\"/tmp/social_media_usage.parquet\") // write to parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/tmp/social_media_usage.parquet\")) // there is one part-00000 file inside the parquet folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val social_media_usage_readFromParquet_DF = spark.read.parquet(\"/tmp/social_media_usage.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_media_usage_readFromParquet_DF.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_media_usage_readFromParquet_DF.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_media_usage_readFromParquet_DF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_media_usage_readFromParquet_DF.select(\"platform\").distinct.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_media_usage_readFromParquet_DF\n",
    "  .write\n",
    "  .partitionBy(\"platform\")\n",
    "  .mode(\"overwrite\").parquet(\"/tmp/social_media_usage_partitionedByPlatform.parquet\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/tmp/social_media_usage_partitionedByPlatform.parquet\")) // there are many platform=* folders inside the parquet folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/tmp/social_media_usage_partitionedByPlatform.parquet/platform=Android\")) // threre are part-00000- files with contents inside each platform=* folder in the parquet folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"/tmp/social_media_usage_partitionedByPlatform.parquet\").rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "We can also use a fixed number of buckets and sort by a column within\n",
    "each partition. Such finer control of the dataframe written as a parquet\n",
    "file can help with optimizing downstream operations on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_media_usage_readFromParquet_DF\n",
    "  .write\n",
    "  .partitionBy(\"platform\")\n",
    "  .bucketBy(10, \"date\")\n",
    "  .sortBy(\"date\")\n",
    "  .mode(\"overwrite\")\n",
    "  .saveAsTable(\"social_media_usage_table_partitionedByPlatformBucketedByDate\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.table(\"social_media_usage_table_partitionedByPlatformBucketedByDate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Partition Discovery\n",
    "\n",
    "Table partitioning is a common optimization approach used in systems\n",
    "like Hive. In a partitioned table, data are usually stored in different\n",
    "directories, with partitioning column values encoded in the path of each\n",
    "partition directory. The Parquet data source is now able to discover and\n",
    "infer partitioning information automatically. For example, we can store\n",
    "all our previously used population data (from the programming guide\n",
    "example!) into a partitioned table using the following directory\n",
    "structure, with two extra columns, `gender` and `country` as\n",
    "partitioning columns:\n",
    "`path     └── to         └── table             ├── gender=male             │   ├── ...             │   │             │   ├── country=US             │   │   └── data.parquet             │   ├── country=CN             │   │   └── data.parquet             │   └── ...             └── gender=female                 ├── ...                 │                 ├── country=US                 │   └── data.parquet                 ├── country=CN                 │   └── data.parquet                 └── ...`\n",
    "By passing `path/to/table` to either `SparkSession.read.parquet` or\n",
    "`SparkSession.read.load`, Spark SQL will automatically extract the\n",
    "partitioning information from the paths. Now the schema of the returned\n",
    "DataFrame becomes:\n",
    "`root     |-- name: string (nullable = true)     |-- age: long (nullable = true)     |-- gender: string (nullable = true)     |-- country: string (nullable = true)`\n",
    "Notice that the data types of the partitioning columns are automatically\n",
    "inferred. Currently, numeric data types and string type are supported.\n",
    "Sometimes users may not want to automatically infer the data types of\n",
    "the partitioning columns. For these use cases, the automatic type\n",
    "inference can be configured by\n",
    "`spark.sql.sources.partitionColumnTypeInference.enabled`, which is\n",
    "default to `true`. When type inference is disabled, string type will be\n",
    "used for the partitioning columns.\n",
    "\n",
    "Starting from Spark 1.6.0, partition discovery only finds partitions\n",
    "under the given paths by default. For the above example, if users pass\n",
    "`path/to/table/gender=male` to either `SparkSession.read.parquet` or\n",
    "`SparkSession.read.load`, `gender` will not be considered as a\n",
    "partitioning column. If users need to specify the base path that\n",
    "partition discovery should start with, they can set `basePath` in the\n",
    "data source options. For example, when `path/to/table/gender=male` is\n",
    "the path of the data and users set `basePath` to `path/to/table/`,\n",
    "`gender` will be a partitioning column.\n",
    "\n",
    "### Schema Merging\n",
    "\n",
    "Like ProtocolBuffer, Avro, and Thrift, Parquet also supports schema\n",
    "evolution. Users can start with a simple schema, and gradually add more\n",
    "columns to the schema as needed. In this way, users may end up with\n",
    "multiple Parquet files with different but mutually compatible schemas.\n",
    "The Parquet data source is now able to automatically detect this case\n",
    "and merge schemas of all these files.\n",
    "\n",
    "Since schema merging is a relatively expensive operation, and is not a\n",
    "necessity in most cases, we turned it off by default starting from\n",
    "1.5.0. You may enable it by:\n",
    "\n",
    "1.  setting data source option `mergeSchema` to `true` when reading\n",
    "    Parquet files (as shown in the examples below), or\n",
    "2.  setting the global SQL option `spark.sql.parquet.mergeSchema` to\n",
    "    `true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create a simple DataFrame, stored into a partition directory\n",
    "val df1 = sc.parallelize(1 to 5).map(i => (i, i * 2)).toDF(\"single\", \"double\")\n",
    "df1.write.mode(\"overwrite\").parquet(\"/tmp/data/test_table/key=1\")\n",
    "\n",
    "// Create another DataFrame in a new partition directory, adding a new column and dropping an existing column\n",
    "val df2 = sc.parallelize(6 to 10).map(i => (i, i * 3)).toDF(\"single\", \"triple\")\n",
    "df2.write.mode(\"overwrite\").parquet(\"/tmp/data/test_table/key=2\")\n",
    "\n",
    "// Read the partitioned table\n",
    "val df3 = spark.read.option(\"mergeSchema\", \"true\").parquet(\"/tmp/data/test_table\")\n",
    "df3.printSchema()\n",
    "\n",
    "// The final schema consists of all 3 columns in the Parquet files together\n",
    "// with the partitioning column appeared in the partition directory paths.\n",
    "// root\n",
    "//  |-- single: integer (nullable = true)\n",
    "//  |-- double: integer (nullable = true)\n",
    "//  |-- triple: integer (nullable = true)\n",
    "//  |-- key: integer (nullable = true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Hive metastore Parquet table conversion\n",
    "\n",
    "When reading from and writing to Hive metastore Parquet tables, Spark\n",
    "SQL will try to use its own Parquet support instead of Hive SerDe for\n",
    "better performance. This behavior is controlled by the\n",
    "`spark.sql.hive.convertMetastoreParquet` configuration, and is turned on\n",
    "by default.\n",
    "\n",
    "#### Hive/Parquet Schema Reconciliation\n",
    "\n",
    "There are two key differences between Hive and Parquet from the\n",
    "perspective of table schema processing.\n",
    "\n",
    "1.  Hive is case insensitive, while Parquet is not\n",
    "2.  Hive considers all columns nullable, while nullability in Parquet is\n",
    "    significant\n",
    "\n",
    "Due to this reason, we must reconcile Hive metastore schema with Parquet\n",
    "schema when converting a Hive metastore Parquet table to a Spark SQL\n",
    "Parquet table. The reconciliation rules are:\n",
    "\n",
    "1.  Fields that have the same name in both schema must have the same\n",
    "    data type regardless of nullability. The reconciled field should\n",
    "    have the data type of the Parquet side, so that nullability is\n",
    "    respected.\n",
    "2.  The reconciled schema contains exactly those fields defined in Hive\n",
    "    metastore schema.\n",
    "\n",
    "-   Any fields that only appear in the Parquet schema are dropped in the\n",
    "    reconciled schema.\n",
    "-   Any fileds that only appear in the Hive metastore schema are added\n",
    "    as nullable field in the reconciled schema.\n",
    "\n",
    "#### Metadata Refreshing\n",
    "\n",
    "Spark SQL caches Parquet metadata for better performance. When Hive\n",
    "metastore Parquet table conversion is enabled, metadata of those\n",
    "converted tables are also cached. If these tables are updated by Hive or\n",
    "other external tools, you need to refresh them manually to ensure\n",
    "consistent metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// should refresh table metadata\n",
    "spark.catalog.refreshTable(\"simple_range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Or you can use SQL to refresh table\n",
    "REFRESH TABLE simple_range;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Configuration\n",
    "\n",
    "Configuration of Parquet can be done using the `setConf` method on\n",
    "`SQLContext` or by running `SET key=value` commands using SQL.\n",
    "\n",
    "| Property Name | Default | Meaning | | --- | --- | --- | --- | |\n",
    "`spark.sql.parquet.binaryAsString` | false | Some other\n",
    "Parquet-producing systems, in particular Impala, Hive, and older\n",
    "versions of Spark SQL, do not differentiate between binary data and\n",
    "strings when writing out the Parquet schema. This flag tells Spark SQL\n",
    "to interpret binary data as a string to provide compatibility with these\n",
    "systems. | | `spark.sql.parquet.int96AsTimestamp` | true | Some\n",
    "Parquet-producing systems, in particular Impala and Hive, store\n",
    "Timestamp into INT96. This flag tells Spark SQL to interpret INT96 data\n",
    "as a timestamp to provide compatibility with these systems. | |\n",
    "`spark.sql.parquet.cacheMetadata` | true | Turns on caching of Parquet\n",
    "schema metadata. Can speed up querying of static data. | |\n",
    "`spark.sql.parquet.compression.codec` | gzip | Sets the compression\n",
    "codec use when writing Parquet files. Acceptable values include:\n",
    "uncompressed, snappy, gzip, lzo. | | `spark.sql.parquet.filterPushdown`\n",
    "| true | Enables Parquet filter push-down optimization when set to true.\n",
    "| | `spark.sql.hive.convertMetastoreParquet` | true | When set to false,\n",
    "Spark SQL will use the Hive SerDe for parquet tables instead of the\n",
    "built in support. | | `spark.sql.parquet.output.committer.class` |\n",
    "`org.apache.parquet.hadoop.ParquetOutputCommitter` | The output\n",
    "committer class used by Parquet. The specified class needs to be a\n",
    "subclass of `org.apache.hadoop.mapreduce.OutputCommitter`. Typically,\n",
    "it's also a subclass of\n",
    "`org.apache.parquet.hadoop.ParquetOutputCommitter`. Spark SQL comes with\n",
    "a builtin `org.apache.spark.sql.parquet.DirectParquetOutputCommitter`,\n",
    "which can be more efficient then the default Parquet output committer\n",
    "when writing data to S3. | | `spark.sql.parquet.mergeSchema` | `false` |\n",
    "When true, the Parquet data source merges schemas collected from all\n",
    "data files, otherwise the schema is picked from the summary file or a\n",
    "random data file if no summary file is available. |\n",
    "\n",
    "JSON Datasets\n",
    "-------------\n",
    "\n",
    "Spark SQL can automatically infer the schema of a JSON dataset and load\n",
    "it as a DataFrame. This conversion can be done using\n",
    "`SparkSession.read.json()` on either an RDD of String, or a JSON file.\n",
    "\n",
    "Note that the file that is offered as *a json file* is not a typical\n",
    "JSON file. Each line must contain a separate, self-contained valid JSON\n",
    "object. As a consequence, a regular multi-line JSON file will most often\n",
    "fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// A JSON dataset is pointed to by path.\n",
    "// The path can be either a single text file or a directory storing text files.\n",
    "val path = \"/tmp/platforms.json\"\n",
    "val platforms = spark.read.json(path)\n",
    "\n",
    "// The inferred schema can be visualized using the printSchema() method.\n",
    "platforms.printSchema()\n",
    "// root\n",
    "//  |-- platform: string (nullable = true)\n",
    "//  |-- visits: long (nullable = true)\n",
    "\n",
    "// Register this DataFrame as a table.\n",
    "platforms.createOrReplaceTempView(\"platforms\")\n",
    "\n",
    "// SQL statements can be run by using the sql methods provided by sqlContext.\n",
    "val facebook = spark.sql(\"SELECT platform, visits FROM platforms WHERE platform like 'Face%k'\")\n",
    "facebook.show()\n",
    "\n",
    "// Alternatively, a DataFrame can be created for a JSON dataset represented by\n",
    "// an RDD[String] storing one JSON object per string.\n",
    "val rdd = sc.parallelize(\"\"\"{\"name\":\"IWyn\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}\"\"\" :: Nil)\n",
    "val anotherPlatforms = spark.read.json(rdd)\n",
    "anotherPlatforms.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Hive Tables\n",
    "-----------\n",
    "\n",
    "Spark SQL also supports reading and writing data stored in [Apache\n",
    "Hive](http://hive.apache.org/). However, since Hive has a large number\n",
    "of dependencies, it is not included in the default Spark assembly. Hive\n",
    "support is enabled by adding the `-Phive` and `-Phive-thriftserver`\n",
    "flags to Spark’s build. This command builds a new assembly jar that\n",
    "includes Hive. Note that this Hive assembly jar must also be present on\n",
    "all of the worker nodes, as they will need access to the Hive\n",
    "serialization and deserialization libraries (SerDes) in order to access\n",
    "data stored in Hive.\n",
    "\n",
    "Configuration of Hive is done by placing your `hive-site.xml`,\n",
    "`core-site.xml` (for security configuration), `hdfs-site.xml` (for HDFS\n",
    "configuration) file in `conf/`. Please note when running the query on a\n",
    "YARN cluster (`cluster` mode), the `datanucleus` jars under the\n",
    "`lib_managed/jars` directory and `hive-site.xml` under `conf/` directory\n",
    "need to be available on the driver and all executors launched by the\n",
    "YARN cluster. The convenient way to do this is adding them through the\n",
    "`--jars` option and `--file` option of the `spark-submit` command.\n",
    "\n",
    "When working with Hive one must construct a `HiveContext`, which\n",
    "inherits from `SQLContext`, and adds support for finding tables in the\n",
    "MetaStore and writing queries using HiveQL. Users who do not have an\n",
    "existing Hive deployment can still create a `HiveContext`. When not\n",
    "configured by the hive-site.xml, the context automatically creates\n",
    "`metastore_db` in the current directory and creates `warehouse`\n",
    "directory indicated by HiveConf, which defaults to\n",
    "`/user/hive/warehouse`. Note that you may need to grant write privilege\n",
    "on `/user/hive/warehouse` to the user who starts the spark application.\n",
    "\n",
    "\\`\\`\\`scala val spark =\n",
    "SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\")\n",
    "spark.sql(\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt'\n",
    "INTO TABLE src\")\n",
    "\n",
    "// Queries are expressed in HiveQL spark.sql(\"FROM src SELECT key,\n",
    "value\").collect().foreach(println) \\`\\`\\`\n",
    "\n",
    "### Interacting with Different Versions of Hive Metastore\n",
    "\n",
    "One of the most important pieces of Spark SQL’s Hive support is\n",
    "interaction with Hive metastore, which enables Spark SQL to access\n",
    "metadata of Hive tables. Starting from Spark 1.4.0, a single binary\n",
    "build of Spark SQL can be used to query different versions of Hive\n",
    "metastores, using the configuration described below. Note that\n",
    "independent of the version of Hive that is being used to talk to the\n",
    "metastore, internally Spark SQL will compile against Hive 1.2.1 and use\n",
    "those classes for internal execution (serdes, UDFs, UDAFs, etc).\n",
    "\n",
    "The following options can be used to configure the version of Hive that\n",
    "is used to retrieve metadata:\n",
    "\n",
    "| Property Name | Default | Meaning | | --- | --- | --- | |\n",
    "`spark.sql.hive.metastore.version` | `1.2.1` | Version of the Hive\n",
    "metastore. Available options are `0.12.0` through `1.2.1`. | |\n",
    "`spark.sql.hive.metastore.jars` | `builtin` | Location of the jars that\n",
    "should be used to instantiate the HiveMetastoreClient. This property can\n",
    "be one of three options: `builtin`, `maven`, a classpath in the standard\n",
    "format for the JVM. This classpath must include all of Hive and its\n",
    "dependencies, including the correct version of Hadoop. These jars only\n",
    "need to be present on the driver, but if you are running in yarn cluster\n",
    "mode then you must ensure they are packaged with you application. | |\n",
    "`spark.sql.hive.metastore.sharedPrefixes` |\n",
    "`com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc` | A\n",
    "comma separated list of class prefixes that should be loaded using the\n",
    "classloader that is shared between Spark SQL and a specific version of\n",
    "Hive. An example of classes that should be shared is JDBC drivers that\n",
    "are needed to talk to the metastore. Other classes that need to be\n",
    "shared are those that interact with classes that are already shared. For\n",
    "example, custom appenders that are used by log4j. | |\n",
    "`spark.sql.hive.metastore.barrierPrefixes` | `(empty)` | A comma\n",
    "separated list of class prefixes that should explicitly be reloaded for\n",
    "each version of Hive that Spark SQL is communicating with. For example,\n",
    "Hive UDFs that are declared in a prefix that typically would be shared\n",
    "(i.e. `org.apache.spark.*`). |\n",
    "\n",
    "JDBC To Other Databases\n",
    "-----------------------\n",
    "\n",
    "Spark SQL also includes a data source that can read data from other\n",
    "databases using JDBC. This functionality should be preferred over using\n",
    "[JdbcRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.JdbcRDD).\n",
    "This is because the results are returned as a DataFrame and they can\n",
    "easily be processed in Spark SQL or joined with other data sources. The\n",
    "JDBC data source is also easier to use from Java or Python as it does\n",
    "not require the user to provide a ClassTag. (Note that this is different\n",
    "than the Spark SQL JDBC server, which allows other applications to run\n",
    "queries using Spark SQL).\n",
    "\n",
    "To get started you will need to include the JDBC driver for you\n",
    "particular database on the spark classpath. For example, to connect to\n",
    "postgres from the Spark Shell you would run the following command:\n",
    "\n",
    "`SPARK_CLASSPATH=postgresql-9.3-1102-jdbc41.jar bin/spark-shell`\n",
    "\n",
    "Tables from the remote database can be loaded as a DataFrame or Spark\n",
    "SQL Temporary table using the Data Sources API. The following options\n",
    "are supported:\n",
    "\n",
    "| Property Name | Meaning | | --- | --- | --- | | `url` | The JDBC URL\n",
    "to connect to. | | `dbtable` | The JDBC table that should be read. Note\n",
    "that anything that is valid in a `FROM` clause of a SQL query can be\n",
    "used. For example, instead of a full table you could also use a subquery\n",
    "in parentheses. | | `driver` | The class name of the JDBC driver needed\n",
    "to connect to this URL. This class will be loaded on the master and\n",
    "workers before running an JDBC commands to allow the driver to register\n",
    "itself with the JDBC subsystem. | |\n",
    "`partitionColumn, lowerBound, upperBound, numPartitions` | These options\n",
    "must all be specified if any of them is specified. They describe how to\n",
    "partition the table when reading in parallel from multiple workers.\n",
    "`partitionColumn` must be a numeric column from the table in question.\n",
    "Notice that `lowerBound` and `upperBound` are just used to decide the\n",
    "partition stride, not for filtering the rows in table. So all rows in\n",
    "the table will be partitioned and returned. | | `fetchSize` | The JDBC\n",
    "fetch size, which determines how many rows to fetch per round trip. This\n",
    "can help performance on JDBC drivers which default to low fetch size\n",
    "(eg. Oracle with 10 rows). |\n",
    "\n",
    "`// Example of using JDBC datasource val jdbcDF = spark.read.format(\"jdbc\").options(Map(\"url\" -> \"jdbc:postgresql:dbserver\", \"dbtable\" -> \"schema.tablename\")).load()`\n",
    "\n",
    "`-- Or using JDBC datasource in SQL CREATE TEMPORARY TABLE jdbcTable USING org.apache.spark.sql.jdbc OPTIONS (   url \"jdbc:postgresql:dbserver\",   dbtable \"schema.tablename\" )`\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "-   The JDBC driver class must be visible to the primordial class loader\n",
    "    on the client session and on all executors. This is because Java’s\n",
    "    DriverManager class does a security check that results in it\n",
    "    ignoring all drivers not visible to the primordial class loader when\n",
    "    one goes to open a connection. One convenient way to do this is to\n",
    "    modify compute\\_classpath.sh on all worker nodes to include your\n",
    "    driver JARs.\n",
    "-   Some databases, such as H2, convert all names to upper case. You’ll\n",
    "    need to use upper case to refer to those names in Spark SQL."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
