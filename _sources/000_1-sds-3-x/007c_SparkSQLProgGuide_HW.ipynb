{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ScaDaMaLe, Scalable Data Science and Distributed Machine Learning](https://lamastex.github.io/scalable-data-science/sds/3/x/)\n",
    "==============================================================================================================================\n",
    "\n",
    "This is an elaboration of the\n",
    "<http://spark.apache.org/docs/latest/sql-programming-guide.html> by Ivan\n",
    "Sadikov and Raazesh Sainudiin.\n",
    "\n",
    "Getting Started - Exercise\n",
    "==========================\n",
    "\n",
    "After having gone through the simple example dataset in the programming\n",
    "guide, let's try a slightly larger dataset next.\n",
    "\n",
    "Let us first create a table of social media usage from NYC\n",
    "\n",
    "> See the **Load Data** section to create this `social_media_usage`\n",
    "> table from raw data.\n",
    "\n",
    "First let's make sure this table is available for us. If you don't see\n",
    "`social_media_usage` as a `name`d table in the output of the next cell\n",
    "then we first need to ingest this dataset. Let's do it using the\n",
    "databricks' GUI for creating `Data` as done next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Let's find out what tables are already available for loading\n",
    "spark.catalog.listTables.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +----------------+--------+-----------+---------+-----------+\n",
    ">     |            name|database|description|tableType|isTemporary|\n",
    ">     +----------------+--------+-----------+---------+-----------+\n",
    ">     |sentimentlex_csv| default|       null| EXTERNAL|      false|\n",
    ">     |          people|    null|       null|TEMPORARY|       true|\n",
    ">     +----------------+--------+-----------+---------+-----------+\n",
    "\n",
    "  \n",
    "\n",
    "NYC Social Media Usage Data\n",
    "---------------------------\n",
    "\n",
    "This dataset is from\n",
    "<https://datahub.io/JohnSnowLabs/nyc-social-media-usage#readme>\n",
    "\n",
    "The Demographic Reports are produced by the Economic, Demographic and\n",
    "Statistical Research unit within the Countywide Service Integration and\n",
    "Planning Management (CSIPM) Division of the Fairfax County Department of\n",
    "Neighborhood and Community Services. Information produced by the\n",
    "Economic, Demographic and Statistical Research unit is used by every\n",
    "county department, board, authority and the Fairfax County Public\n",
    "Schools. In addition to the small area estimates and forecasts, state\n",
    "and federal data on Fairfax County are collected and summarized, and\n",
    "special studies and Quantitative research are conducted by the unit.\n",
    "\n",
    "We are going to fetch this data, with slightly simplified column names,\n",
    "from the following URL:\n",
    "\n",
    "-   http://lamastex.org/datasets/public/NYCUSA/social-media-usage.csv\n",
    "\n",
    "To turn the dataset into a registered table we will load it using the\n",
    "GUI as follows:\n",
    "\n",
    "-   Download it to your local machine / laptop and then use the 'Data'\n",
    "    button on the left to upload it (we will try this method now).\n",
    "    -   This will put your data in the `Filestore` in databricks'\n",
    "        distributed file system.\n",
    "\n",
    "### Overview\n",
    "\n",
    "Below we will show you how to create and query a table or DataFrame that\n",
    "you uploaded to DBFS.\n",
    "[DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html)\n",
    "is a Databricks File System (their distributed file system) that allows\n",
    "you to store data for querying inside of Databricks. This notebook\n",
    "assumes that you have a file already inside of DBFS that you would like\n",
    "to read from.\n",
    "\n",
    "In other setups, you can have the data in s3 (say in AWS) or in hdfs in\n",
    "your hadoop cluster, etc.\n",
    "\n",
    "Alternatively, you can use `curl` or `wget` to download it to the local\n",
    "file system in `/databricks/driver` and then load it into `dbfs`, after\n",
    "this you can use read it via `spark` session into a dataframe and\n",
    "register it as a hive table.\n",
    "\n",
    "You can also get the data directly from here (but in this case you need\n",
    "to change the column names in the databricks Data upload GUI or\n",
    "programmatically to follow this notebook):\n",
    "\n",
    "-   http://datahub.io/JohnSnowLabs/nyc-social-media-usage\n",
    "\n",
    "Load Data\n",
    "---------\n",
    "\n",
    "### How to uoload csv file and make a table in databricks\n",
    "\n",
    "Okay, so how did we actually make table `social_media_usage`? Databricks\n",
    "allows us to upload/link external data and make it available as\n",
    "registerd SQL table. It involves several steps:\n",
    "\n",
    "1.  Dowload this `social-media-usage.csv` file from the following URL to\n",
    "    your laptop:\n",
    "\n",
    "-   http://lamastex.org/datasets/public/NYCUSA/social-media-usage.csv\n",
    "\n",
    "-   Go to Databricks cloud (where you log in to use Databricks\n",
    "    notebooks) and open tab **Data** on the left panel\n",
    "\n",
    "-   On the very top of the left sub-menu you will see button **+Add\n",
    "    Data**, click on it\n",
    "\n",
    "-   Choose **Upload File** for Data Sources by **Browse** or **Drag and\n",
    "    Drop**, where **File** means any file (Parquet, Avro, CSV), but it\n",
    "    works the best with CSV format\n",
    "\n",
    "-   Upload `social-media-usage.csv` file you just downloaded to\n",
    "    databricks\n",
    "\n",
    "-   Just note the path to the uploaded file, for example in my case:\n",
    "\n",
    "    > File uploaded to `/FileStore/tables/social_media_usage.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "// File location and type\n",
    "// You may need to change the file_location \"social_media_usage-5dbee.csv\" depending on your location given by\n",
    "// File uploaded to /FileStore/tables/social_media_usage.csv\n",
    "val file_location = \"/FileStore/tables/social_media_usage.csv\"\n",
    "val file_type = \"csv\"\n",
    "\n",
    "// CSV options\n",
    "val infer_schema = \"true\"\n",
    "val first_row_is_header = \"true\"\n",
    "val delimiter = \",\"\n",
    "\n",
    "// The applied options are for CSV files. For other file types, these will be ignored.\n",
    "val socialMediaDF = spark.read.format(file_type) \n",
    "  .option(\"inferSchema\", infer_schema) \n",
    "  .option(\"header\", first_row_is_header) \n",
    "  .option(\"sep\", delimiter) \n",
    "  .load(file_location)\n",
    "\n",
    "socialMediaDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +------+----------+--------------------+----------+------+\n",
    ">     |agency|  platform|                 url|      date|visits|\n",
    ">     +------+----------+--------------------+----------+------+\n",
    ">     |   OEM|       SMS|                null|2012-02-17| 61652|\n",
    ">     |   OEM|       SMS|                null|2012-11-09| 44547|\n",
    ">     |   EDC|    Flickr|http://www.flickr...|2012-05-09|  null|\n",
    ">     | NYCHA|Newsletter|                null|2012-05-09|  null|\n",
    ">     |   DHS|   Twitter|www.twitter.com/n...|2012-06-13|   389|\n",
    ">     |   DHS|   Twitter|www.twitter.com/n...|2012-08-02|   431|\n",
    ">     |   DOH|   Android|       Condom Finder|2011-08-08|  5026|\n",
    ">     |   DOT|   Android|         You The Man|2011-08-08|  null|\n",
    ">     |  MOME|   Android|      MiNY Venor app|2011-08-08|   313|\n",
    ">     |   DOT|Broadcastr|                null|2011-08-08|  null|\n",
    ">     +------+----------+--------------------+----------+------+\n",
    ">     only showing top 10 rows\n",
    ">\n",
    ">     file_location: String = /FileStore/tables/social_media_usage.csv\n",
    ">     file_type: String = csv\n",
    ">     infer_schema: String = true\n",
    ">     first_row_is_header: String = true\n",
    ">     delimiter: String = ,\n",
    ">     socialMediaDF: org.apache.spark.sql.DataFrame = [agency: string, platform: string ... 3 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Let's create a view or table\n",
    "\n",
    "val temp_table_name = \"social_media_usage\"\n",
    "\n",
    "socialMediaDF.createOrReplaceTempView(temp_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     temp_table_name: String = social_media_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Let's find out what tables are already available for loading\n",
    "spark.catalog.listTables.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +------------------+--------+-----------+---------+-----------+\n",
    ">     |              name|database|description|tableType|isTemporary|\n",
    ">     +------------------+--------+-----------+---------+-----------+\n",
    ">     |  sentimentlex_csv| default|       null| EXTERNAL|      false|\n",
    ">     |            people|    null|       null|TEMPORARY|       true|\n",
    ">     |social_media_usage|    null|       null|TEMPORARY|       true|\n",
    ">     +------------------+--------+-----------+---------+-----------+\n",
    "\n",
    "  \n",
    "\n",
    "With this registered as a temporary view, `social_media_usage` will only\n",
    "be available to this particular notebook.\n",
    "\n",
    "If you'd like other users to be able to query this table (in the\n",
    "databricks professional shard - not the free community edition; or in a\n",
    "managed on-premise cluster), you can also create a table from the\n",
    "DataFrame.\n",
    "\n",
    "Once saved, this table will persist across cluster restarts as well as\n",
    "allow various users across different notebooks to query this data. To do\n",
    "so, choose your table name and use `saveAsTable` as done in the next\n",
    "cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val permanent_table_name = \"social_media_usage\"\n",
    "socialMediaDF.write.format(\"parquet\").saveAsTable(permanent_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     permanent_table_name: String = social_media_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Let's find out what tables are already available for loading\n",
    "// spark.catalog.listTables.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "It looks like the table `social_media_usage` is available as a permanent\n",
    "table (`isTemporary` set as `false`), if you have not uncommented the\n",
    "last line in the previous cell (otherwise it will be available from a\n",
    "parquet file as a permanent table - we will see more about parquet in\n",
    "the sequel).\n",
    "\n",
    "Next let us do the following:\n",
    "\n",
    "-   load this table as a DataFrame (yes, the dataframe already exists as\n",
    "    `socialMediaDF`, but we want to make a new DataFrame directly from\n",
    "    the table)\n",
    "-   print its schema and\n",
    "-   show the first 20 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +------------------+--------+-----------+---------+-----------+\n",
    ">     |              name|database|description|tableType|isTemporary|\n",
    ">     +------------------+--------+-----------+---------+-----------+\n",
    ">     |  sentimentlex_csv| default|       null| EXTERNAL|      false|\n",
    ">     |            people|    null|       null|TEMPORARY|       true|\n",
    ">     |social_media_usage|    null|       null|TEMPORARY|       true|\n",
    ">     +------------------+--------+-----------+---------+-----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.table(\"social_media_usage\") // Ctrl+Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     df: org.apache.spark.sql.DataFrame = [agency: string, platform: string ... 3 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "As you can see the immutable value `df` is a DataFrame and more\n",
    "specifically it is:\n",
    "\n",
    "> `org.apache.spark.sql.DataFrame = [agency: string, platform: string, url: string, date: timestamp, visits: integer]`.\n",
    "\n",
    "Now let us print schema of the DataFrame `df` and have a look at the\n",
    "actual data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Ctrl+Enter\n",
    "df.printSchema() // prints schema of the DataFrame\n",
    "df.show() // shows first n (default is 20) rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     root\n",
    ">      |-- agency: string (nullable = true)\n",
    ">      |-- platform: string (nullable = true)\n",
    ">      |-- url: string (nullable = true)\n",
    ">      |-- date: string (nullable = true)\n",
    ">      |-- visits: integer (nullable = true)\n",
    ">\n",
    ">     +----------+----------+--------------------+----------+------+\n",
    ">     |    agency|  platform|                 url|      date|visits|\n",
    ">     +----------+----------+--------------------+----------+------+\n",
    ">     |       OEM|       SMS|                null|2012-02-17| 61652|\n",
    ">     |       OEM|       SMS|                null|2012-11-09| 44547|\n",
    ">     |       EDC|    Flickr|http://www.flickr...|2012-05-09|  null|\n",
    ">     |     NYCHA|Newsletter|                null|2012-05-09|  null|\n",
    ">     |       DHS|   Twitter|www.twitter.com/n...|2012-06-13|   389|\n",
    ">     |       DHS|   Twitter|www.twitter.com/n...|2012-08-02|   431|\n",
    ">     |       DOH|   Android|       Condom Finder|2011-08-08|  5026|\n",
    ">     |       DOT|   Android|         You The Man|2011-08-08|  null|\n",
    ">     |      MOME|   Android|      MiNY Venor app|2011-08-08|   313|\n",
    ">     |       DOT|Broadcastr|                null|2011-08-08|  null|\n",
    ">     |       DPR|Broadcastr|http://beta.broad...|2011-08-08|  null|\n",
    ">     |     ENDHT|  Facebook|http://www.facebo...|2011-08-08|     3|\n",
    ">     |       VAC|  Facebook|https://www.faceb...|2011-08-08|    36|\n",
    ">     |    PlaNYC|  Facebook|http://www.facebo...|2011-08-08|    47|\n",
    ">     |      DFTA|  Facebook|http://www.facebo...|2011-08-08|    90|\n",
    ">     | energyNYC|  Facebook|http://www.facebo...|2011-08-08|   105|\n",
    ">     |      MOIA|  Facebook|http://www.facebo...|2011-08-08|   123|\n",
    ">     |City Store|  Facebook|http://www.facebo...|2011-08-08|   119|\n",
    ">     |      OCDV|  Facebook|http://www.facebo...|2011-08-08|   148|\n",
    ">     |       HIA|  Facebook|http://www.facebo...|2011-08-08|   197|\n",
    ">     +----------+----------+--------------------+----------+------+\n",
    ">     only showing top 20 rows\n",
    "\n",
    "  \n",
    "\n",
    "> Note that `(nullable = true)` simply means if the value is allowed to\n",
    "> be `null`.\n",
    "\n",
    "Let us count the number of rows in `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count() // Ctrl+Enter to get 5898"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res16: Long = 5898\n",
    "\n",
    "  \n",
    "\n",
    "So there are 5899 records or rows in the DataFrame `df`. Pretty good!\n",
    "You can also select individual columns using so-called DataFrame API, as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val platforms = df.select(\"platform\") // Shift+Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     platforms: org.apache.spark.sql.DataFrame = [platform: string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platforms.count() // Shift+Enter to count the number of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res18: Long = 5898"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platforms.show(5) // Ctrl+Enter to show top 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +----------+\n",
    ">     |  platform|\n",
    ">     +----------+\n",
    ">     |       SMS|\n",
    ">     |       SMS|\n",
    ">     |    Flickr|\n",
    ">     |Newsletter|\n",
    ">     |   Twitter|\n",
    ">     +----------+\n",
    ">     only showing top 5 rows\n",
    "\n",
    "  \n",
    "\n",
    "You can also apply `.distinct()` to extract only unique entries as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val uniquePlatforms = df.select(\"platform\").distinct() // Shift+Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     uniquePlatforms: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [platform: string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquePlatforms.count() // Ctrl+Enter to count the number of distinct platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res21: Long = 23\n",
    "\n",
    "  \n",
    "\n",
    "Let's see all the rows of the DataFrame `uniquePlatforms`.\n",
    "\n",
    "> Note that `display(uniquePlatforms)` unlike `uniquePlatforms.show()`\n",
    "> displays all rows of the DataFrame + gives you ability to select\n",
    "> different view, e.g. charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(uniquePlatforms) // Ctrl+Enter to show all rows; use the scroll-bar on the right of the display to see all platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "  \n",
    "\n",
    "### Spark SQL and DataFrame API\n",
    "\n",
    "Spark SQL provides DataFrame API that can perform relational operations\n",
    "on both external data sources and internal collections, which is similar\n",
    "to widely used data frame concept in R, but evaluates operations support\n",
    "lazily (remember RDDs?), so that it can perform relational\n",
    "optimizations. This API is also available in Java, Python and R, but\n",
    "some functionality may not be available, although with every release of\n",
    "Spark people minimize this gap.\n",
    "\n",
    "So we give some examples how to query data in Python and R, but continue\n",
    "with Scala. You can do all DataFrame operations in this notebook using\n",
    "Python or R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ctrl+Enter to evaluate this python cell, recall '#' is the pre-comment character in python\n",
    "# Using Python to query our \"social_media_usage\" table\n",
    "pythonDF = spark.table(\"social_media_usage\").select(\"platform\").distinct()\n",
    "pythonDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +--------+\n",
    ">     |platform|\n",
    ">     +--------+\n",
    ">     |  Flickr|\n",
    ">     |     SMS|\n",
    ">     | Twitter|\n",
    ">     +--------+\n",
    ">     only showing top 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Ctrl+Enter to achieve the same result using standard SQL syntax!\n",
    "select distinct platform from social_media_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "  \n",
    "\n",
    "Now it is time for some tips around how you use `select` and what the\n",
    "difference is between `$\"a\"`, `col(\"a\")`, `df(\"a\")`.\n",
    "\n",
    "As you probably have noticed by now, you can specify individual columns\n",
    "to select by providing String values in select statement. But sometimes\n",
    "you need to: - distinguish between columns with the same name - use it\n",
    "to filter (actually you can still filter using full String expression) -\n",
    "do some \"magic\" with joins and user-defined functions (this will be\n",
    "shown later)\n",
    "\n",
    "So Spark gives you ability to actually specify columns when you select.\n",
    "Now the difference between all those three notations is ... none, those\n",
    "things are just aliases for a `Column` in Spark SQL, which means\n",
    "following expressions yield the same result:\n",
    "\n",
    "\\`\\`\\` // Using string expressions df.select(\"agency\", \"visits\")\n",
    "\n",
    "// Using \"$\" alias for column df.select($\"agency\", $\"visits\")\n",
    "\n",
    "// Using \"col\" alias for column df.select(col(\"agency\"), col(\"visits\"))\n",
    "\n",
    "// Using DataFrame name for column df.select(df(\"agency\"), df(\"visits\"))\n",
    "\\`\\`\\`\n",
    "\n",
    "This \"same-difference\" applies to filtering, i.e. you can either use\n",
    "full expression to filter, or column as shown in the following example:\n",
    "\n",
    "\\`\\`\\` // Using column to filter df.select(\"visits\").filter($\"visits\"\n",
    "&gt; 100)\n",
    "\n",
    "// Or you can use full expression as string\n",
    "df.select(\"visits\").filter(\"visits &gt; 100\") \\`\\`\\`\n",
    "\n",
    "> Note that `$\"visits\" > 100` expression looks amazing, but under the\n",
    "> hood it is just another column, and it equals to\n",
    "> `df(\"visits\").>(100)`, where, thanks to Scala paradigm `>` is just\n",
    "> another function that you can define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sms = df.select($\"agency\", $\"platform\", $\"visits\").filter($\"platform\" === \"SMS\")\n",
    "sms.show() // Ctrl+Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +------+--------+------+\n",
    ">     |agency|platform|visits|\n",
    ">     +------+--------+------+\n",
    ">     |   OEM|     SMS| 61652|\n",
    ">     |   OEM|     SMS| 44547|\n",
    ">     |   DOE|     SMS|   382|\n",
    ">     | NYCHA|     SMS|  null|\n",
    ">     |   OEM|     SMS| 61652|\n",
    ">     |   DOE|     SMS|   382|\n",
    ">     | NYCHA|     SMS|  null|\n",
    ">     |   OEM|     SMS| 61652|\n",
    ">     |   OEM|     SMS|  null|\n",
    ">     |   DOE|     SMS|  null|\n",
    ">     | NYCHA|     SMS|  null|\n",
    ">     |   OEM|     SMS|  null|\n",
    ">     |   DOE|     SMS|  null|\n",
    ">     | NYCHA|     SMS|  null|\n",
    ">     |   DOE|     SMS|   382|\n",
    ">     | NYCHA|     SMS|  null|\n",
    ">     |   OEM|     SMS| 61652|\n",
    ">     |   DOE|     SMS|   382|\n",
    ">     | NYCHA|     SMS|  null|\n",
    ">     |   OEM|     SMS| 61652|\n",
    ">     +------+--------+------+\n",
    ">     only showing top 20 rows\n",
    ">\n",
    ">     sms: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [agency: string, platform: string ... 1 more field]\n",
    "\n",
    "  \n",
    "\n",
    "Again you could have written the query above using any column aliases or\n",
    "String names or even writing the query directly.\n",
    "\n",
    "For example, we can do it using String names, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Ctrl+Enter Note that we are using \"platform = 'SMS'\" since it will be evaluated as actual SQL\n",
    "val sms = df.select(df(\"agency\"), df(\"platform\"), df(\"visits\")).filter(\"platform = 'SMS'\")\n",
    "sms.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +------+--------+------+\n",
    ">     |agency|platform|visits|\n",
    ">     +------+--------+------+\n",
    ">     |   OEM|     SMS| 61652|\n",
    ">     |   OEM|     SMS| 44547|\n",
    ">     |   DOE|     SMS|   382|\n",
    ">     | NYCHA|     SMS|  null|\n",
    ">     |   OEM|     SMS| 61652|\n",
    ">     +------+--------+------+\n",
    ">     only showing top 5 rows\n",
    ">\n",
    ">     sms: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [agency: string, platform: string ... 1 more field]\n",
    "\n",
    "  \n",
    "\n",
    "Refer to the [DataFrame\n",
    "API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame)\n",
    "for more detailed API. In addition to simple column references and\n",
    "expressions, DataFrames also have a rich library of functions including\n",
    "string manipulation, date arithmetic, common math operations and more.\n",
    "The complete list is available in the [DataFrame Function\n",
    "Reference](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$).\n",
    "\n",
    "Let's next explore some of the functionality that is available by\n",
    "transforming this DataFrame `df` into a new DataFrame called `fixedDF`.\n",
    "\n",
    "-   First, note that some columns are not exactly what we want them to\n",
    "    be.\n",
    "    -   visits should not contain null values, but `0`s instead.\n",
    "-   Let us fix it using some code that is briefly explained here (don't\n",
    "    worry if you don't get it completely now, you will get the hang of\n",
    "    it by playing more)\n",
    "    -   The `coalesce` function is similar to `if-else` statement, i.e.,\n",
    "        if first column in expression is `null`, then the value of the\n",
    "        second column is used and so on.\n",
    "    -   `lit` just means column of constant value (`lit`erally\n",
    "        speaking!).\n",
    "    -   we also remove `TOTAL` value from `platform` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Ctrl+Enter to make fixedDF\n",
    "\n",
    "// import the needed sql functions\n",
    "import org.apache.spark.sql.functions.{coalesce, lit}\n",
    "\n",
    "// make the fixedDF DataFrame\n",
    "val fixedDF = df.\n",
    "   select(\n",
    "     $\"agency\", \n",
    "     $\"platform\", \n",
    "     $\"url\", \n",
    "     $\"date\", \n",
    "     coalesce($\"visits\", lit(0)).as(\"visits\"))\n",
    "    .filter($\"platform\" =!= \"TOTAL\")\n",
    "\n",
    "fixedDF.printSchema() // print its schema \n",
    "// and show the top 20 records fully\n",
    "fixedDF.show(false) // the false argument does not truncate the rows, so you will not see something like this \"anot...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     root\n",
    ">      |-- agency: string (nullable = true)\n",
    ">      |-- platform: string (nullable = true)\n",
    ">      |-- url: string (nullable = true)\n",
    ">      |-- date: string (nullable = true)\n",
    ">      |-- visits: integer (nullable = false)\n",
    ">\n",
    ">     +----------+----------+---------------------------------------------------------------------------------------+----------+------+\n",
    ">     |agency    |platform  |url                                                                                    |date      |visits|\n",
    ">     +----------+----------+---------------------------------------------------------------------------------------+----------+------+\n",
    ">     |OEM       |SMS       |null                                                                                   |2012-02-17|61652 |\n",
    ">     |OEM       |SMS       |null                                                                                   |2012-11-09|44547 |\n",
    ">     |EDC       |Flickr    |http://www.flickr.com/nycedc                                                           |2012-05-09|0     |\n",
    ">     |NYCHA     |Newsletter|null                                                                                   |2012-05-09|0     |\n",
    ">     |DHS       |Twitter   |www.twitter.com/nycdhs                                                                 |2012-06-13|389   |\n",
    ">     |DHS       |Twitter   |www.twitter.com/nycdhs                                                                 |2012-08-02|431   |\n",
    ">     |DOH       |Android   |Condom Finder                                                                          |2011-08-08|5026  |\n",
    ">     |DOT       |Android   |You The Man                                                                            |2011-08-08|0     |\n",
    ">     |MOME      |Android   |MiNY Venor app                                                                         |2011-08-08|313   |\n",
    ">     |DOT       |Broadcastr|null                                                                                   |2011-08-08|0     |\n",
    ">     |DPR       |Broadcastr|http://beta.broadcastr.com/Echo.html?audioId=670026-4001                               |2011-08-08|0     |\n",
    ">     |ENDHT     |Facebook  |http://www.facebook.com/pages/NYC-Lets-End-Human-Trafficking/125730490795659?sk=wall   |2011-08-08|3     |\n",
    ">     |VAC       |Facebook  |https://www.facebook.com/pages/NYC-Voter-Assistance-Commission/110226709012110         |2011-08-08|36    |\n",
    ">     |PlaNYC    |Facebook  |http://www.facebook.com/pages/New-York-NY/PlaNYC/160454173971169?ref=ts                |2011-08-08|47    |\n",
    ">     |DFTA      |Facebook  |http://www.facebook.com/pages/NYC-Department-for-the-Aging/109028655823590             |2011-08-08|90    |\n",
    ">     |energyNYC |Facebook  |http://www.facebook.com/EnergyNYC?sk=wall                                              |2011-08-08|105   |\n",
    ">     |MOIA      |Facebook  |http://www.facebook.com/ihwnyc                                                         |2011-08-08|123   |\n",
    ">     |City Store|Facebook  |http://www.facebook.com/citystorenyc                                                   |2011-08-08|119   |\n",
    ">     |OCDV      |Facebook  |http://www.facebook.com/pages/NYC-Healthy-Relationship-Training-Academy/134637829901065|2011-08-08|148   |\n",
    ">     |HIA       |Facebook  |http://www.facebook.com/pages/New-York-City-Health-Insurance-Link/145920551598         |2011-08-08|197   |\n",
    ">     +----------+----------+---------------------------------------------------------------------------------------+----------+------+\n",
    ">     only showing top 20 rows\n",
    ">\n",
    ">     import org.apache.spark.sql.functions.{coalesce, lit}\n",
    ">     fixedDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [agency: string, platform: string ... 3 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "Okay, this is better, but `url`s are still inconsistent.\n",
    "\n",
    "Let's fix this by writing our own UDF (user-defined function) to deal\n",
    "with special cases.\n",
    "\n",
    "Note that if you **CAN USE Spark functions library**, do it. But for the\n",
    "sake of the example, custom UDF is shown below.\n",
    "\n",
    "We take value of a column as String type and return the same String\n",
    "type, but ignore values that do not start with `http`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Ctrl+Enter to evaluate this UDF which takes a input String called \"value\"\n",
    "// and converts it into lower-case if it begins with http and otherwise leaves it as null, so we sort of remove non valid web-urls\n",
    "val cleanUrl = udf((value: String) => if (value != null && value.startsWith(\"http\")) value.toLowerCase() else null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     cleanUrl: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$6230/869691273@375f4166,StringType,List(Some(class[value[0]: string])),None,true,true)\n",
    "\n",
    "  \n",
    "\n",
    "Let us apply our UDF on `fixedDF` to create a new DataFrame called\n",
    "`cleanedDF` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Ctrl+Enter\n",
    "val cleanedDF = fixedDF.select($\"agency\", $\"platform\", cleanUrl($\"url\").as(\"url\"), $\"date\", $\"visits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     cleanedDF: org.apache.spark.sql.DataFrame = [agency: string, platform: string ... 3 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "Now, let's check that it actually worked by seeing the first 5 rows of\n",
    "the `cleanedDF` whose `url` `isNull` and `isNotNull`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Shift+Enter\n",
    "cleanedDF.filter($\"url\".isNull).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +------+----------+----+----------+------+\n",
    ">     |agency|  platform| url|      date|visits|\n",
    ">     +------+----------+----+----------+------+\n",
    ">     |   OEM|       SMS|null|2012-02-17| 61652|\n",
    ">     |   OEM|       SMS|null|2012-11-09| 44547|\n",
    ">     | NYCHA|Newsletter|null|2012-05-09|     0|\n",
    ">     |   DHS|   Twitter|null|2012-06-13|   389|\n",
    ">     |   DHS|   Twitter|null|2012-08-02|   431|\n",
    ">     +------+----------+----+----------+------+\n",
    ">     only showing top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Ctrl+Enter\n",
    "cleanedDF.filter($\"url\".isNotNull).show(5, false) // false in .show(5, false) shows rows untruncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +------+----------+------------------------------------------------------------------------------------+----------+------+\n",
    ">     |agency|platform  |url                                                                                 |date      |visits|\n",
    ">     +------+----------+------------------------------------------------------------------------------------+----------+------+\n",
    ">     |EDC   |Flickr    |http://www.flickr.com/nycedc                                                        |2012-05-09|0     |\n",
    ">     |DPR   |Broadcastr|http://beta.broadcastr.com/echo.html?audioid=670026-4001                            |2011-08-08|0     |\n",
    ">     |ENDHT |Facebook  |http://www.facebook.com/pages/nyc-lets-end-human-trafficking/125730490795659?sk=wall|2011-08-08|3     |\n",
    ">     |VAC   |Facebook  |https://www.facebook.com/pages/nyc-voter-assistance-commission/110226709012110      |2011-08-08|36    |\n",
    ">     |PlaNYC|Facebook  |http://www.facebook.com/pages/new-york-ny/planyc/160454173971169?ref=ts             |2011-08-08|47    |\n",
    ">     +------+----------+------------------------------------------------------------------------------------+----------+------+\n",
    ">     only showing top 5 rows\n",
    "\n",
    "  \n",
    "\n",
    "Now there is a suggestion from you manager's manager's manager that due\n",
    "to some perceived privacy concerns we want to replace `agency` with some\n",
    "unique identifier.\n",
    "\n",
    "So we need to do the following:\n",
    "\n",
    "-   create unique list of agencies with ids and\n",
    "-   join them with main DataFrame.\n",
    "\n",
    "Sounds easy, right? Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Crtl+Enter\n",
    "// Import Spark SQL function that will give us unique id across all the records in this DataFrame\n",
    "import org.apache.spark.sql.functions.monotonically_increasing_id\n",
    "\n",
    "// We append column as SQL function that creates unique ids across all records in DataFrames \n",
    "val agencies = cleanedDF.select(cleanedDF(\"agency\"))\n",
    "                        .distinct()\n",
    "                        .withColumn(\"id\", monotonically_increasing_id())\n",
    "agencies.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +--------------------+-----------+\n",
    ">     |              agency|         id|\n",
    ">     +--------------------+-----------+\n",
    ">     |              PlaNYC|34359738368|\n",
    ">     |                 HIA|34359738369|\n",
    ">     |NYC Digital: exte...|34359738370|\n",
    ">     |           NYCGLOBAL|42949672960|\n",
    ">     |              nycgov|68719476736|\n",
    ">     +--------------------+-----------+\n",
    ">     only showing top 5 rows\n",
    ">\n",
    ">     import org.apache.spark.sql.functions.monotonically_increasing_id\n",
    ">     agencies: org.apache.spark.sql.DataFrame = [agency: string, id: bigint]\n",
    "\n",
    "  \n",
    "\n",
    "Those who want to understand left/right inner/outer joins can see the\n",
    "video lectures in Module 3 of Anthony Joseph's Introduction to Big data\n",
    "edX course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Ctrl+Enter\n",
    "// And join with the rest of the data, note how join condition is specified \n",
    "val anonym = cleanedDF.join(agencies, cleanedDF(\"agency\") === agencies(\"agency\"), \"inner\").select(\"id\", \"platform\", \"url\", \"date\", \"visits\")\n",
    "\n",
    "// We also cache DataFrame since it can be quite expensive to recompute join\n",
    "anonym.cache()\n",
    "\n",
    "// Display result\n",
    "anonym.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +-------------+----------+--------------------+----------+------+\n",
    ">     |           id|  platform|                 url|      date|visits|\n",
    ">     +-------------+----------+--------------------+----------+------+\n",
    ">     |1580547964928|       SMS|                null|2012-02-17| 61652|\n",
    ">     |1580547964928|       SMS|                null|2012-11-09| 44547|\n",
    ">     | 412316860416|    Flickr|http://www.flickr...|2012-05-09|     0|\n",
    ">     |1649267441664|Newsletter|                null|2012-05-09|     0|\n",
    ">     |1529008357376|   Twitter|                null|2012-06-13|   389|\n",
    ">     +-------------+----------+--------------------+----------+------+\n",
    ">     only showing top 5 rows\n",
    ">\n",
    ">     anonym: org.apache.spark.sql.DataFrame = [id: bigint, platform: string ... 3 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables().show() // look at the available tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +------------------+--------+-----------+---------+-----------+\n",
    ">     |              name|database|description|tableType|isTemporary|\n",
    ">     +------------------+--------+-----------+---------+-----------+\n",
    ">     |  sentimentlex_csv| default|       null| EXTERNAL|      false|\n",
    ">     |            people|    null|       null|TEMPORARY|       true|\n",
    ">     |social_media_usage|    null|       null|TEMPORARY|       true|\n",
    ">     +------------------+--------+-----------+---------+-----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- to remove a TempTable if it exists already\n",
    "drop table if exists anonym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Register table for Spark SQL, we also import \"month\" function \n",
    "import org.apache.spark.sql.functions.month\n",
    "\n",
    "anonym.createOrReplaceTempView(\"anonym\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql.functions.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Interesting. Now let's do some aggregation. Display platform, month, visits\n",
    "-- Date column allows us to extract month directly\n",
    "\n",
    "select platform, month(date) as month, sum(visits) as visits from anonym group by platform, month(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "Note, that we could have done aggregation using DataFrame API instead of\n",
    "Spark SQL.\n",
    "\n",
    "Alright, now let's see some *cool* operations with window functions.\n",
    "\n",
    "Our next task is to compute `(daily visits / monthly average)` for all\n",
    "platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{dayofmonth, month, row_number, sum}\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val coolDF = anonym.select($\"id\", $\"platform\", dayofmonth($\"date\").as(\"day\"), month($\"date\").as(\"month\"), $\"visits\").\n",
    "  groupBy($\"id\", $\"platform\", $\"day\", $\"month\").agg(sum(\"visits\").as(\"visits\"))\n",
    "\n",
    "// Run window aggregation on visits per month and platform\n",
    "val window = coolDF.select($\"id\", $\"day\", $\"visits\", sum($\"visits\").over(Window.partitionBy(\"platform\", \"month\")).as(\"monthly_visits\"))\n",
    "\n",
    "// Create and register percent table\n",
    "val percent = window.select($\"id\", $\"day\", ($\"visits\" / $\"monthly_visits\").as(\"percent\"))\n",
    "\n",
    "percent.createOrReplaceTempView(\"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql.functions.{dayofmonth, month, row_number, sum}\n",
    ">     import org.apache.spark.sql.expressions.Window\n",
    ">     coolDF: org.apache.spark.sql.DataFrame = [id: bigint, platform: string ... 3 more fields]\n",
    ">     window: org.apache.spark.sql.DataFrame = [id: bigint, day: int ... 2 more fields]\n",
    ">     percent: org.apache.spark.sql.DataFrame = [id: bigint, day: int ... 1 more field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- A little bit of visualization as result of our efforts\n",
    "select id, day, `percent` from percent where `percent` > 0.3 and day = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- You also could just use plain SQL to write query above, note that you might need to group by id and day as well.\n",
    "with aggr as (\n",
    "  select id, dayofmonth(date) as day, visits / sum(visits) over (partition by (platform, month(date))) as percent\n",
    "  from anonym\n",
    ")\n",
    "select * from aggr where day = 2 and percent > 0.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "  \n",
    "\n",
    "Interoperating with RDDs\n",
    "------------------------\n",
    "\n",
    "Spark SQL supports two different methods for converting existing RDDs\n",
    "into DataFrames. The first method uses reflection to infer the schema of\n",
    "an RDD that contains specific types of objects. This reflection based\n",
    "approach leads to more concise code and works well when you already know\n",
    "the schema.\n",
    "\n",
    "The second method for creating DataFrames is through a programmatic\n",
    "interface that allows you to construct a schema and then apply it to an\n",
    "existing RDD. While this method is more verbose, it allows you to\n",
    "construct DataFrames when the columns and their types are not known\n",
    "until runtime.\n",
    "\n",
    "### Inferring the Schema Using Reflection\n",
    "\n",
    "The Scala interface for Spark SQL supports automatically converting an\n",
    "RDD containing case classes to a DataFrame. The case class defines the\n",
    "schema of the table. The names of the arguments to the case class are\n",
    "read using reflection and become the names of the columns. Case classes\n",
    "can also be nested or contain complex types such as Sequences or Arrays.\n",
    "This RDD can be implicitly converted to a DataFrame and then be\n",
    "registered as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Define case class that will be our schema for DataFrame\n",
    "case class Hubot(name: String, year: Int, manufacturer: String, version: Array[Int], details: Map[String, String])\n",
    "\n",
    "// You can process a text file, for example, to convert every row to our Hubot, but we will create RDD manually\n",
    "val rdd = sc.parallelize(\n",
    "  Array(\n",
    "    Hubot(\"Jerry\", 2015, \"LCorp\", Array(1, 2, 3), Map(\"eat\" -> \"yes\", \"sleep\" -> \"yes\", \"drink\" -> \"yes\")),\n",
    "    Hubot(\"Mozart\", 2010, \"LCorp\", Array(1, 2), Map(\"eat\" -> \"no\", \"sleep\" -> \"no\", \"drink\" -> \"no\")),\n",
    "    Hubot(\"Einstein\", 2012, \"LCorp\", Array(1, 2, 3), Map(\"eat\" -> \"yes\", \"sleep\" -> \"yes\", \"drink\" -> \"no\"))\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     defined class Hubot\n",
    ">     rdd: org.apache.spark.rdd.RDD[Hubot] = ParallelCollectionRDD[136] at parallelize at command-1267216879634399:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// In order to convert RDD into DataFrame you need to do this:\n",
    "val hubots = rdd.toDF()\n",
    "\n",
    "// Display DataFrame, note how array and map fields are displayed\n",
    "hubots.printSchema()\n",
    "hubots.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     root\n",
    ">      |-- name: string (nullable = true)\n",
    ">      |-- year: integer (nullable = false)\n",
    ">      |-- manufacturer: string (nullable = true)\n",
    ">      |-- version: array (nullable = true)\n",
    ">      |    |-- element: integer (containsNull = false)\n",
    ">      |-- details: map (nullable = true)\n",
    ">      |    |-- key: string\n",
    ">      |    |-- value: string (valueContainsNull = true)\n",
    ">\n",
    ">     +--------+----+------------+---------+--------------------+\n",
    ">     |    name|year|manufacturer|  version|             details|\n",
    ">     +--------+----+------------+---------+--------------------+\n",
    ">     |   Jerry|2015|       LCorp|[1, 2, 3]|[eat -> yes, slee...|\n",
    ">     |  Mozart|2010|       LCorp|   [1, 2]|[eat -> no, sleep...|\n",
    ">     |Einstein|2012|       LCorp|[1, 2, 3]|[eat -> yes, slee...|\n",
    ">     +--------+----+------------+---------+--------------------+\n",
    ">\n",
    ">     hubots: org.apache.spark.sql.DataFrame = [name: string, year: int ... 3 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// You can query complex type the same as you query any other column\n",
    "// By the way you can use `sql` function to invoke Spark SQL to create DataFrame\n",
    "hubots.createOrReplaceTempView(\"hubots\")\n",
    "\n",
    "val onesThatEat = sqlContext.sql(\"select name, details.eat from hubots where details.eat = 'yes'\")\n",
    "\n",
    "onesThatEat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +--------+---+\n",
    ">     |    name|eat|\n",
    ">     +--------+---+\n",
    ">     |   Jerry|yes|\n",
    ">     |Einstein|yes|\n",
    ">     +--------+---+\n",
    ">\n",
    ">     onesThatEat: org.apache.spark.sql.DataFrame = [name: string, eat: string]\n",
    "\n",
    "  \n",
    "\n",
    "### Programmatically Specifying the Schema\n",
    "\n",
    "When case classes cannot be defined ahead of time (for example, the\n",
    "structure of records is encoded in a string, or a text dataset will be\n",
    "parsed and fields will be projected differently for different users), a\n",
    "`DataFrame` can be created programmatically with three steps.\n",
    "\n",
    "1.  Create an RDD of `Row`s from the original RDD\n",
    "2.  Create the schema represented by a\n",
    "    [StructType](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.types.StructType)\n",
    "    and\n",
    "    [StructField](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.types.StructField)\n",
    "    classes matching the structure of `Row`s in the RDD created in\n",
    "    Step 1.\n",
    "3.  Apply the schema to the RDD of `Row`s via `createDataFrame` method\n",
    "    provided by `SQLContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// Let's say we have an RDD of String and we need to convert it into a DataFrame with schema \"name\", \"year\", and \"manufacturer\"\n",
    "// As you can see every record is space-separated.\n",
    "val rdd = sc.parallelize(\n",
    "  Array(\n",
    "    \"Jerry 2015 LCorp\",\n",
    "    \"Mozart 2010 LCorp\",\n",
    "    \"Einstein 2012 LCorp\"\n",
    "  )\n",
    ")\n",
    "\n",
    "// Create schema as StructType //\n",
    "val schema = StructType(\n",
    "  StructField(\"name\", StringType, false) :: \n",
    "  StructField(\"year\", IntegerType, false) :: \n",
    "  StructField(\"manufacturer\", StringType, false) :: \n",
    "  Nil\n",
    ")\n",
    "\n",
    "// Prepare RDD[Row]\n",
    "val rows = rdd.map { entry => \n",
    "  val arr = entry.split(\"\\\\s+\")\n",
    "  val name = arr(0)\n",
    "  val year = arr(1).toInt\n",
    "  val manufacturer = arr(2)\n",
    "  \n",
    "  Row(name, year, manufacturer)\n",
    "}\n",
    "\n",
    "// Create DataFrame\n",
    "val bots = sqlContext.createDataFrame(rows, schema)\n",
    "bots.printSchema()\n",
    "bots.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     root\n",
    ">      |-- name: string (nullable = false)\n",
    ">      |-- year: integer (nullable = false)\n",
    ">      |-- manufacturer: string (nullable = false)\n",
    ">\n",
    ">     +--------+----+------------+\n",
    ">     |    name|year|manufacturer|\n",
    ">     +--------+----+------------+\n",
    ">     |   Jerry|2015|       LCorp|\n",
    ">     |  Mozart|2010|       LCorp|\n",
    ">     |Einstein|2012|       LCorp|\n",
    ">     +--------+----+------------+\n",
    ">\n",
    ">     import org.apache.spark.sql.types._\n",
    ">     rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[141] at parallelize at command-1267216879634403:5\n",
    ">     schema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,false), StructField(year,IntegerType,false), StructField(manufacturer,StringType,false))\n",
    ">     rows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[142] at map at command-1267216879634403:22\n",
    ">     bots: org.apache.spark.sql.DataFrame = [name: string, year: int ... 1 more field]\n",
    "\n",
    "  \n",
    "\n",
    "Creating Datasets\n",
    "-----------------\n",
    "\n",
    "A\n",
    "[Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)\n",
    "is a strongly-typed, immutable collection of objects that are mapped to\n",
    "a relational schema. At the core of the Dataset API is a new concept\n",
    "called an\n",
    "[encoder](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder),\n",
    "which is responsible for converting between JVM objects and tabular\n",
    "representation. The tabular representation is stored using Spark’s\n",
    "internal Tungsten binary format, allowing for operations on serialized\n",
    "data and improved memory utilization. Spark 2.2 comes with support for\n",
    "automatically generating encoders for a wide variety of types, including\n",
    "primitive types (e.g. String, Integer, Long), and Scala case classes.\n",
    "\n",
    "> Simply put, you will get all the benefits of DataFrames with fair\n",
    "> amount of flexibility of RDD API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// We can start working with Datasets by using our \"hubots\" table\n",
    "\n",
    "// To create Dataset from DataFrame do this (assuming that case class Hubot exists):\n",
    "val ds = hubots.as[Hubot]\n",
    "ds.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +--------+----+------------+---------+--------------------+\n",
    ">     |    name|year|manufacturer|  version|             details|\n",
    ">     +--------+----+------------+---------+--------------------+\n",
    ">     |   Jerry|2015|       LCorp|[1, 2, 3]|[eat -> yes, slee...|\n",
    ">     |  Mozart|2010|       LCorp|   [1, 2]|[eat -> no, sleep...|\n",
    ">     |Einstein|2012|       LCorp|[1, 2, 3]|[eat -> yes, slee...|\n",
    ">     +--------+----+------------+---------+--------------------+\n",
    ">\n",
    ">     ds: org.apache.spark.sql.Dataset[Hubot] = [name: string, year: int ... 3 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "> **Side-note:** Dataset API is first-class citizen in Spark, and\n",
    "> DataFrame is an alias for Dataset\\[Row\\]. Note that Python and R use\n",
    "> DataFrames (since they are dynamically typed), but it is essentially a\n",
    "> Dataset.\n",
    "\n",
    "Finally\n",
    "-------\n",
    "\n",
    "DataFrames and Datasets can simplify and improve most of the\n",
    "applications pipelines by bringing concise syntax and performance\n",
    "optimizations. We would highly recommend you to check out the official\n",
    "API documentation, specifically around\n",
    "\n",
    "-   [DataFrame\n",
    "    API](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame),\n",
    "-   [Spark SQL functions\n",
    "    library](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$),\n",
    "-   [GroupBy clause and aggregated\n",
    "    functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.GroupedData).\n",
    "\n",
    "Unfortunately, this is just *a getting started quickly* course, and we\n",
    "skip features like custom aggregations, types, pivoting, etc., but if\n",
    "you are keen to know then start from the links above and this notebook\n",
    "and others in this directory."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
