{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ScaDaMaLe, Scalable Data Science and Distributed Machine Learning](https://lamastex.github.io/scalable-data-science/sds/3/x/)\n",
    "==============================================================================================================================\n",
    "\n",
    "Old Bailey Online Data Analysis in Apache Spark\n",
    "===============================================\n",
    "\n",
    "2016, by Raaz Sainudiin and James Smithies is licensed under [Creative\n",
    "Commons Attribution-NonCommercial 4.0 International\n",
    "License](http://creativecommons.org/licenses/by-nc/4.0/).\n",
    "\n",
    "#### Old Bailey, London's Central Criminal Court, 1674 to 1913\n",
    "\n",
    "-   with Full XML Data for another great project. This is a starting\n",
    "    point for ETL of Old Bailey Online Data from\n",
    "    <http://lamastex.org/datasets/public/OldBailey/index.html>.\n",
    "\n",
    "This work merely builds on [Old Bailey Online by Clive Emsley, Tim\n",
    "Hitchcock and Robert Shoemaker](https://www.oldbaileyonline.org/) that\n",
    "is licensed under a Creative Commons Attribution-NonCommercial 4.0\n",
    "International License. Permissions beyond the scope of this license may\n",
    "be available at https://www.oldbaileyonline.org/static/Legal-info.jsp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//This allows easy embedding of publicly available information into any other notebook\n",
    "//when viewing in git-book just ignore this block - you may have to manually chase the URL in frameIt(\"URL\").\n",
    "//Example usage:\n",
    "// displayHTML(frameIt(\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA\",250))\n",
    "def frameIt( u:String, h:Int ) : String = {\n",
    "      \"\"\"<iframe \n",
    " src=\"\"\"\"+ u+\"\"\"\"\n",
    " width=\"95%\" height=\"\"\"\" + h + \"\"\"\"\n",
    " sandbox>\n",
    "  <p>\n",
    "    <a href=\"http://spark.apache.org/docs/latest/index.html\">\n",
    "      Fallback link for browsers that, unlikely, don't support frames\n",
    "    </a>\n",
    "  </p>\n",
    "</iframe>\"\"\"\n",
    "   }\n",
    "displayHTML(frameIt(\"https://www.oldbaileyonline.org/\", 450))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### This exciting dataset is here for a course project in digital humanities\n",
    "\n",
    "#### To understand the extraction job we are about to do here:\n",
    "\n",
    "-   see [Jasper Mackenzie, Raazesh Sainudiin, James Smithies and Heather\n",
    "    Wolffram, A nonparametric view of the civilizing process in London's\n",
    "    Old Bailey, Research Report UCDMS2015/1, 32 pages,\n",
    "    2015](http://lamastex.org/preprints/20150828_civilizingProcOBO.pdf).\n",
    "\n",
    "The data is already loaded in dbfs (see dowloading and loading section\n",
    "below for these details).\n",
    "\n",
    "Analysing the Full Old Bailey Online Sessions Papers Dataset\n",
    "============================================================\n",
    "\n",
    "First **Step 0: Dowloading and Loading Data (The Full Dataset)** below\n",
    "should have been done on the shard.  \n",
    "This currently cannot be done in Community Edition as the dataset is not\n",
    "loaded into the dbfs available in CE yet. But the datset is in the\n",
    "academic shard and this is a walkthorugh of the Old Bailey Online data\n",
    "in the academic shard.\n",
    "\n",
    "Let's first check that the datasets are there in the distributed file\n",
    "system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/datasets/obo/tei/\")) // full data if you have it - not in CE!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/datasets/obo/tei/ordinarysAccounts\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/datasets/obo/tei/sessionsPapers\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayHTML(frameIt(\"https://en.wikipedia.org/wiki/XML\", 450))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Step 1: Exploring data first: xml parsing in scala\n",
    "--------------------------------------------------\n",
    "\n",
    "But, first let's understand the data and its structure.\n",
    "\n",
    "**Step 0: Dowloading and Loading Data (The Full Dataset)** should have\n",
    "been done already with data in dbfs alread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val raw = sc.wholeTextFiles(\"dbfs:/datasets/obo/tei/ordinarysAccounts/OA17070912.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val raw = sc.wholeTextFiles(\"dbfs:/datasets/obo/tei/sessionsPapers/17280717.xml\") // has data on crimes and punishments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val oboTest = sc.wholeTextFiles(\"dbfs:/datasets/obo/tei/ordinaryAccounts/OA1693072*.xml\")\n",
    "val xml = raw.map( x => x._2 )\n",
    "val x = xml.take(1)(0) // getting content of xml file as a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val elem = scala.xml.XML.loadString(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Quick Preparation\n",
    "-----------------\n",
    "\n",
    "#### Some examples to learn xml and scala in a hurry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val p = new scala.xml.PrettyPrinter(80, 2)\n",
    "\n",
    "p.format(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Better examples:\n",
    "\n",
    "http://alvinalexander.com/scala/how-to-extract-data-from-xml-nodes-in-scala\n",
    "\n",
    "http://alvinalexander.com/scala/scala-xml-xpath-example\n",
    "\n",
    "#### More advanced topics:\n",
    "\n",
    "https://alvinalexander.com/scala/serializing-deserializing-xml-scala-classes\n",
    "\n",
    "#### XML to JSON, if you want to go this route:\n",
    "\n",
    "https://stackoverflow.com/questions/9516973/xml-to-json-with-scala\n",
    "\n",
    "Our Parsing Problem\n",
    "-------------------\n",
    "\n",
    "Let's dive deep on this data right away. See links above to learn xml\n",
    "more systematically to be able to parse other subsets of the data for\n",
    "your own project.\n",
    "\n",
    "For now, we will jump in to parse the input data of counts used in\n",
    "[Jasper Mackenzie, Raazesh Sainudiin, James Smithies and Heather\n",
    "Wolffram, A nonparametric view of the civilizing process in London's Old\n",
    "Bailey, Research Report UCDMS2015/1, 32 pages,\n",
    "2015](http://lamastex.org/preprints/20150828_civilizingProcOBO.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(elem \\\\ \"div0\").map(Node => (Node \\ \"@type\").text) // types of div0 node, the singleton root node for the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(elem \\\\ \"div1\").map(Node => (Node \\ \"@type\").text) // types of div1 node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(elem \\\\ \"div1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(elem \\\\ \"div1\").filter(Node => ((Node \\ \"@type\").text == \"trialAccount\"))\n",
    "                 .map(Node => (Node \\ \"@type\", Node \\ \"@id\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trials = (elem \\\\ \"div1\").filter(Node => ((Node \\ \"@type\").text == \"trialAccount\"))\n",
    "                 .map(Node => (Node \\ \"@type\", Node \\ \"@id\", (Node \\\\ \"rs\" \\\\ \"interp\").map( n => ((n \\\\ \"@type\").text, (n \\\\ \"@value\").text ))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wantedFields = Seq(\"verdictCategory\",\"punishmentCategory\",\"offenceCategory\").toSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trials = (elem \\\\ \"div1\").filter(Node => ((Node \\ \"@type\").text == \"trialAccount\"))\n",
    "                 .map(Node => ((Node \\ \"@type\").text, (Node \\ \"@id\").text, (Node \\\\ \"rs\" \\\\ \"interp\")\n",
    "                                                               .filter(n => wantedFields.contains( (n \\\\ \"@type\").text))\n",
    "                                                               .map( n => ((n \\\\ \"@type\").text, (n \\\\ \"@value\").text ))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Since there can be more than one defendant in a trial, we need to reduce\n",
    "by key as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceByKey(collection: Traversable[Tuple2[String, Int]]) = {    \n",
    "    collection\n",
    "      .groupBy(_._1)\n",
    "      .map { case (group: String, traversable) => traversable.reduce{(a,b) => (a._1, a._2 + b._2)} }\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Let's process the coarsest data on the trial as json strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trials = (elem \\\\ \"div1\").filter(Node => ((Node \\ \"@type\").text == \"trialAccount\"))\n",
    "                 .map(Node => {val trialId = (Node \\ \"@id\").text;\n",
    "                               val trialInterps = (Node \\\\ \"rs\" \\\\ \"interp\")\n",
    "                                                                 .filter(n => wantedFields.contains( (n \\\\ \"@type\").text))\n",
    "                                                                 //.map( n => ((n \\\\ \"@type\").text, (n \\\\ \"@value\").text ));\n",
    "                                                                 .map( n => ((n \\\\ \"@value\").text , 1 ));\n",
    "                               val trialCounts = reduceByKey(trialInterps).toMap;\n",
    "                               //(trialId, trialInterps, trialCounts)\n",
    "                               scala.util.parsing.json.JSONObject(trialCounts updated (\"id\", trialId))\n",
    "                              })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Step 2: Extract, Transform and Load XML files to get DataFrame of counts\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "We have played enough (see **Step 1: Exploring data first: xml parsing\n",
    "in scala** above first) to understand what to do now with our xml data\n",
    "in order to get it converted to counts of crimes, verdicts and\n",
    "punishments.\n",
    "\n",
    "Let's parse the xml files and turn into Dataframe in one block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rawWTF = sc.wholeTextFiles(\"dbfs:/datasets/obo/tei/sessionsPapers/*.xml\") // has all data on crimes and punishments\n",
    "val raw = rawWTF.map( x => x._2 )\n",
    "val trials = raw.flatMap( x => { \n",
    "                       val elem = scala.xml.XML.loadString(x);\n",
    "                       val outJson = (elem \\\\ \"div1\").filter(Node => ((Node \\ \"@type\").text == \"trialAccount\"))\n",
    "                           .map(Node => {val trialId = (Node \\ \"@id\").text;\n",
    "                               val trialInterps = (Node \\\\ \"rs\" \\\\ \"interp\")\n",
    "                                                                 .filter(n => wantedFields.contains( (n \\\\ \"@type\").text))\n",
    "                                                                 //.map( n => ((n \\\\ \"@type\").text, (n \\\\ \"@value\").text ));\n",
    "                                                                 .map( n => ((n \\\\ \"@value\").text , 1 ));\n",
    "                               val trialCounts = reduceByKey(trialInterps).toMap;\n",
    "                               //(trialId, trialInterps, trialCounts)\n",
    "                               scala.util.parsing.json.JSONObject(trialCounts updated (\"id\", trialId)).toString()\n",
    "                              })\n",
    "  outJson\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"dbfs:/datasets/obo/processed/trialCounts\",recurse=true) // let's remove the files from the previous analysis\n",
    "trials.saveAsTextFile(\"dbfs:/datasets/obo/processed/trialCounts\") // now let's save the trial counts - aboout 220 seconds to pars all data and get counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/datasets/obo/processed/trialCounts\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trialCountsDF = sqlContext.read.json(\"dbfs:/datasets/obo/processed/trialCounts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trialCountsDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trialCountsDF.count // total number of trials = 197751"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trialCountsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trDF = trialCountsDF.na.fill(0) // filling nulls with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "This is already available as the following csv file:\n",
    "\n",
    "-   <http://lamastex.org/datasets/public/OldBailey/oboOffencePunnishmentCountsFrom-sds-2-2-ApacheSparkScalaProcessingOfOBOXMLDoneByRaazOn20180405.csv>\n",
    "\n",
    "Please cite this URL if you use this data or the Apache licensed codes\n",
    "in the databricks notebook above for your own non-commerical analysis:\n",
    "\n",
    "-   <http://lamastex.org/datasets/public/OldBailey/>\n",
    "\n",
    "Raazesh Sainudiin generated this header **Old bailey Processing in\n",
    "Apache Spark** on Thu Apr 5 18:22:43 CEST 2018 in Uppsala, Sweden.\n",
    "\n",
    "Step 0: Dowloading and Loading Data (The Full Dataset)\n",
    "------------------------------------------------------\n",
    "\n",
    "First we will be downloading data from\n",
    "<http://lamastex.org/datasets/public/OldBailey/index.html>.\n",
    "\n",
    "The steps below need to be done once for a give shard!\n",
    "\n",
    "You can download the tiny dataset\n",
    "`obo-tiny/OB-tiny_tei_7-2_CC-BY-NC.zip` **to save time and space in db\n",
    "CE**\n",
    "\n",
    "**Optional TODOs:**\n",
    "\n",
    "-   one could just read the zip files directly (see week 10 on Beijing\n",
    "    taxi trajectories example from the scalable-data-science course in\n",
    "    2016 or read 'importing zip files' in the Guide).\n",
    "-   one could just download from s3 directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to download the tiny dataset\n",
    "wget https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/datasets/obo-tiny/OB-tiny_tei_7-2_CC-BY-NC.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the full dataset - necessary for a project on this dataset\n",
    "wget http://lamastex.org/datasets/public/OldBailey/OB_tei_7-2_CC-BY-NC.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd && ls -al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Make sure you comment/uncomment the right files depending on wheter you\n",
    "have downloaded the tiny dataset or the big one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzip OB-tiny_tei_7-2_CC-BY-NC.zip\n",
    "#unzip OB_tei_7-2_CC-BY-NC.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Let's put the files in dbfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:/datasets/obo/tei\") //need not be done again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//dbutils.fs.rm(\"dbfs:/datasets/obo/tei\",true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls \n",
    "#ls obo-tiny/tei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " dbutils.fs.cp(\"file:/databricks/driver/obo-tiny/tei\", \"dbfs:/datasets/obo/tei/\",recurse=true) // already done and it takes 1500 seconds - a while!\n",
    " //dbutils.fs.cp(\"file:/databricks/driver/tei\", \"dbfs:/datasets/obo/tei/\",recurse=true) // already done and it takes 19 minutes - a while!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//dbutils.fs.rm(\"dbfs:/datasets/tweets\",true) // remove files to make room for the OBO dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/datasets/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/datasets/obo/tei/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.Properties.versionString // check scala version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
