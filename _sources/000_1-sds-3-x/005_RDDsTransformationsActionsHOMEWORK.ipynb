{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ScaDaMaLe, Scalable Data Science and Distributed Machine Learning](https://lamastex.github.io/scalable-data-science/sds/3/x/)\n",
    "==============================================================================================================================\n",
    "\n",
    "HOMEWORK notebook - RDDs Transformations and Actions\n",
    "====================================================\n",
    "\n",
    "Just go through the notebook and familiarize yourself with these\n",
    "transformations and actions.\n",
    "\n",
    "##### 1. Perform the `takeOrdered` action on the RDD\n",
    "\n",
    "To illustrate `take` and `takeOrdered` actions, let's create a bigger\n",
    "RDD named `rdd0_1000000` that is made up of a million integers from 0 to\n",
    "1000000.  \n",
    "We will `sc.parallelize` the `Seq` Scala collection by using its\n",
    "`.range(startInteger,stopInteger)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd0_1000000 = sc.parallelize(Seq.range(0, 1000000)) // <Shift+Enter> to create an RDD of million integers: 0,1,2,...,10^6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     rdd0_1000000: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[26] at parallelize at command-3398110674017778:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd0_1000000.take(5) // <Ctrl+Enter> gives the first 5 elements of the RDD, (0, 1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res0: Array[Int] = Array(0, 1, 2, 3, 4)\n",
    "\n",
    "  \n",
    "\n",
    "`takeordered(n)` returns `n` elements ordered in ascending order (by\n",
    "default) or as specified by the optional key function, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd0_1000000.takeOrdered(5) // <Shift+Enter> is same as rdd0_1000000.take(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res1: Array[Int] = Array(0, 1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd0_1000000.takeOrdered(5)(Ordering[Int].reverse) // <Ctrl+Enter> to get the last 5 elements of the RDD 999999, 999998, ..., 999995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res2: Array[Int] = Array(999999, 999998, 999997, 999996, 999995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// HOMEWORK: edit the numbers below to get the last 20 elements of an RDD made of a sequence of integers from 669966 to 969696\n",
    "sc.parallelize(Seq.range(0, 10)).takeOrdered(5)(Ordering[Int].reverse) // <Ctrl+Enter> evaluate this cell after editing it for the right answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res3: Array[Int] = Array(9, 8, 7, 6, 5)\n",
    "\n",
    "  \n",
    "\n",
    "##### 2. More examples of `map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd = sc.parallelize(Seq(1, 2, 3, 4))    // <Shift+Enter> to evaluate this cell (using default number of partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at command-3398110674017785:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map( x => x*2) // <Ctrl+Enter> to transform rdd by map that doubles each element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res4: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[29] at map at command-3398110674017786:1\n",
    "\n",
    "  \n",
    "\n",
    "To see what's in the transformed RDD, let's perform the actions of\n",
    "`count` and `collect` on the `rdd.map( x => x*2)`, the transformation of\n",
    "`rdd` by the `map` given by the closure `x => x*2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map( x => x*2).count()    // <Shift+Enter> to perform count (action) the element of the RDD = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res5: Long = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map( x => x*2).collect()    // <Shift+Enter> to perform collect (action) to show 2, 4, 6, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res6: Array[Int] = Array(2, 4, 6, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// HOMEWORK: uncomment the last line in this cell and modify the '<Fill-In-Here>' in the code below to collect and display the square (x*x) of each element of the RDD\n",
    "// the answer should be Array[Int] = Array(1, 4, 9, 16) Press <Cntrl+Enter> to evaluate the cell after modifying '???'\n",
    "\n",
    "//sc.parallelize(Seq(1, 2, 3, 4)).map( x => <Fill-In-Here> ).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "##### 3. More examples of `filter`\n",
    "\n",
    "Let's declare another `val` RDD named `rddFiltered` by transforming our\n",
    "first RDD named `rdd` via the `filter` transformation `x%2==0` (of being\n",
    "even).\n",
    "\n",
    "This filter transformation based on the closure `x => x%2==0` will\n",
    "return `true` if the element, modulo two, equals zero. The closure is\n",
    "automatically passed on to the workers for evaluation (when an action is\n",
    "called later). So this will take our RDD of (1,2,3,4) and return RDD of\n",
    "(2, 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rddFiltered = rdd.filter( x => x%2==0 )    // <Ctrl+Enter> to declare rddFiltered from transforming rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     rddFiltered: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[32] at filter at command-3398110674017792:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddFiltered.collect()    // <Ctrl+Enter> to collect (action) elements of rddFiltered; should be (2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res8: Array[Int] = Array(2, 4)\n",
    "\n",
    "  \n",
    "\n",
    "##### 4. More examples of `reduce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd = sc.parallelize(Array(1,2,3,4,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[33] at parallelize at command-3398110674017795:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.reduce( (x,y)=>x+y ) // <Shift+Enter> to do reduce (action) to sum and return Int = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res9: Int = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.reduce( _ + _ )    // <Shift+Enter> to do same sum as above and return Int = 15 (undescore syntax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res10: Int = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.reduce( (x,y)=>x*y ) // <Shift+Enter> to do reduce (action) to multiply and return Int = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res11: Int = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd0_1000000 = sc.parallelize(Seq.range(0, 1000000)) // <Shift+Enter> to create an RDD of million integers: 0,1,2,...,10^6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     rdd0_1000000: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at command-3398110674017799:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd0_1000000.reduce( (x,y)=>x+y ) // <Ctrl+Enter> to do reduce (action) to sum and return Int 1783293664"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res12: Int = 1783293664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// the following correctly returns Int = 0 although for wrong reason \n",
    "// we have flowed out of Int's numeric limits!!! (but got lucky with 0*x=0 for any Int x)\n",
    "// <Shift+Enter> to do reduce (action) to multiply and return Int = 0\n",
    "rdd0_1000000.reduce( (x,y)=>x*y ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res13: Int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// <Ctrl+Enter> to do reduce (action) to multiply 1*2*...*9*10 and return correct answer Int = 3628800\n",
    "sc.parallelize(Seq.range(1, 11)).reduce( (x,y)=>x*y ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res14: Int = 3628800\n",
    "\n",
    "  \n",
    "\n",
    "**CAUTION: Know the limits of your numeric types!**\n",
    "\n",
    "The minimum and maximum value of `Int` and `Long` types are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Int.MinValue , Int.MaxValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res15: (Int, Int) = (-2147483648,2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Long.MinValue, Long.MaxValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res16: (Long, Long) = (-9223372036854775808,9223372036854775807)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// <Ctrl+Enter> to do reduce (action) to multiply 1*2*...*20 and return wrong answer as Int = -2102132736\n",
    "//  we have overflowed out of Int's in a circle back to negative Ints!!! (rigorous distributed numerics, anyone?)\n",
    "sc.parallelize(Seq.range(1, 21)).reduce( (x,y)=>x*y ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res17: Int = -2102132736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//<Ctrl+Enter> we can accomplish the multiplication using Long Integer types \n",
    "// by adding 'L' ro integer values, Scala infers that it is type Long\n",
    "sc.parallelize(Seq.range(1L, 21L)).reduce( (x,y)=>x*y ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res18: Long = 2432902008176640000\n",
    "\n",
    "  \n",
    "\n",
    "As the following products over Long Integers indicate, they are limited\n",
    "too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " // <Shift+Enter> for wrong answer Long = -8718968878589280256 (due to Long's numeric limits)\n",
    "sc.parallelize(Seq.range(1L, 61L)).reduce( (x,y)=>x*y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res19: Long = -8718968878589280256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// <Cntrl+Enter> for wrong answer Long = 0 (due to Long's numeric limits)\n",
    "sc.parallelize(Seq.range(1L, 100L)).reduce( (x,y)=>x*y ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res20: Long = 0\n",
    "\n",
    "  \n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "##### 5. Let us do a bunch of transformations to our RDD and perform an action\n",
    "\n",
    "-   start from a Scala `Seq`,\n",
    "-   `sc.parallelize` the list to create an RDD,\n",
    "-   `filter` that RDD, creating a new filtered RDD,\n",
    "-   do a `map` transformation that maps that RDD to a new mapped RDD,\n",
    "-   and finally, perform a `reduce` action to sum the elements in the\n",
    "    RDD.\n",
    "\n",
    "This last `reduce` action causes the `parallelize`, the `filter`, and\n",
    "the `map` transformations to actually be executed, and return a result\n",
    "back to the driver machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(Seq(1, 2, 3, 4))    // <Ctrl+Enter> will return Array(4, 8)\n",
    "  .filter(x => x%2==0)             // (2, 4) is the filtered RDD\n",
    "  .map(x => x*2)                   // (4, 8) is the mapped RDD\n",
    "  .reduce(_+_)                     // 4+8=12 is the final result from reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res21: Int = 12\n",
    "\n",
    "  \n",
    "\n",
    "##### 6. Transform the RDD by `distinct` to make another RDD\n",
    "\n",
    "Let's declare another RDD named `rdd2` that has some repeated elements\n",
    "to apply the `distinct` transformation to it. That would give us a new\n",
    "RDD that only contains the distinct elements of the input RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd2 = sc.parallelize(Seq(4, 1, 3, 2, 2, 2, 3, 4))    // <Ctrl+Enter> to declare rdd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[43] at parallelize at command-3398110674017815:1\n",
    "\n",
    "  \n",
    "\n",
    "Let's apply the `distinct` transformation to `rdd2` and have it return a\n",
    "new RDD named `rdd2Distinct` that contains the distinct elements of the\n",
    "source RDD `rdd2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd2Distinct = rdd2.distinct() // <Ctrl+Enter> transformation: distinct gives distinct elements of rdd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     rdd2Distinct: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[46] at distinct at command-3398110674017817:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2Distinct.collect()    // <Ctrl+Enter> to collect (action) as Array(4, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res22: Array[Int] = Array(4, 1, 2, 3)\n",
    "\n",
    "  \n",
    "\n",
    "##### 7. more flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd = sc. parallelize(Array(1,2,3)) // <Shift+Enter> to create an RDD of three Int elements 1,2,3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[47] at parallelize at command-3398110674017820:1\n",
    "\n",
    "  \n",
    "\n",
    "Let us pass the `rdd` above to a map with a closure that will take in\n",
    "each element `x` and return `Array(x, x+5)`. So each element of the\n",
    "mapped RDD named `rddOfArrays` is an `Array[Int]`, an array of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// <Shift+Enter> to make RDD of Arrays, i.e., RDD[Array[int]]\n",
    "val rddOfArrays = rdd.map( x => Array(x, x+5) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     rddOfArrays: org.apache.spark.rdd.RDD[Array[Int]] = MapPartitionsRDD[48] at map at command-3398110674017822:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddOfArrays.collect() // <Ctrl+Enter> to see it is RDD[Array[int]] = (Array(1, 6), Array(2, 7), Array(3, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res23: Array[Array[Int]] = Array(Array(1, 6), Array(2, 7), Array(3, 8))\n",
    "\n",
    "  \n",
    "\n",
    "Now let's observer what happens when we use `flatMap` to transform the\n",
    "same `rdd` and create another RDD called `rddfM`.\n",
    "\n",
    "Interestingly, `flatMap` *flattens* our `rdd` by taking each `Array` (or\n",
    "sequence in general) and truning it into individual elements.\n",
    "\n",
    "Thus, we end up with the RDD `rddfM` consisting of the elements (1, 6,\n",
    "2, 7, 3, 8) as shown from the output of `rddfM.collect` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rddfM = rdd.flatMap(x => Array(x, x+5))    // <Shift+Enter> to flatMap the rdd using closure (x => Array(x, x+5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     rddfM: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[49] at flatMap at command-3398110674017825:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddfM.collect    // <Ctrl+Enter> to collect rddfM = (1, 6, 2, 7, 3, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res24: Array[Int] = Array(1, 6, 2, 7, 3, 8)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
