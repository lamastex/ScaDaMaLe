{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ScaDaMaLe, Scalable Data Science and Distributed Machine Learning](https://lamastex.github.io/scalable-data-science/sds/3/x/)\n",
    "==============================================================================================================================\n",
    "\n",
    "### Diamonds ML Pipeline Workflow - DataFrame ETL and EDA Part\n",
    "\n",
    "This is the Spark SQL parts that are focussed on extract-transform-Load\n",
    "(ETL) and exploratory-data-analysis (EDA) parts of an end-to-end example\n",
    "of a Machine Learning (ML) workflow.\n",
    "\n",
    "**Why are we using DataFrames?** *This is because of the\n",
    "**Announcement** in the Spark MLlib Main Guide for Spark 2.2*\n",
    "<https://spark.apache.org/docs/latest/ml-guide.html> that\n",
    "*\"DataFrame-based API is primary API\"*.\n",
    "\n",
    "This notebook is a scala*rific* break-down of the python*ic* 'Diamonds\n",
    "ML Pipeline Workflow' from the Databricks Guide.\n",
    "\n",
    "**We will see this example again in the sequel**\n",
    "\n",
    "For this example, we analyze the Diamonds dataset from the R Datasets\n",
    "hosted on DBC.\n",
    "\n",
    "Later on, we will use the [DecisionTree\n",
    "algorithm](http://spark.apache.org/docs/latest/ml-classification-regression.html#decision-trees)\n",
    "to predict the price of a diamond from its characteristics.\n",
    "\n",
    "Here is an outline of our pipeline:\n",
    "\n",
    "-   **Step 1. *Load data*: Load data as DataFrame**\n",
    "-   **Step 2. *Understand the data*: Compute statistics and create\n",
    "    visualizations to get a better understanding of the data.**\n",
    "-   Step 3. *Hold out data*: Split the data randomly into training and\n",
    "    test sets. We will not look at the test data until *after* learning.\n",
    "-   Step 4. On the training dataset:\n",
    "    -   *Extract features*: We will index categorical (String-valued)\n",
    "        features so that DecisionTree can handle them.\n",
    "    -   *Learn a model*: Run DecisionTree to learn how to predict a\n",
    "        diamond's price from a description of the diamond.\n",
    "    -   *Tune the model*: Tune the tree depth (complexity) using the\n",
    "        training data. (This process is also called *model selection*.)\n",
    "-   Step 5. *Evaluate the model*: Now look at the test dataset. Compare\n",
    "    the initial model with the tuned model to see the benefit of tuning\n",
    "    parameters.\n",
    "-   Step 6. *Understand the model*: We will examine the learned model\n",
    "    and results to gain further insight.\n",
    "\n",
    "In this notebook, we will only cover **Step 1** and **Step 2.** above.\n",
    "The other Steps will be revisited in the sequel.\n",
    "\n",
    "### Step 1. Load data as DataFrame\n",
    "\n",
    "This section loads a dataset as a DataFrame and examines a few rows of\n",
    "it to understand the schema.\n",
    "\n",
    "For more info, see the DB guide on [importing\n",
    "data](https://docs.databricks.com/user-guide/importing-data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// We'll use the Diamonds dataset from the R datasets hosted on DBC.\n",
    "val diamondsFilePath = \"dbfs:/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile(diamondsFilePath).take(2) // looks like a csv file as it should"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val diamondsRawDF = sqlContext.read    // we can use sqlContext instead of SparkSession for backwards compatibility to 1.x\n",
    "    .format(\"com.databricks.spark.csv\") // use spark.csv package\n",
    "    .option(\"header\", \"true\") // Use first line of all files as header\n",
    "    .option(\"inferSchema\", \"true\") // Automatically infer data types\n",
    "    //.option(\"delimiter\", \",\") // Specify the delimiter as comma or ',' DEFAULT\n",
    "    .load(diamondsFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//There are 10 columns.  We will try to predict the price of diamonds, treating the other 9 columns as features.\n",
    "diamondsRawDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "*Note:* `(nullable = true)` simply means if the value is allowed to be\n",
    "`null`.\n",
    "\n",
    "Let us count the number of rows in `diamondsDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamondsRawDF.count() // Ctrl+Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "So there are 53940 records or rows in the DataFrame.\n",
    "\n",
    "Use the `show(n)` method to see the first `n` (default is 20) rows of\n",
    "the DataFrame, as folows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamondsRawDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "If you notice the schema of `diamondsRawDF` you will see that the\n",
    "automatic schema inference of `SqlContext.read` method has cast the\n",
    "values in the column `price` as `integer`.\n",
    "\n",
    "To cleanup:\n",
    "\n",
    "-   let's recast the column `price` as `double` for downstream ML tasks\n",
    "    later and\n",
    "-   let's also get rid of the first column of row indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.DoubleType\n",
    "//we will convert price column from int to double for being able to model, fit and predict in downstream ML task\n",
    "val diamondsDF = diamondsRawDF.select($\"carat\", $\"cut\", $\"color\", $\"clarity\", $\"depth\", $\"table\",$\"price\".cast(DoubleType).as(\"price\"), $\"x\", $\"y\", $\"z\")\n",
    "diamondsDF.cache() // let's cache it for reuse\n",
    "diamondsDF.printSchema // print schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamondsDF.show(10,false) // notice that price column has Double values that end in '.0' now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//View DataFrame in databricks\n",
    "// note this 'display' is a databricks notebook specific command that is quite powerful for visual interaction with the data\n",
    "// other notebooks like zeppelin have similar commands for interactive visualisation\n",
    "display(diamondsDF) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Step 2. Understand the data\n",
    "\n",
    "Let's examine the data to get a better understanding of what is there.\n",
    "We only examine a couple of features (columns), but it gives an idea of\n",
    "the type of exploration you might do to understand a new dataset.\n",
    "\n",
    "For more examples of using Databricks's visualization (even across\n",
    "languages) see\n",
    "<https://docs.databricks.com/user-guide/visualizations/index.html> NOW.\n",
    "\n",
    "We can see that we have a mix of\n",
    "\n",
    "-   categorical features (`cut`, `color`, `clarity`) and\n",
    "-   continuous features (`depth`, `x`, `y`, `z`).\n",
    "\n",
    "##### Let's first look at the categorical features.\n",
    "\n",
    "You can also select one or more individual columns using so-called\n",
    "DataFrame API.\n",
    "\n",
    "Let us `select` the column `cut` from `diamondsDF` and create a new\n",
    "DataFrame called `cutsDF` and then display it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cutsDF = diamondsDF.select(\"cut\") // Shift+Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutsDF.show(10) // Ctrl+Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Let us use `distinct` to find the distinct types of `cut`'s in the\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// View distinct diamond cuts in dataset\n",
    "val cutsDistinctDF = diamondsDF.select(\"cut\").distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutsDistinctDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Clearly, there are just 5 kinds of cuts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// View distinct diamond colors in dataset\n",
    "val colorsDistinctDF = diamondsDF.select(\"color\").distinct() //.collect()\n",
    "colorsDistinctDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// View distinct diamond clarities in dataset\n",
    "val claritiesDistinctDF = diamondsDF.select(\"clarity\").distinct() // .collect()\n",
    "claritiesDistinctDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "We can examine the distribution of a particular feature by using\n",
    "display(),\n",
    "\n",
    "**You Try!**\n",
    "\n",
    "1.  Click on the chart icon and Plot Options, and setting:\n",
    "\n",
    "-   Value=`<id>`\n",
    "-   Series groupings='cut'\n",
    "-   and Aggregation=`COUNT`.\n",
    "\n",
    "1.  You can also try this using columns \"color\" and \"clarity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(diamondsDF.select(\"cut\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// come on do the same for color NOW!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// and clarity too..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "\\*\\* You Try!\\*\\*\n",
    "\n",
    "Now play around with display of the entire DF and choosing what you want\n",
    "in the GUI as opposed to a `.select(...)` statement earlier.\n",
    "\n",
    "For instance, the following `display(diamondsDF)` shows the counts of\n",
    "the colors by choosing in the `Plot Options` a `bar-chart` that is\n",
    "`grouped` with `Series Grouping` as `color`, `values` as `<id>` and\n",
    "`Aggregation` as `COUNT`. You can click on `Plot Options` to see these\n",
    "settings and can change them as you wish by dragging and dropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " display(diamondsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Now let's examine one of the continuous features as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Select: \"Plot Options...\" --> \"Display type\" --> \"histogram plot\" and choose to \"Plot over all results\" OTHERWISE you get the image from first 1000 rows only\n",
    "display(diamondsDF.select(\"carat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "The above histogram of the diamonds' carat ratings shows that carats\n",
    "have a skewed distribution: Many diamonds are small, but there are a\n",
    "number of diamonds in the dataset which are much larger.\n",
    "\n",
    "-   Extremely skewed distributions can cause problems for some\n",
    "    algorithms (e.g., Linear Regression).  \n",
    "-   However, Decision Trees handle skewed distributions very naturally.\n",
    "\n",
    "Note: When you call `display` to create a histogram like that above,\n",
    "**it will plot using a subsample from the dataset** (for efficiency),\n",
    "but you can plot using the full dataset by selecting \"Plot over all\n",
    "results\". For our dataset, the two plots can actually look very\n",
    "different due to the long-tailed distribution.\n",
    "\n",
    "We will not examine the label distribution for now. It can be helpful to\n",
    "examine the label distribution, but it is best to do so only on the\n",
    "training set, not on the test set which we will hold out for evaluation.\n",
    "These will be seen in the sequel\n",
    "\n",
    "**You Try!** Of course knock youself out visually exploring the dataset\n",
    "more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(diamondsDF.select(\"cut\",\"carat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Try scatter plot to see pairwise scatter plots of continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(diamondsDF) //Ctrl+Enter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Note that columns of type string are not in the scatter plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamondsDF.printSchema // Ctrl+Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Let us run through some basic inteactive SQL queries next\n",
    "\n",
    "-   HiveQL supports =, &lt;, &gt;, &lt;=, &gt;= and != operators. It\n",
    "    also supports LIKE operator for fuzzy matching of Strings\n",
    "-   Enclose Strings in single quotes\n",
    "-   Multiple conditions can be combined using `and` and `or`\n",
    "-   Enclose conditions in `()` for precedence\n",
    "-   ...\n",
    "-   ...\n",
    "\n",
    "**Why do I need to learn interactive SQL queries?**\n",
    "\n",
    "Such queries in the widely known declarative SQL language can help us\n",
    "explore the data and thereby inform the modeling process!!!\n",
    "\n",
    "Using DataFrame API, we can apply a `filter` after `select` to transform\n",
    "the DataFrame `diamondsDF` to the new DataFrame `diamondsDColoredDF`.\n",
    "\n",
    "Below, `$` is an alias for column.\n",
    "\n",
    "Let as select the columns named `carat`, `colour`, `price` where `color`\n",
    "value is equal to `D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val diamondsDColoredDF = diamondsDF.select(\"carat\", \"color\", \"price\").filter($\"color\" === \"D\") // Shift+Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamondsDColoredDF.show(10) // Ctrl+Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "As you can see all the colors are now 'D'. But to really confirm this we\n",
    "can do the following for fun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamondsDColoredDF.select(\"color\").distinct().show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Let's try to do the same in SQL for those who know SQL from before.\n",
    "\n",
    "First we need to see if the table is registerd (not just the DataFrame),\n",
    "and if not we ened to register our DataFrame as a temporary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.tables.show() // Ctrl+Enter to see available tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Looks like diamonds is already there (if not just execute the following\n",
    "cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamondsDF.createOrReplaceTempView(\"diamonds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.tables.show() // Ctrl+Enter to see available tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Shift+Enter to do the same in SQL\n",
    "select carat, color, price from diamonds where color='D'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Alternatively, one could just write the SQL statement in scala to create\n",
    "a new DataFrame `diamondsDColoredDF_FromTable` from the table `diamonds`\n",
    "and display it, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val diamondsDColoredDF_FromTable = sqlContext.sql(\"select carat, color, price from diamonds where color='D'\") // Shift+Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// or if you like use upper case for SQL then this is equivalent\n",
    "val diamondsDColoredDF_FromTable = sqlContext.sql(\"SELECT carat, color, price FROM diamonds WHERE color='D'\") // Shift+Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// from version 2.x onwards you can call from SparkSession, the pre-made spark in spark-shell or databricks notebook\n",
    "val diamondsDColoredDF_FromTable = spark.sql(\"SELECT carat, color, price FROM diamonds WHERE color='D'\") // Shift+Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(diamondsDColoredDF_FromTable) // Ctrl+Enter to see the same DF!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// You can also use the familiar wildchard character '%' when matching Strings\n",
    "display(spark.sql(\"SELECT * FROM diamonds WHERE clarity LIKE 'V%'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Combining conditions\n",
    "display(spark.sql(\"SELECT * FROM diamonds WHERE clarity LIKE 'V%' AND price > 10000\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// selecting a subset of fields\n",
    "display(spark.sql(\"SELECT carat, clarity, price FROM diamonds WHERE color = 'D'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//renaming a field using as\n",
    "display(spark.sql(\"SELECT carat AS carrot, clarity, price FROM diamonds\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//sorting\n",
    "display(spark.sql(\"SELECT carat, clarity, price FROM diamonds ORDER BY price DESC\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamondsDF.printSchema // since price is double in the DF that was turned into table we can rely on the descenting sort on doubles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// sort by multiple fields\n",
    "display(spark.sql(\"SELECT carat, clarity, price FROM diamonds ORDER BY carat ASC, price DESC\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// use this to type cast strings into Int when the table is loaded with string-valued columns\n",
    "//display(spark.sql(\"select cast(carat as Int) as carat, clarity, cast(price as Int) as price from diamond order by carat asc, price desc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// sort by multiple fields and limit to first 5\n",
    "// I prefer lowercase for SQL - and you can use either in this course - but in the field do what your Boss or your colleagues prefer :)\n",
    "display(spark.sql(\"select carat, clarity, price from diamonds order by carat desc, price desc limit 5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//aggregate functions\n",
    "display(spark.sql(\"select avg(price) as avgprice from diamonds\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//average operator is doing an auto-type conversion from int to double\n",
    "display(spark.sql(\"select avg(cast(price as Integer)) as avgprice from diamonds\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//aggregate function and grouping\n",
    "display(spark.sql(\"select color, avg(price) as avgprice from diamonds group by color\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Why do we need to know these interactive SQL queries?\n",
    "\n",
    "Such queries can help us explore the data and thereby inform the\n",
    "modeling process!!!\n",
    "\n",
    "Of course, if you don't know SQL then don't worry, we will be doing\n",
    "these things in scala using DataFrames.\n",
    "\n",
    "Finally, those who are planning to take the Spark Developer Exams\n",
    "online, then you can't escape from SQL questions there...\n",
    "\n",
    "### We will continue later with ML pipelines to do prediction with a fitted model from this dataset"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
