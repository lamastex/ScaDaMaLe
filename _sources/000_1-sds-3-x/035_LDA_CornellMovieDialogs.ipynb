{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ScaDaMaLe Course\n",
    "[site](https://lamastex.github.io/scalable-data-science/sds/3/x/) and\n",
    "[book](https://lamastex.github.io/ScaDaMaLe/index.html)\n",
    "\n",
    "Topic Modeling of Movie Dialogs with Latent Dirichlet Allocation\n",
    "================================================================\n",
    "\n",
    "**Let us cluster the conversations from different movies!**\n",
    "\n",
    "This notebook will provide a brief algorithm summary, links for further\n",
    "reading, and an example of how to use LDA for Topic Modeling.\n",
    "\n",
    "**not tested in Spark 2.2+ yet (see 034 notebook for syntactic issues,\n",
    "if any)**\n",
    "\n",
    "Algorithm Summary\n",
    "-----------------\n",
    "\n",
    "-   **Task**: Identify topics from a collection of text documents\n",
    "-   **Input**: Vectors of word counts\n",
    "-   **Optimizers**:\n",
    "    -   EMLDAOptimizer using [Expectation\n",
    "        Maximization](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)\n",
    "    -   OnlineLDAOptimizer using Iterative Mini-Batch Sampling for\n",
    "        [Online Variational\n",
    "        Bayes](https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf)\n",
    "\n",
    "Links\n",
    "-----\n",
    "\n",
    "-   Spark API docs\n",
    "    -   Scala:\n",
    "        [LDA](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.LDA)\n",
    "    -   Python:\n",
    "        [LDA](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.clustering.LDA)\n",
    "-   [MLlib Programming\n",
    "    Guide](http://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda)\n",
    "-   [ML Feature Extractors &\n",
    "    Transformers](http://spark.apache.org/docs/latest/ml-features.html)\n",
    "-   [Wikipedia: Latent Dirichlet\n",
    "    Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n",
    "\n",
    "Readings for LDA\n",
    "----------------\n",
    "\n",
    "-   A high-level introduction to the topic from Communications of the\n",
    "    ACM\n",
    "    -   <http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf>\n",
    "-   A very good high-level humanities introduction to the topic\n",
    "    (recommended by Chris Thomson in English Department at UC, Ilam):\n",
    "    -   <http://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/>\n",
    "\n",
    "Also read the methodological and more formal papers cited in the above\n",
    "links if you want to know more.\n",
    "\n",
    "Let's get a bird's eye view of LDA from\n",
    "http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf next.\n",
    "\n",
    "-   See pictures (hopefully you read the paper last night!)\n",
    "-   Algorithm of the generative model (this is unsupervised clustering)\n",
    "-   For a careful introduction to the topic see Section 27.3 and 27.4\n",
    "    (pages 950-970) pf Murphy's *Machine Learning: A Probabilistic\n",
    "    Perspective, MIT Press, 2012*.\n",
    "-   We will be quite application focussed or applied here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//This allows easy embedding of publicly available information into any other notebook\n",
    "//when viewing in git-book just ignore this block - you may have to manually chase the URL in frameIt(\"URL\").\n",
    "//Example usage:\n",
    "// displayHTML(frameIt(\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA\",250))\n",
    "def frameIt( u:String, h:Int ) : String = {\n",
    "      \"\"\"<iframe \n",
    " src=\"\"\"\"+ u+\"\"\"\"\n",
    " width=\"95%\" height=\"\"\"\" + h + \"\"\"\"\n",
    " sandbox>\n",
    "  <p>\n",
    "    <a href=\"http://spark.apache.org/docs/latest/index.html\">\n",
    "      Fallback link for browsers that, unlikely, don't support frames\n",
    "    </a>\n",
    "  </p>\n",
    "</iframe>\"\"\"\n",
    "   }\n",
    "displayHTML(frameIt(\"http://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/\",900))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayHTML(frameIt(\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA\",250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayHTML(frameIt(\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Model\",600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayHTML(frameIt(\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Mathematical_definition\",910))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Probabilistic Topic Modeling Example\n",
    "------------------------------------\n",
    "\n",
    "This is an outline of our Topic Modeling workflow. Feel free to jump to\n",
    "any subtopic to find out more.\n",
    "\n",
    "-   Step 0. Dataset Review\n",
    "-   Step 1. Downloading and Loading Data into DBFS\n",
    "    -   (Step 1. only needs to be done once per shard - see details at\n",
    "        the end of the notebook for Step 1.)\n",
    "-   Step 2. Loading the Data and Data Cleaning\n",
    "-   Step 3. Text Tokenization\n",
    "-   Step 4. Remove Stopwords\n",
    "-   Step 5. Vector of Token Counts\n",
    "-   Step 6. Create LDA model with Online Variational Bayes\n",
    "-   Step 7. Review Topics\n",
    "-   Step 8. Model Tuning - Refilter Stopwords\n",
    "-   Step 9. Create LDA model with Expectation Maximization\n",
    "-   Step 10. Visualize Results\n",
    "\n",
    "Step 0. Dataset Review\n",
    "----------------------\n",
    "\n",
    "In this example, we will use the [Cornell Movie Dialogs\n",
    "Corpus](https://people.mpi-sws.org/~cristian/Cornell_Movie-Dialogs_Corpus.html).\n",
    "\n",
    "Here is the `README.txt`:\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "Cornell Movie-Dialogs Corpus\n",
    "\n",
    "Distributed together with:\n",
    "\n",
    "\"Chameleons in imagined conversations: A new approach to understanding\n",
    "coordination of linguistic style in dialogs\" Cristian\n",
    "Danescu-Niculescu-Mizil and Lillian Lee Proceedings of the Workshop on\n",
    "Cognitive Modeling and Computational Linguistics, ACL 2011.\n",
    "\n",
    "(this paper is included in this zip file)\n",
    "\n",
    "NOTE: If you have results to report on these corpora, please send email\n",
    "to cristian@cs.cornell.edu or llee@cs.cornell.edu so we can add you to\n",
    "our list of people using this data. Thanks!\n",
    "\n",
    "Contents of this README:\n",
    "\n",
    "        A) Brief description\n",
    "        B) Files description\n",
    "        C) Details on the collection procedure\n",
    "        D) Contact\n",
    "\n",
    "A\\) Brief description:\n",
    "\n",
    "This corpus contains a metadata-rich collection of fictional\n",
    "conversations extracted from raw movie scripts:\n",
    "\n",
    "-   220,579 conversational exchanges between 10,292 pairs of movie\n",
    "    characters\n",
    "-   involves 9,035 characters from 617 movies\n",
    "-   in total 304,713 utterances\n",
    "-   movie metadata included: - genres - release year - IMDB rating -\n",
    "    number of IMDB votes - IMDB rating\n",
    "-   character metadata included: - gender (for 3,774 characters) -\n",
    "    position on movie credits (3,321 characters)\n",
    "\n",
    "B\\) Files description:\n",
    "\n",
    "In all files the field separator is \" +++$+++ \"\n",
    "\n",
    "-   movie*titles*metadata.txt - contains information about each movie\n",
    "    title - fields: - movieID, - movie title, - movie year, - IMDB\n",
    "    rating, - no. IMDB votes, - genres in the format\n",
    "    \\['genre1','genre2',...,'genreN'\\]\n",
    "\n",
    "-   movie*characters*metadata.txt - contains information about each\n",
    "    movie character - fields: - characterID - character name - movieID -\n",
    "    movie title - gender (\"?\" for unlabeled cases) - position in credits\n",
    "    (\"?\" for unlabeled cases)\n",
    "\n",
    "-   movie\\_lines.txt - contains the actual text of each utterance -\n",
    "    fields: - lineID - characterID (who uttered this phrase) - movieID -\n",
    "    character name - text of the utterance\n",
    "\n",
    "-   movie*conversations.txt - the structure of the conversations -\n",
    "    fields - characterID of the first character involved in the\n",
    "    conversation - characterID of the second character involved in the\n",
    "    conversation - movieID of the movie in which the conversation\n",
    "    occurred - list of the utterances that make the conversation, in\n",
    "    chronological order: \\['lineID1','lineID2',...,'lineIDN'\\] has to be\n",
    "    matched with movie*lines.txt to reconstruct the actual content\n",
    "\n",
    "-   raw*script*urls.txt - the urls from which the raw sources were\n",
    "    retrieved\n",
    "\n",
    "C\\) Details on the collection procedure:\n",
    "\n",
    "We started from raw publicly available movie scripts (sources\n",
    "acknowledged in raw*script*urls.txt). In order to collect the metadata\n",
    "necessary for this study and to distinguish between two script versions\n",
    "of the same movie, we automatically matched each script with an entry in\n",
    "movie database provided by IMDB (The Internet Movie Database; data\n",
    "interfaces available at http://www.imdb.com/interfaces). Some amount of\n",
    "manual correction was also involved. When more than one movie with the\n",
    "same title was found in IMBD, the match was made with the most popular\n",
    "title (the one that received most IMDB votes)\n",
    "\n",
    "After discarding all movies that could not be matched or that had less\n",
    "than 5 IMDB votes, we were left with 617 unique titles with metadata\n",
    "including genre, release year, IMDB rating and no. of IMDB votes and\n",
    "cast distribution. We then identified the pairs of characters that\n",
    "interact and separated their conversations automatically using simple\n",
    "data processing heuristics. After discarding all pairs that exchanged\n",
    "less than 5 conversational exchanges there were 10,292 left, exchanging\n",
    "220,579 conversational exchanges (304,713 utterances). After\n",
    "automatically matching the names of the 9,035 involved characters to the\n",
    "list of cast distribution, we used the gender of each interpreting actor\n",
    "to infer the fictional gender of a subset of 3,321 movie characters (we\n",
    "raised the number of gendered 3,774 characters through manual\n",
    "annotation). Similarly, we collected the end credit position of a subset\n",
    "of 3,321 characters as a proxy for their status.\n",
    "\n",
    "D\\) Contact:\n",
    "\n",
    "Please email any questions to: cristian@cs.cornell.edu (Cristian\n",
    "Danescu-Niculescu-Mizil)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "Step 2. Loading the Data and Data Cleaning\n",
    "------------------------------------------\n",
    "\n",
    "We have already used the wget command to download the file, and put it\n",
    "in our distributed file system (this process takes about 1 minute). To\n",
    "repeat these steps or to download data from another source follow the\n",
    "steps at the bottom of this worksheet on **Step 1. Downloading and\n",
    "Loading Data into DBFS**.\n",
    "\n",
    "Let's make sure these files are in dbfs now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// this is where the data resides in dbfs (see below to download it first, if you go to a new shard!)\n",
    "display(dbutils.fs.ls(\"dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Conversations Data\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile(\"dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_conversations.txt\").top(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Load text file, leave out file paths, convert all strings to lowercase\n",
    "val conversationsRaw = sc.textFile(\"dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_conversations.txt\").zipWithIndex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Review first 5 lines to get a sense for the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversationsRaw.top(5).foreach(println) // the first five Strings in the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversationsRaw.count // there are over 83,000 conversations in total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.{Failure, Success}\n",
    "\n",
    "val regexConversation = \"\"\"\\s*(\\w+)\\s+(\\+{3}\\$\\+{3})\\s*(\\w+)\\s+(\\2)\\s*(\\w+)\\s+(\\2)\\s*(\\[.*\\]\\s*$)\"\"\".r\n",
    "\n",
    "case class conversationLine(a: String, b: String, c: String, d: String)\n",
    "\n",
    "val conversationsRaw = sc.textFile(\"dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_conversations.txt\")\n",
    " .zipWithIndex()\n",
    "  .map(x => \n",
    "          {\n",
    "            val id:Long = x._2\n",
    "            val line = x._1\n",
    "            val pLine = regexConversation.findFirstMatchIn(line)\n",
    "                               .map(m => conversationLine(m.group(1), m.group(3), m.group(5), m.group(7))) \n",
    "                                  match {\n",
    "                                    case Some(l) => Success(l)\n",
    "                                    case None => Failure(new Exception(s\"Non matching input: $line\"))\n",
    "                                  }\n",
    "              (id,pLine)\n",
    "           }\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversationsRaw.filter(x => x._2.isSuccess).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversationsRaw.filter(x => x._2.isFailure).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "The conversation number and line numbers of each conversation are in one\n",
    "line in `conversationsRaw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversationsRaw.filter(x => x._2.isSuccess).take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Let's create `conversations` that have just the coversation id and\n",
    "line-number with order information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val conversations \n",
    "    = conversationsRaw\n",
    "      .filter(x => x._2.isSuccess)\n",
    "      .flatMap { \n",
    "        case (id,Success(l))  \n",
    "                  => { val conv = l.d.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace(\" \",\"\")\n",
    "                       val convLinesIndexed = conv.split(\",\").zipWithIndex\n",
    "                       convLinesIndexed.map( cLI => (id, cLI._2, cLI._1))\n",
    "                      }\n",
    "       }.toDF(\"conversationID\",\"intraConversationID\",\"lineID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Movie Titles\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val moviesMetaDataRaw = sc.textFile(\"dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_titles_metadata.txt\")\n",
    "moviesMetaDataRaw.top(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesMetaDataRaw.count() // number of movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.{Failure, Success}\n",
    "\n",
    "/*  - contains information about each movie title\n",
    "  - fields:\n",
    "          - movieID,\n",
    "          - movie title,\n",
    "          - movie year,\n",
    "          - IMDB rating,\n",
    "          - no. IMDB votes,\n",
    "          - genres in the format ['genre1','genre2',...,'genreN']\n",
    "          */\n",
    "val regexMovieMetaData = \"\"\"\\s*(\\w+)\\s+(\\+{3}\\$\\+{3})\\s*(.+)\\s+(\\2)\\s+(.+)\\s+(\\2)\\s+(.+)\\s+(\\2)\\s+(.+)\\s+(\\2)\\s+(\\[.*\\]\\s*$)\"\"\".r\n",
    "\n",
    "case class lineInMovieMetaData(movieID: String, movieTitle: String, movieYear: String, IMDBRating: String, NumIMDBVotes: String, genres: String)\n",
    "\n",
    "val moviesMetaDataRaw = sc.textFile(\"dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_titles_metadata.txt\")\n",
    "  .map(line => \n",
    "          {\n",
    "            val pLine = regexMovieMetaData.findFirstMatchIn(line)\n",
    "                               .map(m => lineInMovieMetaData(m.group(1), m.group(3), m.group(5), m.group(7), m.group(9), m.group(11))) \n",
    "                                  match {\n",
    "                                    case Some(l) => Success(l)\n",
    "                                    case None => Failure(new Exception(s\"Non matching input: $line\"))\n",
    "                                  }\n",
    "              pLine\n",
    "           }\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesMetaDataRaw.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesMetaDataRaw.filter(x => x.isSuccess).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesMetaDataRaw.filter(x => x.isSuccess).take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//moviesMetaDataRaw.filter(x => x.isFailure).take(10).foreach(println) // to regex refine for casting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val moviesMetaData \n",
    "    = moviesMetaDataRaw\n",
    "      .filter(x => x.isSuccess)\n",
    "      .map { case Success(l) => l }\n",
    "      .toDF().select(\"movieID\",\"movieTitle\",\"movieYear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesMetaData.show(10,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Lines Data\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val linesRaw = sc.textFile(\"dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_lines.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesRaw.count() // number of lines making up the conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Review first 5 lines to get a sense for the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesRaw.top(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "To see 5 random lines in the `lines.txt` evaluate the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesRaw.takeSample(false, 5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.{Failure, Success}\n",
    "\n",
    "/*  field in line.txt are:\n",
    "          - lineID\n",
    "          - characterID (who uttered this phrase)\n",
    "          - movieID\n",
    "          - character name\n",
    "          - text of the utterance\n",
    "          */\n",
    "val regexLine = \"\"\"\\s*(\\w+)\\s+(\\+{3}\\$\\+{3})\\s*(\\w+)\\s+(\\2)\\s*(\\w+)\\s+(\\2)\\s*(.+)\\s+(\\2)\\s*(.*$)\"\"\".r\n",
    "\n",
    "case class lineInMovie(lineID: String, characterID: String, movieID: String, characterName: String, text: String)\n",
    "\n",
    "val linesRaw = sc.textFile(\"dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_lines.txt\")\n",
    "  .map(line => \n",
    "          {\n",
    "            val pLine = regexLine.findFirstMatchIn(line)\n",
    "                               .map(m => lineInMovie(m.group(1), m.group(3), m.group(5), m.group(7), m.group(9))) \n",
    "                                  match {\n",
    "                                    case Some(l) => Success(l)\n",
    "                                    case None => Failure(new Exception(s\"Non matching input: $line\"))\n",
    "                                  }\n",
    "              pLine\n",
    "           }\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesRaw.filter(x => x.isSuccess).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesRaw.filter(x => x.isFailure).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesRaw.filter(x => x.isSuccess).take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Let's make a DataFrame out of the successfully parsed line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lines \n",
    "    = linesRaw\n",
    "      .filter(x => x.isSuccess)\n",
    "      .map { case Success(l) => l }\n",
    "      .toDF()\n",
    "      .join(moviesMetaData, \"movieID\") // and join it to get movie meta data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Dialogs with Lines\n",
    "------------------\n",
    "\n",
    "Let's join ght two DataFrames on `lineID` next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val convLines = conversations.join(lines, \"lineID\").sort($\"conversationID\", $\"intraConversationID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convLines.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(convLines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Let's amalgamate the texts utered in the same conversations together.\n",
    "\n",
    "By doing this we loose all the information in the order of utterance.\n",
    "\n",
    "But this is fine as we are going to do LDA with just the *first-order\n",
    "information of words uttered in each conversation* by anyone involved in\n",
    "the dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{collect_list, udf, lit, concat_ws}\n",
    "\n",
    "val corpusDF = convLines.groupBy($\"conversationID\",$\"movieID\")\n",
    "  .agg(concat_ws(\" :-()-: \",collect_list($\"text\")).alias(\"corpus\"))\n",
    "  .join(moviesMetaData, \"movieID\") // and join it to get movie meta data\n",
    "  .select($\"conversationID\".as(\"id\"),$\"corpus\",$\"movieTitle\",$\"movieYear\")\n",
    "  .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusDF.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(corpusDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Feature extraction and transformation APIs\n",
    "------------------------------------------\n",
    "\n",
    "We will use the convenient [Feature extraction and transformation\n",
    "APIs](http://spark.apache.org/docs/latest/ml-features.html).\n",
    "\n",
    "Step 3. Text Tokenization\n",
    "-------------------------\n",
    "\n",
    "We will use the RegexTokenizer to split each document into tokens. We\n",
    "can setMinTokenLength() here to indicate a minimum token length, and\n",
    "filter away all tokens that fall below the minimum. See:\n",
    "\n",
    "-   <http://spark.apache.org/docs/latest/ml-features.html#tokenizer>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "\n",
    "// Set params for RegexTokenizer\n",
    "val tokenizer = new RegexTokenizer()\n",
    ".setPattern(\"[\\\\W_]+\") // break by white space character(s)\n",
    ".setMinTokenLength(4) // Filter away tokens with length < 4\n",
    ".setInputCol(\"corpus\") // name of the input column\n",
    ".setOutputCol(\"tokens\") // name of the output column\n",
    "\n",
    "// Tokenize document\n",
    "val tokenized_df = tokenizer.transform(corpusDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tokenized_df.sample(false,0.001,1234L)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tokenized_df.sample(false,0.001,123L).select(\"tokens\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Step 4. Remove Stopwords\n",
    "------------------------\n",
    "\n",
    "We can easily remove stopwords using the StopWordsRemover(). See:\n",
    "\n",
    "-   <http://spark.apache.org/docs/latest/ml-features.html#stopwordsremover>.\n",
    "\n",
    "If a list of stopwords is not provided, the StopWordsRemover() will use\n",
    "[this list of\n",
    "stopwords](http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words),\n",
    "also shown below, by default.\n",
    "\n",
    "`a,about,above,across,after,afterwards,again,against,all,almost,alone,along,already,also,although,always,am,among,amongst,amoungst,amount,an,and,another,any,anyhow,anyone,anything,anyway,anywhere, are,around,as,at,back,be,became,because,become,becomes,becoming,been,before,beforehand,behind,being,below,beside,besides,between,beyond,bill,both,bottom,but,by,call,can,cannot,cant,co,computer,con,could, couldnt,cry,de,describe,detail,do,done,down,due,during,each,eg,eight,either,eleven,else,elsewhere,empty,enough,etc,even,ever,every,everyone,everything,everywhere,except,few,fifteen,fify,fill,find,fire,first, five,for,former,formerly,forty,found,four,from,front,full,further,get,give,go,had,has,hasnt,have,he,hence,her,here,hereafter,hereby,herein,hereupon,hers,herself,him,himself,his,how,however,hundred,i,ie,if, in,inc,indeed,interest,into,is,it,its,itself,keep,last,latter,latterly,least,less,ltd,made,many,may,me,meanwhile,might,mill,mine,more,moreover,most,mostly,move,much,must,my,myself,name,namely,neither,never, nevertheless,next,nine,no,nobody,none,noone,nor,not,nothing,now,nowhere,of,off,often,on,once,one,only,onto,or,other,others,otherwise,our,ours,ourselves,out,over,own,part,per,perhaps,please,put,rather,re,same, see,seem,seemed,seeming,seems,serious,several,she,should,show,side,since,sincere,six,sixty,so,some,somehow,someone,something,sometime,sometimes,somewhere,still,such,system,take,ten,than,that,the,their,them, themselves,then,thence,there,thereafter,thereby,therefore,therein,thereupon,these,they,thick,thin,third,this,those,though,three,through,throughout,thru,thus,to,together,too,top,toward,towards,twelve,twenty,two, un,under,until,up,upon,us,very,via,was,we,well,were,what,whatever,when,whence,whenever,where,whereafter,whereas,whereby,wherein,whereupon,wherever,whether,which,while,whither,who,whoever,whole,whom,whose,why,will, with,within,without,would,yet,you,your,yours,yourself,yourselves`\n",
    "\n",
    "You can use `getStopWords()` to see the list of stopwords that will be\n",
    "used.\n",
    "\n",
    "In this example, we will specify a list of stopwords for the\n",
    "StopWordsRemover() to use. We do this so that we can add on to the list\n",
    "later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/tmp/stopwords\")) // check if the file already exists from earlier wget and dbfs-load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "If the file `dbfs:/tmp/stopwords` already exists then skip the next two\n",
    "cells, otherwise download and load it into DBFS by uncommenting and\n",
    "evaluating the next two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words -O /tmp/stopwords # uncomment '//' at the beginning and repeat only if needed again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp file:/tmp/stopwords dbfs:/tmp/stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// List of stopwords\n",
    "val stopwords = sc.textFile(\"/tmp/stopwords\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.length // find the number of stopwords in the scala Array[String]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Finally, we can just remove the stopwords using the `StopWordsRemover`\n",
    "as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "\n",
    "// Set params for StopWordsRemover\n",
    "val remover = new StopWordsRemover()\n",
    ".setStopWords(stopwords) // This parameter is optional\n",
    ".setInputCol(\"tokens\")\n",
    ".setOutputCol(\"filtered\")\n",
    "\n",
    "// Create new DF with Stopwords removed\n",
    "val filtered_df = remover.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Step 5. Vector of Token Counts\n",
    "------------------------------\n",
    "\n",
    "LDA takes in a vector of token counts as input. We can use the\n",
    "`CountVectorizer()` to easily convert our text documents into vectors of\n",
    "token counts.\n",
    "\n",
    "The `CountVectorizer` will return\n",
    "`(VocabSize, Array(Indexed Tokens), Array(Token Frequency))`.\n",
    "\n",
    "Two handy parameters to note:\n",
    "\n",
    "-   `setMinDF`: Specifies the minimum number of different documents a\n",
    "    term must appear in to be included in the vocabulary.\n",
    "-   `setMinTF`: Specifies the minimum number of times a term has to\n",
    "    appear in a document to be included in the vocabulary.\n",
    "\n",
    "See:\n",
    "\n",
    "-   <http://spark.apache.org/docs/latest/ml-features.html#countvectorizer>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.CountVectorizer\n",
    "\n",
    "// Set params for CountVectorizer\n",
    "val vectorizer = new CountVectorizer()\n",
    ".setInputCol(\"filtered\")\n",
    ".setOutputCol(\"features\")\n",
    ".setVocabSize(10000) \n",
    ".setMinDF(5) // the minimum number of different documents a term must appear in to be included in the vocabulary.\n",
    ".fit(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create vector of token counts\n",
    "val countVectors = vectorizer.transform(filtered_df).select(\"id\", \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// see the first countVectors\n",
    "countVectors.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "To use the LDA algorithm in the MLlib library, we have to convert the\n",
    "DataFrame back into an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Convert DF to RDD - ideally we should use ml for everything an not ml and mllib ; DAN\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, RegexTokenizer, StopWordsRemover}\n",
    "import org.apache.spark.ml.linalg.{Vector => MLVector}\n",
    "import org.apache.spark.mllib.clustering.{LDA, OnlineLDAOptimizer}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.sql.{Row, SparkSession}\n",
    "\n",
    "val lda_countVector = countVectors.map { case Row(id: Long, countVector: MLVector) => (id, Vectors.fromML(countVector)) }.rdd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// format: Array(id, (VocabSize, Array(indexedTokens), Array(Token Frequency)))\n",
    "lda_countVector.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Let's get an overview of LDA in Spark's MLLIB\n",
    "---------------------------------------------\n",
    "\n",
    "See:\n",
    "\n",
    "-   <http://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda>.\n",
    "\n",
    "Create LDA model with Online Variational Bayes\n",
    "----------------------------------------------\n",
    "\n",
    "We will now set the parameters for LDA. We will use the\n",
    "OnlineLDAOptimizer() here, which implements Online Variational Bayes.\n",
    "\n",
    "Choosing the number of topics for your LDA model requires a bit of\n",
    "domain knowledge. As we do not know the number of \"topics\", we will set\n",
    "numTopics to be 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val numTopics = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "We will set the parameters needed to build our LDA model. We can also\n",
    "setMiniBatchFraction for the OnlineLDAOptimizer, which sets the fraction\n",
    "of corpus sampled and used at each iteration. In this example, we will\n",
    "set this to 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.clustering.{LDA, OnlineLDAOptimizer}\n",
    "\n",
    "// Set LDA params\n",
    "val lda = new LDA()\n",
    ".setOptimizer(new OnlineLDAOptimizer().setMiniBatchFraction(0.8))\n",
    ".setK(numTopics)\n",
    ".setMaxIterations(3)\n",
    ".setDocConcentration(-1) // use default values\n",
    ".setTopicConcentration(-1) // use default values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Create the LDA model with Online Variational Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ldaModel = lda.run(lda_countVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Watch **Online Learning for Latent Dirichlet Allocation** in NIPS2010 by\n",
    "Matt Hoffman (right click and open in new tab)\n",
    "\n",
    "[!\\[Matt Hoffman's NIPS 2010 Talk Online\n",
    "LDA\\]](http://videolectures.net/nips2010_hoffman_oll/thumb.jpg)\\](http://videolectures.net/nips2010*hoffman*oll/)\n",
    "\n",
    "Also see the paper on *Online varioational Bayes* by Matt linked for\n",
    "more details (from the above URL):\n",
    "[http://videolectures.net/site/normal*dl/tag=83534/nips2010*1291.pdf](http://videolectures.net/site/normal_dl/tag=83534/nips2010_1291.pdf)\n",
    "\n",
    "Note that using the OnlineLDAOptimizer returns us a\n",
    "[LocalLDAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.LocalLDAModel),\n",
    "which stores the inferred topics of your corpus.\n",
    "\n",
    "Review Topics\n",
    "-------------\n",
    "\n",
    "We can now review the results of our LDA model. We will print out all 20\n",
    "topics with their corresponding term probabilities.\n",
    "\n",
    "Note that you will get slightly different results every time you run an\n",
    "LDA model since LDA includes some randomization.\n",
    "\n",
    "Let us review results of LDA model with Online Variational Bayes, step\n",
    "by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val vocabList = vectorizer.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val topics = topicIndices.map { case (terms, termWeights) =>\n",
    "  terms.map(vocabList(_)).zip(termWeights)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Feel free to take things apart to understand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicIndices(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicIndices(0)._1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicIndices(0)._1(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabList(topicIndices(0)._1(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Review Results of LDA model with Online Variational Bayes - Doing all\n",
    "four steps earlier at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 5)\n",
    "val vocabList = vectorizer.vocabulary\n",
    "val topics = topicIndices.map { case (terms, termWeights) =>\n",
    "  terms.map(vocabList(_)).zip(termWeights)\n",
    "}\n",
    "println(s\"$numTopics topics:\")\n",
    "topics.zipWithIndex.foreach { case (topic, i) =>\n",
    "  println(s\"TOPIC $i\")\n",
    "  topic.foreach { case (term, weight) => println(s\"$term\\t$weight\") }\n",
    "  println(s\"==========\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Going through the results, you may notice that some of the topic words\n",
    "returned are actually stopwords that are specific to our dataset (for\n",
    "eg: \"writes\", \"article\"...). Let's try improving our model.\n",
    "\n",
    "Step 8. Model Tuning - Refilter Stopwords\n",
    "-----------------------------------------\n",
    "\n",
    "We will try to improve the results of our model by identifying some\n",
    "stopwords that are specific to our dataset. We will filter these\n",
    "stopwords out and rerun our LDA model to see if we get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val add_stopwords = Array(\"whatever\") // add  more stop-words like the name of your company!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Combine newly identified stopwords to our exising list of stopwords\n",
    "val new_stopwords = stopwords.union(add_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "\n",
    "// Set Params for StopWordsRemover with new_stopwords\n",
    "val remover = new StopWordsRemover()\n",
    ".setStopWords(new_stopwords)\n",
    ".setInputCol(\"tokens\")\n",
    ".setOutputCol(\"filtered\")\n",
    "\n",
    "// Create new df with new list of stopwords removed\n",
    "val new_filtered_df = remover.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set Params for CountVectorizer\n",
    "val vectorizer = new CountVectorizer()\n",
    ".setInputCol(\"filtered\")\n",
    ".setOutputCol(\"features\")\n",
    ".setVocabSize(10000)\n",
    ".setMinDF(5)\n",
    ".fit(new_filtered_df)\n",
    "\n",
    "// Create new df of countVectors\n",
    "val new_countVectors = vectorizer.transform(new_filtered_df).select(\"id\", \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Convert DF to RDD\n",
    "val new_lda_countVector = new_countVectors.map { case Row(id: Long, countVector: MLVector) => (id, Vectors.fromML(countVector)) }.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "We will also increase MaxIterations to 10 to see if we get better\n",
    "results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set LDA parameters\n",
    "val new_lda = new LDA()\n",
    ".setOptimizer(new OnlineLDAOptimizer().setMiniBatchFraction(0.8))\n",
    ".setK(numTopics)\n",
    ".setMaxIterations(10)\n",
    ".setDocConcentration(-1) // use default values\n",
    ".setTopicConcentration(-1) // use default values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "#### How to find what the default values are?\n",
    "\n",
    "Dive into the source!!!\n",
    "\n",
    "1.  Let's find the default value for `docConcentration` now.\n",
    "2.  Got to Apache Spark package Root:\n",
    "    <https://spark.apache.org/docs/latest/api/scala/#package>\n",
    "\n",
    "-   search for 'ml' in the search box on the top left (ml is for ml\n",
    "    library)\n",
    "-   Then find the `LDA` by scrolling below on the left to mllib's\n",
    "    `clustering` methods and click on `LDA`\n",
    "-   Then click on the source code link which should take you here:\n",
    "    -   <https://github.com/apache/spark/blob/v1.6.1/mllib/src/main/scala/org/apache/spark/ml/clustering/LDA.scala>\n",
    "    -   Now, simply go to the right function and see the following\n",
    "        comment block:\n",
    "\n",
    "    \\`\\`\\` /\\*\\*\n",
    "    -   Concentration parameter (commonly named \"alpha\") for the prior\n",
    "        placed on documents'\n",
    "\n",
    "    -   distributions over topics (\"theta\").\n",
    "\n",
    "    -   \n",
    "\n",
    "    -   This is the parameter to a Dirichlet distribution, where larger\n",
    "        values mean more smoothing\n",
    "\n",
    "    -   (more regularization).\n",
    "\n",
    "    -   \n",
    "\n",
    "    -   If not set by the user, then docConcentration is set\n",
    "        automatically. If set to\n",
    "\n",
    "    -   singleton vector \\[alpha\\], then alpha is replicated to a vector\n",
    "        of length k in fitting.\n",
    "\n",
    "    -   Otherwise, the \\[\\[docConcentration\\]\\] vector must be length k.\n",
    "\n",
    "    -   (default = automatic)\n",
    "\n",
    "    -   \n",
    "\n",
    "    -   Optimizer-specific parameter settings:\n",
    "\n",
    "    -   -   EM\n",
    "\n",
    "    -   - Currently only supports symmetric distributions, so all values in the vector should be\n",
    "\n",
    "    -     the same.\n",
    "\n",
    "    -   - Values should be > 1.0\n",
    "\n",
    "    -   - default = uniformly (50 / k) + 1, where 50/k is common in LDA libraries and +1 follows\n",
    "\n",
    "    -     from Asuncion et al. (2009), who recommend a +1 adjustment for EM.\n",
    "\n",
    "    -   -   Online\n",
    "\n",
    "    -   - Values should be >= 0\n",
    "\n",
    "    -   - default = uniformly (1.0 / k), following the implementation from\n",
    "\n",
    "    -     [[https://github.com/Blei-Lab/onlineldavb]].\n",
    "\n",
    "    -   @group param \\*/ \\`\\`\\`\n",
    "\n",
    "**HOMEWORK:** Try to find the default value for `TopicConcentration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create LDA model with stopwords refiltered\n",
    "val new_ldaModel = new_lda.run(new_lda_countVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val topicIndices = new_ldaModel.describeTopics(maxTermsPerTopic = 5)\n",
    "val vocabList = vectorizer.vocabulary\n",
    "val topics = topicIndices.map { case (terms, termWeights) =>\n",
    "  terms.map(vocabList(_)).zip(termWeights)\n",
    "}\n",
    "println(s\"$numTopics topics:\")\n",
    "topics.zipWithIndex.foreach { case (topic, i) =>\n",
    "  println(s\"TOPIC $i\")\n",
    "  topic.foreach { case (term, weight) => println(s\"$term\\t$weight\") }\n",
    "  println(s\"==========\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Step 9. Create LDA model with Expectation Maximization\n",
    "------------------------------------------------------\n",
    "\n",
    "Let's try creating an LDA model with Expectation Maximization on the\n",
    "data that has been refiltered for additional stopwords. We will also\n",
    "increase MaxIterations here to 100 to see if that improves results. See:\n",
    "\n",
    "-   <http://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.clustering.EMLDAOptimizer\n",
    "\n",
    "// Set LDA parameters\n",
    "val em_lda = new LDA()\n",
    ".setOptimizer(new EMLDAOptimizer())\n",
    ".setK(numTopics)\n",
    ".setMaxIterations(100)\n",
    ".setDocConcentration(-1) // use default values\n",
    ".setTopicConcentration(-1) // use default values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val em_ldaModel = em_lda.run(new_lda_countVector) // takes a long long time 22 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.clustering.DistributedLDAModel;\n",
    "val em_DldaModel = em_ldaModel.asInstanceOf[DistributedLDAModel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val top10ConversationsPerTopic = em_DldaModel.topDocumentsPerTopic(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10ConversationsPerTopic.length // number of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//em_DldaModel.topicDistributions.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Note that the EMLDAOptimizer produces a DistributedLDAModel, which\n",
    "stores not only the inferred topics but also the full training corpus\n",
    "and topic distributions for each document in the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val topicIndices = em_ldaModel.describeTopics(maxTermsPerTopic = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val vocabList = vectorizer.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabList.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val topics = topicIndices.map { case (terms, termWeights) =>\n",
    "  terms.map(vocabList(_)).zip(termWeights)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabList(47) // 47 is the index of the term 'university' or the first term in topics - this may change due to randomness in algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "This is just doing it all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val topicIndices = em_ldaModel.describeTopics(maxTermsPerTopic = 5)\n",
    "val vocabList = vectorizer.vocabulary\n",
    "val topics = topicIndices.map { case (terms, termWeights) =>\n",
    "  terms.map(vocabList(_)).zip(termWeights)\n",
    "}\n",
    "println(s\"$numTopics topics:\")\n",
    "topics.zipWithIndex.foreach { case (topic, i) =>\n",
    "  println(s\"TOPIC $i\")\n",
    "  topic.foreach { case (term, weight) => println(s\"$term\\t$weight\") }\n",
    "  println(s\"==========\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10ConversationsPerTopic(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10ConversationsPerTopic(2)._1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val scenesForTopic2 = sc.parallelize(top10ConversationsPerTopic(2)._1).toDF(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scenesForTopic2.join(corpusDF,\"id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(top10ConversationsPerTopic(2)._1).toDF(\"id\").join(corpusDF,\"id\").show(10,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(top10ConversationsPerTopic(5)._1).toDF(\"id\").join(corpusDF,\"id\").show(10,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "We've managed to get some good results here. For example, we can easily\n",
    "infer that Topic 2 is about space, Topic 3 is about israel, etc.\n",
    "\n",
    "We still get some ambiguous results like Topic 0.\n",
    "\n",
    "To improve our results further, we could employ some of the below\n",
    "methods:\n",
    "\n",
    "-   Refilter data for additional data-specific stopwords\n",
    "-   Use Stemming or Lemmatization to preprocess data\n",
    "-   Experiment with a smaller number of topics, since some of these\n",
    "    topics in the 20 Newsgroups are pretty similar\n",
    "-   Increase model's MaxIterations\n",
    "\n",
    "Visualize Results\n",
    "-----------------\n",
    "\n",
    "We will try visualizing the results obtained from the EM LDA model with\n",
    "a d3 bubble chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Zip topic terms with topic IDs\n",
    "val termArray = topics.zipWithIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Transform data into the form (term, probability, topicId)\n",
    "val termRDD = sc.parallelize(termArray)\n",
    "val termRDD2 =termRDD.flatMap( (x: (Array[(String, Double)], Int)) => {\n",
    "  val arrayOfTuple = x._1\n",
    "  val topicId = x._2\n",
    "  arrayOfTuple.map(el => (el._1, el._2, topicId))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create DF with proper column names\n",
    "val termDF = termRDD2.toDF.withColumnRenamed(\"_1\", \"term\").withColumnRenamed(\"_2\", \"probability\").withColumnRenamed(\"_3\", \"topicId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(termDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "We will convert the DataFrame into a JSON format, which will be passed\n",
    "into d3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create JSON data\n",
    "val rawJson = termDF.toJSON.collect().mkString(\",\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "We are now ready to use D3 on the rawJson data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayHTML(s\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<meta charset=\"utf-8\">\n",
    "<style>\n",
    "\n",
    "circle {\n",
    "  fill: rgb(31, 119, 180);\n",
    "  fill-opacity: 0.5;\n",
    "  stroke: rgb(31, 119, 180);\n",
    "  stroke-width: 1px;\n",
    "}\n",
    "\n",
    ".leaf circle {\n",
    "  fill: #ff7f0e;\n",
    "  fill-opacity: 1;\n",
    "}\n",
    "\n",
    "text {\n",
    "  font: 14px sans-serif;\n",
    "}\n",
    "\n",
    "</style>\n",
    "<body>\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\"></script>\n",
    "<script>\n",
    "\n",
    "var json = {\n",
    " \"name\": \"data\",\n",
    " \"children\": [\n",
    "  {\n",
    "     \"name\": \"topics\",\n",
    "     \"children\": [\n",
    "      ${rawJson}\n",
    "     ]\n",
    "    }\n",
    "   ]\n",
    "};\n",
    "\n",
    "var r = 1500,\n",
    "    format = d3.format(\",d\"),\n",
    "    fill = d3.scale.category20c();\n",
    "\n",
    "var bubble = d3.layout.pack()\n",
    "    .sort(null)\n",
    "    .size([r, r])\n",
    "    .padding(1.5);\n",
    "\n",
    "var vis = d3.select(\"body\").append(\"svg\")\n",
    "    .attr(\"width\", r)\n",
    "    .attr(\"height\", r)\n",
    "    .attr(\"class\", \"bubble\");\n",
    "\n",
    "  \n",
    "var node = vis.selectAll(\"g.node\")\n",
    "    .data(bubble.nodes(classes(json))\n",
    "    .filter(function(d) { return !d.children; }))\n",
    "    .enter().append(\"g\")\n",
    "    .attr(\"class\", \"node\")\n",
    "    .attr(\"transform\", function(d) { return \"translate(\" + d.x + \",\" + d.y + \")\"; })\n",
    "    color = d3.scale.category20();\n",
    "  \n",
    "  node.append(\"title\")\n",
    "      .text(function(d) { return d.className + \": \" + format(d.value); });\n",
    "\n",
    "  node.append(\"circle\")\n",
    "      .attr(\"r\", function(d) { return d.r; })\n",
    "      .style(\"fill\", function(d) {return color(d.topicName);});\n",
    "\n",
    "var text = node.append(\"text\")\n",
    "    .attr(\"text-anchor\", \"middle\")\n",
    "    .attr(\"dy\", \".3em\")\n",
    "    .text(function(d) { return d.className.substring(0, d.r / 3)});\n",
    "  \n",
    "  text.append(\"tspan\")\n",
    "      .attr(\"dy\", \"1.2em\")\n",
    "      .attr(\"x\", 0)\n",
    "      .text(function(d) {return Math.ceil(d.value * 10000) /10000; });\n",
    "\n",
    "// Returns a flattened hierarchy containing all leaf nodes under the root.\n",
    "function classes(root) {\n",
    "  var classes = [];\n",
    "\n",
    "  function recurse(term, node) {\n",
    "    if (node.children) node.children.forEach(function(child) { recurse(node.term, child); });\n",
    "    else classes.push({topicName: node.topicId, className: node.term, value: node.probability});\n",
    "  }\n",
    "\n",
    "  recurse(null, root);\n",
    "  return {children: classes};\n",
    "}\n",
    "</script>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Step 1. Downloading and Loading Data into DBFS\n",
    "----------------------------------------------\n",
    "\n",
    "Here are the steps taken for downloading and saving data to the\n",
    "distributed file system. Uncomment them for repeating this process on\n",
    "your databricks cluster or for downloading a new source of data.\n",
    "\n",
    "Unfortunately, the original data at:\n",
    "\n",
    "-   [http://www.mpi-sws.org/~cristian/data/cornell*movie*dialogs\\_corpus.zip](http://www.mpi-sws.org/~cristian/data/cornell_movie_dialogs_corpus.zip)\n",
    "\n",
    "is not suited for manipulation and loading into dbfs easily. So the data\n",
    "has been downloaded, directory renamed without white spaces, superfluous\n",
    "OS-specific files removed, `dos2unix`'d, `tar -zcvf`'d and uploaded to\n",
    "the following URL for an easily dbfs-loadable download:\n",
    "\n",
    "-   [http://lamastex.org/datasets/public/nlp/cornell*movie*dialogs\\_corpus.tgz](http://lamastex.org/datasets/public/nlp/cornell_movie_dialogs_corpus.tgz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget http://lamastex.org/datasets/public/nlp/cornell_movie_dialogs_corpus.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Untar the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar zxvf cornell_movie_dialogs_corpus.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Let us list and load all the files into dbfs after `dbfs.fs.mkdirs(...)`\n",
    "to create the directory\n",
    "`dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd && ls -al cornell_movie_dialogs_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/\",true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dbutils.fs.cp(\"file:///databricks/driver/cornell_movie_dialogs_corpus/movie_characters_metadata.txt\",\"dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_characters_metadata.txt\")\n",
    "dbutils.fs.cp(\"file:///databricks/driver/cornell_movie_dialogs_corpus/movie_conversations.txt\",\"dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_conversations.txt\")\n",
    "dbutils.fs.cp(\"file:///databricks/driver/cornell_movie_dialogs_corpus/movie_lines.txt\",\"dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_lines.txt\")\n",
    "dbutils.fs.cp(\"file:///databricks/driver/cornell_movie_dialogs_corpus/movie_titles_metadata.txt\",\"dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_titles_metadata.txt\")\n",
    "dbutils.fs.cp(\"file:///databricks/driver/cornell_movie_dialogs_corpus/raw_script_urls.txt\",\"dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/raw_script_urls.txt\")\n",
    "dbutils.fs.cp(\"file:///databricks/driver/cornell_movie_dialogs_corpus/README.txt\",\"dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/README.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
