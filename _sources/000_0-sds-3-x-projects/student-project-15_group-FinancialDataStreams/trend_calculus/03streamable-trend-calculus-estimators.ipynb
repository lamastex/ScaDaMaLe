{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Model for Trend Calculus\n",
    "===============================\n",
    "\n",
    "Johannes Graner, Albert Nilsson and Raazesh Sainudiin\n",
    "\n",
    "2020, Uppsala, Sweden\n",
    "\n",
    "This project was supported by Combient Mix AB through summer internships\n",
    "at:\n",
    "\n",
    "Combient Competence Centre for Data Engineering Sciences, Department of\n",
    "Mathematics, Uppsala University, Uppsala, Sweden\n",
    "\n",
    "Resources\n",
    "---------\n",
    "\n",
    "This builds on the following library and its antecedents therein:\n",
    "\n",
    "-   <https://github.com/lamastex/spark-trend-calculus>\n",
    "\n",
    "This work was inspired by:\n",
    "--------------------------\n",
    "\n",
    "-   Antoine Aamennd's\n",
    "    [texata-2017](https://github.com/aamend/texata-r2-2017)\n",
    "-   Andrew Morgan's [Trend Calculus\n",
    "    Library](https://github.com/ByteSumoLtd/TrendCalculus-lua)\n",
    "\n",
    "We use the dataset generated in the last notebook to build a simple,\n",
    "proof of concept Markov model for predicting trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.sql.Timestamp\n",
    "import io.delta.tables._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.streaming.{GroupState, GroupStateTimeout, OutputMode, Trigger}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.expressions.{Window, WindowSpec}\n",
    "import org.lamastex.spark.trendcalculus._\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import java.sql.Timestamp\n",
    ">     import io.delta.tables._\n",
    ">     import org.apache.spark.sql._\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     import org.apache.spark.sql.streaming.{GroupState, GroupStateTimeout, OutputMode, Trigger}\n",
    ">     import org.apache.spark.sql.types._\n",
    ">     import org.apache.spark.sql.expressions.{Window, WindowSpec}\n",
    ">     import org.lamastex.spark.trendcalculus._\n",
    ">     import scala.util.Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"m\", \"5\", (1 to 10).map(_.toString).toSeq ++ Seq(15,20,25,30).map(_.toString) :+ \"100\")\n",
    "dbutils.widgets.dropdown(\"n\", \"1\", (1 to 3).map(_.toString).toSeq)\n",
    "dbutils.widgets.dropdown(\"k\", \"max\", (1 to 10).map(_.toString).toSeq :+ \"max\")\n",
    "dbutils.widgets.dropdown(\"numTrainingSets\", \"10\", (1 to 20).map( i => (i*5).toString).toSeq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Reading the joined dataset from the last notebook.\n",
    "\n",
    "We train the model using both oil and gold data and predict trends in\n",
    "oil data. We show that this yields better results than just training on\n",
    "the oil data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val maxRevPath = \"s3a://osint-gdelt-reado/canwrite/summerinterns2020/johannes/streamable-trend-calculus/maxRev\"\n",
    "val maxRevDS = spark.read.format(\"delta\").load(maxRevPath).as[FlatReversal]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "We want to predict what the trend of the next data point will be given\n",
    "the trend reversals we have observed.\n",
    "\n",
    "For this, we use an m-th order Markov model. We look at the reversal\n",
    "state of the last `m` points and use this to predict the trends in the\n",
    "next `n` points. `k` is the maximum order of reversal that is considered\n",
    "when training the model.\n",
    "\n",
    "`trainingRatio` is the ratio of the data used for training the model,\n",
    "the rest is used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val modelPath = \"s3a://osint-gdelt-reado/canwrite/summerinterns2020/johannes/streamable-trend-calculus/estimators/\"\n",
    "val maxRevDSWithLagCountPath = modelPath + \"maxRevDSWithLag\"\n",
    "\n",
    "val numPartitions = dbutils.widgets.get(\"numTrainingSets\").toInt // 5\n",
    "val partialModelPaths = (1 to numPartitions).map( i => modelPath + s\"partialModel${i}\" )\n",
    "val fullModelPath = modelPath + \"fullModel\"\n",
    "\n",
    "val m = dbutils.widgets.get(\"m\").toInt // 5\n",
    "val n = dbutils.widgets.get(\"n\").toInt // 1\n",
    "val k = dbutils.widgets.get(\"k\") match { // 17\n",
    "  case \"max\" => math.abs(maxRevDS.orderBy(abs($\"reversal\").desc).first.reversal) + 1\n",
    "  case _ => dbutils.widgets.get(\"k\").toInt\n",
    "}\n",
    "val trainingRatio = 0.7\n",
    "type FinalModel = Map[Seq[Int], Map[Seq[Int], Double]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncRev(k: Int)(rev: Int): Int = {\n",
    "  if (math.abs(rev) > k) k*rev.signum else rev\n",
    "}\n",
    "val truncRevUDF = udf{ rev: Int => rev.signum }\n",
    "def truncRevsUDF(k: Int) = udf{ revs: Seq[Int] => revs.map(truncRev(k)) }\n",
    "\n",
    "def lagColumn(df: DataFrame, orderColumnName: String, lagKeyName: String, lagValueName: String, m: Int, n: Int): DataFrame = {\n",
    "  val windowSpec = Window.partitionBy(\"ticker\").orderBy(orderColumnName)\n",
    "  val laggedKeyColNames = (1 to m).map( i => s\"lagKey$i\" ).toSeq\n",
    "  val laggedValueColNames = (1 to n).map( i => s\"lagValue$i\" ).toSeq\n",
    "  val dfWithLaggedKeyColumns = (n+1 to m+n)\n",
    "    .foldLeft(df)( (df: DataFrame, i: Int) => df.withColumn(laggedKeyColNames(i-n-1), lag(lagKeyName, i-1, Int.MaxValue).over(windowSpec)) )\n",
    "  val dfWithLaggedKeyValueColumns = (1 to n)\n",
    "    .foldLeft(dfWithLaggedKeyColumns)( (df: DataFrame, i: Int) => df.withColumn(laggedValueColNames(i-1), lag(lagValueName, i-1, Int.MaxValue).over(windowSpec)) )\n",
    "  \n",
    "  dfWithLaggedKeyValueColumns\n",
    "    .withColumn(\"lagKey\", array(laggedKeyColNames.reverse.take(m).map(col(_)):_*))\n",
    "    .withColumn(\"lagValue\", array(laggedValueColNames.reverse.takeRight(n).map(col(_)):_*))\n",
    "    .withColumn(\"lagKeyFirst\", col(laggedKeyColNames.last))\n",
    "    .filter($\"lagKeyFirst\" =!= Int.MaxValue)\n",
    "    .drop(\"lagKeyFirst\")\n",
    "    .drop(laggedKeyColNames:_*)\n",
    "    .drop(laggedValueColNames:_*)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "The trend at each point can be extracted from the trend reversals by\n",
    "taking the sum of all previous 1-st order trend reversals. This sum will\n",
    "always be either 0 (up trend) or -1 (down trend) and 0 is therefore\n",
    "mapped to 1 to get (1, -1) as (up, down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val maxRevDSWithLag = lagColumn(\n",
    "  maxRevDS\n",
    "    .orderBy(\"x\")\n",
    "    .toDF\n",
    "    .withColumn(\"truncRev\", truncRevUDF($\"reversal\"))\n",
    "    .withColumn(\"tmpTrend\", sum(\"truncRev\").over(Window.partitionBy(\"ticker\").orderBy(\"x\").rowsBetween(Window.unboundedPreceding, Window.currentRow)))\n",
    "    .withColumn(\"trend\", when($\"tmpTrend\" === 0, 1).otherwise(-1))\n",
    "    .drop(\"truncRev\", \"tmpTrend\"),\n",
    "  \"x\", \n",
    "  \"reversal\",\n",
    "  \"trend\",\n",
    "  m, \n",
    "  n\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "We now want to predict `lagValue` from `lagKey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(maxRevDSWithLag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Cleaning up last run and writing model training input to delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.rm(maxRevDSWithLagCountPath, recurse=true)\n",
    "\n",
    "maxRevDSWithLag\n",
    "  //.withColumn(\"lagValueTrunc\", truncRevsUDF(1)($\"lagValue\"))\n",
    "  .withColumn(\"count\", lit(1L))\n",
    "  .write\n",
    "  .format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .save(maxRevDSWithLagCountPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partialModelPaths.map(dbutils.fs.rm(_, recurse=true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val divUDF = udf{ (a: Long, b: Long) => a.toDouble/b }\n",
    "val maxRevDSWithLagCount = spark.read.format(\"delta\").load(maxRevDSWithLagCountPath)\n",
    "val numberOfRows = maxRevDSWithLagCount.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "The data is split into training and testing data. This is *not* done\n",
    "randomly as there is a dependence on previous data points. We don't want\n",
    "to train on data that is dependent on the testing data and therefore the\n",
    "training data consists on looking at the first (for example) 70% of the\n",
    "data and the last 30% is saved for testing. This also reflects how the\n",
    "model would be used since we can only train on data points that have\n",
    "already been observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val tickers = maxRevDSWithLagCount.select(\"ticker\").distinct.as[String].collect.toSeq\n",
    "val tickerDFs = tickers.map( ticker => maxRevDSWithLagCount.filter($\"ticker\" === ticker))\n",
    "val trainingDF = tickerDFs.map( df => df.limit((df.count*trainingRatio).toInt) ).reduce( _.union(_) ).orderBy(\"x\")\n",
    "val trainingRows = trainingDF.count\n",
    "\n",
    "val testingDF = maxRevDSWithLagCount.except(trainingDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Create `numTrainingSets` training set of increasing size to get\n",
    "snapshots of how a partially trained model looks like. The sizes are\n",
    "spaced logarithmically since the improvement in the model is fastest in\n",
    "the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rowsInPartitions = (1 to numPartitions).map{ i: Int => (math.exp(math.log(trainingRows)*i/numPartitions)).toInt }//.scanLeft(0.0)(_-_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Model is trained by counting how many times each (`lagKey`, `lagValue`)\n",
    "pair is observed and dividing by how many times `lagKey` is observed to\n",
    "get an estimation of the transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val keyValueCountPartialDFs = rowsInPartitions.map(\n",
    "  trainingDF\n",
    "    .limit(_)\n",
    "    .withColumn(\"keyValueObs\", sum(\"count\").over(Window.partitionBy($\"lagKey\", $\"lagValue\")))\n",
    "    .withColumn(\"totalKeyObs\", sum(\"count\").over(Window.partitionBy($\"lagKey\")))\n",
    "    .drop(\"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(keyValueCountPartialDFs.last.orderBy($\"keyValueObs\".desc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyValueCountPartialDFs\n",
    "  .map( df =>\n",
    "    df\n",
    "      .withColumn(\"probability\", divUDF($\"keyValueObs\", $\"totalKeyObs\"))\n",
    "      .drop(\"keyValueObs\", \"totalKeyObs\")\n",
    "  )\n",
    "  .zip(partialModelPaths).map{ case (df: DataFrame, path: String) =>\n",
    "    df.write.mode(\"overwrite\").format(\"delta\").save(path)  \n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val probDFs = partialModelPaths.map(spark.read.format(\"delta\").load(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(probDFs.last.orderBy(\"probability\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "The prediction is given by taking\n",
    "\n",
    "$$ V \\\\in argmax(P\\_K(V)) $$\n",
    "\n",
    "where \\*P\\_K(V)\\* is the probability that *V* is the next trend when the\n",
    "last `m` points have had reversals *K*. If there are more than one\n",
    "elements in argmax, an element is chosen uniformly at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val aggWindow = Window.partitionBy(\"lagKey\").orderBy('probability desc)\n",
    "val testedDFs = probDFs\n",
    "  .map { df =>\n",
    "     val predictionDF = df\n",
    "      .select(\"lagKey\", \"lagValue\", \"probability\")\n",
    "      .distinct\n",
    "      .withColumn(\"rank\", rank().over(aggWindow))\n",
    "      .filter(\"rank == 1\")\n",
    "      .groupBy(\"lagKey\")\n",
    "      .agg(collect_list(\"lagValue\"))\n",
    "    \n",
    "    testingDF\n",
    "      .filter($\"ticker\" === \"BCOUSD\")\n",
    "      .join(predictionDF, Seq(\"lagKey\"), \"left\")\n",
    "      .withColumnRenamed(\"collect_list(lagValue)\", \"test\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "We use a binary loss function that indicates if the prediction was\n",
    "correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val getRandomUDF = udf( (arr: Seq[Seq[Int]]) => {\n",
    "  val safeArr = Option(arr).getOrElse(Seq[Seq[Int]]())\n",
    "  if (safeArr.isEmpty) Seq[Int]() else safeArr(Random.nextInt(safeArr.size))\n",
    "} )\n",
    "\n",
    "val lossUDF = udf{ (value: Seq[Int], pred: Seq[Int]) =>\n",
    "  if (value == pred) 0 else 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lossDFs = testedDFs.map(_.withColumn(\"prediction\", getRandomUDF($\"test\")).withColumn(\"loss\", lossUDF($\"lagValue\", $\"prediction\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(lossDFs.last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val testLength = testingDF.count\n",
    "val oilTestDF = testingDF.filter($\"ticker\" === \"BCOUSD\")\n",
    "val oilTestLength = oilTestDF.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "We find the mean loss for each training dataset of increasing size. As\n",
    "one can see, the loss decreases as more data is supplied.\n",
    "\n",
    "Further, training on both oil and gold data yields a better result than\n",
    "just oil, suggesting that trends behave similarly in the two\n",
    "commodities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val losses = lossDFs.map( _.agg(sum(\"loss\")).select($\"sum(loss)\".as(\"totalLoss\")).collect.head.getLong(0).toDouble/oilTestLength )\n",
    "// Data up to 2019\n",
    "// k=max, m=2 ,n=1: (0.5489615950080244, 0.4014721726541194, 0.3660973816818738, 0.36497087640146797, 0.36485163238050344)\n",
    "// k=max, m=10,n=1: (0.9812730246821787, 0.8523390131056561, 0.5819573469954631, 0.3552177121357085, 0.2807503135425113)\n",
    "// k=max,m=100,n=1: (1.0, 1.0, 1.0, 1.0, 1.0)\n",
    "// k=5  , m=10,n=1: (0.9812730246821787, 0.8522267831239777, 0.5820597568537447, 0.3550507700379618, 0.2806352778112909)\n",
    "// k=1  , m=10,n=1: (0.9812730246821787, 0.852200128503329, 0.5821242890932098, 0.3553383593660128, 0.2806549180580846)\n",
    "\n",
    "// k=max, m=10,n=2: (0.9879380657977248, 0.9049354606556204, 0.7243136776273427, 0.5472986906951395, 0.4783823708897465)\n",
    "\n",
    "// k=max(17), m=10,n=1, 10 training sets: (0.9977203284971564, 0.9812730246821787, 0.9455375956409875, 0.8523810993487855, 0.7183644724769999, 0.5819320952495854, 0.44607349380350214, 0.35513915114853356, 0.3029480010437388, 0.280629666312207)\n",
    "\n",
    "// Data up to last month\n",
    "// k=max(18), m=10,n=1, 10 training sets: (0.9973104051296998, 0.9843882250072865, 0.9510789857184494, 0.8574351501020111, 0.7515744680851064, 0.6360547945205479, 0.5099656076945497, 0.4249921305741766, 0.3735668901194987, 0.34609501603031184)\n",
    "// Could the difference be due to Corona?\n",
    "\n",
    "// Trained on both oil and gold. testing on oil as previously.\n",
    "// k=max(18), m=10,n=1, 10 training sets: (0.9999778490236083, 0.9980728650539201, 0.9527158262897114, 0.8921317400174876, 0.7559988341591373, 0.5820915185077237, 0.46675488195861264, 0.3923101136694841, 0.3574876129408336, 0.3355837948120082)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trainingSizes = probDFs.map(_.count)\n",
    "val lossesDS = sc\n",
    "  .parallelize(losses.zip(trainingSizes))\n",
    "  .toDF(\"loss\", \"size\")\n",
    "  .withColumn(\"training\", lit(\"Oil and Gold\"))\n",
    "  .union(\n",
    "    sc\n",
    "      .parallelize(Seq(0.9973104051296998, 0.9843882250072865, 0.9510789857184494, 0.8574351501020111, 0.7515744680851064, 0.6360547945205479, 0.5099656076945497, 0.4249921305741766, 0.3735668901194987, 0.34609501603031184).zip(Seq(4, 18, 77, 331, 1414, 6036, 25759, 109918, 469036, 2001430)))\n",
    "      .toDF(\"loss\", \"size\")\n",
    "      .withColumn(\"training\", lit(\"Oil\"))\n",
    "  )\n",
    "  .as[(Double,Long,String)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(lossesDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "We collect the models in order to calculate the total variation distance\n",
    "between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val partialModels = probDFs.map{df => \n",
    "  val tmpMap = df.select(\"lagKey\", \"lagValue\", \"probability\").distinct.collect.map{ r => \n",
    "    (r.getAs[Seq[Int]](0), r.getAs[Seq[Int]](1), r.getDouble(2))\n",
    "  }.groupBy(_._1).mapValues(_.map(tup => Map(tup._2 -> tup._3)).flatten.toMap)\n",
    "  \n",
    "  tmpMap: FinalModel\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totalVarDist(m1: FinalModel, m2: FinalModel): Map[Seq[Int], Double] = {\n",
    "  val allKeys = m1.keys.toSet.union(m2.keys.toSet)\n",
    "  val sharedKeys = m1.keys.toSet.intersect(m2.keys.toSet)\n",
    "  val totalVarDists = allKeys.toSeq.map{ key =>\n",
    "    if (!sharedKeys.contains(key)) {\n",
    "      1.0\n",
    "    } else {\n",
    "      val val1 = m1.getOrElse(key, Map())\n",
    "      val val2 = m2.getOrElse(key, Map())\n",
    "      val allValKeys = val1.keys.toSet.union(val2.keys.toSet)\n",
    "      allValKeys.map( valKey => 0.5*math.abs(val1.getOrElse(valKey, 0.0) - val2.getOrElse(valKey, 0.0)) ).sum\n",
    "    }\n",
    "  }\n",
    "  allKeys.zip(totalVarDists).toMap\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val totalVariationDistances = partialModels.map( m1 => partialModels.map( m2 => totalVarDist(m1,m2) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggToMatrix(totalVarDists: Seq[Seq[Map[Seq[Int],Double]]], aggFunc: Seq[Double] => Double): Seq[Seq[Double]] = {\n",
    "  totalVariationDistances.map(_.map( t => aggFunc(t.values.toSeq)))\n",
    "}\n",
    "\n",
    "def printMatrix(mat: Seq[Seq[Double]]): Unit = {\n",
    "  mat.map( s => { s.map( a => print(f\"$a%2.3f \") ); println() } )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val maxDists = aggToMatrix(totalVariationDistances, s => s.max)\n",
    "val minDists = aggToMatrix(totalVariationDistances, s => s.min)\n",
    "val meanDists = aggToMatrix(totalVariationDistances, s => s.sum/s.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Each model is a mapping `{key: {value: probability}}` where `key` is a\n",
    "sequence of reversals and non-reversals of length `m`, `value` is a\n",
    "sequence of trends of length `n` and `probability` is the estimated\n",
    "probability that `value` is observed directly after `key`.\n",
    "\n",
    "Hence, for any two models A and B, we can calculate the total variation\n",
    "distance between the mappings `{value: probability}` for a given key in\n",
    "the union of the keys for A and B. If a key is not present in one of the\n",
    "models, the total variation distance is 1 for that key.\n",
    "\n",
    "In the matrix below, the (i,j)-th position is the arithmetic mean of the\n",
    "total variation distances for all keys in the union of the keysets. The\n",
    "matrix is symmetric with the smallest model in the top row and leftmost\n",
    "column and the largest model in the bottom row and rightmost column.\n",
    "\n",
    "If there are three models labeled M*1, M*2, M*3 and V*i\\_j is the\n",
    "arithmetic mean described above, the matrix is\n",
    "\n",
    "$$ \\\\begin{matrix} V*{1,1} & V*{1,2} & V*{1,3} \\\\end{matrix} $$ $$\n",
    "\\\\begin{matrix} V*{2,1} & V*{2,2} & V*{2,3} \\\\end{matrix} $$ $$\n",
    "\\\\begin{matrix} V*{3,1} & V*{3,2} & V\\_{3,3} \\\\end{matrix} $$\n",
    "\n",
    "As one can see, the models differ a lot from each other, suggesting that\n",
    "the estimate can still be improved given more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMatrix(meanDists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
