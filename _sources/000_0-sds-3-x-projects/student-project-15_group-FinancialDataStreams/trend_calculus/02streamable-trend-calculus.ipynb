{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming Trend Calculus with Maximum Necessary Reversals\n",
    "=========================================================\n",
    "\n",
    "Johannes Graner, Albert Nilsson and Raazesh Sainudiin\n",
    "\n",
    "2020, Uppsala, Sweden\n",
    "\n",
    "This project was supported by Combient Mix AB through summer internships\n",
    "at:\n",
    "\n",
    "Combient Competence Centre for Data Engineering Sciences, Department of\n",
    "Mathematics, Uppsala University, Uppsala, Sweden\n",
    "\n",
    "Resources\n",
    "---------\n",
    "\n",
    "This builds on the following library and its antecedents therein:\n",
    "\n",
    "-   <https://github.com/lamastex/spark-trend-calculus>\n",
    "\n",
    "This work was inspired by:\n",
    "--------------------------\n",
    "\n",
    "-   Antoine Aamennd's\n",
    "    [texata-2017](https://github.com/aamend/texata-r2-2017)\n",
    "-   Andrew Morgan's [Trend Calculus\n",
    "    Library](https://github.com/ByteSumoLtd/TrendCalculus-lua)\n",
    "\n",
    "We use the spark-trend-calculus library and Spark structured streams\n",
    "over delta.io files to obtain a representation of the complete time\n",
    "series of trends with their k-th order reversal.\n",
    "\n",
    "This representation is a sufficient statistic for a Markov model of\n",
    "trends that we show in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.sql.Timestamp\n",
    "import io.delta.tables._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.streaming.{GroupState, GroupStateTimeout, OutputMode, Trigger}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.expressions.{Window, WindowSpec}\n",
    "import org.lamastex.spark.trendcalculus._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Input data in s3. The data contains oil price data from 2010 to last\n",
    "month and gold price data from 2009 to last month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rootPath = \"s3a://osint-gdelt-reado/canwrite/summerinterns2020/johannes/streamable-trend-calculus/\"\n",
    "val oilGoldPath = rootPath + \"oilGoldDelta\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.read.format(\"delta\").load(oilGoldPath).orderBy(\"x\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Reading the data from s3 as a Structured Stream to simulate streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val input = spark\n",
    "  .readStream\n",
    "  .format(\"delta\")\n",
    "  .load(oilGoldPath)\n",
    "  .as[TickerPoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Using the trendcalculus library to 1. Apply Trend Calculus to the\n",
    "streaming dataset. - Save the result as a delta table. - Read the result\n",
    "as a stream. - Repeat from 1. using the latest result as input. Stop\n",
    "when result is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val windowSize = 2\n",
    "\n",
    "// Initializing variables for while loop.\n",
    "var i = 1\n",
    "var prevSinkPath = \"\"\n",
    "var sinkPath = rootPath + \"multiSinks/reversal\" + (i)\n",
    "var chkptPath = rootPath + \"multiSinks/checkpoint/\" + (i)\n",
    "\n",
    "// The first order reversal.\n",
    "var stream = new TrendCalculus2(input, windowSize, spark)\n",
    "  .reversals\n",
    "  .select(\"tickerPoint.ticker\", \"tickerPoint.x\", \"tickerPoint.y\", \"reversal\")\n",
    "  .as[FlatReversal]\n",
    "  .writeStream\n",
    "  .format(\"delta\")\n",
    "  .option(\"path\", sinkPath)\n",
    "  .option(\"checkpointLocation\", chkptPath)\n",
    "  .trigger(Trigger.Once())\n",
    "  .start\n",
    "\n",
    "stream.processAllAvailable\n",
    "\n",
    "i += 1\n",
    "\n",
    "var lastReversalSeries = spark.emptyDataset[TickerPoint]\n",
    "while (!spark.read.format(\"delta\").load(sinkPath).isEmpty) {\n",
    "  \n",
    "  prevSinkPath = rootPath + \"multiSinks/reversal\" + (i-1)\n",
    "  sinkPath = rootPath + \"multiSinks/reversal\" + (i)\n",
    "  chkptPath = rootPath + \"multiSinks/checkpoint/\" + (i)\n",
    "  \n",
    "  // Reading last result as stream\n",
    "  lastReversalSeries = spark\n",
    "    .readStream\n",
    "    .format(\"delta\")\n",
    "    .load(prevSinkPath)\n",
    "    .drop(\"reversal\")\n",
    "    .as[TickerPoint]\n",
    "\n",
    "  // Writing next result\n",
    "  stream = new TrendCalculus2(lastReversalSeries, windowSize, spark)\n",
    "    .reversals\n",
    "    .select(\"tickerPoint.ticker\", \"tickerPoint.x\", \"tickerPoint.y\", \"reversal\")\n",
    "    .as[FlatReversal]\n",
    "    .map( rev => rev.copy(reversal=i*rev.reversal))\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"path\", sinkPath)\n",
    "    .option(\"checkpointLocation\", chkptPath)\n",
    "    .partitionBy(\"ticker\")\n",
    "    .trigger(Trigger.Once())\n",
    "    .start\n",
    "  \n",
    "  stream.processAllAvailable()\n",
    "  \n",
    "  i += 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// DELETES THE SINKS\n",
    "//dbutils.fs.rm(rootPath + \"multiSinks\", recurse=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The total number of reversals written\n",
    "val i = dbutils.fs.ls(rootPath + \"multiSinks\").length - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "The written delta tables can be read as streams but for now we read them\n",
    "as static datasets to be able to join them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sinkPaths = (1 to i-1).map(rootPath + \"multiSinks/reversal\" + _)\n",
    "val maxRevPath = rootPath + \"maxRev\"\n",
    "val revTables = sinkPaths.map(DeltaTable.forPath(_).toDF.as[FlatReversal])\n",
    "val oilGoldTable = DeltaTable.forPath(oilGoldPath).toDF.as[TickerPoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "The number of reversals decrease rapidly as the reversal order\n",
    "increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revTables.map(_.cache.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Joining all results to get a dataset with all reversals in a single\n",
    "column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxByAbs(a: Int, b: Int): Int = {\n",
    "  Seq(a,b).maxBy(math.abs)\n",
    "}\n",
    "\n",
    "val maxByAbsUDF = udf((a: Int, b: Int) => maxByAbs(a,b))\n",
    "\n",
    "val maxRevDS = revTables.foldLeft(oilGoldTable.toDF.withColumn(\"reversal\", lit(0)).as[FlatReversal]){ (acc: Dataset[FlatReversal], ds: Dataset[FlatReversal]) => \n",
    "  acc\n",
    "    .toDF\n",
    "    .withColumnRenamed(\"reversal\", \"oldMaxRev\")\n",
    "    .join(ds.select($\"ticker\" as \"tmpt\", $\"x\" as \"tmpx\", $\"reversal\" as \"newRev\"), $\"ticker\" === $\"tmpt\" && $\"x\" === $\"tmpx\", \"left\")\n",
    "    .drop(\"tmpt\", \"tmpx\")\n",
    "    .na.fill(0,Seq(\"newRev\"))\n",
    "    .withColumn(\"reversal\", maxByAbsUDF($\"oldMaxRev\", $\"newRev\"))\n",
    "    .select(\"ticker\", \"x\", \"y\", \"reversal\")\n",
    "    .as[FlatReversal]    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Writing the joined dataset to a delta table.\n",
    "maxRevDS.write.format(\"delta\").partitionBy(\"ticker\").save(maxRevPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "The reversal column in the joined dataset contains the information of\n",
    "all orders of reversals.\n",
    "\n",
    "`0` indicates that no reversal happens while a non-zero value indicates\n",
    "that this is a reversal point for that order and every lower order.\n",
    "\n",
    "For example, row 33 contains the value `-4`, meaning that this point is\n",
    "trend reversal downwards for orders 1, 2, 3, and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(DeltaTable.forPath(maxRevPath).toDF.as[FlatReversal].filter(\"ticker == 'BCOUSD'\").orderBy(\"x\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
