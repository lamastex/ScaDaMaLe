{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scratch environment\n",
    "==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.lamastex.spark.trendcalculus._\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import java.sql.Timestamp\n",
    "import org.apache.spark.sql.expressions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.lamastex.spark.trendcalculus._\n",
    ">     import spark.implicits._\n",
    ">     import org.apache.spark.sql._\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     import java.sql.Timestamp\n",
    ">     import org.apache.spark.sql.expressions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val oilDS = spark.read.fx1m(\"dbfs:/FileStore/shared_uploads/fabiansi@kth.se/*csv.gz\").toDF.withColumn(\"ticker\", lit(\"BCOUSD\")).select($\"ticker\", $\"time\" as \"x\", $\"close\" as \"y\").as[TickerPoint].orderBy(\"time\")\n",
    "\n",
    "// store loaded data as temp view\n",
    "// oilDS.createOrReplaceTempView(\"temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     oilDS: org.apache.spark.sql.Dataset[org.lamastex.spark.trendcalculus.TickerPoint] = [ticker: string, x: timestamp ... 1 more field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//add additional usefull columns to the dataframe\n",
    "\n",
    "import org.apache.spark.sql.expressions._\n",
    "\n",
    "val windowSpec = Window.orderBy(\"x\")\n",
    "\n",
    "val oilDS2 = oilDS\n",
    ".withColumn(\"diff_close\", $\"y\" - when((lag(\"y\", 1).over(windowSpec)).isNull, 0).otherwise(lag(\"y\", 1).over(windowSpec)))\n",
    ".withColumn(\"index\", row_number().over(windowSpec))\n",
    "\n",
    "\n",
    "// store loaded data as temp view\n",
    "oilDS2.createOrReplaceTempView(\"temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql.expressions._\n",
    ">     windowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@7206bf30\n",
    ">     oilDS2: org.apache.spark.sql.DataFrame = [ticker: string, x: timestamp ... 3 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cell is used for testing pyspark syntax only\n",
    "'''\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "oilDS_py = spark.table(\"temp\") # data: Row(ticker=u'BCOUSD', x=datetime.datetime(2010, 11, 14, 20, 21), y=86.74), ...\n",
    "print(oilDS_py.take(10))\n",
    "start = datetime.datetime(2010, 11, 14, 20, 19)\n",
    "end = datetime.datetime(2010, 11, 14, 20, 32)\n",
    "\n",
    "# oilDS_py.filter(oilDS_py.index.between(3, 6))\n",
    "# np.array(oilDS_py.filter(oilDS_py[\"x\"] > start).filter(oilDS_py[\"x\"] <= end).select('y').collect())\n",
    "\n",
    "np.array(oilDS_py.where(oilDS_py.index == 5).select('diff_close').collect()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     [Row(ticker=u'BCOUSD', x=datetime.datetime(2010, 11, 14, 20, 15), y=86.74, diff_close=86.74, index=1), Row(ticker=u'BCOUSD', x=datetime.datetime(2010, 11, 14, 20, 17), y=86.75, diff_close=0.010000000000005116, index=2), Row(ticker=u'BCOUSD', x=datetime.datetime(2010, 11, 14, 20, 18), y=86.76, diff_close=0.010000000000005116, index=3), Row(ticker=u'BCOUSD', x=datetime.datetime(2010, 11, 14, 20, 19), y=86.74, diff_close=-0.020000000000010232, index=4), Row(ticker=u'BCOUSD', x=datetime.datetime(2010, 11, 14, 20, 21), y=86.74, diff_close=0.0, index=5), Row(ticker=u'BCOUSD', x=datetime.datetime(2010, 11, 14, 20, 24), y=86.75, diff_close=0.010000000000005116, index=6), Row(ticker=u'BCOUSD', x=datetime.datetime(2010, 11, 14, 20, 26), y=86.77, diff_close=0.01999999999999602, index=7), Row(ticker=u'BCOUSD', x=datetime.datetime(2010, 11, 14, 20, 27), y=86.75, diff_close=-0.01999999999999602, index=8), Row(ticker=u'BCOUSD', x=datetime.datetime(2010, 11, 14, 20, 28), y=86.79, diff_close=0.04000000000000625, index=9), Row(ticker=u'BCOUSD', x=datetime.datetime(2010, 11, 14, 20, 32), y=86.81, diff_close=0.01999999999999602, index=10)]\n",
    ">     Out[1]: 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "PENALTY = 1  # 0.999756079\n",
    "\n",
    "class MarketEnv(gym.Env):\n",
    "    def __init__(self, full_data, start_date, end_date, scope=60, cumulative_reward=False):\n",
    "        self.cumulative_reward = cumulative_reward\n",
    "        self.actions = [\"LONG\", \"SHORT\"] # pass not required, if already owned, buy is hold\n",
    "        self.action_space = gym.spaces.Discrete(len(self.actions))\n",
    "        self.state = None\n",
    "        self.time_range = full_data.filter(full_data[\"x\"] > start_date).filter(full_data[\"x\"] <= end_date)\n",
    "\n",
    "        self.scope = scope\n",
    "        self.time_index = 0\n",
    "        self.reset()\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self.state, self.reward, self.done, {}\n",
    "    \n",
    "        self.reward = 0\n",
    "        if self.actions[action] == \"LONG\":\n",
    "            if sum(self.boughts) < 0:\n",
    "                for b in self.boughts:\n",
    "                    self.reward += -(b + 1) \n",
    "                if self.cumulative_reward:\n",
    "                    self.reward = self.reward / max(1, len(self.boughts))\n",
    "                self.boughts = []\n",
    "            self.boughts.append(1.0)\n",
    "        elif self.actions[action] == \"SHORT\":\n",
    "            if sum(self.boughts) > 0:\n",
    "                for b in self.boughts:\n",
    "                    self.reward += b - 1\n",
    "                if self.cumulative_reward:\n",
    "                    self.reward = self.reward / max(1, len(self.boughts))\n",
    "                self.boughts = []\n",
    "            self.boughts.append(-1.0)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        print(0)\n",
    "        diff_close = np.array(self.time_range.where(self.time_range.index == self.time_index).select('diff_close').collect()).item()\n",
    "\n",
    "        print(1)\n",
    "        for i in range(len(self.boughts)):\n",
    "            self.boughts[i] = self.boughts[i] * PENALTY * (1 + diff_close * (-1 if sum(self.boughts) < 0 else 1))\n",
    "\n",
    "        print(2)\n",
    "        self.define_state()\n",
    "        self.time_index += 1\n",
    "        if self.time_index > self.time_range.count() - self.scope:\n",
    "            self.done = True\n",
    "\n",
    "        print(3)\n",
    "        if self.done:\n",
    "            for b in self.boughts:\n",
    "                self.reward += (b * (1 if sum(self.boughts) > 0 else -1)) - 1\n",
    "            if self.cumulative_reward:\n",
    "                self.reward = self.reward / max(1, len(self.boughts))\n",
    "            self.boughts = []\n",
    "\n",
    "        return self.state, self.reward, self.done\n",
    "  \n",
    "    def reset(self):\n",
    "        self.boughts = []\n",
    "        self.done = False\n",
    "        self.reward = 0 \n",
    "        self.time_index = self.scope\n",
    "        self.define_state()\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def define_state(self):\n",
    "        print(\"call define_state\")\n",
    "        tmpState = []\n",
    "\n",
    "        budget = (sum(self.boughts) / len(self.boughts)) if len(self.boughts) > 0 else 1.\n",
    "        size = math.log(max(1., len(self.boughts)), 100)\n",
    "        position = 1. if sum(self.boughts) > 0 else 0.\n",
    "        tmpState.append([[budget, size, position]])\n",
    "\n",
    "        # need to change here!! \n",
    "        df_back = self.time_range.filter(self.time_range.index.between(self.time_index, self.time_index + self.scope - 1))\n",
    "        # TODO check if we go out of range\n",
    "\n",
    "        np_back = np.array(df_back.select('diff_close').collect())\n",
    "        # print('[!!] {}'.format(np_back))\n",
    "\n",
    "        tmpState.append(np_back)\n",
    "\n",
    "        tmpState = [np.array(i) for i in tmpState]\n",
    "        self.state = tmpState\n",
    "\n",
    "    def seed(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime(2010, 11, 14, 20, 19)\n",
    "end = datetime.datetime(2012, 11, 14, 20, 32)\n",
    "\n",
    "env = MarketEnv(oilDS_py, start, end)\n",
    "\n",
    "print(env.step(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     ([array([[ 1.,  0.,  1.]]), array([[ 0.  ],\n",
    ">            [ 0.01],\n",
    ">            [ 0.  ],\n",
    ">            [ 0.  ],\n",
    ">            [-0.01],\n",
    ">            [ 0.01],\n",
    ">            [ 0.04],\n",
    ">            [ 0.01],\n",
    ">            [-0.01],\n",
    ">            [-0.03],\n",
    ">            [-0.01],\n",
    ">            [ 0.  ],\n",
    ">            [-0.01],\n",
    ">            [ 0.01],\n",
    ">            [ 0.  ],\n",
    ">            [-0.01],\n",
    ">            [-0.01],\n",
    ">            [-0.03],\n",
    ">            [ 0.  ],\n",
    ">            [ 0.03],\n",
    ">            [ 0.02],\n",
    ">            [ 0.05],\n",
    ">            [ 0.07],\n",
    ">            [ 0.01],\n",
    ">            [ 0.01],\n",
    ">            [ 0.  ],\n",
    ">            [ 0.  ],\n",
    ">            [-0.01],\n",
    ">            [-0.01],\n",
    ">            [ 0.01],\n",
    ">            [-0.05],\n",
    ">            [-0.02],\n",
    ">            [-0.02],\n",
    ">            [-0.01],\n",
    ">            [ 0.02],\n",
    ">            [-0.01],\n",
    ">            [ 0.02],\n",
    ">            [-0.01],\n",
    ">            [-0.01],\n",
    ">            [-0.06],\n",
    ">            [-0.02],\n",
    ">            [ 0.  ],\n",
    ">            [ 0.  ],\n",
    ">            [-0.01],\n",
    ">            [ 0.  ],\n",
    ">            [ 0.04],\n",
    ">            [-0.01],\n",
    ">            [-0.02],\n",
    ">            [ 0.  ],\n",
    ">            [ 0.02],\n",
    ">            [ 0.01],\n",
    ">            [-0.01],\n",
    ">            [ 0.  ],\n",
    ">            [-0.02],\n",
    ">            [ 0.  ],\n",
    ">            [-0.02],\n",
    ">            [ 0.01],\n",
    ">            [-0.01],\n",
    ">            [ 0.  ],\n",
    ">            [-0.03]])], 0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.step(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     ([array([[ 1.01    ,  0.150515,  1.      ]]), array([[ 0.01],\n",
    ">            [ 0.  ],\n",
    ">            [ 0.  ],\n",
    ">            [-0.01],\n",
    ">            [ 0.01],\n",
    ">            [ 0.04],\n",
    ">            [ 0.01],\n",
    ">            [-0.01],\n",
    ">            [-0.03],\n",
    ">            [-0.01],\n",
    ">            [ 0.  ],\n",
    ">            [-0.01],\n",
    ">            [ 0.01],\n",
    ">            [ 0.  ],\n",
    ">            [-0.01],\n",
    ">            [-0.01],\n",
    ">            [-0.03],\n",
    ">            [ 0.  ],\n",
    ">            [ 0.03],\n",
    ">            [ 0.02],\n",
    ">            [ 0.05],\n",
    ">            [ 0.07],\n",
    ">            [ 0.01],\n",
    ">            [ 0.01],\n",
    ">            [ 0.  ],\n",
    ">            [ 0.  ],\n",
    ">            [-0.01],\n",
    ">            [-0.01],\n",
    ">            [ 0.01],\n",
    ">            [-0.05],\n",
    ">            [-0.02],\n",
    ">            [-0.02],\n",
    ">            [-0.01],\n",
    ">            [ 0.02],\n",
    ">            [-0.01],\n",
    ">            [ 0.02],\n",
    ">            [-0.01],\n",
    ">            [-0.01],\n",
    ">            [-0.06],\n",
    ">            [-0.02],\n",
    ">            [ 0.  ],\n",
    ">            [ 0.  ],\n",
    ">            [-0.01],\n",
    ">            [ 0.  ],\n",
    ">            [ 0.04],\n",
    ">            [-0.01],\n",
    ">            [-0.02],\n",
    ">            [ 0.  ],\n",
    ">            [ 0.02],\n",
    ">            [ 0.01],\n",
    ">            [-0.01],\n",
    ">            [ 0.  ],\n",
    ">            [-0.02],\n",
    ">            [ 0.  ],\n",
    ">            [-0.02],\n",
    ">            [ 0.01],\n",
    ">            [-0.01],\n",
    ">            [ 0.  ],\n",
    ">            [-0.03],\n",
    ">            [ 0.01]])], 0, False)\n",
    "\n",
    "  \n",
    "\n",
    "reinforcement learning part\n",
    "===========================\n",
    "\n",
    "-   see\n",
    "    https://dbc-635ca498-e5f1.cloud.databricks.com/?o=445287446643905\\#notebook/4201196137758409/command/4201196137758420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, done):\n",
    "        self.memory.append([states, done])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = model.output_shape[-1]\n",
    "    \n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim, 1))\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory, size=inputs.shape[0])):\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            done = self.memory[idx][1]\n",
    "\n",
    "            inputs[i:i+1] = state_t\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = model.predict(state_t)[0]\n",
    "            Q_sa = np.max(model.predict(state_tp1)[0])\n",
    "            if done:  # if done is True\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                # reward_t + gamma * max_a' Q(s', a')\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Conv1D, MaxPool1D, Flatten\n",
    "from keras.optimizers import sgd\n",
    "import collections\n",
    "import datetime\n",
    "\n",
    "epsilon = .1  # exploration\n",
    "num_actions = 2  # [long, short]\n",
    "epoch = 400\n",
    "max_memory = 500\n",
    "hidden_size = 128\n",
    "batch_size = 64\n",
    "sequence_scope = 60\n",
    "\n",
    "input_shape = (batch_size, sequence_scope, 1)\n",
    "\n",
    "# Create Q Network (s,a --> Q)\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, (5), strides=2, input_shape=input_shape[1:], activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2, strides=1))\n",
    "model.add(Conv1D(32, (5), strides=1, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2, strides=1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(hidden_size, activation='relu'))\n",
    "model.add(Dense(num_actions))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Define train interval\n",
    "start = datetime.datetime(2010, 11, 14, 20, 19)\n",
    "end = datetime.datetime(2010, 12, 14, 20, 32)\n",
    "\n",
    "print(\"initialize MarketEnv\")\n",
    "env = MarketEnv(oilDS_py, start, end, scope=sequence_scope, cumulative_reward=False)\n",
    "\n",
    "# Initialize experience replay object\n",
    "print(\"initialize Experience Replay\")\n",
    "exp_replay = ExperienceReplay(max_memory=max_memory)\n",
    "\n",
    "# Train\n",
    "rewards = []\n",
    "for e in range(epoch):\n",
    "    loss = 0.\n",
    "    reward_sum = 0.\n",
    "    print(\"reset\")\n",
    "    state = env.reset()\n",
    "    input_t = state[1].reshape(1, sequence_scope, 1) # can't this be moved into the Environment class?\n",
    "    \n",
    "    counter = 0\n",
    "    done = False\n",
    "    # Start an episode\n",
    "    while not done:\n",
    "        counter += 1\n",
    "        \n",
    "        input_tm1 = input_t\n",
    "        # Epsilon Greedy step\n",
    "        if np.random.rand() <= epsilon:\n",
    "            # Take randon action\n",
    "            action = np.random.randint(0, num_actions, size=1)\n",
    "        else:\n",
    "            # Take action that maximizes Q value\n",
    "            print(\"predict\")\n",
    "            q = model.predict(input_tm1)\n",
    "            action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        print(\"step\")\n",
    "        state, reward, done = env.step(action)\n",
    "        input_t = state[1].reshape(1, sequence_scope, 1) # can't this be moved into the Environment class?\n",
    "        reward_sum += reward\n",
    "          \n",
    "        # store experience\n",
    "        print(\"remember\")\n",
    "        exp_replay.remember([input_tm1, action, reward, input_t], done)\n",
    "\n",
    "        # adapt model\n",
    "        print(\"get batch\")\n",
    "        inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
    "\n",
    "        print(\"train on batch\")\n",
    "        loss += model.train_on_batch(inputs, targets)\n",
    "\n",
    "    print(\"Epoch {:03d}/{:d} | Loss {:.4f} | Average Reward {:.4f}\".format(e, epoch - 1, loss, reward_sum/counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     initialize MarketEnv\n",
    ">     call define_state\n",
    ">     initialize ExpRepl\n",
    ">     reset\n",
    ">     call define_state\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    ">     3\n",
    ">     remember\n",
    ">     get batch\n",
    ">     train on batch\n",
    ">     predict\n",
    ">     step\n",
    ">     0\n",
    ">     1\n",
    ">     2\n",
    ">     call define_state\n",
    "\n",
    "  \n",
    "\n",
    "read data as structures stream\n",
    "=============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val oil_path = \"dbfs:/FileStore/shared_uploads/fabiansi@kth.se/*csv.gz\"\n",
    "\n",
    "val input = spark\n",
    "  .readStream\n",
    "  .format(\"delta\")\n",
    "  .load(oil_path)\n",
    "  .as[TickerPoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedDS = spark.read.parquet(\"dbfs:/FileStore/shared_uploads/fabiansi@kth.se/joinedDSWithMaxRev\").orderBy(\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
