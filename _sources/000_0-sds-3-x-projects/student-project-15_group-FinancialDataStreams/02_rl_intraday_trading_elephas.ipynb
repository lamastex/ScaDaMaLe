{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning for Intraday Trading - Distributed model tuning with Elephas\n",
    "===================================================================================\n",
    "\n",
    "Group members:\n",
    "--------------\n",
    "\n",
    "-   Fabian Sinzinger\n",
    "-   Karl Bäckström\n",
    "-   Rita Laezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Scala imports\n",
    "import org.lamastex.spark.trendcalculus._\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import java.sql.Timestamp\n",
    "import org.apache.spark.sql.expressions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.lamastex.spark.trendcalculus._\n",
    ">     import spark.implicits._\n",
    ">     import org.apache.spark.sql._\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     import java.sql.Timestamp\n",
    ">     import org.apache.spark.sql.expressions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Load dataset\n",
    "val oilDS = spark.read.fx1m(\"dbfs:/FileStore/shared_uploads/fabiansi@kth.se/*csv.gz\").toDF.withColumn(\"ticker\", lit(\"BCOUSD\")).select($\"ticker\", $\"time\" as \"x\", $\"close\" as \"y\").as[TickerPoint].orderBy(\"time\")\n",
    "\n",
    "// Add column with difference from previous close value (expected 'x', 'y' column names)\n",
    "val windowSpec = Window.orderBy(\"x\")\n",
    "val oilDS1 = oilDS \n",
    ".withColumn(\"diff_close\", $\"y\" - when((lag(\"y\", 1).over(windowSpec)).isNull, 0).otherwise(lag(\"y\", 1).over(windowSpec)))\n",
    "\n",
    "// Rename variables\n",
    "val oilDS2 = oilDS1.withColumnRenamed(\"x\",\"time\").withColumnRenamed(\"y\",\"close\")\n",
    "\n",
    "// Remove incomplete data from first day (2010-11-14) and last day (2019-06-21)\n",
    "val oilDS3 = oilDS2.filter(to_date(oilDS2(\"time\")) >= lit(\"2010-11-15\") && to_date(oilDS2(\"time\")) <= lit(\"2019-06-20\"))\n",
    "\n",
    "// Add index column\n",
    "val windowSpec1 = Window.orderBy(\"time\")\n",
    "val oilDS4 = oilDS3\n",
    ".withColumn(\"index\", row_number().over(windowSpec1))\n",
    "\n",
    "// Drop ticker column\n",
    "val oilDS5 = oilDS4.drop(\"ticker\")\n",
    "\n",
    "// Store loaded data as temp view, to be accessible in Python\n",
    "oilDS5.createOrReplaceTempView(\"temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     oilDS: org.apache.spark.sql.Dataset[org.lamastex.spark.trendcalculus.TickerPoint] = [ticker: string, x: timestamp ... 1 more field]\n",
    ">     windowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@6dd55d65\n",
    ">     oilDS1: org.apache.spark.sql.DataFrame = [ticker: string, x: timestamp ... 2 more fields]\n",
    ">     oilDS2: org.apache.spark.sql.DataFrame = [ticker: string, time: timestamp ... 2 more fields]\n",
    ">     oilDS3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ticker: string, time: timestamp ... 2 more fields]\n",
    ">     windowSpec1: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@5be434ff\n",
    ">     oilDS4: org.apache.spark.sql.DataFrame = [ticker: string, time: timestamp ... 3 more fields]\n",
    ">     oilDS5: org.apache.spark.sql.DataFrame = [time: timestamp, close: double ... 2 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python imports\n",
    "import datetime\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Conv1D, MaxPool1D, Flatten, BatchNormalization\n",
    "from keras import optimizers\n",
    "\n",
    "from elephas.utils.rdd_utils import to_simple_rdd\n",
    "from elephas.spark_model import SparkModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Using TensorFlow backend.\n",
    ">     WARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe from temp data\n",
    "oilDF_py = spark.table(\"temp\")\n",
    "\n",
    "# Select the 10 first Rows of data and print them\n",
    "ten_oilDF_py = oilDF_py.limit(10)\n",
    "ten_oilDF_py.show()\n",
    "\n",
    "# Check number of data points\n",
    "last_index = oilDF_py.count()\n",
    "print(\"Number of data points: {}\".format(last_index))\n",
    "\n",
    "# Select the date of the last data point\n",
    "print(\"Last data point: {}\".format(np.array(oilDF_py.where(oilDF_py.index == last_index).select('time').collect()).item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +-------------------+-----+--------------------+-----+\n",
    ">     |               time|close|          diff_close|index|\n",
    ">     +-------------------+-----+--------------------+-----+\n",
    ">     |2010-11-15 00:00:00| 86.6|-0.01000000000000...|    1|\n",
    ">     |2010-11-15 00:01:00| 86.6|                 0.0|    2|\n",
    ">     |2010-11-15 00:02:00|86.63|0.030000000000001137|    3|\n",
    ">     |2010-11-15 00:03:00|86.61|-0.01999999999999602|    4|\n",
    ">     |2010-11-15 00:05:00|86.61|                 0.0|    5|\n",
    ">     |2010-11-15 00:07:00| 86.6|-0.01000000000000...|    6|\n",
    ">     |2010-11-15 00:08:00|86.58|-0.01999999999999602|    7|\n",
    ">     |2010-11-15 00:09:00|86.58|                 0.0|    8|\n",
    ">     |2010-11-15 00:10:00|86.58|                 0.0|    9|\n",
    ">     |2010-11-15 00:12:00|86.57|-0.01000000000000...|   10|\n",
    ">     +-------------------+-----+--------------------+-----+\n",
    ">\n",
    ">     Number of data points: 2523078\n",
    ">     Last data point: 2019-06-20 23:59:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://github.com/kh-kim/stock_market_reinforcement_learning/blob/master/market_env.py\n",
    "\n",
    "\n",
    "class MarketEnv(gym.Env):\n",
    "    def __init__(self, full_data, start_date, end_date, episode_size=30*24*60, scope=60):\n",
    "        self.episode_size = episode_size\n",
    "        self.actions = [\"LONG\", \"SHORT\"] \n",
    "        self.action_space = gym.spaces.Discrete(len(self.actions))\n",
    "        self.state_space = gym.spaces.Box(np.ones(scope) * -1, np.ones(scope))\n",
    "\n",
    "        self.diff_close = np.array(full_data.filter(full_data[\"time\"] > start_date).filter(full_data[\"time\"] <= end_date).select('diff_close').collect())\n",
    "        max_diff_close = np.max(self.diff_close)\n",
    "        self.diff_close = self.diff_close*max_diff_close\n",
    "        self.close = np.array(full_data.filter(full_data[\"time\"] > start_date).filter(full_data[\"time\"] <= end_date).select('close').collect())\n",
    "        self.num_ticks_train = np.shape(self.diff_close)[0]\n",
    "\n",
    "        self.scope = scope # N values to be included in a state vector\n",
    "        self.time_index = self.scope  # start N steps in, to ensure that we have enough past values for history \n",
    "        self.episode_init_time = self.time_index  # initial time index of the episode\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        info = {'index': int(self.time_index), 'close': float(self.close[self.time_index])}\n",
    "        self.time_index += 1\n",
    "        self.state = self.diff_close[self.time_index - self.scope:self.time_index]\n",
    "        self.reward = float( - (2 * action - 1) * self.state[-1] )\n",
    "        \n",
    "        # Check if done\n",
    "        if self.time_index - self.episode_init_time > self.episode_size:\n",
    "            self.done = True\n",
    "        if self.time_index > self.diff_close.shape[0] - self.scope -1:\n",
    "            self.done = True\n",
    "\n",
    "        return self.state, self.reward, self.done, info\n",
    "\n",
    "    def reset(self, random_starttime=True):\n",
    "        self.done = False\n",
    "        self.reward = 0.\n",
    "        self.time_index = self.scope \n",
    "        self.state = self.diff_close[self.time_index - self.scope:self.time_index]\n",
    "        \n",
    "        if random_starttime:\n",
    "            self.time_index += random.randint(0, self.num_ticks_train - self.scope)\n",
    "        \n",
    "        self.episode_init_time = self.time_index\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "    def seed(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://dbc-635ca498-e5f1.cloud.databricks.com/?o=445287446643905#notebook/4201196137758409/command/4201196137758410\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, done):\n",
    "        self.memory.append([states, done])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = model.output_shape[-1]\n",
    "\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim, 1))\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory, size=inputs.shape[0])):\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            done = self.memory[idx][1]\n",
    "\n",
    "            inputs[i:i + 1] = state_t\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = model.predict(state_t)[0]\n",
    "            Q_sa = np.max(model.predict(state_tp1)[0])\n",
    "            if done: # if done is True\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                # reward_t + gamma * max_a' Q(s', a')\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://dbc-635ca498-e5f1.cloud.databricks.com/?o=445287446643905#notebook/4201196137758409/command/4201196137758410\n",
    "\n",
    "# RL parameters\n",
    "epsilon = .5  # exploration\n",
    "min_epsilon = 0.1\n",
    "max_memory = 5000\n",
    "batch_size = 512\n",
    "discount = 0.8\n",
    "\n",
    "# Environment parameters\n",
    "num_actions = 2  # [long, short]\n",
    "episodes = 500 # 100000\n",
    "episode_size = 1 * 1 * 60  # roughly an hour worth of data in each training episode\n",
    "\n",
    "# Define state sequence scope (approx. 1 hour)\n",
    "sequence_scope = 60\n",
    "input_shape = (batch_size, sequence_scope, 1)\n",
    "\n",
    "# Create Q Network\n",
    "hidden_size = 128\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, (5), strides=2, input_shape=input_shape[1:], activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2, strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(32, (5), strides=1, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2, strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(hidden_size, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_actions))\n",
    "opt = optimizers.Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "# Define training interval\n",
    "start = datetime.datetime(2010, 11, 15, 0, 0)\n",
    "end = datetime.datetime(2018, 12, 31, 23, 59)\n",
    "\n",
    "# Initialize Environment\n",
    "env = MarketEnv(oilDF_py, start, end, episode_size=episode_size, scope=sequence_scope)\n",
    "\n",
    "# Initialize experience replay object\n",
    "exp_replay = ExperienceReplay(max_memory=max_memory, discount=discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
    ">     Instructions for updating:\n",
    ">     Colocations handled automatically by placer.\n",
    ">     /databricks/python/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: WARN: Box bound precision lowered by casting to float32\n",
    ">       warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
    "\n",
    "  \n",
    "\n",
    "Elephas\n",
    "=======\n",
    "\n",
    "https://github.com/danielenricocahall/elephas\n",
    "\n",
    "Elephas is a third party library that allows to train distributed Keras\n",
    "models on Spark. To run a distributed training session, a Keras model is\n",
    "first declared and compiled on one (singular) master node. Then, copies\n",
    "of the Master model are serialized and shipped to an arbitrary number of\n",
    "worker nodes. Elephas uses RDD's internally to make the data dynamically\n",
    "available to the workers when required. After gradient computating and\n",
    "the update of the weights, the updated model parameters are pushed to\n",
    "the master model.\n",
    "\n",
    "&lt;img\n",
    "src=https://raw.githubusercontent.com/danielenricocahall/elephas/master/elephas.gif&gt;\n",
    "\n",
    "For updating the parameters of the master\\_model, Elephas provides three\n",
    "modes, Synchronous, Asynchronous and HOGWILD\n",
    "(https://arxiv.org/abs/1106.5730).\n",
    "\n",
    "Integrating Distributed model training in our RL-framework\n",
    "----------------------------------------------------------\n",
    "\n",
    "Elephas supports in the current version only supervised model training.\n",
    "We therefore opt to distribute the supervised training step based on the\n",
    "experience replay buffer and keep the surrounding for loops from the\n",
    "previeous RL-implementation.\n",
    "\n",
    "### Training data conversion\n",
    "\n",
    "Data must be provided as either RDD or pyspark dataframe (that will\n",
    "internally be converted to RDD's). A more elaborate pipeline might slice\n",
    "and evaluate replay buffer instances from the original dataframe,\n",
    "however, since most of our implementation expects numpy arrays, we\n",
    "convert the buffer to an RDD manually each step.\n",
    "\n",
    "### Elephas SparkModel for retraining in the RL-loop\n",
    "\n",
    "When Elephas finished its training epochs (here, one Experiancereplay\n",
    "buffer training in one of the RL-loop sweeps), the used processes get\n",
    "terminated. This leads to a crash when trying to retrain a already\n",
    "trained model. As a workaround, we initialize theelephas in each\n",
    "training step newly by using the keras model from the previous training\n",
    "step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elephas variables\n",
    "ele_epochs = 10\n",
    "ele_batchsize = 32\n",
    "ele_verbose = 0\n",
    "ele_valsplit = 0.1\n",
    "\n",
    "# Train\n",
    "returns = []\n",
    "for e in range(1, episodes):\n",
    "    loss = 0.\n",
    "    counter = 0\n",
    "    reward_sum = 0.\n",
    "    done = False\n",
    "    \n",
    "    state = env.reset()\n",
    "    input_t = state.reshape(1, sequence_scope, 1) \n",
    "    \n",
    "    while not done:     \n",
    "        counter += 1\n",
    "        input_tm1 = input_t\n",
    "        # get next action\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(0, num_actions, size=1)\n",
    "        else:\n",
    "            q = model.predict(input_tm1)\n",
    "            action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        state, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        input_t = state.reshape(1, sequence_scope, 1)         \n",
    "\n",
    "        # store experience\n",
    "        exp_replay.remember([input_tm1, action, reward, input_t], done)\n",
    "\n",
    "        # adapt model\n",
    "        inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
    "        \n",
    "        # elephas calls for distributed gradient optimization\n",
    "        train_rdd = to_simple_rdd(sc, inputs, targets)  # note that we provide the spark context sc (sc variable automatically set in databricks)\n",
    "        \n",
    "        spark_model = SparkModel(model, frequency='epoch', mode='asynchronous')  # 'asynchronous', 'hogwild' or 'synchronous'\n",
    "        spark_model.fit(train_rdd, epochs=ele_epochs, batch_size=ele_batchsize, verbose=ele_verbose, validation_split=ele_valsplit)\n",
    "        model = spark_model._master_network # hacky!\n",
    "        \n",
    "        loss += model.train_on_batch(inputs, targets)\n",
    "    \n",
    "    \n",
    "    print(\"Episode {:03d}/{:d} | Average Loss {:.4f} | Cumulative Reward {:.4f}\".format(e, episodes, loss / counter, reward_sum))\n",
    "    epsilon = max(min_epsilon, epsilon * 0.99)\n",
    "    returns.append(reward_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
    ">     Instructions for updating:\n",
    ">     Use tf.cast instead.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    ">     >>> Async training complete.\n",
    ">     >>> Fit model\n",
    ">     >>> Initialize workers\n",
    ">     >>> Distribute load\n",
    "\n",
    "  \n",
    "\n",
    "Notes to the elephas training\n",
    "=============================\n",
    "\n",
    "The pipeline in this notebook serves as a proof of concept to\n",
    "demonstrate how RL-training can be distributed on a spark cluster.\n",
    "During testing, we observed that with the gived Experience replay buffer\n",
    "size, the distributed model training is rather a bottleneck, compared to\n",
    "running keras out-of the box in parallel.\n",
    "\n",
    "Error that sometimes (non-reproducible) appears when running on databricks\n",
    "--------------------------------------------------------------------------\n",
    "\n",
    "We notices that sometimes, the distributed training crashes at line 180\n",
    "in\n",
    "https://github.com/danielenricocahall/elephas/blob/master/elephas/spark\\_model.py,\n",
    "concretely at `rdd.mapPartitions(worker.train).collect()`, with a\n",
    "`Py4JJavaError`. Restarting the cluster does not resolve the issue,\n",
    "however, sometimes it was possible to re-run a training after a bit of\n",
    "time succesfully. We assume that it might be connected with the jvm, but\n",
    "lacking precise insight regarding the nature of this bug.\n",
    "\n",
    "### KB updated:\n",
    "\n",
    "Notes to the elephas training\n",
    "=============================\n",
    "\n",
    "The pipeline in this notebook serves as a proof of concept to\n",
    "demonstrate how RL-training can be distributed on a spark cluster.\n",
    "During testing, we observed that the experience replay is a bottleneck\n",
    "during distributed model training, when comparing to running keras\n",
    "out-of the box in parallel.\n",
    "\n",
    "Error that sometimes appears when running on databricks\n",
    "-------------------------------------------------------\n",
    "\n",
    "We notice occasional crashes in the distributed training at line 180 in\n",
    "https://github.com/danielenricocahall/elephas/blob/master/elephas/spark\\_model.py,\n",
    "more precisely at `rdd.mapPartitions(worker.train).collect()`, with a\n",
    "`Py4JJavaError`. Restarting the cluster does not resolve the issue,\n",
    "however sometimes it was possible to re-run a training succesfully after\n",
    "a bit of time. We assume that it is connected with the jvm, but lacking\n",
    "precise insight regarding the nature of this bug."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
