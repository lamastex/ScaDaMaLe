{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SDS-2.x, Scalable Data Engineering Science](https://lamastex.github.io/scalable-data-science/sds/2/x/)\n",
    "=======================================================================================================\n",
    "\n",
    "This is a 2019 augmentation and update of [Adam\n",
    "Breindel](https://www.linkedin.com/in/adbreind)'s initial notebooks.\n",
    "\n",
    "TensorFlow\n",
    "==========\n",
    "\n",
    "### ... is a general math framework\n",
    "\n",
    "TensorFlow is designed to accommodate...\n",
    "\n",
    "-   Easy operations on tensors (n-dimensional arrays)\n",
    "-   Mappings to performant low-level implementations, including native\n",
    "    CPU and GPU\n",
    "-   Optimization via gradient descent variants\n",
    "    -   Including high-performance differentiation\n",
    "\n",
    "Low-level math primitives called \"Ops\"\n",
    "\n",
    "From these primitives, linear algebra and other higher-level constructs\n",
    "are formed.\n",
    "\n",
    "Going up one more level common neural-net components have been built and\n",
    "included.\n",
    "\n",
    "At an even higher level of abstraction, various libraries have been\n",
    "created that simplify building and wiring common network patterns. Over\n",
    "the last 2 years, we've seen 3-5 such libraries.\n",
    "\n",
    "We will focus later on one, Keras, which has now been adopted as the\n",
    "\"official\" high-level wrapper for TensorFlow.\n",
    "\n",
    "### We'll get familiar with TensorFlow so that it is not a \"magic black box\"\n",
    "\n",
    "But for most of our work, it will be more productive to work with the\n",
    "higher-level wrappers. At the end of this notebook, we'll make the\n",
    "connection between the Keras API we've used and the TensorFlow code\n",
    "underneath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant(100, name='x')\n",
    "y = tf.Variable(x + 50, name='y')\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     WARNING:tensorflow:From /databricks/python/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
    ">     Instructions for updating:\n",
    ">     Colocations handled automatically by placer.\n",
    ">     <tf.Variable 'y:0' shape=() dtype=int32_ref>\n",
    "\n",
    "  \n",
    "\n",
    "### There's a bit of \"ceremony\" there...\n",
    "\n",
    "... and ... where's the actual output?\n",
    "\n",
    "For performance reasons, TensorFlow separates the design of the\n",
    "computation from the actual execution.\n",
    "\n",
    "TensorFlow programs describe a computation graph -- an abstract DAG of\n",
    "data flow -- that can then be analyzed, optimized, and implemented on a\n",
    "variety of hardware, as well as potentially scheduled across a cluster\n",
    "of separate machines.\n",
    "\n",
    "Like many query engines and compute graph engines, evaluation is\n",
    "**lazy** ... so we don't get \"real numbers\" until we force TensorFlow to\n",
    "run the calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_node = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init_node)\n",
    "    print(session.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     150\n",
    "\n",
    "  \n",
    "\n",
    "### TensorFlow integrates tightly with NumPy\n",
    "\n",
    "and we typically use NumPy to create and manage the tensors (vectors,\n",
    "matrices, etc.) that will \"flow\" through our graph\n",
    "\n",
    "New to NumPy? Grab a cheat sheet:\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog*assets/Numpy*Python*Cheat*Sheet.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.normal(loc=10.0, scale=2.0, size=[3,3]) # mean 10, std dev 2\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     [[11.18652918 12.58198461 11.91879134]\n",
    ">      [13.46965549 11.58228636 12.62747219]\n",
    ">      [12.25074639 11.84947419 11.2810347 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all nodes get added to default graph (unless we specify otherwise)\n",
    "# we can reset the default graph -- so it's not cluttered up:\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.constant(data, name='x')\n",
    "y = tf.Variable(x * 10, name='y')\n",
    "\n",
    "init_node = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init_node)\n",
    "    print(session.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     [[111.86529181 125.81984612 119.18791344]\n",
    ">      [134.69655487 115.82286357 126.27472191]\n",
    ">      [122.50746386 118.49474186 112.810347  ]]\n",
    "\n",
    "  \n",
    "\n",
    "### We will often iterate on a calculation ...\n",
    "\n",
    "Calling `session.run` runs just one step, so we can iterate using Python\n",
    "as a control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    for i in range(3):\n",
    "        x = x + 1\n",
    "        print(session.run(x))\n",
    "        print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     [[12.18652918 13.58198461 12.91879134]\n",
    ">      [14.46965549 12.58228636 13.62747219]\n",
    ">      [13.25074639 12.84947419 12.2810347 ]]\n",
    ">     ----------------------------------------------\n",
    ">     [[13.18652918 14.58198461 13.91879134]\n",
    ">      [15.46965549 13.58228636 14.62747219]\n",
    ">      [14.25074639 13.84947419 13.2810347 ]]\n",
    ">     ----------------------------------------------\n",
    ">     [[14.18652918 15.58198461 14.91879134]\n",
    ">      [16.46965549 14.58228636 15.62747219]\n",
    ">      [15.25074639 14.84947419 14.2810347 ]]\n",
    ">     ----------------------------------------------\n",
    "\n",
    "  \n",
    "\n",
    "### Optimizers\n",
    "\n",
    "TF includes a set of built-in algorithm implementations (though you\n",
    "could certainly write them yourself) for performing optimization.\n",
    "\n",
    "These are oriented around gradient-descent methods, with a set of handy\n",
    "extension flavors to make things converge faster.\n",
    "\n",
    "#### Using TF optimizer to solve problems\n",
    "\n",
    "We can use the optimizers to solve anything (not just neural networks)\n",
    "so let's start with a simple equation.\n",
    "\n",
    "We supply a bunch of data points, that represent inputs. We will\n",
    "generate them based on a known, simple equation (y will always be 2\\*x +\n",
    "6) but we won't tell TF that. Instead, we will give TF a function\n",
    "structure ... linear with 2 parameters, and let TF try to figure out the\n",
    "parameters by minimizing an error function.\n",
    "\n",
    "What is the error function?\n",
    "\n",
    "The \"real\" error is the absolute value of the difference between TF's\n",
    "current approximation and our ground-truth y value.\n",
    "\n",
    "But absolute value is not a friendly function to work with there, so\n",
    "instead we'll square it. That gets us a nice, smooth function that TF\n",
    "can work with, and it's just as good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[7]: 0.7233325331320565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\") \n",
    "y = tf.placeholder(\"float\")\n",
    "\n",
    "m = tf.Variable([1.0], name=\"m-slope-coefficient\") # initial values ... for now they don't matter much\n",
    "b = tf.Variable([1.0], name=\"b-intercept\")\n",
    "\n",
    "y_model = tf.multiply(x, m) + b\n",
    "\n",
    "error = tf.square(y - y_model)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(error)\n",
    "\n",
    "model = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(model)\n",
    "    for i in range(10):\n",
    "        x_value = np.random.rand()\n",
    "        y_value = x_value * 2 + 6 # we know these params, but we're making TF learn them\n",
    "        session.run(train_op, feed_dict={x: x_value, y: y_value})\n",
    "\n",
    "    out = session.run([m, b])\n",
    "    print(out)\n",
    "    print(\"Model: {r:.3f}x + {s:.3f}\".format(r=out[0][0], s=out[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     [array([1.4387712], dtype=float32), array([1.9758162], dtype=float32)]\n",
    ">     Model: 1.439x + 1.976\n",
    "\n",
    "  \n",
    "\n",
    "#### That's pretty terrible :)\n",
    "\n",
    "Try two experiments. Change the number of iterations the optimizer runs,\n",
    "and -- independently -- try changing the learning rate (that's the\n",
    "number we passed to `GradientDescentOptimizer`)\n",
    "\n",
    "See what happens with different values.\n",
    "\n",
    "#### These are scalars. Where do the tensors come in?\n",
    "\n",
    "Using matrices allows us to represent (and, with the right hardware,\n",
    "compute) the data-weight dot products for lots of data vectors (a mini\n",
    "batch) and lots of weight vectors (neurons) at the same time.\n",
    "\n",
    "Tensors are useful because some of our data \"vectors\" are really\n",
    "multidimensional -- for example, with a color image we may want to\n",
    "preserve height, width, and color planes. We can hold multiple color\n",
    "images, with their shapes, in a 4-D (or 4 \"axis\") tensor.\n",
    "\n",
    "### Let's also make the connection from Keras down to Tensorflow.\n",
    "\n",
    "We used a Keras class called `Dense`, which represents a\n",
    "\"fully-connected\" layer of -- in this case -- linear perceptrons. Let's\n",
    "look at the source code to that, just to see that there's no mystery.\n",
    "\n",
    "https://github.com/fchollet/keras/blob/master/keras/layers/core.py\n",
    "\n",
    "It calls down to the \"back end\" by calling\n",
    "`output = K.dot(inputs, self.kernel)` where `kernel` means this layer's\n",
    "weights.\n",
    "\n",
    "`K` represents the pluggable backend wrapper. You can trace K.dot on\n",
    "Tensorflow by looking at\n",
    "\n",
    "https://github.com/fchollet/keras/blob/master/keras/backend/tensorflow\\_backend.py\n",
    "\n",
    "Look for `def dot(x, y):` and look right toward the end of the method.\n",
    "The math is done by calling `tf.matmul(x, y)`\n",
    "\n",
    "#### What else helps Tensorflow (and other frameworks) run fast?\n",
    "\n",
    "-   A fast, simple mechanism for calculating all of the partial\n",
    "    derivatives we need, called *reverse-mode autodifferentiation*\n",
    "-   Implementations of low-level operations in optimized CPU code (e.g.,\n",
    "    C++, MKL) and GPU code (CUDA/CuDNN/HLSL)\n",
    "-   Support for distributed parallel training, although parallelizing\n",
    "    deep learning is non-trivial ... not automagic like with, e.g.,\n",
    "    Apache Spark\n",
    "\n",
    "### That is the essence of TensorFlow!\n",
    "\n",
    "There are three principal directions to explore further:\n",
    "\n",
    "-   Working with tensors instead of scalars: this is not intellectually\n",
    "    difficult, but takes some practice to wrangle the shaping and\n",
    "    re-shaping of tensors. If you get the shape of a tensor wrong, your\n",
    "    script will blow up. Just takes practice.\n",
    "\n",
    "-   Building more complex models. You can write these yourself using\n",
    "    lower level \"Ops\" -- like matrix multiply -- or using higher level\n",
    "    classes like `tf.layers.dense` *Use the source, Luke!*\n",
    "\n",
    "-   Operations and integration ecosystem: as TensorFlow has matured, it\n",
    "    is easier to integrate additional tools and solve the peripheral\n",
    "    problems:\n",
    "\n",
    "    -   TensorBoard for visualizing training\n",
    "    -   tfdbg command-line debugger\n",
    "    -   Distributed TensorFlow for clustered training\n",
    "    -   GPU integration\n",
    "    -   Feeding large datasets from external files\n",
    "    -   Tensorflow Serving for serving models (i.e., using an existing\n",
    "        model to predict on new incoming data)\n",
    "\n",
    "Distirbuted Training for DL\n",
    "===========================\n",
    "\n",
    "-   https://docs.azuredatabricks.net/applications/deep-learning/distributed-training/horovod-runner.html\n",
    "    -   https://docs.azuredatabricks.net/applications/deep-learning/distributed-training/mnist-tensorflow-keras.html\n",
    "-   https://software.intel.com/en-us/articles/bigdl-distributed-deep-learning-on-apache-spark"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
