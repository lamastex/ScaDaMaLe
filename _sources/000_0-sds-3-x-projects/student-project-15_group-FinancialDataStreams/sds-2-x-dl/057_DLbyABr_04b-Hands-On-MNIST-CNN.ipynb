{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SDS-2.x, Scalable Data Engineering Science](https://lamastex.github.io/scalable-data-science/sds/2/x/)\n",
    "=======================================================================================================\n",
    "\n",
    "This is a 2019 augmentation and update of [Adam\n",
    "Breindel](https://www.linkedin.com/in/adbreind)'s initial notebooks.\n",
    "\n",
    "Try out the first ConvNet -- the one we looked at earlier.\n",
    "\n",
    "This code is the same, but we'll run to 20 epochs so we can get a better\n",
    "feel for fitting/validation/overfitting trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import sklearn.datasets\n",
    "\n",
    "train_libsvm = \"/dbfs/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt\"\n",
    "test_libsvm = \"/dbfs/databricks-datasets/mnist-digits/data-001/mnist-digits-test.txt\"\n",
    "\n",
    "X_train, y_train = sklearn.datasets.load_svmlight_file(train_libsvm, n_features=784)\n",
    "X_train = X_train.toarray()\n",
    "\n",
    "X_test, y_test = sklearn.datasets.load_svmlight_file(test_libsvm, n_features=784)\n",
    "X_test = X_test.toarray()\n",
    "\n",
    "X_train = X_train.reshape( (X_train.shape[0], 28, 28, 1) )\n",
    "X_train = X_train.astype('float32')\n",
    "X_train /= 255\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "\n",
    "X_test = X_test.reshape( (X_test.shape[0], 28, 28, 1) )\n",
    "X_test = X_test.astype('float32')\n",
    "X_test /= 255\n",
    "y_test = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Using TensorFlow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(8, # number of kernels \n",
    "\t\t\t\t(4, 4), # kernel size\n",
    "                padding='valid', # no padding; output will be smaller than input\n",
    "                input_shape=(28, 28, 1)))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu')) # alternative syntax for applying activation\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=20, verbose=2, validation_split=0.1)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print\n",
    "for i in range(len(model.metrics_names)):\n",
    "\tprint(\"%s: %f\" % (model.metrics_names[i], scores[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     WARNING:tensorflow:From /databricks/python/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
    ">     Instructions for updating:\n",
    ">     Colocations handled automatically by placer.\n",
    ">     WARNING:tensorflow:From /databricks/python/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
    ">     Instructions for updating:\n",
    ">     Use tf.cast instead.\n",
    ">     Train on 54000 samples, validate on 6000 samples\n",
    ">     Epoch 1/20\n",
    ">      - 11s - loss: 0.3233 - acc: 0.9068 - val_loss: 0.1206 - val_acc: 0.9653\n",
    ">     Epoch 2/20\n",
    ">      - 10s - loss: 0.1111 - acc: 0.9673 - val_loss: 0.0753 - val_acc: 0.9808\n",
    ">     Epoch 3/20\n",
    ">      - 10s - loss: 0.0687 - acc: 0.9791 - val_loss: 0.0625 - val_acc: 0.9818\n",
    ">     Epoch 4/20\n",
    ">      - 10s - loss: 0.0488 - acc: 0.9855 - val_loss: 0.0521 - val_acc: 0.9855\n",
    ">     Epoch 5/20\n",
    ">      - 10s - loss: 0.0385 - acc: 0.9883 - val_loss: 0.0507 - val_acc: 0.9858\n",
    ">     Epoch 6/20\n",
    ">      - 10s - loss: 0.0311 - acc: 0.9903 - val_loss: 0.0485 - val_acc: 0.9862\n",
    ">     Epoch 7/20\n",
    ">      - 10s - loss: 0.0257 - acc: 0.9920 - val_loss: 0.0486 - val_acc: 0.9880\n",
    ">     Epoch 8/20\n",
    ">      - 10s - loss: 0.0210 - acc: 0.9934 - val_loss: 0.0462 - val_acc: 0.9875\n",
    ">     Epoch 9/20\n",
    ">      - 10s - loss: 0.0173 - acc: 0.9948 - val_loss: 0.0500 - val_acc: 0.9865\n",
    ">     Epoch 10/20\n",
    ">      - 10s - loss: 0.0150 - acc: 0.9954 - val_loss: 0.0471 - val_acc: 0.9878\n",
    ">     Epoch 11/20\n",
    ">      - 10s - loss: 0.0116 - acc: 0.9966 - val_loss: 0.0608 - val_acc: 0.9840\n",
    ">     Epoch 12/20\n",
    ">      - 10s - loss: 0.0106 - acc: 0.9968 - val_loss: 0.0459 - val_acc: 0.9885\n",
    ">     Epoch 13/20\n",
    ">      - 10s - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0511 - val_acc: 0.9873\n",
    ">     Epoch 14/20\n",
    ">      - 10s - loss: 0.0080 - acc: 0.9976 - val_loss: 0.0586 - val_acc: 0.9857\n",
    ">     Epoch 15/20\n",
    ">      - 10s - loss: 0.0074 - acc: 0.9979 - val_loss: 0.0495 - val_acc: 0.9890\n",
    ">     Epoch 16/20\n",
    ">      - 10s - loss: 0.0052 - acc: 0.9984 - val_loss: 0.0545 - val_acc: 0.9890\n",
    ">     Epoch 17/20\n",
    ">      - 10s - loss: 0.0068 - acc: 0.9977 - val_loss: 0.0591 - val_acc: 0.9860\n",
    ">     Epoch 18/20\n",
    ">      - 10s - loss: 0.0076 - acc: 0.9974 - val_loss: 0.0567 - val_acc: 0.9890\n",
    ">     Epoch 19/20\n",
    ">      - 10s - loss: 0.0038 - acc: 0.9990 - val_loss: 0.0528 - val_acc: 0.9885\n",
    ">     Epoch 20/20\n",
    ">      - 10s - loss: 0.0029 - acc: 0.9992 - val_loss: 0.0629 - val_acc: 0.9865\n",
    ">\n",
    ">        32/10000 [..............................] - ETA: 1s\n",
    ">       416/10000 [>.............................] - ETA: 1s\n",
    ">       800/10000 [=>............................] - ETA: 1s\n",
    ">      1184/10000 [==>...........................] - ETA: 1s\n",
    ">      1600/10000 [===>..........................] - ETA: 1s\n",
    ">      2016/10000 [=====>........................] - ETA: 1s\n",
    ">      2432/10000 [======>.......................] - ETA: 0s\n",
    ">      2848/10000 [=======>......................] - ETA: 0s\n",
    ">      3232/10000 [========>.....................] - ETA: 0s\n",
    ">      3584/10000 [=========>....................] - ETA: 0s\n",
    ">      4000/10000 [===========>..................] - ETA: 0s\n",
    ">      4416/10000 [============>.................] - ETA: 0s\n",
    ">      4832/10000 [=============>................] - ETA: 0s\n",
    ">      5216/10000 [==============>...............] - ETA: 0s\n",
    ">      5600/10000 [===============>..............] - ETA: 0s\n",
    ">      5984/10000 [================>.............] - ETA: 0s\n",
    ">      6400/10000 [==================>...........] - ETA: 0s\n",
    ">      6816/10000 [===================>..........] - ETA: 0s\n",
    ">      7232/10000 [====================>.........] - ETA: 0s\n",
    ">      7648/10000 [=====================>........] - ETA: 0s\n",
    ">      8064/10000 [=======================>......] - ETA: 0s\n",
    ">      8480/10000 [========================>.....] - ETA: 0s\n",
    ">      8864/10000 [=========================>....] - ETA: 0s\n",
    ">      9248/10000 [==========================>...] - ETA: 0s\n",
    ">      9664/10000 [===========================>..] - ETA: 0s\n",
    ">     10000/10000 [==============================] - 1s 130us/step\n",
    ">\n",
    ">     loss: 0.052445\n",
    ">     acc: 0.986500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((5,5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Next, let's try adding another convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(8, # number of kernels \n",
    "\t\t\t\t\t\t(4, 4), # kernel size\n",
    "                        padding='valid',\n",
    "                        input_shape=(28, 28, 1)))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(8, (4, 4))) # <-- additional Conv layer\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=15, verbose=2, validation_split=0.1)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print\n",
    "for i in range(len(model.metrics_names)):\n",
    "\tprint(\"%s: %f\" % (model.metrics_names[i], scores[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Train on 54000 samples, validate on 6000 samples\n",
    ">     Epoch 1/15\n",
    ">      - 20s - loss: 0.2958 - acc: 0.9148 - val_loss: 0.0759 - val_acc: 0.9790\n",
    ">     Epoch 2/15\n",
    ">      - 20s - loss: 0.0710 - acc: 0.9784 - val_loss: 0.0542 - val_acc: 0.9863\n",
    ">     Epoch 3/15\n",
    ">      - 20s - loss: 0.0478 - acc: 0.9854 - val_loss: 0.0474 - val_acc: 0.9870\n",
    ">     Epoch 4/15\n",
    ">      - 20s - loss: 0.0351 - acc: 0.9891 - val_loss: 0.0421 - val_acc: 0.9890\n",
    ">     Epoch 5/15\n",
    ">      - 20s - loss: 0.0270 - acc: 0.9917 - val_loss: 0.0524 - val_acc: 0.9857\n",
    ">     Epoch 6/15\n",
    ">      - 20s - loss: 0.0213 - acc: 0.9933 - val_loss: 0.0434 - val_acc: 0.9883\n",
    ">     Epoch 7/15\n",
    ">      - 20s - loss: 0.0184 - acc: 0.9942 - val_loss: 0.0447 - val_acc: 0.9888\n",
    ">     Epoch 8/15\n",
    ">      - 20s - loss: 0.0132 - acc: 0.9959 - val_loss: 0.0529 - val_acc: 0.9867\n",
    ">     Epoch 9/15\n",
    ">      - 20s - loss: 0.0111 - acc: 0.9964 - val_loss: 0.0481 - val_acc: 0.9882\n",
    ">     Epoch 10/15\n",
    ">      - 20s - loss: 0.0099 - acc: 0.9967 - val_loss: 0.0469 - val_acc: 0.9887\n",
    ">     Epoch 11/15\n",
    ">      - 20s - loss: 0.0080 - acc: 0.9971 - val_loss: 0.0697 - val_acc: 0.9843\n",
    ">     Epoch 12/15\n",
    ">      - 20s - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0468 - val_acc: 0.9897\n",
    ">     Epoch 13/15\n",
    ">      - 20s - loss: 0.0052 - acc: 0.9982 - val_loss: 0.0526 - val_acc: 0.9893\n",
    ">     Epoch 14/15\n",
    ">      - 20s - loss: 0.0071 - acc: 0.9978 - val_loss: 0.0532 - val_acc: 0.9892\n",
    ">     Epoch 15/15\n",
    ">      - 19s - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0510 - val_acc: 0.9902\n",
    ">\n",
    ">        32/10000 [..............................] - ETA: 1s\n",
    ">       352/10000 [>.............................] - ETA: 1s\n",
    ">       672/10000 [=>............................] - ETA: 1s\n",
    ">       992/10000 [=>............................] - ETA: 1s\n",
    ">      1312/10000 [==>...........................] - ETA: 1s\n",
    ">      1632/10000 [===>..........................] - ETA: 1s\n",
    ">      1952/10000 [====>.........................] - ETA: 1s\n",
    ">      2272/10000 [=====>........................] - ETA: 1s\n",
    ">      2592/10000 [======>.......................] - ETA: 1s\n",
    ">      2880/10000 [=======>......................] - ETA: 1s\n",
    ">      3200/10000 [========>.....................] - ETA: 1s\n",
    ">      3520/10000 [=========>....................] - ETA: 1s\n",
    ">      3840/10000 [==========>...................] - ETA: 1s\n",
    ">      4160/10000 [===========>..................] - ETA: 0s\n",
    ">      4480/10000 [============>.................] - ETA: 0s\n",
    ">      4800/10000 [=============>................] - ETA: 0s\n",
    ">      5120/10000 [==============>...............] - ETA: 0s\n",
    ">      5440/10000 [===============>..............] - ETA: 0s\n",
    ">      5760/10000 [================>.............] - ETA: 0s\n",
    ">      6080/10000 [=================>............] - ETA: 0s\n",
    ">      6400/10000 [==================>...........] - ETA: 0s\n",
    ">      6688/10000 [===================>..........] - ETA: 0s\n",
    ">      7008/10000 [====================>.........] - ETA: 0s\n",
    ">      7328/10000 [====================>.........] - ETA: 0s\n",
    ">      7648/10000 [=====================>........] - ETA: 0s\n",
    ">      7968/10000 [======================>.......] - ETA: 0s\n",
    ">      8288/10000 [=======================>......] - ETA: 0s\n",
    ">      8608/10000 [========================>.....] - ETA: 0s\n",
    ">      8896/10000 [=========================>....] - ETA: 0s\n",
    ">      9216/10000 [==========================>...] - ETA: 0s\n",
    ">      9536/10000 [===========================>..] - ETA: 0s\n",
    ">      9856/10000 [============================>.] - ETA: 0s\n",
    ">     10000/10000 [==============================] - 2s 168us/step\n",
    ">\n",
    ">     loss: 0.047584\n",
    ">     acc: 0.988000\n",
    "\n",
    "  \n",
    "\n",
    "### Still Overfitting\n",
    "\n",
    "We're making progress on our test error -- about 99% -- but just a bit\n",
    "for all the additional time, due to the network overfitting the data.\n",
    "\n",
    "There are a variety of techniques we can take to counter this -- forms\n",
    "of regularization.\n",
    "\n",
    "Let's try a relatively simple solution that works surprisingly well: add\n",
    "a pair of `Dropout` filters, a layer that randomly omits a fraction of\n",
    "neurons from each training batch (thus exposing each neuron to only part\n",
    "of the training data).\n",
    "\n",
    "We'll add more convolution kernels but shrink them to 3x3 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, # number of kernels \n",
    "\t\t\t\t\t\t(3, 3), # kernel size\n",
    "                        padding='valid',\n",
    "                        input_shape=(28, 28, 1)))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=15, verbose=2)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "print\n",
    "for i in range(len(model.metrics_names)):\n",
    "\tprint(\"%s: %f\" % (model.metrics_names[i], scores[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     WARNING:tensorflow:From /databricks/python/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
    ">     Instructions for updating:\n",
    ">     Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
    ">     Epoch 1/15\n",
    ">      - 66s - loss: 0.2854 - acc: 0.9140\n",
    ">     Epoch 2/15\n",
    ">      - 66s - loss: 0.0945 - acc: 0.9715\n",
    ">     Epoch 3/15\n",
    ">      - 66s - loss: 0.0727 - acc: 0.9783\n",
    ">     Epoch 4/15\n",
    ">      - 65s - loss: 0.0592 - acc: 0.9817\n",
    ">     Epoch 5/15\n",
    ">      - 66s - loss: 0.0501 - acc: 0.9840\n",
    ">     Epoch 6/15\n",
    ">      - 66s - loss: 0.0463 - acc: 0.9855\n",
    ">     Epoch 7/15\n",
    ">      - 66s - loss: 0.0420 - acc: 0.9865\n",
    ">     Epoch 8/15\n",
    ">      - 66s - loss: 0.0373 - acc: 0.9881\n",
    ">     Epoch 9/15\n",
    ">      - 65s - loss: 0.0346 - acc: 0.9893\n",
    ">     Epoch 10/15\n",
    ">      - 66s - loss: 0.0337 - acc: 0.9893\n",
    ">     Epoch 11/15\n",
    ">      - 66s - loss: 0.0305 - acc: 0.9901\n",
    ">     Epoch 12/15\n",
    ">      - 66s - loss: 0.0253 - acc: 0.9920\n",
    ">     Epoch 13/15\n",
    ">      - 66s - loss: 0.0246 - acc: 0.9918\n",
    ">     Epoch 14/15\n",
    ">      - 66s - loss: 0.0241 - acc: 0.9919\n",
    ">     Epoch 15/15\n",
    ">      - 65s - loss: 0.0244 - acc: 0.9923\n",
    ">\n",
    ">     loss: 0.027750\n",
    ">     acc: 0.991800\n",
    "\n",
    "  \n",
    "\n",
    "*Lab Wrapup*\n",
    "------------\n",
    "\n",
    "From the last lab, you should have a test accuracy of over 99.1%\n",
    "\n",
    "For one more activity, try changing the optimizer to old-school \"sgd\" --\n",
    "just to see how far we've come with these modern gradient descent\n",
    "techniques in the last few years.\n",
    "\n",
    "Accuracy will end up noticeably worse ... about 96-97% test accuracy.\n",
    "Two key takeaways:\n",
    "\n",
    "-   Without a good optimizer, even a very powerful network design may\n",
    "    not achieve results\n",
    "-   In fact, we could replace the word \"optimizer\" there with\n",
    "    -   initialization\n",
    "    -   activation\n",
    "    -   regularization\n",
    "    -   (etc.)\n",
    "-   All of these elements we've been working with operate together in a\n",
    "    complex way to determine final performance"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
