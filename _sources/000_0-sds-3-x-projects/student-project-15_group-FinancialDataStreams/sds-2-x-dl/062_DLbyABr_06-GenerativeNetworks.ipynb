{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SDS-2.x, Scalable Data Engineering Science](https://lamastex.github.io/scalable-data-science/sds/2/x/)\n",
    "=======================================================================================================\n",
    "\n",
    "This is a 2019 augmentation and update of [Adam\n",
    "Breindel](https://www.linkedin.com/in/adbreind)'s initial notebooks.\n",
    "\n",
    "Generative Networks\n",
    "===================\n",
    "\n",
    "### Concept:\n",
    "\n",
    "If a set of network weights can convert an image of the numeral 8 or a\n",
    "cat &lt;br/&gt;into the classification \"8\" or \"cat\" ...\n",
    "\n",
    "### Does it contain enough information to do the reverse?\n",
    "\n",
    "I.e., can we ask a network what \"8\" looks like and get a picture?\n",
    "\n",
    "Let's think about this for a second. Clearly the classifications have\n",
    "far fewer bits of entropy than the source images' theoretical limit.\n",
    "\n",
    "-   Cat (in cat-vs-dog) has just 1 bit, where perhaps a 256x256\n",
    "    grayscale image has up to 512k bits.\n",
    "-   8 (in MNIST) has ${log \\_2 10}$ or a little over 3 bits, where a\n",
    "    28x28 grayscale image has over 6000 bits.\n",
    "\n",
    "So at first, this would seem difficult or impossible.\n",
    "\n",
    "**But** ... let's do a thought experiment.\n",
    "\n",
    "-   Children can do this easily\n",
    "-   We could create a lookup table of, say, digit -&gt; image trivially,\n",
    "    and use that as a first approximation\n",
    "\n",
    "Those approaches seem like cheating. But let's think about how they\n",
    "work.\n",
    "\n",
    "If a child (or adult) draws a cat (or number 8), they are probably not\n",
    "drawing any specific cat (or 8). They are drawing a general\n",
    "approximation of a cat based on\n",
    "\n",
    "1.  All of the cats they've seen\n",
    "2.  What they remember as the key elements of a cat (4 legs, tail,\n",
    "    pointy ears)\n",
    "3.  A lookup table substitutes one specific cat or 8 ... and, especially\n",
    "    in the case of the 8, that may be fine. The only thing we \"lose\" is\n",
    "    the diversity of things that all mapped to cat (or 8) -- and\n",
    "    discarding that information was exactly our goal when building a\n",
    "    classifier\n",
    "\n",
    "The \"8\" is even simpler: we learn that a number is an idea, not a\n",
    "specific instance, so anything that another human recognizes as 8 is\n",
    "good enough. We are not even trying to make a particular shape, just one\n",
    "that represents our encoded information that distinguishes 8 from other\n",
    "possible symbols in the context and the eye of the viewer.\n",
    "\n",
    "This should remind you a bit of the KL Divergence we talked about at the\n",
    "start: we are providing just enough information (entropy or surprise) to\n",
    "distinguish the cat or \"8\" from the other items that a human receiver\n",
    "might be expecting to see.\n",
    "\n",
    "And where do our handwritten \"pixels\" come from? No image in particular\n",
    "-- they are totally synthetic based on a probability distribution.\n",
    "\n",
    "*These considerations make it sound more likely that a computer could\n",
    "perform the same task.*\n",
    "\n",
    "But what would the weights really represent? what could they generate?\n",
    "\n",
    "**The weights we learn in classification represent the distinguishing\n",
    "features of a class, across all training examples of the class, modified\n",
    "to overlap minimally with features of other classes.** KL divergence\n",
    "again.\n",
    "\n",
    "To be concrete, if we trained a model on just a few dozen MNIST images\n",
    "using pixels, it would probably learn the 3 or 4 \"magic\" pixels that\n",
    "*happened* to distinguish the 8s in that dataset. Trying to generate\n",
    "from that information would yield strong confidence about those magic\n",
    "pixels, but would look like dots to us humans.\n",
    "\n",
    "On the other hand, if we trained on a very large number of MNIST images\n",
    "-- say we use the convnet this time -- the model's weights should\n",
    "represent general filters of masks for features that distinguish an 8.\n",
    "And if we try to reverse the process by amplifying just those filters,\n",
    "we should get a blurry statistical distribution of those very features.\n",
    "The approximate shape of the Platonic \"8\"!\n",
    "\n",
    "Mechanically, How Could This Work?\n",
    "----------------------------------\n",
    "\n",
    "Let's start with a simpler model called an auto-encoder.\n",
    "\n",
    "An autoencoder's job is to take a large representation of a record and\n",
    "find weights that represent that record in a smaller encoding, subject\n",
    "to the constraint that the decoded version should match the original as\n",
    "closely as possible.\n",
    "\n",
    "A bit like training a JPEG encoder to compress images by scoring it with\n",
    "the loss between the original image and the decompressed version of the\n",
    "lossy compressed image.\n",
    "\n",
    "&lt;img src=\"http://i.imgur.com/oTRvlB6.png\" width=450&gt;\n",
    "\n",
    "One nice aspect of this is that it is *unsupervised* -- i.e., we do not\n",
    "need any ground-truth or human-generated labels in order to find the\n",
    "error and train. The error is always the difference between the output\n",
    "and the input, and the goal is to minimize this over many examples, thus\n",
    "minimize in the general case.\n",
    "\n",
    "We can do this with a simple multilayer perceptron network. Or, we can\n",
    "get fancier and do this with a convolutional network. In reverse, the\n",
    "convolution (typically called \"transposed convolution\" or\n",
    "\"deconvolution\") is an upsampling operation across space (in images) or\n",
    "space & time (in audio/video)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "import sklearn.datasets\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train_libsvm = \"/dbfs/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt\"\n",
    "test_libsvm = \"/dbfs/databricks-datasets/mnist-digits/data-001/mnist-digits-test.txt\"\n",
    "\n",
    "X_train, y_train = sklearn.datasets.load_svmlight_file(train_libsvm, n_features=784)\n",
    "X_train = X_train.toarray()\n",
    "X_test, y_test = sklearn.datasets.load_svmlight_file(test_libsvm, n_features=784)\n",
    "X_test = X_test.toarray()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=784, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(784, kernel_initializer='normal', activation='relu'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error', 'binary_crossentropy'])\n",
    "\n",
    "start = datetime.datetime.today()\n",
    "\n",
    "history = model.fit(X_train, X_train, epochs=5, batch_size=100, validation_split=0.1, verbose=2)\n",
    "\n",
    "scores = model.evaluate(X_test, X_test)\n",
    "\n",
    "print\n",
    "for i in range(len(model.metrics_names)):\n",
    "\tprint(\"%s: %f\" % (model.metrics_names[i], scores[i]))\n",
    "\n",
    "print (\"Start: \" + str(start))\n",
    "end = datetime.datetime.today()\n",
    "print (\"End: \" + str(end))\n",
    "print (\"Elapse: \" + str(end-start))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((4,4))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.clear()\n",
    "fig.set_size_inches((5,5))\n",
    "ax.imshow(np.reshape(X_test[61], (28,28)), cmap='gray')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_decode = model.predict(np.reshape(X_test[61], (1, 784)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.clear()\n",
    "ax.imshow(np.reshape(encode_decode, (28,28)), cmap='gray')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Any ideas about those black dots in the upper right?\n",
    "\n",
    "### Pretty cool. So we're all done now, right? Now quite...\n",
    "\n",
    "The problem with the autoencoder is it's \"too good\" at its task.\n",
    "\n",
    "It is optimized to compress exactly the input record set, so it is\n",
    "trained only to create records it has seen. If the middle layer, or\n",
    "information bottleneck, is tight enough, the coded records use all of\n",
    "the information space in the middle layer.\n",
    "\n",
    "So any value in the middle layer decodes to exactly one already-seen\n",
    "exemplar.\n",
    "\n",
    "In our example, and most autoencoders, there is more space in the middle\n",
    "layer but the coded values are not distributed in any sensible way. So\n",
    "we can decode a random vector and we'll probably just get garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.random.randn(30)\n",
    "v = np.array(np.reshape(v, (1, 30)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "t = tf.convert_to_tensor(v, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.layers[1](t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "with K.get_session().as_default():\n",
    "    output = out.eval()\n",
    "    ax.clear()\n",
    "    ax.imshow(np.reshape(output, (28,28)), cmap='gray')\n",
    "    display(fig)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### The Goal is to Generate a Variety of New Output From a Variety of New Inputs\n",
    "\n",
    "... Where the Class/Category is Common (i.e., all 8s or Cats)\n",
    "\n",
    "Some considerations:\n",
    "\n",
    "-   Is \"generative content\" something new? Or something true?\n",
    "    -   In a Platonic sense, maybe, but in reality it's literally a\n",
    "        probabilistic guess based on the training data!\n",
    "    -   E.g., law enforcement photo enhancment\n",
    "-   How do we train?\n",
    "    -   If we score directly against the training data (like in the\n",
    "        autoencoder), the network will be very conservative, generating\n",
    "        only examples that it has seen.\n",
    "    -   In extreme cases, it will always generate a single (or small\n",
    "        number) of examples, since those score well. This is known as\n",
    "        **mode collapse**, since the network learns to locate the modes\n",
    "        in the input distribution.\n",
    "\n",
    "### Two principal approaches / architectures (2015-)\n",
    "\n",
    "**Generative Adversarial Networks (GAN)** and **Variational Autoencoders\n",
    "(VAE)**\n",
    "\n",
    "Variational Autoencoder (VAE)\n",
    "-----------------------------\n",
    "\n",
    "Our autoencoder was able to generate images, but the problem was that\n",
    "arbitrary input vectors don't map to anything meaningful. As discussed,\n",
    "this is partly by design -- the training of the VAE is for effectively\n",
    "for compressing a specific input dataset.\n",
    "\n",
    "What we would like, is that if we start with a valid input vector and\n",
    "move a bit on some direction, we get a plausible output that is also\n",
    "changed in some way.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "> **ASIDE: Manifold Hypothesis**\n",
    "\n",
    "> The manifold hypothesis is that the interesting, relevant, or critical\n",
    "> subspaces in the space of all vector inputs are actually low(er)\n",
    "> dimensional manifolds. A manifold is a space where each point has a\n",
    "> neighborhood that behaves like (is homeomorphic to) ${\\\\Bbb R^n}$. So\n",
    "> we would like to be able to move a small amount and have only a small\n",
    "> amount of change, not a sudden discontinuous change.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "The key feature of Variational Autoencoders is that we add a constraint\n",
    "on the encoded representation of our data: namely, that it follows a\n",
    "Gaussian distribution. Since the Gaussian is determined by its mean and\n",
    "variance (or standard deviation), we can model it as a k-variate\n",
    "Gaussian with these two parameters (${\\\\mu}$ and ${\\\\sigma}$) for each\n",
    "value of k.\n",
    "\n",
    "&lt;img src=\"http://i.imgur.com/OFLDweH.jpg\" width=600&gt; &lt;div\n",
    "style=\"text-align: right\"&gt;&lt;sup&gt;(credit to Miram Shiffman,\n",
    "http://blog.fastforwardlabs.com/2016/08/22/under-the-hood-of-the-variational-autoencoder-in.html)&lt;/sup&gt;&lt;/div&gt;\n",
    "\n",
    "&lt;img src=\"http://i.imgur.com/LbvJI5q.jpg\"&gt; &lt;div\n",
    "style=\"text-align: right\"&gt;&lt;sup&gt;(credit to Kevin Franz,\n",
    "http://kvfrans.com/variational-autoencoders-explained/)&lt;/sup&gt;&lt;/div&gt;\n",
    "\n",
    "One challenge is how to balance accurate reproduction of the input\n",
    "(traditional autoencoder loss) with the requirement that we match a\n",
    "Gaussian distribution. We can force the network to optimize both of\n",
    "these goals by creating a custom error function that sums up two\n",
    "components:\n",
    "\n",
    "-   How well we match the input, calculated as binary crossentropy or\n",
    "    MSE loss\n",
    "-   How well we match a Gaussian, calculated as KL divergence from the\n",
    "    Gaussian distribution\n",
    "\n",
    "We can easily implement a custom loss function and pass it as a\n",
    "parameter to the optimizer in Keras.\n",
    "\n",
    "The Keras source examples folder contains an elegant simple\n",
    "implementation, which we'll discuss below. It's a little more complex\n",
    "than the code we've seen so far, but we'll clarify the innovations:\n",
    "\n",
    "-   Custom loss functions that combined KL divergence and cross-entropy\n",
    "    loss\n",
    "-   Custom \"Lambda\" layer that provides the sampling from the encoded\n",
    "    distribution\n",
    "\n",
    "Overall it's probably simpler than you might expect. Let's start it\n",
    "(since it takes a few minutes to train) and discuss the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist\n",
    "import sklearn.datasets\n",
    "\n",
    "batch_size = 100\n",
    "original_dim = 784\n",
    "latent_dim = 2\n",
    "intermediate_dim = 256\n",
    "nb_epoch = 50\n",
    "epsilon_std = 1.0\n",
    "\n",
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = original_dim * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae = Model(x, x_decoded_mean)\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "\n",
    "train_libsvm = \"/dbfs/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt\"\n",
    "test_libsvm = \"/dbfs/databricks-datasets/mnist-digits/data-001/mnist-digits-test.txt\"\n",
    "\n",
    "x_train, y_train = sklearn.datasets.load_svmlight_file(train_libsvm, n_features=784)\n",
    "x_train = x_train.toarray()\n",
    "x_test, y_test = sklearn.datasets.load_svmlight_file(test_libsvm, n_features=784)\n",
    "x_test = x_test.toarray()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "vae.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        epochs=nb_epoch,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test), verbose=2)\n",
    "\n",
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Train on 60000 samples, validate on 10000 samples\n",
    ">     Epoch 1/50\n",
    ">      - 7s - loss: 190.9768 - val_loss: 173.1796\n",
    ">     Epoch 2/50\n",
    ">      - 7s - loss: 170.5004 - val_loss: 168.3274\n",
    ">     Epoch 3/50\n",
    ">      - 7s - loss: 166.9483 - val_loss: 165.7523\n",
    ">     Epoch 4/50\n",
    ">      - 7s - loss: 164.8370 - val_loss: 164.2769\n",
    ">     Epoch 5/50\n",
    ">      - 7s - loss: 163.3543 - val_loss: 162.8434\n",
    ">     Epoch 6/50\n",
    ">      - 7s - loss: 162.1008 - val_loss: 161.5877\n",
    ">     Epoch 7/50\n",
    ">      - 7s - loss: 161.0557 - val_loss: 160.8535\n",
    ">     Epoch 8/50\n",
    ">      - 7s - loss: 160.0666 - val_loss: 159.8321\n",
    ">     Epoch 9/50\n",
    ">      - 7s - loss: 159.1418 - val_loss: 159.0197\n",
    ">     Epoch 10/50\n",
    ">      - 7s - loss: 158.3370 - val_loss: 158.1361\n",
    ">     Epoch 11/50\n",
    ">      - 7s - loss: 157.6069 - val_loss: 157.9401\n",
    ">     Epoch 12/50\n",
    ">      - 7s - loss: 156.9792 - val_loss: 156.8949\n",
    ">     Epoch 13/50\n",
    ">      - 7s - loss: 156.4237 - val_loss: 156.6119\n",
    ">     Epoch 14/50\n",
    ">      - 7s - loss: 155.9121 - val_loss: 156.3393\n",
    ">     Epoch 15/50\n",
    ">      - 7s - loss: 155.4880 - val_loss: 156.1204\n",
    ">     Epoch 16/50\n",
    ">      - 7s - loss: 155.0392 - val_loss: 156.1275\n",
    ">     Epoch 17/50\n",
    ">      - 7s - loss: 154.6869 - val_loss: 155.0182\n",
    ">     Epoch 18/50\n",
    ">      - 7s - loss: 154.3163 - val_loss: 154.7265\n",
    ">     Epoch 19/50\n",
    ">      - 7s - loss: 154.0314 - val_loss: 154.6318\n",
    ">     Epoch 20/50\n",
    ">      - 7s - loss: 153.7218 - val_loss: 154.3242\n",
    ">     Epoch 21/50\n",
    ">      - 7s - loss: 153.4679 - val_loss: 154.4581\n",
    ">     Epoch 22/50\n",
    ">      - 7s - loss: 153.1870 - val_loss: 153.9840\n",
    ">     Epoch 23/50\n",
    ">      - 7s - loss: 152.9839 - val_loss: 153.8052\n",
    ">     Epoch 24/50\n",
    ">      - 7s - loss: 152.7605 - val_loss: 153.9252\n",
    ">     Epoch 25/50\n",
    ">      - 7s - loss: 152.5473 - val_loss: 153.7088\n",
    ">     Epoch 26/50\n",
    ">      - 7s - loss: 152.3364 - val_loss: 153.6085\n",
    ">     Epoch 27/50\n",
    ">      - 7s - loss: 152.1534 - val_loss: 153.2579\n",
    ">     Epoch 28/50\n",
    ">      - 7s - loss: 152.0203 - val_loss: 153.1721\n",
    ">     Epoch 29/50\n",
    ">      - 7s - loss: 151.8157 - val_loss: 153.1620\n",
    ">     Epoch 30/50\n",
    ">      - 7s - loss: 151.6618 - val_loss: 153.1152\n",
    ">     Epoch 31/50\n",
    ">      - 7s - loss: 151.5067 - val_loss: 152.8042\n",
    ">     Epoch 32/50\n",
    ">      - 7s - loss: 151.3810 - val_loss: 152.8947\n",
    ">     Epoch 33/50\n",
    ">      - 7s - loss: 151.2489 - val_loss: 152.5349\n",
    ">     Epoch 34/50\n",
    ">      - 7s - loss: 151.0784 - val_loss: 152.9623\n",
    ">     Epoch 35/50\n",
    ">      - 7s - loss: 150.9885 - val_loss: 152.5544\n",
    ">     Epoch 36/50\n",
    ">      - 7s - loss: 150.8500 - val_loss: 152.6461\n",
    ">     Epoch 37/50\n",
    ">      - 7s - loss: 150.7122 - val_loss: 152.0259\n",
    ">     Epoch 38/50\n",
    ">      - 7s - loss: 150.6166 - val_loss: 152.8115\n",
    ">     Epoch 39/50\n",
    ">      - 7s - loss: 150.4893 - val_loss: 152.0757\n",
    ">     Epoch 40/50\n",
    ">      - 7s - loss: 150.4013 - val_loss: 152.3985\n",
    ">     Epoch 41/50\n",
    ">      - 7s - loss: 150.3099 - val_loss: 151.8574\n",
    ">     Epoch 42/50\n",
    ">      - 7s - loss: 150.1686 - val_loss: 151.9436\n",
    ">     Epoch 43/50\n",
    ">      - 7s - loss: 150.1139 - val_loss: 152.1689\n",
    ">     Epoch 44/50\n",
    ">      - 7s - loss: 150.0130 - val_loss: 151.7565\n",
    ">     Epoch 45/50\n",
    ">      - 7s - loss: 149.8982 - val_loss: 151.7926\n",
    ">     Epoch 46/50\n",
    ">      - 7s - loss: 149.8157 - val_loss: 152.5258\n",
    ">     Epoch 47/50\n",
    ">      - 7s - loss: 149.7415 - val_loss: 151.6356\n",
    ">     Epoch 48/50\n",
    ">      - 7s - loss: 149.6592 - val_loss: 151.9871\n",
    ">     Epoch 49/50\n",
    ">      - 7s - loss: 149.5619 - val_loss: 151.6842\n",
    ">     Epoch 50/50\n",
    ">      - 7s - loss: 149.4900 - val_loss: 151.6131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((8,7))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian\n",
    "# to produce values of the latent variables z, since the prior of the latent space is Gaussian\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = generator.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((7,7))\n",
    "ax.imshow(figure, cmap='Greys_r')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Note that it is blurry, and \"manipulable\" by moving through the latent\n",
    "space!\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "> It is *not* intuitively obvious where the calculation of the KL\n",
    "> divergence comes from, and in general there is not a simple analytic\n",
    "> way to derive KL divergence for arbitrary distributions. Because we\n",
    "> have assumptions about Gaussians here, this is a special case -- the\n",
    "> derivation is included in the Auto-Encoding Variational Bayes paper\n",
    "> (2014; https://arxiv.org/pdf/1312.6114.pdf)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "Generative Adversarial Network (GAN)\n",
    "------------------------------------\n",
    "\n",
    "The GAN, popularized recently by Ian Goodfellow's work, consists of\n",
    "**two networks**:\n",
    "\n",
    "1.  Generator network (that initially generates output from noise)\n",
    "2.  Discriminator network (trained with real data, to simply distinguish\n",
    "    2 class: real and fake)\n",
    "    -   The discriminator is also sometimes called the \"A\" or\n",
    "        adversarial network\n",
    "\n",
    "The basic procedure for building a GAN is to train both neworks in\n",
    "tandem according to the following simple procedure:\n",
    "\n",
    "1.  Generate bogus output from \"G\"\n",
    "2.  Train \"D\" with real and bogus data, labeled properly\n",
    "3.  Train \"G\" to target the \"real/true/1\" label by\n",
    "    -   taking the \"stacked\" G + D model\n",
    "    -   feeding noise in at the start (G) end\n",
    "    -   and backpropagating from the real/true/1 distribution at the\n",
    "        output (D) end\n",
    "\n",
    "As always, there are lots of variants! But this is the core idea, as\n",
    "illustrated in the following code.\n",
    "\n",
    "Zackory Erickson's example is so elegant and clear, I've used included\n",
    "it from https://github.com/Zackory/Keras-MNIST-GAN\n",
    "\n",
    "Once again, we'll start it running first, since it takes a while to\n",
    "train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Reshape, Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Convolution2D, UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1, l1_l2\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "import sklearn.datasets\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "# Deterministic output.\n",
    "# Tired of seeing the same results every time? Remove the line below.\n",
    "np.random.seed(1000)\n",
    "\n",
    "# The results are a little better when the dimensionality of the random vector is only 10.\n",
    "# The dimensionality has been left at 100 for consistency with other GAN implementations.\n",
    "randomDim = 100\n",
    "\n",
    "train_libsvm = \"/dbfs/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt\"\n",
    "test_libsvm = \"/dbfs/databricks-datasets/mnist-digits/data-001/mnist-digits-test.txt\"\n",
    "\n",
    "X_train, y_train = sklearn.datasets.load_svmlight_file(train_libsvm, n_features=784)\n",
    "X_train = X_train.toarray()\n",
    "X_test, y_test = sklearn.datasets.load_svmlight_file(test_libsvm, n_features=784)\n",
    "X_test = X_test.toarray()\n",
    "\n",
    "X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "\n",
    "# Function for initializing network weights\n",
    "def initNormal():\n",
    "    return initializers.normal(stddev=0.02)\n",
    "\n",
    "# Optimizer\n",
    "adam = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "generator = Sequential()\n",
    "generator.add(Dense(256, input_dim=randomDim, kernel_initializer=initializers.normal(stddev=0.02)))\n",
    "generator.add(LeakyReLU(0.2))\n",
    "generator.add(Dense(512))\n",
    "generator.add(LeakyReLU(0.2))\n",
    "generator.add(Dense(1024))\n",
    "generator.add(LeakyReLU(0.2))\n",
    "generator.add(Dense(784, activation='tanh'))\n",
    "generator.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Dense(1024, input_dim=784, kernel_initializer=initializers.normal(stddev=0.02)))\n",
    "discriminator.add(LeakyReLU(0.2))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Dense(512))\n",
    "discriminator.add(LeakyReLU(0.2))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Dense(256))\n",
    "discriminator.add(LeakyReLU(0.2))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "\n",
    "# Combined network\n",
    "discriminator.trainable = False\n",
    "ganInput = Input(shape=(randomDim,))\n",
    "x = generator(ganInput)\n",
    "ganOutput = discriminator(x)\n",
    "gan = Model(inputs=ganInput, outputs=ganOutput)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "\n",
    "dLosses = []\n",
    "gLosses = []\n",
    "\n",
    "# Plot the loss from each batch\n",
    "def plotLoss(epoch):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(dLosses, label='Discriminitive loss')\n",
    "    plt.plot(gLosses, label='Generative loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('/dbfs/FileStore/gan_loss_epoch_%d.png' % epoch)\n",
    "\n",
    "# Create a wall of generated MNIST images\n",
    "def plotGeneratedImages(epoch, examples=100, dim=(10, 10), figsize=(10, 10)):\n",
    "    noise = np.random.normal(0, 1, size=[examples, randomDim])\n",
    "    generatedImages = generator.predict(noise)\n",
    "    generatedImages = generatedImages.reshape(examples, 28, 28)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generatedImages.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generatedImages[i], interpolation='nearest', cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/dbfs/FileStore/gan_generated_image_epoch_%d.png' % epoch)\n",
    "\n",
    "# Save the generator and discriminator networks (and weights) for later use\n",
    "def saveModels(epoch):\n",
    "    generator.save('/tmp/gan_generator_epoch_%d.h5' % epoch)\n",
    "    discriminator.save('/tmp/gan_discriminator_epoch_%d.h5' % epoch)\n",
    "\n",
    "def train(epochs=1, batchSize=128):\n",
    "    batchCount = X_train.shape[0] // batchSize\n",
    "    print('Epochs:', epochs)\n",
    "    print('Batch size:', batchSize)\n",
    "    print('Batches per epoch:', batchCount)\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        print('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "        for _ in range(batchCount):\n",
    "            # Get a random set of input noise and images\n",
    "            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n",
    "            imageBatch = X_train[np.random.randint(0, X_train.shape[0], size=batchSize)]\n",
    "\n",
    "            # Generate fake MNIST images\n",
    "            generatedImages = generator.predict(noise)\n",
    "            # print np.shape(imageBatch), np.shape(generatedImages)\n",
    "            X = np.concatenate([imageBatch, generatedImages])\n",
    "\n",
    "            # Labels for generated and real data\n",
    "            yDis = np.zeros(2*batchSize)\n",
    "            # One-sided label smoothing\n",
    "            yDis[:batchSize] = 0.9\n",
    "\n",
    "            # Train discriminator\n",
    "            discriminator.trainable = True\n",
    "            dloss = discriminator.train_on_batch(X, yDis)\n",
    "\n",
    "            # Train generator\n",
    "            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n",
    "            yGen = np.ones(batchSize)\n",
    "            discriminator.trainable = False\n",
    "            gloss = gan.train_on_batch(noise, yGen)\n",
    "\n",
    "        # Store loss of most recent batch from this epoch\n",
    "        dLosses.append(dloss)\n",
    "        gLosses.append(gloss)\n",
    "\n",
    "        if e == 1 or e % 10 == 0:\n",
    "            plotGeneratedImages(e)\n",
    "            saveModels(e)\n",
    "\n",
    "    # Plot losses from every epoch\n",
    "    plotLoss(e)\n",
    "\n",
    "train(10, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     WARNING:tensorflow:From /databricks/python/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
    ">     Instructions for updating:\n",
    ">     Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
    ">     ('Epochs:', 10)\n",
    ">     ('Batch size:', 128)\n",
    ">     ('Batches per epoch:', 468)\n",
    ">     ('---------------', 'Epoch 1', '---------------')\n",
    ">     ('---------------', 'Epoch 2', '---------------')\n",
    ">     ('---------------', 'Epoch 3', '---------------')\n",
    ">     ('---------------', 'Epoch 4', '---------------')\n",
    ">     ('---------------', 'Epoch 5', '---------------')\n",
    ">     ('---------------', 'Epoch 6', '---------------')\n",
    ">     ('---------------', 'Epoch 7', '---------------')\n",
    ">     ('---------------', 'Epoch 8', '---------------')\n",
    ">     ('---------------', 'Epoch 9', '---------------')\n",
    ">     ('---------------', 'Epoch 10', '---------------')\n",
    "\n",
    "  \n",
    "\n",
    "### Sample generated digits: epoch 1\n",
    "\n",
    "&lt;img src=\"/files/gan*generated*image*epoch*1.png\" width=800&gt;\n",
    "\n",
    "### Sample generated digits: epoch 10\n",
    "\n",
    "&lt;img src=\"/files/gan*generated*image*epoch*10.png\" width=800&gt;\n",
    "\n",
    "### Generator/Discriminator Loss\n",
    "\n",
    "Which Strategy to Use?\n",
    "----------------------\n",
    "\n",
    "This is definitely an area of active research, so you'll want to\n",
    "experiment with both of these approaches.\n",
    "\n",
    "GANs typically produce \"sharper pictures\" -- the adversarial loss is\n",
    "better than the combined MSE/XE + KL loss used in VAEs, but then again,\n",
    "that's partly by design.\n",
    "\n",
    "VAEs are -- as seen above -- blurrier but more manipulable. One way of\n",
    "thinking about the multivariate Gaussian representation is that VAEs are\n",
    "trained to find some \"meaning\" in variation along each dimensin. And, in\n",
    "fact, with specific training it is possible to get them to associate\n",
    "specific meanings like color, translation, rotation, etc. to those\n",
    "dimensions.\n",
    "\n",
    "Where Next?\n",
    "===========\n",
    "\n",
    "Keep an eye on Medium articles and others at sites like\n",
    "towardsdatascience:\n",
    "\n",
    "Here is one which caught my eve in 2019:\n",
    "\n",
    "-   [AI for\n",
    "    trading](https://towardsdatascience.com/aifortrading-2edd6fac689d)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
