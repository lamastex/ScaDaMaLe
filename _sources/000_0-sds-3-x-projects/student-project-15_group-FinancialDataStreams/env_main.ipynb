{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning for Financial Data Series\n",
    "================================================\n",
    "\n",
    "In this project, our aim is to implement a Reinforcement Learning (RL)\n",
    "strategy for trading stocks. Specifically, we use the DQN -- Deep\n",
    "Q-Network -- algorithm to train an agent which trades Brent Crude Oil\n",
    "(BCOUSD) stocks, in order to maximize long term profit. Finally, we\n",
    "compare the results from the RL agent with a rule-based agent that uses\n",
    "the Trend Calculus predictive algorithm to make decisions.\n",
    "\n",
    "Group members:\n",
    "--------------\n",
    "\n",
    "-   Fabian Sinzinger\n",
    "-   Karl Bäckström\n",
    "-   Rita Laezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Scala imports\n",
    "import org.lamastex.spark.trendcalculus._\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import java.sql.Timestamp\n",
    "import org.apache.spark.sql.expressions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.lamastex.spark.trendcalculus._\n",
    ">     import spark.implicits._\n",
    ">     import org.apache.spark.sql._\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     import java.sql.Timestamp\n",
    ">     import org.apache.spark.sql.expressions._\n",
    "\n",
    "  \n",
    "\n",
    "Brent Crude Oil Dataset\n",
    "-----------------------\n",
    "\n",
    "The dataset consists of historical data starting from the *14th of\n",
    "October 2010* to the *21st of June 2019*. Since the data in the first\n",
    "and last day is incomplete, we remove it from the dataset. The BCUSD\n",
    "data is sampled approximatly every minute with a specific timestamp and\n",
    "registered in US dollars.\n",
    "\n",
    "To read the BCUSD dataset, we use the same parsers provided by the\n",
    "[TrendCalculus](https://github.com/lamastex/spark-trend-calculus)\n",
    "library. This allows us to load the FX data into a Spark Dataset. The\n",
    "original dataset contains **TickerPoint** objects which are made up of a\n",
    "**ticker**, a **time** and a **close** value. The first consists of the\n",
    "name of the stock, the second is the timestamp of the data point and the\n",
    "latter consists of the value of the stock at the end of each 1 minute\n",
    "bin.\n",
    "\n",
    "Finally we add the **index** and the \\*\\*diff\\_close\\*\\* columns. The\n",
    "latter consists of the relative difference between the **close** value\n",
    "at the current and the previous **time**. Note hat since **ticker** is\n",
    "always the same, we remove that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Load dataset\n",
    "val oilDS = spark.read.fx1m(\"dbfs:/FileStore/shared_uploads/fabiansi@kth.se/*csv.gz\").toDF.withColumn(\"ticker\", lit(\"BCOUSD\")).select($\"ticker\", $\"time\" as \"x\", $\"close\" as \"y\").as[TickerPoint].orderBy(\"time\")\n",
    "\n",
    "// Add column with difference from previous close value (expected 'x', 'y' column names)\n",
    "val windowSpec = Window.orderBy(\"x\")\n",
    "val oilDS1 = oilDS \n",
    ".withColumn(\"diff_close\", $\"y\" - when((lag(\"y\", 1).over(windowSpec)).isNull, 0).otherwise(lag(\"y\", 1).over(windowSpec)))\n",
    "\n",
    "// Rename variables\n",
    "val oilDS2 = oilDS1.withColumnRenamed(\"x\",\"time\").withColumnRenamed(\"y\",\"close\")\n",
    "\n",
    "// Remove incomplete data from first day (2010-11-14) and last day (2019-06-21)\n",
    "val oilDS3 = oilDS2.filter(to_date(oilDS2(\"time\")) >= lit(\"2010-11-15\") && to_date(oilDS2(\"time\")) <= lit(\"2019-06-20\"))\n",
    "\n",
    "// Add index column\n",
    "val windowSpec1 = Window.orderBy(\"time\")\n",
    "val oilDS4 = oilDS3\n",
    ".withColumn(\"index\", row_number().over(windowSpec1))\n",
    "\n",
    "// Drop ticker column\n",
    "val oilDS5 = oilDS4.drop(\"ticker\")\n",
    "\n",
    "// Store loaded data as temp view, to be accessible in Python\n",
    "oilDS5.createOrReplaceTempView(\"temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     oilDS: org.apache.spark.sql.Dataset[org.lamastex.spark.trendcalculus.TickerPoint] = [ticker: string, x: timestamp ... 1 more field]\n",
    ">     windowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@5d67736f\n",
    ">     oilDS1: org.apache.spark.sql.DataFrame = [ticker: string, x: timestamp ... 2 more fields]\n",
    ">     oilDS2: org.apache.spark.sql.DataFrame = [ticker: string, time: timestamp ... 2 more fields]\n",
    ">     oilDS3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ticker: string, time: timestamp ... 2 more fields]\n",
    ">     windowSpec1: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@63b5e6bd\n",
    ">     oilDS4: org.apache.spark.sql.DataFrame = [ticker: string, time: timestamp ... 3 more fields]\n",
    ">     oilDS5: org.apache.spark.sql.DataFrame = [time: timestamp, close: double ... 2 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "### Preparing the data in Python\n",
    "\n",
    "Because the\n",
    "[TrendCalculus](https://github.com/lamastex/spark-trend-calculus)\n",
    "library we use is implemented in Scala and we want to do our\n",
    "implementation in Python, we have to make sure that the data loaded in\n",
    "Scala is correctly read in Python, before moving on. To that end, we\n",
    "select the first 10 datapoints and show them in a table.\n",
    "\n",
    "We can see that there are roughly **2.5 million data points** in the\n",
    "BCUSD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Create Dataframe from temp data\n",
    "oilDF_py = spark.table(\"temp\")\n",
    "\n",
    "# Select the 10 first Rows of data and print them\n",
    "ten_oilDF_py = oilDF_py.limit(10)\n",
    "ten_oilDF_py.show()\n",
    "\n",
    "# Check number of data points\n",
    "last_index = oilDF_py.count()\n",
    "print(\"Number of data points: {}\".format(last_index))\n",
    "\n",
    "# Select the date of the last data point\n",
    "print(\"Last data point: {}\".format(np.array(oilDF_py.where(oilDF_py.index == last_index).select('time').collect()).item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +-------------------+-----+--------------------+-----+\n",
    ">     |               time|close|          diff_close|index|\n",
    ">     +-------------------+-----+--------------------+-----+\n",
    ">     |2010-11-15 00:00:00| 86.6|-0.01000000000000...|    1|\n",
    ">     |2010-11-15 00:01:00| 86.6|                 0.0|    2|\n",
    ">     |2010-11-15 00:02:00|86.63|0.030000000000001137|    3|\n",
    ">     |2010-11-15 00:03:00|86.61|-0.01999999999999602|    4|\n",
    ">     |2010-11-15 00:05:00|86.61|                 0.0|    5|\n",
    ">     |2010-11-15 00:07:00| 86.6|-0.01000000000000...|    6|\n",
    ">     |2010-11-15 00:08:00|86.58|-0.01999999999999602|    7|\n",
    ">     |2010-11-15 00:09:00|86.58|                 0.0|    8|\n",
    ">     |2010-11-15 00:10:00|86.58|                 0.0|    9|\n",
    ">     |2010-11-15 00:12:00|86.57|-0.01000000000000...|   10|\n",
    ">     +-------------------+-----+--------------------+-----+\n",
    ">\n",
    ">     Number of data points: 2523078\n",
    ">     Last data point: 2019-06-20 23:59:00\n",
    "\n",
    "  \n",
    "\n",
    "Creating the Environment\n",
    "------------------------\n",
    "\n",
    "In order to train RL agents, we first need to create the environment\n",
    "with which the agent will interact to gather experience. In our case,\n",
    "that consist of a stock market simulation which plays out historical\n",
    "data from the BCUSD dataset. This is valid, under the assumption that\n",
    "the trading on the part of our agent has no affect on the stock market.\n",
    "An RL problem can be formally defined by a Markov Decision Process\n",
    "(MDP).\n",
    "\n",
    "For our application, we have the following MDP: - s = (HOLDING, NOT\n",
    "HOLDING) - a = (LONG, SHORT) - r ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "PENALTY = 1  # 0.999756079\n",
    "\n",
    "\n",
    "class MarketEnv(gym.Env):\n",
    "    def __init__(self, full_data, start_date, end_date, episode_size=30*24*60, scope=60, cumulative_reward=False):\n",
    "\n",
    "        self.cumulative_reward = cumulative_reward\n",
    "        self.actions = [\"LONG\", \"SHORT\"]  # pass not required, if already owned, buy is hold\n",
    "        self.action_space = gym.spaces.Discrete(len(self.actions))\n",
    "        self.state = None\n",
    "\n",
    "        self.diff_close = np.array(full_data.filter(full_data[\"time\"] > start_date).filter(full_data[\"time\"] <= end_date).select('diff_close').collect())\n",
    "        self.close = np.array(full_data.filter(full_data[\"time\"] > start_date).filter(full_data[\"time\"] <= end_date).select('close').collect())\n",
    "        \n",
    "        self.num_ticks_train = np.shape(self.diff_close)[0]\n",
    "        self.episode_size = episode_size\n",
    "\n",
    "        self.scope = scope\n",
    "        self.time_index = self.scope  # start 60 steps in, to ensure that we have hist. values ?\n",
    "        self.episode_init_time = self.time_index  # initial time index of the episode\n",
    "        #self.reset() this should always be called after initilization, any way\n",
    "\n",
    "    def step(self, action):\n",
    "        self.reward = 0\n",
    "        if self.actions[action] == \"LONG\":\n",
    "            if sum(self.boughts) < 0:\n",
    "                for b in self.boughts:\n",
    "                    self.reward += -(b + 1) \n",
    "                if self.cumulative_reward:\n",
    "                    self.reward = self.reward / max(1, len(self.boughts))\n",
    "                self.boughts = []\n",
    "            self.boughts.append(1.0)\n",
    "        elif self.actions[action] == \"SHORT\":\n",
    "            if sum(self.boughts) > 0:\n",
    "                for b in self.boughts:\n",
    "                    self.reward += b - 1\n",
    "                if self.cumulative_reward:\n",
    "                    self.reward = self.reward / max(1, len(self.boughts))\n",
    "                self.boughts = []\n",
    "            self.boughts.append(-1.0)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        vari = self.time_range[self.time_index]\n",
    "\n",
    "        for i in range(len(self.boughts)):\n",
    "            self.boughts[i] = self.boughts[i] * PENALTY * (1 + vari * (-1 if sum(self.boughts) < 0 else 1))\n",
    "\n",
    "        self.define_state()\n",
    "        self.time_index += 1\n",
    "        \n",
    "        # Check if done\n",
    "        if self.time_index - self.episode_init_time > self.episode_size:\n",
    "            self.done = True\n",
    "        if self.time_index > self.diff_close.shape[0] - self.scope:\n",
    "            self.done = True\n",
    "            \n",
    "        if self.done:\n",
    "            for b in self.boughts:\n",
    "                self.reward += (b * (1 if sum(self.boughts) > 0 else -1)) - 1\n",
    "            if self.cumulative_reward:\n",
    "                self.reward = self.reward / max(1, len(self.boughts))\n",
    "\n",
    "            self.boughts = []\n",
    "\n",
    "        return self.state, self.reward, self.done, {'index':'close': int(self.time_index), 'close': float(self.close[self.time_index]), 'boughts': self.boughts}\n",
    "\n",
    "    def reset(self, random_starttime=True):\n",
    "        self.boughts = []\n",
    "        self.done = False\n",
    "        self.reward = 0.\n",
    "        self.time_index = self.scope \n",
    "        self.define_state()\n",
    "        \n",
    "        if random_starttime:\n",
    "            self.time_index += random.randint(0, self.num_ticks_train - self.scope)\n",
    "        \n",
    "        self.episode_init_time = self.time_index\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def define_state(self):\n",
    "        tmp_state = []\n",
    "\n",
    "        budget = (sum(self.boughts) / len(self.boughts)) if len(self.boughts) > 0 else 1.\n",
    "        size = math.log(max(1., len(self.boughts)), 100)\n",
    "        position = 1. if sum(self.boughts) > 0 else 0.\n",
    "        tmp_state.append([[budget, size, position]])\n",
    "\n",
    "        # df_back = self.diff_close.filter(self.diff_close.index.between(self.time_index, self.time_index + self.scope - 1))\n",
    "        np_back = self.diff_close[self.time_index - self.scope:self.time_index]  # verify that we dont provide the actual value that we want to predict here\n",
    "        # TODO check if we go out of range\n",
    "\n",
    "        # np_back = np.array(df_back.select('diff_close').collect())\n",
    "\n",
    "        tmp_state.append(np_back)\n",
    "\n",
    "        tmp_state = [np.array(i) for i in tmp_state]\n",
    "        self.state = tmp_state\n",
    "\n",
    "    def seed(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime(2010, 11, 14, 20, 19)\n",
    "end = datetime.datetime(2012, 11, 14, 20, 32)\n",
    "\n",
    "env = MarketEnv(oilDF_py, start, end)\n",
    "\n",
    "print(env.step(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     ([array([[ 0.99,  0.  ,  1.  ]]), array([[-0.09],\n",
    ">            [ 0.05],\n",
    ">            [-0.03],\n",
    ">            [-0.01],\n",
    ">            [-0.06],\n",
    ">            [-0.02],\n",
    ">            [ 0.15],\n",
    ">            [ 0.01],\n",
    ">            [ 0.06],\n",
    ">            [-0.05],\n",
    ">            [-0.09],\n",
    ">            [ 0.04],\n",
    ">            [-0.02],\n",
    ">            [ 0.02],\n",
    ">            [ 0.03],\n",
    ">            [ 0.08],\n",
    ">            [ 0.04],\n",
    ">            [ 0.01],\n",
    ">            [ 0.  ],\n",
    ">            [-0.02],\n",
    ">            [-0.05],\n",
    ">            [ 0.03],\n",
    ">            [ 0.04],\n",
    ">            [ 0.03],\n",
    ">            [ 0.  ],\n",
    ">            [ 0.01],\n",
    ">            [-0.04],\n",
    ">            [-0.06],\n",
    ">            [ 0.  ],\n",
    ">            [ 0.06],\n",
    ">            [ 0.04],\n",
    ">            [-0.01],\n",
    ">            [ 0.  ],\n",
    ">            [ 0.06],\n",
    ">            [ 0.  ],\n",
    ">            [ 0.02],\n",
    ">            [ 0.  ],\n",
    ">            [-0.03],\n",
    ">            [-0.1 ],\n",
    ">            [ 0.07],\n",
    ">            [-0.08],\n",
    ">            [ 0.04],\n",
    ">            [-0.03],\n",
    ">            [ 0.03],\n",
    ">            [ 0.07],\n",
    ">            [ 0.06],\n",
    ">            [ 0.02],\n",
    ">            [ 0.01],\n",
    ">            [-0.01],\n",
    ">            [ 0.02],\n",
    ">            [-0.01],\n",
    ">            [-0.01],\n",
    ">            [ 0.  ],\n",
    ">            [ 0.04],\n",
    ">            [-0.02],\n",
    ">            [-0.03],\n",
    ">            [-0.01],\n",
    ">            [ 0.  ],\n",
    ">            [-0.01],\n",
    ">            [ 0.21]])], 0.0, False, {'close': -0.020000000000010232, 'boughts': [0.9900000000000091]})\n",
    "\n",
    "  \n",
    "\n",
    "DQN Algorithm\n",
    "-------------\n",
    "\n",
    "&lt;img src=\"https://imgur.com/mvopoh8.png\" width=800&gt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, done):\n",
    "        # memory[i] = [[state_t, action_t, reward_t, state_t+1], done?]\n",
    "        self.memory.append([states, done])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = model.output_shape[-1]\n",
    "\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim, 1))\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory, size=inputs.shape[0])):\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            done = self.memory[idx][1]\n",
    "\n",
    "            inputs[i:i + 1] = state_t\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = model.predict(state_t)[0]\n",
    "            Q_sa = np.max(model.predict(state_tp1)[0])\n",
    "            if done: # if done is True\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                # reward_t + gamma * max_a' Q(s', a')\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the model used in https://github.com/kh-kim/stock_market_reinforcement_learning \n",
    "from keras.models import Model\n",
    "from keras.layers import merge, Convolution2D, MaxPooling2D, Input, Dense, Flatten, Dropout, Reshape, TimeDistributed, BatchNormalization, Merge, merge\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "def build_model(self):\n",
    "    dr_rate = 0.0\n",
    "\n",
    "    B = Input(shape = (3,))\n",
    "    b = Dense(5, activation = \"relu\")(B)\n",
    "\n",
    "    inputs = [B]\n",
    "    merges = [b]\n",
    "\n",
    "    for i in xrange(1):\n",
    "        S = Input(shape=[2, 60, 1])\n",
    "        inputs.append(S)\n",
    "\n",
    "        h = Convolution2D(64, 3, 1, border_mode = 'valid')(S)\n",
    "        h = LeakyReLU(0.001)(h)\n",
    "        h = Convolution2D(128, 5, 1, border_mode = 'valid')(S)\n",
    "        h = LeakyReLU(0.001)(h)\n",
    "        h = Convolution2D(256, 10, 1, border_mode = 'valid')(S)\n",
    "        h = LeakyReLU(0.001)(h)\n",
    "        h = Convolution2D(512, 20, 1, border_mode = 'valid')(S)\n",
    "        h = LeakyReLU(0.001)(h)\n",
    "        h = Convolution2D(1024, 40, 1, border_mode = 'valid')(S)\n",
    "        h = LeakyReLU(0.001)(h)\n",
    "\n",
    "        h = Flatten()(h)\n",
    "        h = Dense(2048)(h)\n",
    "        h = LeakyReLU(0.001)(h)\n",
    "        h = Dropout(dr_rate)(h)\n",
    "        merges.append(h)\n",
    "\n",
    "        h = Convolution2D(2048, 60, 1, border_mode = 'valid')(S)\n",
    "        h = LeakyReLU(0.001)(h)\n",
    "\n",
    "        h = Flatten()(h)\n",
    "        h = Dense(4096)(h)\n",
    "        h = LeakyReLU(0.001)(h)\n",
    "        h = Dropout(dr_rate)(h)\n",
    "        merges.append(h)\n",
    "\n",
    "    m = merge(merges, mode = 'concat', concat_axis = 1)\n",
    "    m = Dense(1024)(m)\n",
    "    m = LeakyReLU(0.001)(m)\n",
    "    m = Dropout(dr_rate)(m)\n",
    "    m = Dense(512)(m)\n",
    "    m = LeakyReLU(0.001)(m)\n",
    "    m = Dropout(dr_rate)(m)\n",
    "    m = Dense(256)(m)\n",
    "    m = LeakyReLU(0.001)(m)\n",
    "    m = Dropout(dr_rate)(m)\n",
    "    V = Dense(2, activation = 'linear', init = 'zero')(m)\n",
    "    model = Model(input = inputs, output = V)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Conv1D, MaxPool1D, Flatten, BatchNormalization\n",
    "import collections\n",
    "import datetime\n",
    "\n",
    "# RL parameters\n",
    "epsilon = .5  # exploration\n",
    "min_epsilon = 0.1\n",
    "max_memory = 5000\n",
    "batch_size = 128\n",
    "discount = 0.8\n",
    "\n",
    "# Environment parameters\n",
    "num_actions = 2  # [long, short]\n",
    "episodes = 1000 # 100000\n",
    "episode_size = 1 * 1 * 60  # roughly a hour worth of data in each training episode\n",
    "\n",
    "# Define state sequence scope (approx. 1 hour)\n",
    "sequence_scope = 60\n",
    "input_shape = (batch_size, sequence_scope, 1)\n",
    "\n",
    "# Create Q Network\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, (5), strides=2, input_shape=input_shape[1:], activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2, strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(32, (5), strides=1, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2, strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(hidden_size, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_actions))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Define training interval\n",
    "start = datetime.datetime(2010, 11, 15, 0, 0)\n",
    "end = datetime.datetime(2018, 12, 31, 23, 59)\n",
    "\n",
    "# Initialize Environment\n",
    "env = MarketEnv(oilDF_py, start, end, episode_size=episode_size, scope=sequence_scope)\n",
    "\n",
    "# Initialize experience replay object\n",
    "exp_replay = ExperienceReplay(max_memory=max_memory)\n",
    "\n",
    "# Train\n",
    "returns = []\n",
    "for e in range(1, episodes):\n",
    "    loss = 0.\n",
    "    counter = 0\n",
    "    reward_sum = 0.\n",
    "    done = False\n",
    "    \n",
    "    state = env.reset()\n",
    "    input_t = state[1].reshape(1, sequence_scope, 1)\n",
    "    \n",
    "    while not done:     \n",
    "        counter += 1\n",
    "        input_tm1 = input_t\n",
    "        # get next action\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(0, num_actions, size=1)\n",
    "        else:\n",
    "            q = model.predict(input_tm1)\n",
    "            action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        state, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        input_t = state[1].reshape(1, sequence_scope, 1)         \n",
    "\n",
    "        # store experience\n",
    "        exp_replay.remember([input_tm1, action, reward, input_t], done)\n",
    "\n",
    "        # adapt model\n",
    "        inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
    "\n",
    "        loss += model.train_on_batch(inputs, targets)\n",
    "        \n",
    "    returns.append(reward_sum)\n",
    "    print(\"Episode {:03d}/{:d} | Average Loss {:.4f} | Cumulative Reward {:.4f}\".format(e, episodes, loss / counter, reward_sum))\n",
    "    epsilon = max(min_epsilon, epsilon * 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Episode 001/400 | Average Loss 0.3689 | Cumulative Reward 0.1637\n",
    ">     Episode 002/400 | Average Loss 0.2221 | Cumulative Reward -0.0233\n",
    ">     Episode 003/400 | Average Loss 0.2839 | Cumulative Reward -0.3709\n",
    ">     Episode 004/400 | Average Loss 0.3601 | Cumulative Reward -1.2132\n",
    ">     Episode 005/400 | Average Loss 0.2441 | Cumulative Reward 0.8580\n",
    ">     Episode 006/400 | Average Loss 0.1887 | Cumulative Reward 0.3902\n",
    ">     Episode 007/400 | Average Loss 0.1640 | Cumulative Reward 1.4007\n",
    ">     Episode 008/400 | Average Loss 0.1131 | Cumulative Reward 0.6776\n",
    ">     Episode 009/400 | Average Loss 0.0888 | Cumulative Reward 0.6964\n",
    ">     Episode 010/400 | Average Loss 0.1072 | Cumulative Reward -0.1378\n",
    ">     Episode 011/400 | Average Loss 0.1203 | Cumulative Reward -0.4680\n",
    ">     Episode 012/400 | Average Loss 0.0950 | Cumulative Reward 0.1480\n",
    ">     Episode 013/400 | Average Loss 0.0773 | Cumulative Reward -0.8090\n",
    ">     Episode 014/400 | Average Loss 0.1827 | Cumulative Reward -0.2950\n",
    ">     Episode 015/400 | Average Loss 227231.1264 | Cumulative Reward -1.7342\n",
    ">     Episode 016/400 | Average Loss 898787.8186 | Cumulative Reward 0.8652\n",
    ">     Episode 017/400 | Average Loss 309887.6135 | Cumulative Reward 0.5920\n",
    ">     Episode 018/400 | Average Loss 42025.0600 | Cumulative Reward 0.7633\n",
    ">     Episode 019/400 | Average Loss 23240.6485 | Cumulative Reward -0.6439\n",
    ">     Episode 020/400 | Average Loss 1548.2308 | Cumulative Reward 0.1562\n",
    ">     Episode 021/400 | Average Loss 3022.9790 | Cumulative Reward 0.1391\n",
    ">     Episode 022/400 | Average Loss 928.1853 | Cumulative Reward 0.0965\n",
    ">     Episode 023/400 | Average Loss 335.2204 | Cumulative Reward -0.0647\n",
    ">     Episode 024/400 | Average Loss 92.0703 | Cumulative Reward -1.0509\n",
    ">     Episode 025/400 | Average Loss 62.6662 | Cumulative Reward 2.6331\n",
    ">     Episode 026/400 | Average Loss 141.5263 | Cumulative Reward 0.7501\n",
    ">     Episode 027/400 | Average Loss 151.6588 | Cumulative Reward -0.7282\n",
    ">     Episode 028/400 | Average Loss 88.7003 | Cumulative Reward -0.4138\n",
    ">     Episode 029/400 | Average Loss 70.3339 | Cumulative Reward -0.3277\n",
    ">     Episode 030/400 | Average Loss 71.3869 | Cumulative Reward 0.2297\n",
    ">     Episode 031/400 | Average Loss 63.7683 | Cumulative Reward -1.2043\n",
    ">     Episode 032/400 | Average Loss 52.0525 | Cumulative Reward 0.3404\n",
    ">     Episode 033/400 | Average Loss 139.2886 | Cumulative Reward 0.5432\n",
    ">     Episode 034/400 | Average Loss 125.1808 | Cumulative Reward -0.0532\n",
    ">     Episode 035/400 | Average Loss 75581.6705 | Cumulative Reward -0.2716\n",
    ">     Episode 036/400 | Average Loss 1499093.0702 | Cumulative Reward -0.0629\n",
    ">     Episode 037/400 | Average Loss 142075.2642 | Cumulative Reward 0.6731\n",
    ">     Episode 038/400 | Average Loss 42704.7186 | Cumulative Reward -1.2040\n",
    ">     Episode 039/400 | Average Loss 4600.6392 | Cumulative Reward -2.4596\n",
    ">     Episode 040/400 | Average Loss 3533.7538 | Cumulative Reward -2.1036\n",
    ">     Episode 041/400 | Average Loss 3485.4257 | Cumulative Reward -0.3009\n",
    ">     Episode 042/400 | Average Loss 1754.1580 | Cumulative Reward -0.0897\n",
    ">     Episode 043/400 | Average Loss 4854.8793 | Cumulative Reward 0.6749\n",
    ">     Episode 044/400 | Average Loss 32073.8605 | Cumulative Reward 0.4181\n",
    ">     Episode 045/400 | Average Loss 27062.1981 | Cumulative Reward 10.9193\n",
    ">     Episode 046/400 | Average Loss 5357.5187 | Cumulative Reward 0.6953\n",
    ">     Episode 047/400 | Average Loss 1618.4097 | Cumulative Reward -0.5666\n",
    ">     Episode 048/400 | Average Loss 527.3351 | Cumulative Reward -1.1120\n",
    ">     Episode 049/400 | Average Loss 238.1634 | Cumulative Reward -1.3369\n",
    ">     Episode 050/400 | Average Loss 175.4740 | Cumulative Reward -0.6742\n",
    ">     Episode 051/400 | Average Loss 123.2425 | Cumulative Reward -0.6937\n",
    ">     Episode 052/400 | Average Loss 104.5643 | Cumulative Reward -0.2996\n",
    ">     Episode 053/400 | Average Loss 110.9646 | Cumulative Reward -0.2320\n",
    ">     Episode 054/400 | Average Loss 133.3820 | Cumulative Reward 0.1100\n",
    ">     Episode 055/400 | Average Loss 116.8288 | Cumulative Reward 1.5496\n",
    ">     Episode 056/400 | Average Loss 97.1488 | Cumulative Reward -0.5533\n",
    ">     Episode 057/400 | Average Loss 97.8561 | Cumulative Reward -2.1378\n",
    ">     Episode 058/400 | Average Loss 142.2611 | Cumulative Reward -0.3099\n",
    ">     Episode 059/400 | Average Loss 223.6259 | Cumulative Reward -0.2781\n",
    ">     Episode 060/400 | Average Loss 1084.1126 | Cumulative Reward 3.4933\n",
    ">     Episode 061/400 | Average Loss 23224.0857 | Cumulative Reward 32.5585\n",
    ">     Episode 062/400 | Average Loss 25930.6092 | Cumulative Reward 0.4708\n",
    ">     Episode 063/400 | Average Loss 322392.9633 | Cumulative Reward -2.5420\n",
    ">     Episode 064/400 | Average Loss 334215.3544 | Cumulative Reward -1.8296\n",
    ">     Episode 065/400 | Average Loss 35868.0845 | Cumulative Reward 0.6106\n",
    ">     Episode 066/400 | Average Loss 117355.0819 | Cumulative Reward 0.2297\n",
    ">     Episode 067/400 | Average Loss 22205.1672 | Cumulative Reward 1.5464\n",
    ">     Episode 068/400 | Average Loss 2638.6969 | Cumulative Reward 1.9100\n",
    ">     Episode 069/400 | Average Loss 853.5517 | Cumulative Reward 6.6405\n",
    ">     Episode 070/400 | Average Loss 554.5541 | Cumulative Reward -1.4309\n",
    ">     Episode 071/400 | Average Loss 428.0672 | Cumulative Reward 1.3316\n",
    ">     Episode 072/400 | Average Loss 357.3235 | Cumulative Reward 0.2123\n",
    ">     Episode 073/400 | Average Loss 414.5044 | Cumulative Reward -2.1793\n",
    ">     Episode 074/400 | Average Loss 333.6023 | Cumulative Reward -0.2519\n",
    ">     Episode 075/400 | Average Loss 311.4634 | Cumulative Reward -0.5926\n",
    ">     Episode 076/400 | Average Loss 315.3541 | Cumulative Reward -0.5030\n",
    ">     Episode 077/400 | Average Loss 308.6532 | Cumulative Reward 1.1239\n",
    ">     Episode 078/400 | Average Loss 312.1528 | Cumulative Reward 1.9444\n",
    ">     Episode 079/400 | Average Loss 330.1110 | Cumulative Reward -1.8888\n",
    ">     Episode 080/400 | Average Loss 445.1022 | Cumulative Reward 0.9536\n",
    ">     Episode 081/400 | Average Loss 331.2435 | Cumulative Reward 1.0538\n",
    ">     Episode 082/400 | Average Loss 276.0205 | Cumulative Reward 0.4262\n",
    ">     Episode 083/400 | Average Loss 255.5548 | Cumulative Reward 4.3867\n",
    ">     Episode 084/400 | Average Loss 350.6699 | Cumulative Reward -0.3012\n",
    ">     Episode 085/400 | Average Loss 315.2827 | Cumulative Reward -1.3592\n",
    ">     Episode 086/400 | Average Loss 428.5230 | Cumulative Reward -3.0848\n",
    ">     Episode 087/400 | Average Loss 426.4023 | Cumulative Reward -2.3325\n",
    ">     Episode 088/400 | Average Loss 323.8480 | Cumulative Reward -0.7839\n",
    ">     Episode 089/400 | Average Loss 288.2806 | Cumulative Reward 0.4595\n",
    ">     Episode 090/400 | Average Loss 375.2701 | Cumulative Reward 6.6938\n",
    ">     Episode 091/400 | Average Loss 327.0411 | Cumulative Reward -3.4743\n",
    ">     Episode 092/400 | Average Loss 307.1827 | Cumulative Reward 3.0540\n",
    ">     Episode 093/400 | Average Loss 547.2796 | Cumulative Reward -2.0188\n",
    ">     Episode 094/400 | Average Loss 1411.6391 | Cumulative Reward 0.6463\n",
    ">     Episode 095/400 | Average Loss 8877.2380 | Cumulative Reward -0.9047\n",
    ">     Episode 096/400 | Average Loss 78876.4797 | Cumulative Reward 0.3335\n",
    ">     Episode 097/400 | Average Loss 35666.4564 | Cumulative Reward -2.8158\n",
    ">     Episode 098/400 | Average Loss 60985.4187 | Cumulative Reward -1.1599\n",
    ">     Episode 099/400 | Average Loss 30969.0067 | Cumulative Reward 0.3606\n",
    ">     Episode 100/400 | Average Loss 120682.6666 | Cumulative Reward 0.2651\n",
    ">     Episode 101/400 | Average Loss 50795.0507 | Cumulative Reward 0.6701\n",
    ">     Episode 102/400 | Average Loss 24420.6597 | Cumulative Reward -2.4215\n",
    ">     Episode 103/400 | Average Loss 33171.9955 | Cumulative Reward -0.8510\n",
    ">     Episode 104/400 | Average Loss 25231.0958 | Cumulative Reward -0.8302\n",
    ">     Episode 105/400 | Average Loss 7064.6491 | Cumulative Reward 5.9196\n",
    ">     Episode 106/400 | Average Loss 2441.4632 | Cumulative Reward 2.9671\n",
    ">     Episode 107/400 | Average Loss 3972.6634 | Cumulative Reward 2.2185\n",
    ">     Episode 108/400 | Average Loss 4525.4060 | Cumulative Reward -0.5613\n",
    ">     Episode 109/400 | Average Loss 4010.8900 | Cumulative Reward 0.3562\n",
    ">     Episode 110/400 | Average Loss 4406.7336 | Cumulative Reward -0.2574\n",
    ">     Episode 111/400 | Average Loss 2514.7808 | Cumulative Reward 0.9526\n",
    ">     Episode 112/400 | Average Loss 1741.7351 | Cumulative Reward -0.4360\n",
    ">     Episode 113/400 | Average Loss 1994.2122 | Cumulative Reward -0.6628\n",
    ">     Episode 114/400 | Average Loss 2122.6169 | Cumulative Reward 1.4274\n",
    ">     Episode 115/400 | Average Loss 2412.2641 | Cumulative Reward -2.7171\n",
    ">     Episode 116/400 | Average Loss 2157.2323 | Cumulative Reward -2.2203\n",
    ">     Episode 117/400 | Average Loss 1763.6523 | Cumulative Reward 5.8168\n",
    ">     Episode 118/400 | Average Loss 2157.3896 | Cumulative Reward 0.7164\n",
    ">     Episode 119/400 | Average Loss 2271.7217 | Cumulative Reward -0.4404\n",
    ">     Episode 120/400 | Average Loss 1806.5632 | Cumulative Reward 4.1580\n",
    ">     Episode 121/400 | Average Loss 1592.2903 | Cumulative Reward -1.7653\n",
    ">     Episode 122/400 | Average Loss 1865.5214 | Cumulative Reward 1.0669\n",
    ">     Episode 123/400 | Average Loss 1590.0795 | Cumulative Reward -5.7212\n",
    ">     Episode 124/400 | Average Loss 1290.1997 | Cumulative Reward -2.0139\n",
    ">     Episode 125/400 | Average Loss 6849.0803 | Cumulative Reward 10.0472\n",
    ">     Episode 126/400 | Average Loss 4108.5280 | Cumulative Reward -0.4549\n",
    ">     Episode 127/400 | Average Loss 2033.6164 | Cumulative Reward -1.9177\n",
    ">     Episode 128/400 | Average Loss 3314.3956 | Cumulative Reward -1.2552\n",
    ">     Episode 129/400 | Average Loss 1554.2960 | Cumulative Reward 1.3309\n",
    ">     Episode 130/400 | Average Loss 1695.8567 | Cumulative Reward -0.6389\n",
    ">     Episode 131/400 | Average Loss 4964.5786 | Cumulative Reward -0.4334\n",
    ">     Episode 132/400 | Average Loss 30351.7264 | Cumulative Reward 3.4232\n",
    ">     Episode 133/400 | Average Loss 292663.9331 | Cumulative Reward -6.1486\n",
    ">     Episode 134/400 | Average Loss 39913.3923 | Cumulative Reward 0.8041\n",
    ">     Episode 135/400 | Average Loss 4525.7708 | Cumulative Reward 0.5128\n",
    ">     Episode 136/400 | Average Loss 5816.5796 | Cumulative Reward 1.0164\n",
    ">     Episode 137/400 | Average Loss 3372.1565 | Cumulative Reward -2.7557\n",
    ">     Episode 138/400 | Average Loss 1255.0318 | Cumulative Reward -1.4759\n",
    ">     Episode 139/400 | Average Loss 1227.6646 | Cumulative Reward -4.4638\n",
    ">     Episode 140/400 | Average Loss 1490.9701 | Cumulative Reward -0.9443\n",
    ">     Episode 141/400 | Average Loss 2175.9060 | Cumulative Reward -0.6079\n",
    ">     Episode 142/400 | Average Loss 3145.8265 | Cumulative Reward -4.0675\n",
    ">     Episode 143/400 | Average Loss 1102.6005 | Cumulative Reward -0.1042\n",
    ">     Episode 144/400 | Average Loss 350.4612 | Cumulative Reward -1.6198\n",
    ">     Episode 145/400 | Average Loss 316.4280 | Cumulative Reward 0.6322\n",
    ">     Episode 146/400 | Average Loss 436.2378 | Cumulative Reward 0.9417\n",
    ">     Episode 147/400 | Average Loss 470.8193 | Cumulative Reward -0.7965\n",
    ">     Episode 148/400 | Average Loss 497.6323 | Cumulative Reward 1.0542\n",
    ">     Episode 149/400 | Average Loss 330.6520 | Cumulative Reward 0.1210\n",
    ">     Episode 150/400 | Average Loss 464.6532 | Cumulative Reward -0.8083\n",
    ">     Episode 151/400 | Average Loss 606.0201 | Cumulative Reward -0.3361\n",
    ">     Episode 152/400 | Average Loss 415.2449 | Cumulative Reward -2.2024\n",
    ">     Episode 153/400 | Average Loss 390.6160 | Cumulative Reward 8.2336\n",
    ">     Episode 154/400 | Average Loss 379.7443 | Cumulative Reward 2.7249\n",
    ">     Episode 155/400 | Average Loss 415.5202 | Cumulative Reward -3.8253\n",
    ">     Episode 156/400 | Average Loss 349.0971 | Cumulative Reward 0.5322\n",
    ">     Episode 157/400 | Average Loss 327.8658 | Cumulative Reward -3.0669\n",
    ">     Episode 158/400 | Average Loss 452.8055 | Cumulative Reward 1.8531\n",
    ">     Episode 159/400 | Average Loss 358.1489 | Cumulative Reward 0.6118\n",
    ">     Episode 160/400 | Average Loss 285.6239 | Cumulative Reward -0.2853\n",
    ">     Episode 161/400 | Average Loss 211.8839 | Cumulative Reward 1.3377\n",
    ">     Episode 162/400 | Average Loss 199.9443 | Cumulative Reward 8.9061\n",
    ">     Episode 163/400 | Average Loss 266.2299 | Cumulative Reward -0.5084\n",
    ">     Episode 164/400 | Average Loss 278.4291 | Cumulative Reward -1.9366\n",
    ">     Episode 165/400 | Average Loss 237.7808 | Cumulative Reward 2.3392\n",
    ">     Episode 166/400 | Average Loss 316.0515 | Cumulative Reward 1.7630\n",
    ">     Episode 167/400 | Average Loss 259.7458 | Cumulative Reward 1.6817\n",
    ">     Episode 168/400 | Average Loss 157.4775 | Cumulative Reward -5.0146\n",
    ">     Episode 169/400 | Average Loss 158.3143 | Cumulative Reward -0.2332\n",
    ">     Episode 170/400 | Average Loss 171.3159 | Cumulative Reward 0.1229\n",
    ">     Episode 171/400 | Average Loss 225.3762 | Cumulative Reward -0.2566\n",
    ">     Episode 172/400 | Average Loss 246.0206 | Cumulative Reward 1.2274\n",
    ">     Episode 173/400 | Average Loss 246.9212 | Cumulative Reward -6.2713\n",
    ">     Episode 174/400 | Average Loss 172.3311 | Cumulative Reward -7.8041\n",
    ">     Episode 175/400 | Average Loss 159.6695 | Cumulative Reward -0.1459\n",
    ">     Episode 176/400 | Average Loss 133.5186 | Cumulative Reward 12.6934\n",
    ">     Episode 177/400 | Average Loss 203.6588 | Cumulative Reward 0.6016\n",
    ">     Episode 178/400 | Average Loss 180.3377 | Cumulative Reward -5.1401\n",
    ">     Episode 179/400 | Average Loss 156.4804 | Cumulative Reward -2.9022\n",
    ">     Episode 180/400 | Average Loss 256.7832 | Cumulative Reward -0.4108\n",
    ">     Episode 181/400 | Average Loss 248.6069 | Cumulative Reward 2.9433\n",
    ">     Episode 182/400 | Average Loss 209.6588 | Cumulative Reward -9.2684\n",
    ">     Episode 183/400 | Average Loss 170.3557 | Cumulative Reward -2.3097\n",
    ">     Episode 184/400 | Average Loss 137.4371 | Cumulative Reward 3.2238\n",
    ">     Episode 185/400 | Average Loss 173.6806 | Cumulative Reward 0.6079\n",
    ">     Episode 186/400 | Average Loss 156.5687 | Cumulative Reward -0.9450\n",
    ">     Episode 187/400 | Average Loss 127.6977 | Cumulative Reward -3.8308\n",
    ">     Episode 188/400 | Average Loss 319.2198 | Cumulative Reward -2.4003\n",
    ">     Episode 189/400 | Average Loss 288.2685 | Cumulative Reward 1.8500\n",
    ">     Episode 190/400 | Average Loss 190.4522 | Cumulative Reward -3.9517\n",
    ">     Episode 191/400 | Average Loss 135.5635 | Cumulative Reward 1.3865\n",
    ">     Episode 192/400 | Average Loss 117.3449 | Cumulative Reward -0.0848\n",
    ">     Episode 193/400 | Average Loss 144.4432 | Cumulative Reward -4.9818\n",
    ">     Episode 194/400 | Average Loss 120.0152 | Cumulative Reward 3.4097\n",
    ">     Episode 195/400 | Average Loss 132.2422 | Cumulative Reward -0.7065\n",
    ">     Episode 196/400 | Average Loss 202.1392 | Cumulative Reward 1.6419\n",
    ">     Episode 197/400 | Average Loss 114.8075 | Cumulative Reward -0.8801\n",
    ">     Episode 198/400 | Average Loss 153.8085 | Cumulative Reward -0.5935\n",
    ">     Episode 199/400 | Average Loss 109.2495 | Cumulative Reward 0.0568\n",
    ">     Episode 200/400 | Average Loss 82.4544 | Cumulative Reward 0.7314\n",
    ">     Episode 201/400 | Average Loss 149.3786 | Cumulative Reward -3.0157\n",
    ">     Episode 202/400 | Average Loss 272.0143 | Cumulative Reward 0.4744\n",
    ">     Episode 203/400 | Average Loss 132.8172 | Cumulative Reward -0.1198\n",
    ">     Episode 204/400 | Average Loss 70.0116 | Cumulative Reward 1.8903\n",
    ">     Episode 205/400 | Average Loss 67.3756 | Cumulative Reward -1.8030\n",
    ">     Episode 206/400 | Average Loss 270.3828 | Cumulative Reward -4.3223\n",
    ">     Episode 207/400 | Average Loss 399.5093 | Cumulative Reward -12.4752\n",
    ">     Episode 208/400 | Average Loss 450.2183 | Cumulative Reward -0.0156\n",
    ">     Episode 209/400 | Average Loss 1671.8967 | Cumulative Reward 2.7647\n",
    ">     Episode 210/400 | Average Loss 16700.3337 | Cumulative Reward 9.8480\n",
    ">     Episode 211/400 | Average Loss 13446.1835 | Cumulative Reward -1.1438\n",
    ">     Episode 212/400 | Average Loss 137932.9261 | Cumulative Reward 0.4590\n",
    ">     Episode 213/400 | Average Loss 921923.0748 | Cumulative Reward -15.9503\n",
    ">     Episode 214/400 | Average Loss 1615567.8120 | Cumulative Reward 1.3957\n",
    ">     Episode 215/400 | Average Loss 3153049.2869 | Cumulative Reward -7.7295\n",
    ">     Episode 216/400 | Average Loss 2272427.4549 | Cumulative Reward 6.7439\n",
    ">     Episode 217/400 | Average Loss 1095519.9990 | Cumulative Reward 11.3729\n",
    ">     Episode 218/400 | Average Loss 432235.9467 | Cumulative Reward 1.4778\n",
    ">     Episode 219/400 | Average Loss 161949.9376 | Cumulative Reward 0.3419\n",
    ">     Episode 220/400 | Average Loss 58845.4062 | Cumulative Reward 2.7168\n",
    ">     Episode 221/400 | Average Loss 21230.0694 | Cumulative Reward -1.6147\n",
    ">     Episode 222/400 | Average Loss 9440.9599 | Cumulative Reward -0.7419\n",
    ">     Episode 223/400 | Average Loss 7721.9410 | Cumulative Reward -1.6850\n",
    ">     Episode 224/400 | Average Loss 6988.2630 | Cumulative Reward -2.1230\n",
    ">     Episode 225/400 | Average Loss 15801.3750 | Cumulative Reward 0.1460\n",
    ">     Episode 226/400 | Average Loss 26985.7430 | Cumulative Reward 0.8148\n",
    ">     Episode 227/400 | Average Loss 33871.8176 | Cumulative Reward 0.7159\n",
    ">     Episode 228/400 | Average Loss 64896.3301 | Cumulative Reward 2.6142\n",
    ">     Episode 229/400 | Average Loss 58708.7660 | Cumulative Reward 1.5416\n",
    ">     Episode 230/400 | Average Loss 23921.1229 | Cumulative Reward -1.3302\n",
    ">     Episode 231/400 | Average Loss 19383.0976 | Cumulative Reward 0.3096\n",
    ">     Episode 232/400 | Average Loss 21257.7983 | Cumulative Reward 1.6749\n",
    ">     Episode 233/400 | Average Loss 9605.4634 | Cumulative Reward 2.6378\n",
    ">     Episode 234/400 | Average Loss 9249.7002 | Cumulative Reward 9.3712\n",
    ">     Episode 235/400 | Average Loss 10296.6081 | Cumulative Reward 1.5610\n",
    ">     Episode 236/400 | Average Loss 15917.5678 | Cumulative Reward -2.1748\n",
    ">     Episode 237/400 | Average Loss 20750.8658 | Cumulative Reward -2.5459\n",
    ">     Episode 238/400 | Average Loss 38039.9481 | Cumulative Reward 8.1172\n",
    ">     Episode 239/400 | Average Loss 32984.9469 | Cumulative Reward -0.0595\n",
    ">     Episode 240/400 | Average Loss 16785.4156 | Cumulative Reward 0.3051\n",
    ">     Episode 241/400 | Average Loss 19140.9990 | Cumulative Reward -31.6967\n",
    ">     Episode 242/400 | Average Loss 50914.1964 | Cumulative Reward -0.2164\n",
    ">     Episode 243/400 | Average Loss 43226.3390 | Cumulative Reward 0.7469\n",
    ">     Episode 244/400 | Average Loss 142370.8437 | Cumulative Reward -0.7410\n",
    ">     Episode 245/400 | Average Loss 271693.5746 | Cumulative Reward 2.9429\n",
    ">     Episode 246/400 | Average Loss 372235.5745 | Cumulative Reward -0.9499\n",
    ">     Episode 247/400 | Average Loss 554116.7420 | Cumulative Reward -2.4131\n",
    ">     Episode 248/400 | Average Loss 689604.6347 | Cumulative Reward 3.9210\n",
    ">     Episode 249/400 | Average Loss 17052161.8473 | Cumulative Reward 1.8883\n",
    ">     Episode 250/400 | Average Loss 13790480.7408 | Cumulative Reward -0.9414\n",
    ">     Episode 251/400 | Average Loss 553313.4969 | Cumulative Reward 1.9731\n",
    ">     Episode 252/400 | Average Loss 193172.5233 | Cumulative Reward 0.4359\n",
    ">     Episode 253/400 | Average Loss 70787.7497 | Cumulative Reward -3.2308\n",
    ">     Episode 254/400 | Average Loss 31558.5159 | Cumulative Reward -0.5023\n",
    ">     Episode 255/400 | Average Loss 20880.9573 | Cumulative Reward -3.3874\n",
    ">     Episode 256/400 | Average Loss 14961.8871 | Cumulative Reward -0.5209\n",
    ">     Episode 257/400 | Average Loss 12658.6910 | Cumulative Reward -1.0665\n",
    ">     Episode 258/400 | Average Loss 10553.0673 | Cumulative Reward -1.0583\n",
    ">     Episode 259/400 | Average Loss 11647.9965 | Cumulative Reward -1.1289\n",
    ">     Episode 260/400 | Average Loss 10819.5231 | Cumulative Reward -0.9052\n",
    ">     Episode 261/400 | Average Loss 9811.8621 | Cumulative Reward 0.6868\n",
    ">     Episode 262/400 | Average Loss 9734.9689 | Cumulative Reward -0.1421\n",
    ">     Episode 263/400 | Average Loss 16806.1076 | Cumulative Reward -0.9311\n",
    ">     Episode 264/400 | Average Loss 51381.4164 | Cumulative Reward 0.5244\n",
    ">     Episode 265/400 | Average Loss 52715.5890 | Cumulative Reward -3.1132\n",
    ">     Episode 266/400 | Average Loss 147370.8256 | Cumulative Reward 6.7689\n",
    ">     Episode 267/400 | Average Loss 349701.5862 | Cumulative Reward -5.3704\n",
    ">     Episode 268/400 | Average Loss 453258.1571 | Cumulative Reward 1.9217\n",
    ">     Episode 269/400 | Average Loss 469045.7159 | Cumulative Reward -1.1183\n",
    ">     Episode 270/400 | Average Loss 216574.8130 | Cumulative Reward -0.2209\n",
    ">     Episode 271/400 | Average Loss 63084.3598 | Cumulative Reward -0.5959\n",
    ">     Episode 272/400 | Average Loss 43544.3739 | Cumulative Reward 2.8877\n",
    ">     Episode 273/400 | Average Loss 54201.8135 | Cumulative Reward -0.7229\n",
    ">     Episode 274/400 | Average Loss 44982.9422 | Cumulative Reward -7.6026\n",
    ">     Episode 275/400 | Average Loss 49190.5107 | Cumulative Reward -2.3663\n",
    ">     Episode 276/400 | Average Loss 38168.9935 | Cumulative Reward 1.3036\n",
    ">     Episode 277/400 | Average Loss 33624.0186 | Cumulative Reward 9.4910\n",
    ">     Episode 278/400 | Average Loss 52808.2118 | Cumulative Reward -2.5743\n",
    ">     Episode 279/400 | Average Loss 99931.7062 | Cumulative Reward 3.7482\n",
    ">     Episode 280/400 | Average Loss 52280.1315 | Cumulative Reward -1.1176\n",
    ">     Episode 281/400 | Average Loss 51300.1439 | Cumulative Reward 0.0196\n",
    ">     Episode 282/400 | Average Loss 46517.2936 | Cumulative Reward -7.0006\n",
    ">     Episode 283/400 | Average Loss 75258.0231 | Cumulative Reward -2.2092\n",
    ">     Episode 284/400 | Average Loss 124144.1463 | Cumulative Reward 2.3907\n",
    ">     Episode 285/400 | Average Loss 166405.0758 | Cumulative Reward 17.9376\n",
    ">     Episode 286/400 | Average Loss 109845.9581 | Cumulative Reward 4.2551\n",
    ">     Episode 287/400 | Average Loss 86368.8667 | Cumulative Reward 0.0941\n",
    ">     Episode 288/400 | Average Loss 48403.3659 | Cumulative Reward -0.6673\n",
    ">     Episode 289/400 | Average Loss 41796.8802 | Cumulative Reward -1.6662\n",
    ">     Episode 290/400 | Average Loss 46337.5346 | Cumulative Reward -3.5903\n",
    ">     Episode 291/400 | Average Loss 131393.1431 | Cumulative Reward -0.0253\n",
    ">     Episode 292/400 | Average Loss 495484.4769 | Cumulative Reward 1.9893\n",
    ">     Episode 293/400 | Average Loss 175873.1958 | Cumulative Reward 2.5618\n",
    ">     Episode 294/400 | Average Loss 179162.6928 | Cumulative Reward 2.0417\n",
    ">     Episode 295/400 | Average Loss 119478.1822 | Cumulative Reward 4.6492\n",
    ">     Episode 296/400 | Average Loss 58801.1919 | Cumulative Reward 0.6851\n",
    ">     Episode 297/400 | Average Loss 32332.3374 | Cumulative Reward -3.0626\n",
    ">     Episode 298/400 | Average Loss 60964.8619 | Cumulative Reward -2.7876\n",
    ">     Episode 299/400 | Average Loss 102951.7667 | Cumulative Reward -0.6980\n",
    ">     Episode 300/400 | Average Loss 219870.4704 | Cumulative Reward -2.9908\n",
    ">     Episode 301/400 | Average Loss 205059.5425 | Cumulative Reward 0.4130\n",
    ">     Episode 302/400 | Average Loss 121851.6144 | Cumulative Reward -6.5875\n",
    ">     Episode 303/400 | Average Loss 55374.6726 | Cumulative Reward -0.7004\n",
    ">     Episode 304/400 | Average Loss 40714.0962 | Cumulative Reward -0.6966\n",
    ">     Episode 305/400 | Average Loss 616077.5248 | Cumulative Reward -6.7208\n",
    ">     Episode 306/400 | Average Loss 1291013.0000 | Cumulative Reward 3.3525\n",
    ">     Episode 307/400 | Average Loss 287649.6938 | Cumulative Reward -1.0102\n",
    ">     Episode 308/400 | Average Loss 26063.9600 | Cumulative Reward -2.2521\n",
    ">     Episode 309/400 | Average Loss 13097.6914 | Cumulative Reward -10.6769\n",
    ">     Episode 310/400 | Average Loss 10210.9897 | Cumulative Reward 1.4032\n",
    ">     Episode 311/400 | Average Loss 9341.3356 | Cumulative Reward 0.5489\n",
    ">     Episode 312/400 | Average Loss 8890.5654 | Cumulative Reward -1.5771\n",
    ">     Episode 313/400 | Average Loss 9969.8120 | Cumulative Reward -6.8167\n",
    ">     Episode 314/400 | Average Loss 16652.6395 | Cumulative Reward 4.5195\n",
    ">     Episode 315/400 | Average Loss 59403.0649 | Cumulative Reward -1.7285\n",
    ">     Episode 316/400 | Average Loss 245777.2312 | Cumulative Reward 2.5883\n",
    ">     Episode 317/400 | Average Loss 817190.2299 | Cumulative Reward -1.8446\n",
    ">     Episode 318/400 | Average Loss 323813.5505 | Cumulative Reward -10.0864\n",
    ">     Episode 319/400 | Average Loss 214917.4482 | Cumulative Reward 3.1694\n",
    ">     Episode 320/400 | Average Loss 64451.5863 | Cumulative Reward -0.7796\n",
    ">     Episode 321/400 | Average Loss 219139.1056 | Cumulative Reward 5.6409\n",
    ">     Episode 322/400 | Average Loss 166843.1720 | Cumulative Reward 1.1437\n",
    ">     Episode 323/400 | Average Loss 643758.2579 | Cumulative Reward 0.9259\n",
    ">     Episode 324/400 | Average Loss 273197.3924 | Cumulative Reward 0.6936\n",
    ">     Episode 325/400 | Average Loss 87513.6069 | Cumulative Reward -1.1678\n",
    ">     Episode 326/400 | Average Loss 37614.0828 | Cumulative Reward 0.2960\n",
    ">     Episode 327/400 | Average Loss 19445.4082 | Cumulative Reward 8.9279\n",
    ">     Episode 328/400 | Average Loss 9239.1169 | Cumulative Reward 0.9224\n",
    ">     Episode 329/400 | Average Loss 7564.7177 | Cumulative Reward -1.6945\n",
    ">     Episode 330/400 | Average Loss 7219.3443 | Cumulative Reward 0.7301\n",
    ">     Episode 331/400 | Average Loss 6740.1247 | Cumulative Reward -0.7489\n",
    ">     Episode 332/400 | Average Loss 6888.5497 | Cumulative Reward 0.7859\n",
    ">     Episode 333/400 | Average Loss 6260.5688 | Cumulative Reward 1.2887\n",
    ">     Episode 334/400 | Average Loss 6881.4800 | Cumulative Reward 0.8997\n",
    ">     Episode 335/400 | Average Loss 6491.5882 | Cumulative Reward 1.2076\n",
    ">     Episode 336/400 | Average Loss 6970.5306 | Cumulative Reward 8.2423\n",
    ">     Episode 337/400 | Average Loss 6692.2639 | Cumulative Reward -10.7015\n",
    ">     Episode 338/400 | Average Loss 10528.1149 | Cumulative Reward -14.2128\n",
    ">     Episode 339/400 | Average Loss 2749960.3616 | Cumulative Reward 3.9547\n",
    ">     Episode 340/400 | Average Loss 6293915.8740 | Cumulative Reward 0.5112\n",
    ">     Episode 341/400 | Average Loss 4181966.3648 | Cumulative Reward 7.5671\n",
    ">     Episode 342/400 | Average Loss 1521715.4903 | Cumulative Reward -0.7421\n",
    ">     Episode 343/400 | Average Loss 259878.5100 | Cumulative Reward -3.9238\n",
    ">     Episode 344/400 | Average Loss 223847.7131 | Cumulative Reward -0.8457\n",
    ">     Episode 345/400 | Average Loss 178520.7246 | Cumulative Reward 9.3477\n",
    ">     Episode 346/400 | Average Loss 91922.6007 | Cumulative Reward 10.6711\n",
    ">     Episode 347/400 | Average Loss 92341.3508 | Cumulative Reward 4.6715\n",
    ">     Episode 348/400 | Average Loss 481404.8008 | Cumulative Reward -0.6806\n",
    ">     Episode 349/400 | Average Loss 619327.0096 | Cumulative Reward -2.5698\n",
    ">     Episode 350/400 | Average Loss 200402.9201 | Cumulative Reward 4.9615\n",
    ">     Episode 351/400 | Average Loss 73401.7965 | Cumulative Reward -1.9998\n",
    ">     Episode 352/400 | Average Loss 353985.6604 | Cumulative Reward -3.0569\n",
    ">     Episode 353/400 | Average Loss 83143597.3859 | Cumulative Reward 2.1511\n",
    ">     Episode 354/400 | Average Loss 6398529.5102 | Cumulative Reward 0.7794\n",
    ">     Episode 355/400 | Average Loss 3473207.1696 | Cumulative Reward -2.7955\n",
    ">     Episode 356/400 | Average Loss 1885632.9462 | Cumulative Reward -2.8542\n",
    ">     Episode 357/400 | Average Loss 1584695.8876 | Cumulative Reward -0.8653\n",
    ">     Episode 358/400 | Average Loss 1803479.0510 | Cumulative Reward 15.1597\n",
    ">     Episode 359/400 | Average Loss 2213418.5752 | Cumulative Reward -2.5831\n",
    ">     Episode 360/400 | Average Loss 2749705.8786 | Cumulative Reward 1.0120\n",
    ">     Episode 361/400 | Average Loss 1289897.7182 | Cumulative Reward 1.4205\n",
    ">     Episode 362/400 | Average Loss 1813762.0330 | Cumulative Reward 1.5898\n",
    ">     Episode 363/400 | Average Loss 1324749.2118 | Cumulative Reward 1.6278\n",
    ">     Episode 364/400 | Average Loss 1762172.3248 | Cumulative Reward -2.4109\n",
    ">     Episode 365/400 | Average Loss 2590903.5681 | Cumulative Reward -5.4126\n",
    ">     Episode 366/400 | Average Loss 2224000.0000 | Cumulative Reward -0.1538\n",
    ">     Episode 367/400 | Average Loss 1441616.1888 | Cumulative Reward -1.6342\n",
    ">     Episode 368/400 | Average Loss 3022001.7933 | Cumulative Reward -5.8340\n",
    ">     Episode 369/400 | Average Loss 2972922.5484 | Cumulative Reward 5.4439\n",
    ">     Episode 370/400 | Average Loss 2003531.0430 | Cumulative Reward -2.6320\n",
    ">     Episode 371/400 | Average Loss 2709109.8594 | Cumulative Reward -12.0559\n",
    ">     Episode 372/400 | Average Loss 3228692.9416 | Cumulative Reward -6.7218\n",
    ">     Episode 373/400 | Average Loss 3700596.4360 | Cumulative Reward 0.5078\n",
    ">     Episode 374/400 | Average Loss 3377426.7874 | Cumulative Reward 0.7276\n",
    ">     Episode 375/400 | Average Loss 3365048.7382 | Cumulative Reward 3.1090\n",
    ">     Episode 376/400 | Average Loss 4587485.8284 | Cumulative Reward -6.7627\n",
    ">     Episode 377/400 | Average Loss 5680297.4426 | Cumulative Reward 3.3688\n",
    ">     Episode 378/400 | Average Loss 3493340.6926 | Cumulative Reward -2.9359\n",
    ">     Episode 379/400 | Average Loss 2484035.3117 | Cumulative Reward 3.1205\n",
    ">     Episode 380/400 | Average Loss 5507217.9449 | Cumulative Reward 3.1457\n",
    ">     Episode 381/400 | Average Loss 3075229.6434 | Cumulative Reward 0.5471\n",
    ">     Episode 382/400 | Average Loss 6119780.0567 | Cumulative Reward -3.7341\n",
    ">     Episode 383/400 | Average Loss 3379319.6055 | Cumulative Reward -0.7432\n",
    ">     Episode 384/400 | Average Loss 4744977.0717 | Cumulative Reward -1.1412\n",
    ">     Episode 385/400 | Average Loss 5937114.8721 | Cumulative Reward -3.9135\n",
    ">     Episode 386/400 | Average Loss 3936486.9326 | Cumulative Reward 1.8648\n",
    ">     Episode 387/400 | Average Loss 4697161.7754 | Cumulative Reward -0.6459\n",
    ">     Episode 388/400 | Average Loss 3918769.9928 | Cumulative Reward -1.3641\n",
    ">     Episode 389/400 | Average Loss 3720405.1218 | Cumulative Reward 5.0283\n",
    ">     Episode 390/400 | Average Loss 3203617.5284 | Cumulative Reward 0.5877\n",
    ">     Episode 391/400 | Average Loss 4206256.2672 | Cumulative Reward -2.0365\n",
    ">     Episode 392/400 | Average Loss 5004207.2419 | Cumulative Reward -0.4567\n",
    ">     Episode 393/400 | Average Loss 3484851.8288 | Cumulative Reward -2.2568\n",
    ">     Episode 394/400 | Average Loss 5117577.1432 | Cumulative Reward -0.3933\n",
    ">     Episode 395/400 | Average Loss 3649000.0143 | Cumulative Reward -1.9124\n",
    ">     Episode 396/400 | Average Loss 3255186.4273 | Cumulative Reward 2.8319\n",
    ">     Episode 397/400 | Average Loss 3177039.6798 | Cumulative Reward -1.6385\n",
    ">     Episode 398/400 | Average Loss 3807617.9805 | Cumulative Reward 1.1477\n",
    ">     Episode 399/400 | Average Loss 2519941.9805 | Cumulative Reward 3.4947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(returns)\n",
    "ax.set_ylabel(\"Return\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "reward_sum = 0.\n",
    "\n",
    "# Define testing interval\n",
    "start = datetime.datetime(2019, 1, 1, 0, 0)\n",
    "end = datetime.datetime(2019, 6, 20, 23, 59)\n",
    "\n",
    "# Test learned model\n",
    "env = MarketEnv(oilDF_py, start, end, episode_size=np.inf, scope=sequence_scope)\n",
    "state = env.reset(random_starttime=False)\n",
    "input_t = state[1].reshape(1, sequence_scope, 1)\n",
    "while not done:    \n",
    "    states.append(state)\n",
    "    q = model.predict(input_t)\n",
    "    action = np.argmax(q[0])\n",
    "    actions.append(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "    input_t = state[1].reshape(1, sequence_scope, 1)      \n",
    "print(\"Return = {}\".format(reward_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(11, 9))\n",
    "ax[0].plot(states,label='s')\n",
    "ax[0].plot(actions,label='a')\n",
    "ax[0].set_ylabel(\"(s,a)\")\n",
    "ax[0].set_xlabel(\"Timestep\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(rewards)\n",
    "ax[1].set_ylabel(\"r\")\n",
    "ax[1].set_xlabel(\"Timestep\")\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "read data as structures stream\n",
    "=============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val oil_path = \"dbfs:/FileStore/shared_uploads/fabiansi@kth.se/*csv.gz\"\n",
    "\n",
    "val input = spark\n",
    "  .readStream\n",
    "  .format(\"delta\")\n",
    "  .load(oil_path)\n",
    "  .as[TickerPoint]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedDS = spark.read.parquet(\"dbfs:/FileStore/shared_uploads/fabiansi@kth.se/joinedDSWithMaxRev\").orderBy(\"x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
