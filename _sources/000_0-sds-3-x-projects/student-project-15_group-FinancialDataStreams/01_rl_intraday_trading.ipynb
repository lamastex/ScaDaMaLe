{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning for Intraday Trading\n",
    "===========================================\n",
    "\n",
    "In this project, our aim is to implement a Reinforcement Learning (RL)\n",
    "strategy for trading stocks. Adopting a learning-based approach, in\n",
    "particular using RL, entails several potential benefits over current\n",
    "approaches. Firstly, several ML methods allow learning-based\n",
    "pre-processing steps, such as convolutional layers which enable\n",
    "automatic feature extraction and detection, and may be used to focus the\n",
    "computation on the most relevant features. Secondly, constructing an\n",
    "end-to-end learning-based pipeline makes the prediction step implicit,\n",
    "and potentially reduces the problem complexity to predicting only\n",
    "certain aspects or features of the time series which are necessary for\n",
    "the control strategy, as opposed to attempting to predict the exact time\n",
    "series values. Thirdly, an end-to-end learning-based approach alleviates\n",
    "potential bounds of the step-wise modularization that a human-designed\n",
    "pipeline would entail, and allows the learning algorithm to\n",
    "automatically deduce the optimal strategy for utilizing any feature\n",
    "signal, in order to execute the most efficient control strategy.\n",
    "\n",
    "The main idea behind RL algorithms is to learn by trial-and-error how to\n",
    "act optimally. An agent gathers experience by iteratively interacting\n",
    "with an environment. Starting in state S*t, the agent takes an action\n",
    "A*t and receives a reward R*t+1 as it moves to state S*t+1, as seen\n",
    "below\n",
    "([source](https://upload.wikimedia.org/wikipedia/commons/d/da/Markov_diagram_v2.svg)).\n",
    "Using this experience, RL algorithms can learn either a value function\n",
    "or a policy directly. We learn the former, which can then be used to\n",
    "compute optimal actions, by chosing the action that maximizes the action\n",
    "value, Q. Specifically, we use the DQN -- Deep Q-Network -- algorithm to\n",
    "train an agent which trades Brent Crude Oil (BCOUSD) stocks, in order to\n",
    "maximize profit.\n",
    "\n",
    "&lt;img\n",
    "src=https://upload.wikimedia.org/wikipedia/commons/d/da/Markov*diagram*v2.svg\n",
    "width=600&gt;\n",
    "\n",
    "Group members:\n",
    "--------------\n",
    "\n",
    "-   Fabian Sinzinger\n",
    "-   Karl Bäckström\n",
    "-   Rita Laezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Scala imports\n",
    "import org.lamastex.spark.trendcalculus._\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import java.sql.Timestamp\n",
    "import org.apache.spark.sql.expressions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.lamastex.spark.trendcalculus._\n",
    ">     import spark.implicits._\n",
    ">     import org.apache.spark.sql._\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     import java.sql.Timestamp\n",
    ">     import org.apache.spark.sql.expressions._\n",
    "\n",
    "  \n",
    "\n",
    "Brent Crude Oil Dataset\n",
    "-----------------------\n",
    "\n",
    "The dataset consists of historical data starting from the *14th of\n",
    "October 2010* to the *21st of June 2019*. Since the data in the first\n",
    "and last day is incomplete, we remove it from the dataset. The BCUSD\n",
    "data is sampled approximatly every minute with a specific timestamp and\n",
    "registered in US dollars.\n",
    "\n",
    "To read the BCUSD dataset, we use the same parsers provided by the\n",
    "[TrendCalculus](https://github.com/lamastex/spark-trend-calculus)\n",
    "library. This allows us to load the FX data into a Spark Dataset. The\n",
    "**fx1m** function returns the dataset as **TickerPoint** objects with\n",
    "values **x** and **y**, which are **time** and a **close** values\n",
    "respectively. The first consists of the name of the stock, the second is\n",
    "the timestamp of the data point and the latter consists of the value of\n",
    "the stock at the end of each 1 minute bin.\n",
    "\n",
    "Finally we add the **index** column to facilitate retrieving values from\n",
    "the table, since there are gaps in the data meaning that not all minutes\n",
    "have an entry. Further a \\*\\*diff\\_close\\*\\* column was added, which\n",
    "consists of the relative difference between the **close** value at the\n",
    "current and the previous **time**. Note hat since **ticker** is always\n",
    "the same, we remove that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Load dataset\n",
    "val oilDS = spark.read.fx1m(\"dbfs:/FileStore/shared_uploads/fabiansi@kth.se/*csv.gz\").toDF.withColumn(\"ticker\", lit(\"BCOUSD\")).select($\"ticker\", $\"time\" as \"x\", $\"close\" as \"y\").as[TickerPoint].orderBy(\"time\")\n",
    "\n",
    "// Add column with difference from previous close value (expected 'x', 'y' column names)\n",
    "val windowSpec = Window.orderBy(\"x\")\n",
    "val oilDS1 = oilDS \n",
    ".withColumn(\"diff_close\", $\"y\" - when((lag(\"y\", 1).over(windowSpec)).isNull, 0).otherwise(lag(\"y\", 1).over(windowSpec)))\n",
    "\n",
    "// Rename variables\n",
    "val oilDS2 = oilDS1.withColumnRenamed(\"x\",\"time\").withColumnRenamed(\"y\",\"close\")\n",
    "\n",
    "// Remove incomplete data from first day (2010-11-14) and last day (2019-06-21)\n",
    "val oilDS3 = oilDS2.filter(to_date(oilDS2(\"time\")) >= lit(\"2010-11-15\") && to_date(oilDS2(\"time\")) <= lit(\"2019-06-20\"))\n",
    "\n",
    "// Add index column\n",
    "val windowSpec1 = Window.orderBy(\"time\")\n",
    "val oilDS4 = oilDS3\n",
    ".withColumn(\"index\", row_number().over(windowSpec1))\n",
    "\n",
    "// Drop ticker column\n",
    "val oilDS5 = oilDS4.drop(\"ticker\")\n",
    "\n",
    "// Store loaded data as temp view, to be accessible in Python\n",
    "oilDS5.createOrReplaceTempView(\"temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     oilDS: org.apache.spark.sql.Dataset[org.lamastex.spark.trendcalculus.TickerPoint] = [ticker: string, x: timestamp ... 1 more field]\n",
    ">     windowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@347f9beb\n",
    ">     oilDS1: org.apache.spark.sql.DataFrame = [ticker: string, x: timestamp ... 2 more fields]\n",
    ">     oilDS2: org.apache.spark.sql.DataFrame = [ticker: string, time: timestamp ... 2 more fields]\n",
    ">     oilDS3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ticker: string, time: timestamp ... 2 more fields]\n",
    ">     windowSpec1: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@3c3c818a\n",
    ">     oilDS4: org.apache.spark.sql.DataFrame = [ticker: string, time: timestamp ... 3 more fields]\n",
    ">     oilDS5: org.apache.spark.sql.DataFrame = [time: timestamp, close: double ... 2 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "### Preparing the data in Python\n",
    "\n",
    "Because the\n",
    "[TrendCalculus](https://github.com/lamastex/spark-trend-calculus)\n",
    "library we use is implemented in Scala and we want to do our\n",
    "implementation in Python, we have to make sure that the data loaded in\n",
    "Scala is correctly read in Python, before moving on. To that end, we\n",
    "select the first 10 data points and show them in a table.\n",
    "\n",
    "We can see that there are roughly **2.5 million data points** in the\n",
    "BCUSD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python imports\n",
    "import datetime\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Conv1D, MaxPool1D, Flatten, BatchNormalization\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe from temp data\n",
    "oilDF_py = spark.table(\"temp\")\n",
    "\n",
    "# Select the 10 first Rows of data and print them\n",
    "ten_oilDF_py = oilDF_py.limit(10)\n",
    "ten_oilDF_py.show()\n",
    "\n",
    "# Check number of data points\n",
    "last_index = oilDF_py.count()\n",
    "print(\"Number of data points: {}\".format(last_index))\n",
    "\n",
    "# Select the date of the last data point\n",
    "print(\"Last data point: {}\".format(np.array(oilDF_py.where(oilDF_py.index == last_index).select('time').collect()).item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +-------------------+-----+--------------------+-----+\n",
    ">     |               time|close|          diff_close|index|\n",
    ">     +-------------------+-----+--------------------+-----+\n",
    ">     |2010-11-15 00:00:00| 86.6|-0.01000000000000...|    1|\n",
    ">     |2010-11-15 00:01:00| 86.6|                 0.0|    2|\n",
    ">     |2010-11-15 00:02:00|86.63|0.030000000000001137|    3|\n",
    ">     |2010-11-15 00:03:00|86.61|-0.01999999999999602|    4|\n",
    ">     |2010-11-15 00:05:00|86.61|                 0.0|    5|\n",
    ">     |2010-11-15 00:07:00| 86.6|-0.01000000000000...|    6|\n",
    ">     |2010-11-15 00:08:00|86.58|-0.01999999999999602|    7|\n",
    ">     |2010-11-15 00:09:00|86.58|                 0.0|    8|\n",
    ">     |2010-11-15 00:10:00|86.58|                 0.0|    9|\n",
    ">     |2010-11-15 00:12:00|86.57|-0.01000000000000...|   10|\n",
    ">     +-------------------+-----+--------------------+-----+\n",
    ">\n",
    ">     Number of data points: 2523078\n",
    ">     Last data point: 2019-06-20 23:59:00\n",
    "\n",
    "  \n",
    "\n",
    "RL Environment\n",
    "--------------\n",
    "\n",
    "In order to train RL agents, we first need to create the environment\n",
    "with which the agent will interact to gather experience. In our case,\n",
    "that consist of a stock market simulation which plays out historical\n",
    "data from the BCUSD dataset. This is valid, under the assumption that\n",
    "the trading on the part of our agent has no affect on the stock market.\n",
    "An RL problem can be formally defined by a Markov Decision Process\n",
    "(MDP).\n",
    "\n",
    "For our application, we have the following MDP: - State, s: a window of\n",
    "**diff*close\\*\\* values for a given **scope**, i.e. the current value\n",
    "and history leading up to it. - Action, a: either **LONG** for buying\n",
    "stock, or **SHORT** for selling stock. Note that **PASS** is not\n",
    "required, since if stock is already owned, buying means holding and if\n",
    "stock is not owned then shorting means pass. - Reward, r: if\n",
    "a*t=**LONG\\*\\* r*t=s*t+1=\\*\\*diff\\_close\\*\\*; if a*t=**SHORT**\n",
    "r*t=-s\\_t+1=-\\*\\*diff\\_close\\*\\*. Essentially, the reward is negative if\n",
    "we sell and the stock goes up or if we buy and the stock goes down in\n",
    "the next timestep. Conversely, the reward is positive if we buy and the\n",
    "stock goes up or if we sell and the stock goes down in the next\n",
    "timestep.\n",
    "\n",
    "This environment is very simplified, with only binary actions. An\n",
    "alternative could be to use continuos actions to determine how much\n",
    "stock to buy or sell. However, since we aim to compare to TrendCalculus\n",
    "results which only predict reversals, these actions are more adequate.\n",
    "For the implementation, we used OpenAI Gym's formalism, which includes a\n",
    "**done** variable to indicate the end of an episode. In **MarketEnv**,\n",
    "by setting the \\*\\*start\\_date\\*\\* and \\*\\*end\\_date\\*\\* atttributes, we\n",
    "can select the part of the dataset we wish to use. Finally, the and\n",
    "\\*\\*episode\\_size\\*\\* parameter determines the episode size. An\n",
    "episode's starting point can be sampled at random or not, which is\n",
    "defined when calling **reset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://github.com/kh-kim/stock_market_reinforcement_learning/blob/master/market_env.py\n",
    "\n",
    "\n",
    "class MarketEnv(gym.Env):\n",
    "    def __init__(self, full_data, start_date, end_date, episode_size=30*24*60, scope=60):\n",
    "        self.episode_size = episode_size\n",
    "        self.actions = [\"LONG\", \"SHORT\"] \n",
    "        self.action_space = gym.spaces.Discrete(len(self.actions))\n",
    "        self.state_space = gym.spaces.Box(np.ones(scope) * -1, np.ones(scope))\n",
    "\n",
    "        self.diff_close = np.array(full_data.filter(full_data[\"time\"] > start_date).filter(full_data[\"time\"] <= end_date).select('diff_close').collect())\n",
    "        max_diff_close = np.max(self.diff_close)\n",
    "        self.diff_close = self.diff_close*max_diff_close\n",
    "        self.close = np.array(full_data.filter(full_data[\"time\"] > start_date).filter(full_data[\"time\"] <= end_date).select('close').collect())\n",
    "        self.num_ticks_train = np.shape(self.diff_close)[0]\n",
    "\n",
    "        self.scope = scope # N values to be included in a state vector\n",
    "        self.time_index = self.scope  # start N steps in, to ensure that we have enough past values for history \n",
    "        self.episode_init_time = self.time_index  # initial time index of the episode\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        info = {'index': int(self.time_index), 'close': float(self.close[self.time_index])}\n",
    "        self.time_index += 1\n",
    "        self.state = self.diff_close[self.time_index - self.scope:self.time_index]\n",
    "        self.reward = float( - (2 * action - 1) * self.state[-1] )\n",
    "        \n",
    "        # Check if done\n",
    "        if self.time_index - self.episode_init_time > self.episode_size:\n",
    "            self.done = True\n",
    "        if self.time_index > self.diff_close.shape[0] - self.scope -1:\n",
    "            self.done = True\n",
    "\n",
    "        return self.state, self.reward, self.done, info\n",
    "\n",
    "    def reset(self, random_starttime=True):\n",
    "        self.done = False\n",
    "        self.reward = 0.\n",
    "        self.time_index = self.scope \n",
    "        self.state = self.diff_close[self.time_index - self.scope:self.time_index]\n",
    "        \n",
    "        if random_starttime:\n",
    "            self.time_index += random.randint(0, self.num_ticks_train - self.scope)\n",
    "        \n",
    "        self.episode_init_time = self.time_index\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "    def seed(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "reward_sum = 0.\n",
    "\n",
    "# Verify environment for 1 hour\n",
    "start = datetime.datetime(2010, 11, 15, 0, 0)\n",
    "end = datetime.datetime(2010, 11, 15, 1, 0)\n",
    "\n",
    "env = MarketEnv(oilDF_py, start, end, episode_size=np.inf, scope=1)\n",
    "state = env.reset(random_starttime=False)\n",
    "done = False\n",
    "while not done:\n",
    "    states.append(state[-1])\n",
    "    # Take random actions\n",
    "    action = env.action_space.sample()\n",
    "    actions.append(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "print(\"Return = {}\".format(reward_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Return = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot samples\n",
    "timesteps = np.linspace(1,len(states),len(states))\n",
    "longs = np.argwhere(np.asarray(actions) ==  0)\n",
    "shorts = np.argwhere(np.asarray(actions) ==  1)\n",
    "states = np.asarray(states)\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n",
    "ax[0].grid(True)\n",
    "ax[0].plot(timesteps, states, label='diff_close')\n",
    "ax[0].plot(timesteps[longs], states[longs].flatten(), '*g', markersize=12, label='long')\n",
    "ax[0].plot(timesteps[shorts], states[shorts].flatten(), '*r', markersize=12, label='short')\n",
    "ax[0].set_ylabel(\"(s,a)\")\n",
    "ax[0].set_xlabel(\"Timestep\")\n",
    "ax[0].set_xlim(1,len(states))\n",
    "ax[0].set_xticks(np.arange(1, len(states), 1.0))\n",
    "ax[0].legend()\n",
    "ax[1].grid(True)\n",
    "ax[1].plot(timesteps, rewards, 'o-r')\n",
    "ax[1].set_ylabel(\"r\")\n",
    "ax[1].set_xlabel(\"Timestep\")\n",
    "ax[1].set_xlim(1,len(states))\n",
    "ax[1].set_xticks(np.arange(1, len(states), 1.0))\n",
    "plt.tight_layout()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "DQN Algorithm\n",
    "-------------\n",
    "\n",
    "Since we have discrete actions, we can use Q-learning to train our\n",
    "agent. Specifically we use the DQN algorithm with Experience Replay,\n",
    "which was first described in DeepMind's: [Playing Atari with Deep\n",
    "Reinforcement Learning](https://arxiv.org/pdf/1312.5602.pdf). The\n",
    "algorithm is described below, where equation \\[3\\], refers to the\n",
    "gradient: &lt;img src=\"https://imgur.com/eGhNC9m.png\" width=650&gt;\n",
    "\n",
    "&lt;img src=\"https://imgur.com/mvopoh8.png\" width=800&gt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://dbc-635ca498-e5f1.cloud.databricks.com/?o=445287446643905#notebook/4201196137758409/command/4201196137758410\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, done):\n",
    "        self.memory.append([states, done])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = model.output_shape[-1]\n",
    "\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim, 1))\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory, size=inputs.shape[0])):\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            done = self.memory[idx][1]\n",
    "\n",
    "            inputs[i:i + 1] = state_t\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = model.predict(state_t)[0]\n",
    "            Q_sa = np.max(model.predict(state_tp1)[0])\n",
    "            if done: # if done is True\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                # reward_t + gamma * max_a' Q(s', a')\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Training RL agent\n",
    "-----------------\n",
    "\n",
    "In order to train the RL agent, we use the data from 2014 to 2018,\n",
    "leaving the data from 2019 for testing. RL implementations are quite\n",
    "difficult to train, due to the large amount of parameters which need to\n",
    "be tuned. We have spent little time seraching for better hyperparameters\n",
    "as this was beyond the scope of the course. We have picked parameters\n",
    "based on a similar implementation of RL for trading, however we have\n",
    "designed an new Q-network, since the state is different in our\n",
    "implementation. Sine we are dealing wih sequential data, we could have\n",
    "opted for an RNN, however 1-dimensional CNNs are also a common choice\n",
    "which is less computationally heavy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://dbc-635ca498-e5f1.cloud.databricks.com/?o=445287446643905#notebook/4201196137758409/command/4201196137758410\n",
    "\n",
    "# RL parameters\n",
    "epsilon = .5  # exploration\n",
    "min_epsilon = 0.1\n",
    "max_memory = 5000\n",
    "batch_size = 512\n",
    "discount = 0.8\n",
    "\n",
    "# Environment parameters\n",
    "num_actions = 2  # [long, short]\n",
    "episodes = 500 # 100000\n",
    "episode_size = 1 * 1 * 60  # roughly an hour worth of data in each training episode\n",
    "\n",
    "# Define state sequence scope (approx. 1 hour)\n",
    "sequence_scope = 60\n",
    "input_shape = (batch_size, sequence_scope, 1)\n",
    "\n",
    "# Create Q Network\n",
    "hidden_size = 128\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, (5), strides=2, input_shape=input_shape[1:], activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2, strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(32, (5), strides=1, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2, strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(hidden_size, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_actions))\n",
    "opt = optimizers.Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "# Define training interval\n",
    "start = datetime.datetime(2010, 11, 15, 0, 0)\n",
    "end = datetime.datetime(2018, 12, 31, 23, 59)\n",
    "\n",
    "# Initialize Environment\n",
    "env = MarketEnv(oilDF_py, start, end, episode_size=episode_size, scope=sequence_scope)\n",
    "\n",
    "# Initialize experience replay object\n",
    "exp_replay = ExperienceReplay(max_memory=max_memory, discount=discount)\n",
    "\n",
    "# Train\n",
    "returns = []\n",
    "for e in range(1, episodes):\n",
    "    loss = 0.\n",
    "    counter = 0\n",
    "    reward_sum = 0.\n",
    "    done = False\n",
    "    \n",
    "    state = env.reset()\n",
    "    input_t = state.reshape(1, sequence_scope, 1) \n",
    "    \n",
    "    while not done:     \n",
    "        counter += 1\n",
    "        input_tm1 = input_t\n",
    "        # get next action\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(0, num_actions, size=1)\n",
    "        else:\n",
    "            q = model.predict(input_tm1)\n",
    "            action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        state, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        input_t = state.reshape(1, sequence_scope, 1)         \n",
    "\n",
    "        # store experience\n",
    "        exp_replay.remember([input_tm1, action, reward, input_t], done)\n",
    "\n",
    "        # adapt model\n",
    "        inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
    "        loss += model.train_on_batch(inputs, targets)\n",
    "    \n",
    "    \n",
    "    print(\"Episode {:03d}/{:d} | Average Loss {:.4f} | Cumulative Reward {:.4f}\".format(e, episodes, loss / counter, reward_sum))\n",
    "    epsilon = max(min_epsilon, epsilon * 0.99)\n",
    "    returns.append(reward_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Episode 001/500 | Average Loss 0.9243 | Cumulative Reward -0.0492\n",
    ">     Episode 002/500 | Average Loss 0.0431 | Cumulative Reward -0.2952\n",
    ">     Episode 003/500 | Average Loss 0.0102 | Cumulative Reward 0.0246\n",
    ">     Episode 004/500 | Average Loss 0.0332 | Cumulative Reward -0.3444\n",
    ">     Episode 005/500 | Average Loss 0.0366 | Cumulative Reward 0.4920\n",
    ">     Episode 006/500 | Average Loss 44.9978 | Cumulative Reward -0.0984\n",
    ">     Episode 007/500 | Average Loss 13892.3869 | Cumulative Reward -0.7626\n",
    ">     Episode 008/500 | Average Loss 695661.5894 | Cumulative Reward 1.9434\n",
    ">     Episode 009/500 | Average Loss 12746050.9732 | Cumulative Reward -0.2952\n",
    ">     Episode 010/500 | Average Loss 31002932.3811 | Cumulative Reward 0.0492\n",
    ">     Episode 011/500 | Average Loss 272621435.5164 | Cumulative Reward 0.4182\n",
    ">     Episode 012/500 | Average Loss 171599336.8525 | Cumulative Reward 0.4674\n",
    ">     Episode 013/500 | Average Loss 27726520.4590 | Cumulative Reward 0.1968\n",
    ">     Episode 014/500 | Average Loss 20788127.3443 | Cumulative Reward 0.0984\n",
    ">     Episode 015/500 | Average Loss 10797102.9508 | Cumulative Reward -0.0738\n",
    ">     Episode 016/500 | Average Loss 4081633.1680 | Cumulative Reward -0.0984\n",
    ">     Episode 017/500 | Average Loss 3617864.5246 | Cumulative Reward 0.5166\n",
    ">     Episode 018/500 | Average Loss 2102176.5451 | Cumulative Reward -0.9348\n",
    ">     Episode 019/500 | Average Loss 1819763.1424 | Cumulative Reward -0.9348\n",
    ">     Episode 020/500 | Average Loss 525313.2244 | Cumulative Reward 0.3198\n",
    ">     Episode 021/500 | Average Loss 590327.7751 | Cumulative Reward -0.5412\n",
    ">     Episode 022/500 | Average Loss 643538.9570 | Cumulative Reward -1.2792\n",
    ">     Episode 023/500 | Average Loss 506215.3632 | Cumulative Reward 0.2214\n",
    ">     Episode 024/500 | Average Loss 229747.6570 | Cumulative Reward 0.3690\n",
    ">     Episode 025/500 | Average Loss 118323.1301 | Cumulative Reward -0.1968\n",
    ">     Episode 026/500 | Average Loss 90964.7006 | Cumulative Reward 0.1476\n",
    ">     Episode 027/500 | Average Loss 47487.1318 | Cumulative Reward -0.4428\n",
    ">     Episode 028/500 | Average Loss 90195246.1188 | Cumulative Reward -3.2718\n",
    ">     Episode 029/500 | Average Loss 32692468.4262 | Cumulative Reward 0.1230\n",
    ">     Episode 030/500 | Average Loss 2332925.2500 | Cumulative Reward -0.3690\n",
    ">     Episode 031/500 | Average Loss 834450.7592 | Cumulative Reward -0.1968\n",
    ">     Episode 032/500 | Average Loss 283082.5282 | Cumulative Reward -0.3198\n",
    ">     Episode 033/500 | Average Loss 283707.8343 | Cumulative Reward 0.0000\n",
    ">     Episode 034/500 | Average Loss 231921.2454 | Cumulative Reward -0.1230\n",
    ">     Episode 035/500 | Average Loss 147320.1317 | Cumulative Reward 0.9594\n",
    ">     Episode 036/500 | Average Loss 128691.0975 | Cumulative Reward -0.5904\n",
    ">     Episode 037/500 | Average Loss 103319.1212 | Cumulative Reward 0.2952\n",
    ">     Episode 038/500 | Average Loss 89891.4415 | Cumulative Reward 2.8782\n",
    ">     Episode 039/500 | Average Loss 328345656.4238 | Cumulative Reward -0.6150\n",
    ">     Episode 040/500 | Average Loss 10943518558.4262 | Cumulative Reward -0.6888\n",
    ">     Episode 041/500 | Average Loss 162841831.4754 | Cumulative Reward 0.1476\n",
    ">     Episode 042/500 | Average Loss 37689431.5000 | Cumulative Reward 0.4182\n",
    ">     Episode 043/500 | Average Loss 72136614.4344 | Cumulative Reward 0.0738\n",
    ">     Episode 044/500 | Average Loss 56635724.9180 | Cumulative Reward -0.2706\n",
    ">     Episode 045/500 | Average Loss 10563238.0512 | Cumulative Reward 0.0492\n",
    ">     Episode 046/500 | Average Loss 132264149.6066 | Cumulative Reward 1.0824\n",
    ">     Episode 047/500 | Average Loss 41677930.5574 | Cumulative Reward 0.0246\n",
    ">     Episode 048/500 | Average Loss 9066351.1148 | Cumulative Reward -0.1230\n",
    ">     Episode 049/500 | Average Loss 2358803.1926 | Cumulative Reward 0.0246\n",
    ">     Episode 050/500 | Average Loss 1814690.9365 | Cumulative Reward -4.8708\n",
    ">     Episode 051/500 | Average Loss 1223300511.6383 | Cumulative Reward -1.4514\n",
    ">     Episode 052/500 | Average Loss 1202551068.6885 | Cumulative Reward -0.6888\n",
    ">     Episode 053/500 | Average Loss 48371521.6721 | Cumulative Reward -0.0738\n",
    ">     Episode 054/500 | Average Loss 3760147.6393 | Cumulative Reward -1.7712\n",
    ">     Episode 055/500 | Average Loss 1791821.9693 | Cumulative Reward -0.0246\n",
    ">     Episode 056/500 | Average Loss 2677952.7643 | Cumulative Reward -0.1230\n",
    ">     Episode 057/500 | Average Loss 4241063.5451 | Cumulative Reward 0.4674\n",
    ">     Episode 058/500 | Average Loss 3664478.7951 | Cumulative Reward -0.3198\n",
    ">     Episode 059/500 | Average Loss 2111481.0102 | Cumulative Reward 0.0492\n",
    ">     Episode 060/500 | Average Loss 1312407.7039 | Cumulative Reward -0.3690\n",
    ">     Episode 061/500 | Average Loss 1215967.5225 | Cumulative Reward -1.6974\n",
    ">     Episode 062/500 | Average Loss 1206593.6096 | Cumulative Reward -0.8118\n",
    ">     Episode 063/500 | Average Loss 1242360.7818 | Cumulative Reward 0.9348\n",
    ">     Episode 064/500 | Average Loss 911372.9959 | Cumulative Reward 0.8856\n",
    ">     Episode 065/500 | Average Loss 983090.6424 | Cumulative Reward 0.2460\n",
    ">     Episode 066/500 | Average Loss 56758439.2228 | Cumulative Reward -0.5166\n",
    ">     Episode 067/500 | Average Loss 1265408.1035 | Cumulative Reward -0.0246\n",
    ">     Episode 068/500 | Average Loss 5961294.6898 | Cumulative Reward -0.1722\n",
    ">     Episode 069/500 | Average Loss 142104936.0943 | Cumulative Reward -1.8450\n",
    ">     Episode 070/500 | Average Loss 604659424.3279 | Cumulative Reward -1.7466\n",
    ">     Episode 071/500 | Average Loss 2669601345.0492 | Cumulative Reward -0.7872\n",
    ">     Episode 072/500 | Average Loss 6518834110.9508 | Cumulative Reward -0.3198\n",
    ">     Episode 073/500 | Average Loss 660690072.6557 | Cumulative Reward 0.3936\n",
    ">     Episode 074/500 | Average Loss 49482324.3770 | Cumulative Reward 0.2706\n",
    ">     Episode 075/500 | Average Loss 12702479.1721 | Cumulative Reward 2.2140\n",
    ">     Episode 076/500 | Average Loss 31520307.4426 | Cumulative Reward -0.6642\n",
    ">     Episode 077/500 | Average Loss 30264551.5410 | Cumulative Reward 0.1230\n",
    ">     Episode 078/500 | Average Loss 23348717.7705 | Cumulative Reward -0.8610\n",
    ">     Episode 079/500 | Average Loss 10719680.3934 | Cumulative Reward -1.5006\n",
    ">     Episode 080/500 | Average Loss 5350704.6926 | Cumulative Reward 0.2952\n",
    ">     Episode 081/500 | Average Loss 4198182.6107 | Cumulative Reward 0.5412\n",
    ">     Episode 082/500 | Average Loss 3379803.6189 | Cumulative Reward 3.6162\n",
    ">     Episode 083/500 | Average Loss 129381317.8607 | Cumulative Reward -0.0246\n",
    ">     Episode 084/500 | Average Loss 220882119.9344 | Cumulative Reward -0.0000\n",
    ">     Episode 085/500 | Average Loss 66604575.5410 | Cumulative Reward -0.0738\n",
    ">     Episode 086/500 | Average Loss 274207500.3934 | Cumulative Reward -1.1808\n",
    ">     Episode 087/500 | Average Loss 256016848.9549 | Cumulative Reward -0.5904\n",
    ">     Episode 088/500 | Average Loss 150811378.3607 | Cumulative Reward -0.0984\n",
    ">     Episode 089/500 | Average Loss 64320583.2131 | Cumulative Reward 0.1476\n",
    ">     Episode 090/500 | Average Loss 30557816.7213 | Cumulative Reward 0.3690\n",
    ">     Episode 091/500 | Average Loss 14250694.0328 | Cumulative Reward -0.2214\n",
    ">     Episode 092/500 | Average Loss 7108390.2541 | Cumulative Reward 0.4428\n",
    ">     Episode 093/500 | Average Loss 3342842.4262 | Cumulative Reward 0.8610\n",
    ">     Episode 094/500 | Average Loss 1480352.2623 | Cumulative Reward -0.1230\n",
    ">     Episode 095/500 | Average Loss 665809.5815 | Cumulative Reward -0.1968\n",
    ">     Episode 096/500 | Average Loss 357767.4959 | Cumulative Reward 1.1562\n",
    ">     Episode 097/500 | Average Loss 239387.9193 | Cumulative Reward 0.1968\n",
    ">     Episode 098/500 | Average Loss 204709.5968 | Cumulative Reward -0.7380\n",
    ">     Episode 099/500 | Average Loss 166877.2141 | Cumulative Reward 0.5658\n",
    ">     Episode 100/500 | Average Loss 155712.9769 | Cumulative Reward 0.9102\n",
    ">     Episode 101/500 | Average Loss 144984.7732 | Cumulative Reward 1.2300\n",
    ">     Episode 102/500 | Average Loss 138251.9153 | Cumulative Reward 0.2214\n",
    ">     Episode 103/500 | Average Loss 127553.8490 | Cumulative Reward 0.3444\n",
    ">     Episode 104/500 | Average Loss 124246.1187 | Cumulative Reward 0.9348\n",
    ">     Episode 105/500 | Average Loss 118589.2683 | Cumulative Reward 1.5498\n",
    ">     Episode 106/500 | Average Loss 125549.7988 | Cumulative Reward 0.6888\n",
    ">     Episode 107/500 | Average Loss 134375.3892 | Cumulative Reward 0.1722\n",
    ">     Episode 108/500 | Average Loss 148787.7741 | Cumulative Reward -0.1230\n",
    ">     Episode 109/500 | Average Loss 164604.2537 | Cumulative Reward 1.2054\n",
    ">     Episode 110/500 | Average Loss 199937.5569 | Cumulative Reward 0.0000\n",
    ">     Episode 111/500 | Average Loss 191672.2460 | Cumulative Reward -0.3444\n",
    ">     Episode 112/500 | Average Loss 258762.1554 | Cumulative Reward 0.1230\n",
    ">     Episode 113/500 | Average Loss 289838.2514 | Cumulative Reward -0.5904\n",
    ">     Episode 114/500 | Average Loss 465530.3043 | Cumulative Reward -1.7220\n",
    ">     Episode 115/500 | Average Loss 415042.3491 | Cumulative Reward 0.0246\n",
    ">     Episode 116/500 | Average Loss 309605.8963 | Cumulative Reward -0.3444\n",
    ">     Episode 117/500 | Average Loss 262915.5269 | Cumulative Reward -0.0246\n",
    ">     Episode 118/500 | Average Loss 276877.2044 | Cumulative Reward 0.1722\n",
    ">     Episode 119/500 | Average Loss 297945.2974 | Cumulative Reward -0.8610\n",
    ">     Episode 120/500 | Average Loss 252190.8712 | Cumulative Reward 0.3198\n",
    ">     Episode 121/500 | Average Loss 209873.6153 | Cumulative Reward -0.0984\n",
    ">     Episode 122/500 | Average Loss 200975.3696 | Cumulative Reward -0.4182\n",
    ">     Episode 123/500 | Average Loss 224392.9705 | Cumulative Reward 0.6396\n",
    ">     Episode 124/500 | Average Loss 175272.5402 | Cumulative Reward -0.1230\n",
    ">     Episode 125/500 | Average Loss 134914.6336 | Cumulative Reward 0.0738\n",
    ">     Episode 126/500 | Average Loss 118408.8128 | Cumulative Reward 0.9102\n",
    ">     Episode 127/500 | Average Loss 119856.1210 | Cumulative Reward -0.1476\n",
    ">     Episode 128/500 | Average Loss 128668.1934 | Cumulative Reward 0.4182\n",
    ">     Episode 129/500 | Average Loss 121379.1356 | Cumulative Reward 0.3444\n",
    ">     Episode 130/500 | Average Loss 122105.9212 | Cumulative Reward 0.8364\n",
    ">     Episode 131/500 | Average Loss 126979.4586 | Cumulative Reward 0.4428\n",
    ">     Episode 132/500 | Average Loss 159263.2796 | Cumulative Reward 1.4268\n",
    ">     Episode 133/500 | Average Loss 249200.5343 | Cumulative Reward -0.3690\n",
    ">     Episode 134/500 | Average Loss 351498.9006 | Cumulative Reward 0.2460\n",
    ">     Episode 135/500 | Average Loss 344710.3376 | Cumulative Reward 0.4920\n",
    ">     Episode 136/500 | Average Loss 290292.0177 | Cumulative Reward 0.3444\n",
    ">     Episode 137/500 | Average Loss 203908.2672 | Cumulative Reward -1.1808\n",
    ">     Episode 138/500 | Average Loss 152643.9161 | Cumulative Reward -0.3690\n",
    ">     Episode 139/500 | Average Loss 103234.8929 | Cumulative Reward 0.3690\n",
    ">     Episode 140/500 | Average Loss 81400.9171 | Cumulative Reward -0.8364\n",
    ">     Episode 141/500 | Average Loss 70417.8615 | Cumulative Reward -0.0246\n",
    ">     Episode 142/500 | Average Loss 62871.1321 | Cumulative Reward -0.1968\n",
    ">     Episode 143/500 | Average Loss 66203.8933 | Cumulative Reward 1.3284\n",
    ">     Episode 144/500 | Average Loss 80965.3102 | Cumulative Reward -0.7380\n",
    ">     Episode 145/500 | Average Loss 85374.2427 | Cumulative Reward 0.0738\n",
    ">     Episode 146/500 | Average Loss 74547.1055 | Cumulative Reward -0.2952\n",
    ">     Episode 147/500 | Average Loss 61201.4695 | Cumulative Reward -0.0000\n",
    ">     Episode 148/500 | Average Loss 58691.6379 | Cumulative Reward -0.7872\n",
    ">     Episode 149/500 | Average Loss 53623.2068 | Cumulative Reward -0.1968\n",
    ">     Episode 150/500 | Average Loss 52295.3436 | Cumulative Reward 0.5658\n",
    ">     Episode 151/500 | Average Loss 54563.6969 | Cumulative Reward -0.7528\n",
    ">     Episode 152/500 | Average Loss 56892.9392 | Cumulative Reward -1.5990\n",
    ">     Episode 153/500 | Average Loss 56162.3126 | Cumulative Reward 0.1968\n",
    ">     Episode 154/500 | Average Loss 57015.3386 | Cumulative Reward -0.1722\n",
    ">     Episode 155/500 | Average Loss 61344.5501 | Cumulative Reward 0.2214\n",
    ">     Episode 156/500 | Average Loss 63970.4605 | Cumulative Reward 0.1968\n",
    ">     Episode 157/500 | Average Loss 64938.6926 | Cumulative Reward 1.3776\n",
    ">     Episode 158/500 | Average Loss 71837.3588 | Cumulative Reward -0.4428\n",
    ">     Episode 159/500 | Average Loss 69606.6803 | Cumulative Reward 1.1316\n",
    ">     Episode 160/500 | Average Loss 51512.9089 | Cumulative Reward -0.4428\n",
    ">     Episode 161/500 | Average Loss 42763.9575 | Cumulative Reward -0.6642\n",
    ">     Episode 162/500 | Average Loss 34133.1615 | Cumulative Reward 0.0492\n",
    ">     Episode 163/500 | Average Loss 26055.7404 | Cumulative Reward -0.7380\n",
    ">     Episode 164/500 | Average Loss 17645.5418 | Cumulative Reward 0.3198\n",
    ">     Episode 165/500 | Average Loss 23771.6887 | Cumulative Reward -0.6888\n",
    ">     Episode 166/500 | Average Loss 33822.2709 | Cumulative Reward -1.4760\n",
    ">     Episode 167/500 | Average Loss 36789.7859 | Cumulative Reward -0.3444\n",
    ">     Episode 168/500 | Average Loss 30012.2132 | Cumulative Reward 0.3198\n",
    ">     Episode 169/500 | Average Loss 25915.5318 | Cumulative Reward 0.0246\n",
    ">     Episode 170/500 | Average Loss 21197.9396 | Cumulative Reward 0.4182\n",
    ">     Episode 171/500 | Average Loss 18828.6276 | Cumulative Reward 0.6642\n",
    ">     Episode 172/500 | Average Loss 12931.5156 | Cumulative Reward 0.1230\n",
    ">     Episode 173/500 | Average Loss 9384.6666 | Cumulative Reward -0.5412\n",
    ">     Episode 174/500 | Average Loss 9953.6688 | Cumulative Reward 0.0246\n",
    ">     Episode 175/500 | Average Loss 14217.0748 | Cumulative Reward 0.0738\n",
    ">     Episode 176/500 | Average Loss 18767.0269 | Cumulative Reward -0.2706\n",
    ">     Episode 177/500 | Average Loss 17302.4695 | Cumulative Reward 0.4674\n",
    ">     Episode 178/500 | Average Loss 12386.1551 | Cumulative Reward 0.0984\n",
    ">     Episode 179/500 | Average Loss 8794.3302 | Cumulative Reward 0.9348\n",
    ">     Episode 180/500 | Average Loss 8130.0289 | Cumulative Reward -0.2214\n",
    ">     Episode 181/500 | Average Loss 8009.3913 | Cumulative Reward -0.3198\n",
    ">     Episode 182/500 | Average Loss 6421.1121 | Cumulative Reward -0.0492\n",
    ">     Episode 183/500 | Average Loss 5636.7044 | Cumulative Reward 0.1968\n",
    ">     Episode 184/500 | Average Loss 8854.0772 | Cumulative Reward 0.4920\n",
    ">     Episode 185/500 | Average Loss 10231.7754 | Cumulative Reward 0.9840\n",
    ">     Episode 186/500 | Average Loss 8491.7384 | Cumulative Reward -0.1722\n",
    ">     Episode 187/500 | Average Loss 5335.5632 | Cumulative Reward -0.8856\n",
    ">     Episode 188/500 | Average Loss 4152.4838 | Cumulative Reward 1.1316\n",
    ">     Episode 189/500 | Average Loss 3644.6625 | Cumulative Reward 1.3284\n",
    ">     Episode 190/500 | Average Loss 4318.2997 | Cumulative Reward -0.2460\n",
    ">     Episode 191/500 | Average Loss 4694.9497 | Cumulative Reward -0.0492\n",
    ">     Episode 192/500 | Average Loss 3490.6077 | Cumulative Reward 0.4674\n",
    ">     Episode 193/500 | Average Loss 3043.5791 | Cumulative Reward 0.2903\n",
    ">     Episode 194/500 | Average Loss 2338.0377 | Cumulative Reward 0.4182\n",
    ">     Episode 195/500 | Average Loss 2115.3294 | Cumulative Reward 1.5990\n",
    ">     Episode 196/500 | Average Loss 3127.7692 | Cumulative Reward -1.0578\n",
    ">     Episode 197/500 | Average Loss 4421.3908 | Cumulative Reward -0.2214\n",
    ">     Episode 198/500 | Average Loss 2560.3734 | Cumulative Reward 0.2706\n",
    ">     Episode 199/500 | Average Loss 2147.2506 | Cumulative Reward -1.5006\n",
    ">     Episode 200/500 | Average Loss 2207.9893 | Cumulative Reward -0.5658\n",
    ">     Episode 201/500 | Average Loss 3174.7853 | Cumulative Reward -1.4760\n",
    ">     Episode 202/500 | Average Loss 11331.6762 | Cumulative Reward 0.3198\n",
    ">     Episode 203/500 | Average Loss 37899.0565 | Cumulative Reward -0.6150\n",
    ">     Episode 204/500 | Average Loss 36401.8491 | Cumulative Reward -0.1230\n",
    ">     Episode 205/500 | Average Loss 12925.8751 | Cumulative Reward -0.2706\n",
    ">     Episode 206/500 | Average Loss 12854.6668 | Cumulative Reward -0.3936\n",
    ">     Episode 207/500 | Average Loss 11591.4458 | Cumulative Reward -0.1968\n",
    ">     Episode 208/500 | Average Loss 21179.2468 | Cumulative Reward 0.4920\n",
    ">     Episode 209/500 | Average Loss 20376.6122 | Cumulative Reward 0.1968\n",
    ">     Episode 210/500 | Average Loss 18849.6060 | Cumulative Reward 0.0000\n",
    ">     Episode 211/500 | Average Loss 13982.4748 | Cumulative Reward -0.2214\n",
    ">     Episode 212/500 | Average Loss 87311.4121 | Cumulative Reward -0.8856\n",
    ">     Episode 213/500 | Average Loss 654473.7298 | Cumulative Reward 0.0492\n",
    ">     Episode 214/500 | Average Loss 201913.9874 | Cumulative Reward -0.2214\n",
    ">     Episode 215/500 | Average Loss 44836.9004 | Cumulative Reward 0.1968\n",
    ">     Episode 216/500 | Average Loss 35657.7584 | Cumulative Reward 0.1968\n",
    ">     Episode 217/500 | Average Loss 32079.6191 | Cumulative Reward -0.0246\n",
    ">     Episode 218/500 | Average Loss 30304.5018 | Cumulative Reward -0.0492\n",
    ">     Episode 219/500 | Average Loss 30373.2290 | Cumulative Reward -0.1968\n",
    ">     Episode 220/500 | Average Loss 29203.3995 | Cumulative Reward 0.0492\n",
    ">     Episode 221/500 | Average Loss 31887.5387 | Cumulative Reward 0.4428\n",
    ">     Episode 222/500 | Average Loss 27344.9023 | Cumulative Reward -0.2706\n",
    ">     Episode 223/500 | Average Loss 26080.0415 | Cumulative Reward 1.2546\n",
    ">     Episode 224/500 | Average Loss 27963.8084 | Cumulative Reward -0.2952\n",
    ">     Episode 225/500 | Average Loss 22244.3293 | Cumulative Reward 0.4674\n",
    ">     Episode 226/500 | Average Loss 19426.3250 | Cumulative Reward 0.1476\n",
    ">     Episode 227/500 | Average Loss 19027.4235 | Cumulative Reward 3.0504\n",
    ">     Episode 228/500 | Average Loss 22579.4337 | Cumulative Reward 1.3284\n",
    ">     Episode 229/500 | Average Loss 35449.1802 | Cumulative Reward -0.0000\n",
    ">     Episode 230/500 | Average Loss 36867.8054 | Cumulative Reward -0.3936\n",
    ">     Episode 231/500 | Average Loss 49915.4290 | Cumulative Reward -0.3936\n",
    ">     Episode 232/500 | Average Loss 32619.6804 | Cumulative Reward -0.6396\n",
    ">     Episode 233/500 | Average Loss 12634.1564 | Cumulative Reward 1.6728\n",
    ">     Episode 234/500 | Average Loss 11710.0142 | Cumulative Reward -0.2952\n",
    ">     Episode 235/500 | Average Loss 11534.9517 | Cumulative Reward 0.0246\n",
    ">     Episode 236/500 | Average Loss 21333.6566 | Cumulative Reward -0.1476\n",
    ">     Episode 237/500 | Average Loss 14613.8575 | Cumulative Reward -1.2792\n",
    ">     Episode 238/500 | Average Loss 18945.5421 | Cumulative Reward 0.1968\n",
    ">     Episode 239/500 | Average Loss 11825.2040 | Cumulative Reward 1.4268\n",
    ">     Episode 240/500 | Average Loss 7433.5705 | Cumulative Reward -0.9594\n",
    ">     Episode 241/500 | Average Loss 4902.9148 | Cumulative Reward -0.4674\n",
    ">     Episode 242/500 | Average Loss 3179.8172 | Cumulative Reward 0.6888\n",
    ">     Episode 243/500 | Average Loss 3503.6598 | Cumulative Reward -0.6150\n",
    ">     Episode 244/500 | Average Loss 3431.9749 | Cumulative Reward -0.7134\n",
    ">     Episode 245/500 | Average Loss 2877.5648 | Cumulative Reward -0.0246\n",
    ">     Episode 246/500 | Average Loss 2608.7916 | Cumulative Reward -0.3690\n",
    ">     Episode 247/500 | Average Loss 2049.1479 | Cumulative Reward -0.1476\n",
    ">     Episode 248/500 | Average Loss 1290.3437 | Cumulative Reward -0.0246\n",
    ">     Episode 249/500 | Average Loss 925.7207 | Cumulative Reward 0.5412\n",
    ">     Episode 250/500 | Average Loss 823.1600 | Cumulative Reward 0.3444\n",
    ">     Episode 251/500 | Average Loss 1006.1303 | Cumulative Reward 0.0246\n",
    ">     Episode 252/500 | Average Loss 1000.9536 | Cumulative Reward 0.1968\n",
    ">     Episode 253/500 | Average Loss 910.4431 | Cumulative Reward 0.0492\n",
    ">     Episode 254/500 | Average Loss 793.4383 | Cumulative Reward -0.0738\n",
    ">     Episode 255/500 | Average Loss 960.7262 | Cumulative Reward -0.0492\n",
    ">     Episode 256/500 | Average Loss 1210.5615 | Cumulative Reward 0.7626\n",
    ">     Episode 257/500 | Average Loss 1456.9758 | Cumulative Reward -0.0984\n",
    ">     Episode 258/500 | Average Loss 1492.6155 | Cumulative Reward -0.2706\n",
    ">     Episode 259/500 | Average Loss 800.3773 | Cumulative Reward -0.1968\n",
    ">     Episode 260/500 | Average Loss 663.3038 | Cumulative Reward -0.5166\n",
    ">     Episode 261/500 | Average Loss 570.7030 | Cumulative Reward 0.5412\n",
    ">     Episode 262/500 | Average Loss 515.7673 | Cumulative Reward -0.0246\n",
    ">     Episode 263/500 | Average Loss 914.1635 | Cumulative Reward 0.3936\n",
    ">     Episode 264/500 | Average Loss 635.1886 | Cumulative Reward 0.5166\n",
    ">     Episode 265/500 | Average Loss 503.3200 | Cumulative Reward 0.3936\n",
    ">     Episode 266/500 | Average Loss 442.9404 | Cumulative Reward 0.1722\n",
    ">     Episode 267/500 | Average Loss 372.8969 | Cumulative Reward -0.1722\n",
    ">     Episode 268/500 | Average Loss 342.8540 | Cumulative Reward 0.8610\n",
    ">     Episode 269/500 | Average Loss 336.0440 | Cumulative Reward 1.1562\n",
    ">     Episode 270/500 | Average Loss 325.4765 | Cumulative Reward 1.4022\n",
    ">     Episode 271/500 | Average Loss 639.1140 | Cumulative Reward 0.0738\n",
    ">     Episode 272/500 | Average Loss 467.9056 | Cumulative Reward -1.3530\n",
    ">     Episode 273/500 | Average Loss 691.2283 | Cumulative Reward -0.8856\n",
    ">     Episode 274/500 | Average Loss 442.4127 | Cumulative Reward -0.0246\n",
    ">     Episode 275/500 | Average Loss 672.4676 | Cumulative Reward -0.0000\n",
    ">     Episode 276/500 | Average Loss 373.9056 | Cumulative Reward 0.7626\n",
    ">     Episode 277/500 | Average Loss 510.2552 | Cumulative Reward 0.0492\n",
    ">     Episode 278/500 | Average Loss 476.9959 | Cumulative Reward -0.0246\n",
    ">     Episode 279/500 | Average Loss 374.8663 | Cumulative Reward 0.4182\n",
    ">     Episode 280/500 | Average Loss 285.5737 | Cumulative Reward 0.2214\n",
    ">     Episode 281/500 | Average Loss 359.3125 | Cumulative Reward 0.0738\n",
    ">     Episode 282/500 | Average Loss 509.2474 | Cumulative Reward 0.0738\n",
    ">     Episode 283/500 | Average Loss 509.7669 | Cumulative Reward -0.9840\n",
    ">     Episode 284/500 | Average Loss 848.4750 | Cumulative Reward -0.1968\n",
    ">     Episode 285/500 | Average Loss 746.0073 | Cumulative Reward 0.6396\n",
    ">     Episode 286/500 | Average Loss 354.4148 | Cumulative Reward 0.4920\n",
    ">     Episode 287/500 | Average Loss 744.5271 | Cumulative Reward -1.2546\n",
    ">     Episode 288/500 | Average Loss 1177.0523 | Cumulative Reward -0.0246\n",
    ">     Episode 289/500 | Average Loss 912.7724 | Cumulative Reward -0.0000\n",
    ">     Episode 290/500 | Average Loss 493.3918 | Cumulative Reward 0.7872\n",
    ">     Episode 291/500 | Average Loss 666.1050 | Cumulative Reward -0.4428\n",
    ">     Episode 292/500 | Average Loss 855.3613 | Cumulative Reward -2.3124\n",
    ">     Episode 293/500 | Average Loss 558.7653 | Cumulative Reward -0.6396\n",
    ">     Episode 294/500 | Average Loss 418.6678 | Cumulative Reward -0.6396\n",
    ">     Episode 295/500 | Average Loss 534.7206 | Cumulative Reward 0.1722\n",
    ">     Episode 296/500 | Average Loss 264.4277 | Cumulative Reward 0.1230\n",
    ">     Episode 297/500 | Average Loss 389.4987 | Cumulative Reward -0.2214\n",
    ">     Episode 298/500 | Average Loss 728.0504 | Cumulative Reward 0.7134\n",
    ">     Episode 299/500 | Average Loss 632.7627 | Cumulative Reward -0.3198\n",
    ">     Episode 300/500 | Average Loss 637.5207 | Cumulative Reward -0.9348\n",
    ">     Episode 301/500 | Average Loss 1028.3571 | Cumulative Reward -0.2952\n",
    ">     Episode 302/500 | Average Loss 1685.8779 | Cumulative Reward 0.0246\n",
    ">     Episode 303/500 | Average Loss 1392.4933 | Cumulative Reward 0.6150\n",
    ">     Episode 304/500 | Average Loss 788.6780 | Cumulative Reward -0.2952\n",
    ">     Episode 305/500 | Average Loss 1226.8964 | Cumulative Reward 0.7134\n",
    ">     Episode 306/500 | Average Loss 771.0202 | Cumulative Reward -0.1968\n",
    ">     Episode 307/500 | Average Loss 1038.6742 | Cumulative Reward 1.4514\n",
    ">     Episode 308/500 | Average Loss 2320.5486 | Cumulative Reward -0.5658\n",
    ">     Episode 309/500 | Average Loss 1688.7067 | Cumulative Reward -0.8364\n",
    ">     Episode 310/500 | Average Loss 1235.5575 | Cumulative Reward 1.0824\n",
    ">     Episode 311/500 | Average Loss 314.3540 | Cumulative Reward 0.4920\n",
    ">     Episode 312/500 | Average Loss 221.4066 | Cumulative Reward -0.4674\n",
    ">     Episode 313/500 | Average Loss 149.2911 | Cumulative Reward -1.4022\n",
    ">     Episode 314/500 | Average Loss 251.1208 | Cumulative Reward 0.2952\n",
    ">     Episode 315/500 | Average Loss 295.4088 | Cumulative Reward 0.0984\n",
    ">     Episode 316/500 | Average Loss 353.4387 | Cumulative Reward 0.2214\n",
    ">     Episode 317/500 | Average Loss 471.9230 | Cumulative Reward -0.1968\n",
    ">     Episode 318/500 | Average Loss 481.8251 | Cumulative Reward -0.0246\n",
    ">     Episode 319/500 | Average Loss 277.8028 | Cumulative Reward -0.2706\n",
    ">     Episode 320/500 | Average Loss 458.1981 | Cumulative Reward 0.7134\n",
    ">     Episode 321/500 | Average Loss 554.8020 | Cumulative Reward 0.0984\n",
    ">     Episode 322/500 | Average Loss 712.4146 | Cumulative Reward -0.1722\n",
    ">     Episode 323/500 | Average Loss 533.7365 | Cumulative Reward 1.5252\n",
    ">     Episode 324/500 | Average Loss 361.0181 | Cumulative Reward 0.8118\n",
    ">     Episode 325/500 | Average Loss 447.6775 | Cumulative Reward 1.0332\n",
    ">     Episode 326/500 | Average Loss 1355.3373 | Cumulative Reward 0.0000\n",
    ">     Episode 327/500 | Average Loss 908.2948 | Cumulative Reward -0.0984\n",
    ">     Episode 328/500 | Average Loss 959.7786 | Cumulative Reward -1.2054\n",
    ">     Episode 329/500 | Average Loss 450.1697 | Cumulative Reward -0.7134\n",
    ">     Episode 330/500 | Average Loss 1212.9733 | Cumulative Reward -0.5166\n",
    ">     Episode 331/500 | Average Loss 5285.4580 | Cumulative Reward 0.0738\n",
    ">     Episode 332/500 | Average Loss 3094.1890 | Cumulative Reward 0.3690\n",
    ">     Episode 333/500 | Average Loss 4562.0196 | Cumulative Reward -0.4674\n",
    ">     Episode 334/500 | Average Loss 8924.8034 | Cumulative Reward 1.0824\n",
    ">     Episode 335/500 | Average Loss 30994.9012 | Cumulative Reward -1.1808\n",
    ">     Episode 336/500 | Average Loss 534143731.8066 | Cumulative Reward -0.1476\n",
    ">     Episode 337/500 | Average Loss 5921050444109.0820 | Cumulative Reward -0.4428\n",
    ">     Episode 338/500 | Average Loss 4505985552.3279 | Cumulative Reward 0.1230\n",
    ">     Episode 339/500 | Average Loss 1964492739.4754 | Cumulative Reward 0.1968\n",
    ">     Episode 340/500 | Average Loss 1593002800.1967 | Cumulative Reward 0.5486\n",
    ">     Episode 341/500 | Average Loss 1377590111.9344 | Cumulative Reward 0.0984\n",
    ">     Episode 342/500 | Average Loss 1024536314.2295 | Cumulative Reward 0.3936\n",
    ">     Episode 343/500 | Average Loss 714535699.4098 | Cumulative Reward -1.0824\n",
    ">     Episode 344/500 | Average Loss 581171856.7541 | Cumulative Reward -0.7626\n",
    ">     Episode 345/500 | Average Loss 1080491153.4754 | Cumulative Reward -0.3444\n",
    ">     Episode 346/500 | Average Loss 1290483386.2951 | Cumulative Reward -0.1968\n",
    ">     Episode 347/500 | Average Loss 638285799.5984 | Cumulative Reward 0.0492\n",
    ">     Episode 348/500 | Average Loss 533999203.1926 | Cumulative Reward 0.1968\n",
    ">     Episode 349/500 | Average Loss 1186640501.3566 | Cumulative Reward -0.4674\n",
    ">     Episode 350/500 | Average Loss 836347220.6230 | Cumulative Reward 2.1402\n",
    ">     Episode 351/500 | Average Loss 19564334.5430 | Cumulative Reward 0.2460\n",
    ">     Episode 352/500 | Average Loss 15821917.5266 | Cumulative Reward -0.0738\n",
    ">     Episode 353/500 | Average Loss 8794998.8955 | Cumulative Reward 0.7380\n",
    ">     Episode 354/500 | Average Loss 16293582.7008 | Cumulative Reward -0.1722\n",
    ">     Episode 355/500 | Average Loss 17792936.4262 | Cumulative Reward -0.7872\n",
    ">     Episode 356/500 | Average Loss 19392693.2541 | Cumulative Reward 0.0246\n",
    ">     Episode 357/500 | Average Loss 12951887.3934 | Cumulative Reward -0.3690\n",
    ">     Episode 358/500 | Average Loss 21376279.0902 | Cumulative Reward -0.1968\n",
    ">     Episode 359/500 | Average Loss 18399117.6311 | Cumulative Reward 0.3198\n",
    ">     Episode 360/500 | Average Loss 16349954.5328 | Cumulative Reward -0.0492\n",
    ">     Episode 361/500 | Average Loss 10212129.2971 | Cumulative Reward 0.0246\n",
    ">     Episode 362/500 | Average Loss 9528178.3489 | Cumulative Reward 0.5166\n",
    ">     Episode 363/500 | Average Loss 10891389.6680 | Cumulative Reward -0.2952\n",
    ">     Episode 364/500 | Average Loss 8848364.4677 | Cumulative Reward -0.1722\n",
    ">     Episode 365/500 | Average Loss 7967370.4139 | Cumulative Reward -0.7872\n",
    ">     Episode 366/500 | Average Loss 10170590.0517 | Cumulative Reward 1.1808\n",
    ">     Episode 367/500 | Average Loss 7691382.5763 | Cumulative Reward 0.6150\n",
    ">     Episode 368/500 | Average Loss 19905810.8770 | Cumulative Reward -0.4920\n",
    ">     Episode 369/500 | Average Loss 29691840.6557 | Cumulative Reward -0.1476\n",
    ">     Episode 370/500 | Average Loss 26487591.1967 | Cumulative Reward 0.1230\n",
    ">     Episode 371/500 | Average Loss 23137364.8689 | Cumulative Reward -0.3936\n",
    ">     Episode 372/500 | Average Loss 30061004.7541 | Cumulative Reward -0.3444\n",
    ">     Episode 373/500 | Average Loss 40822205.1148 | Cumulative Reward 0.2460\n",
    ">     Episode 374/500 | Average Loss 31239984.0328 | Cumulative Reward 0.5166\n",
    ">     Episode 375/500 | Average Loss 32901997.1639 | Cumulative Reward 0.0246\n",
    ">     Episode 376/500 | Average Loss 22117612.4918 | Cumulative Reward -0.1968\n",
    ">     Episode 377/500 | Average Loss 21721578.4795 | Cumulative Reward 0.3690\n",
    ">     Episode 378/500 | Average Loss 19514039.5656 | Cumulative Reward -0.4428\n",
    ">     Episode 379/500 | Average Loss 18671316.6721 | Cumulative Reward -0.2214\n",
    ">     Episode 380/500 | Average Loss 19410190.3115 | Cumulative Reward 0.3936\n",
    ">     Episode 381/500 | Average Loss 37436785.1475 | Cumulative Reward 0.6642\n",
    ">     Episode 382/500 | Average Loss 12312954.6025 | Cumulative Reward 0.1476\n",
    ">     Episode 383/500 | Average Loss 12721993.5220 | Cumulative Reward -1.1808\n",
    ">     Episode 384/500 | Average Loss 13490703.0113 | Cumulative Reward -0.1476\n",
    ">     Episode 385/500 | Average Loss 12027525.5820 | Cumulative Reward -0.0984\n",
    ">     Episode 386/500 | Average Loss 8776895.7362 | Cumulative Reward 0.1722\n",
    ">     Episode 387/500 | Average Loss 27030448.9508 | Cumulative Reward -1.2546\n",
    ">     Episode 388/500 | Average Loss 17247655.4836 | Cumulative Reward 0.7134\n",
    ">     Episode 389/500 | Average Loss 13761868.2480 | Cumulative Reward -0.8856\n",
    ">     Episode 390/500 | Average Loss 15799021.5820 | Cumulative Reward -1.0332\n",
    ">     Episode 391/500 | Average Loss 24067677.2961 | Cumulative Reward -2.0664\n",
    ">     Episode 392/500 | Average Loss 15877324.8064 | Cumulative Reward -0.0000\n",
    ">     Episode 393/500 | Average Loss 10898226.3576 | Cumulative Reward 0.2460\n",
    ">     Episode 394/500 | Average Loss 11247735.4857 | Cumulative Reward 0.5904\n",
    ">     Episode 395/500 | Average Loss 14609065.8217 | Cumulative Reward 0.1968\n",
    ">     Episode 396/500 | Average Loss 2343010.0123 | Cumulative Reward 0.1968\n",
    ">     Episode 397/500 | Average Loss 3205300.5410 | Cumulative Reward -0.4182\n",
    ">     Episode 398/500 | Average Loss 8199556.5533 | Cumulative Reward 0.0492\n",
    ">     Episode 399/500 | Average Loss 21762857.0984 | Cumulative Reward -0.3444\n",
    ">     Episode 400/500 | Average Loss 15102527.0164 | Cumulative Reward -0.0246\n",
    ">     Episode 401/500 | Average Loss 12123659.1414 | Cumulative Reward -0.1230\n",
    ">     Episode 402/500 | Average Loss 2576558.2551 | Cumulative Reward 0.2706\n",
    ">     Episode 403/500 | Average Loss 2949066.3484 | Cumulative Reward -0.0984\n",
    ">     Episode 404/500 | Average Loss 4379467.7418 | Cumulative Reward 0.1722\n",
    ">     Episode 405/500 | Average Loss 2939416.2049 | Cumulative Reward 1.7712\n",
    ">     Episode 406/500 | Average Loss 10072456.5164 | Cumulative Reward -0.5412\n",
    ">     Episode 407/500 | Average Loss 11368148.5574 | Cumulative Reward -1.6728\n",
    ">     Episode 408/500 | Average Loss 4251800.9057 | Cumulative Reward 0.0246\n",
    ">     Episode 409/500 | Average Loss 2425709.3094 | Cumulative Reward 1.5990\n",
    ">     Episode 410/500 | Average Loss 1586738.5707 | Cumulative Reward -0.4920\n",
    ">     Episode 411/500 | Average Loss 1070607.6132 | Cumulative Reward -1.2054\n",
    ">     Episode 412/500 | Average Loss 5415716.3217 | Cumulative Reward 0.0000\n",
    ">     Episode 413/500 | Average Loss 1737057.5220 | Cumulative Reward 0.5904\n",
    ">     Episode 414/500 | Average Loss 1278951.9534 | Cumulative Reward 0.4182\n",
    ">     Episode 415/500 | Average Loss 728374.3837 | Cumulative Reward 0.5658\n",
    ">     Episode 416/500 | Average Loss 9779674.6527 | Cumulative Reward -0.8856\n",
    ">     Episode 417/500 | Average Loss 39090349.0164 | Cumulative Reward -0.1230\n",
    ">     Episode 418/500 | Average Loss 17068589.6311 | Cumulative Reward 3.0258\n",
    ">     Episode 419/500 | Average Loss 19433508.8033 | Cumulative Reward 0.6396\n",
    ">     Episode 420/500 | Average Loss 8357417.2172 | Cumulative Reward -0.3936\n",
    ">     Episode 421/500 | Average Loss 11197790.3607 | Cumulative Reward 0.2706\n",
    ">     Episode 422/500 | Average Loss 6756482.8443 | Cumulative Reward 1.2054\n",
    ">     Episode 423/500 | Average Loss 3976738.0656 | Cumulative Reward -0.4182\n",
    ">     Episode 424/500 | Average Loss 3616583.8033 | Cumulative Reward -0.2706\n",
    ">     Episode 425/500 | Average Loss 2603585.6906 | Cumulative Reward 0.0246\n",
    ">     Episode 426/500 | Average Loss 2082487.2746 | Cumulative Reward 0.6396\n",
    ">     Episode 427/500 | Average Loss 2534304.0727 | Cumulative Reward -0.5658\n",
    ">     Episode 428/500 | Average Loss 12130751.7418 | Cumulative Reward -0.2214\n",
    ">     Episode 429/500 | Average Loss 69780277.8033 | Cumulative Reward 0.4182\n",
    ">     Episode 430/500 | Average Loss 78495889.1803 | Cumulative Reward -0.8610\n",
    ">     Episode 431/500 | Average Loss 45958012.8525 | Cumulative Reward -0.0984\n",
    ">     Episode 432/500 | Average Loss 136452393.9016 | Cumulative Reward 0.1476\n",
    ">     Episode 433/500 | Average Loss 38613835.7541 | Cumulative Reward -0.5166\n",
    ">     Episode 434/500 | Average Loss 33883618.4098 | Cumulative Reward -1.0332\n",
    ">     Episode 435/500 | Average Loss 41067984.8197 | Cumulative Reward -0.1230\n",
    ">     Episode 436/500 | Average Loss 91378111.4098 | Cumulative Reward -1.4760\n",
    ">     Episode 437/500 | Average Loss 94752215.4098 | Cumulative Reward -0.4674\n",
    ">     Episode 438/500 | Average Loss 40647725.0492 | Cumulative Reward 0.3444\n",
    ">     Episode 439/500 | Average Loss 35812338.5574 | Cumulative Reward -0.1230\n",
    ">     Episode 440/500 | Average Loss 29541471.6230 | Cumulative Reward 0.3198\n",
    ">     Episode 441/500 | Average Loss 60021648.1885 | Cumulative Reward -1.1070\n",
    ">     Episode 442/500 | Average Loss 158118092.1967 | Cumulative Reward -0.0984\n",
    ">     Episode 443/500 | Average Loss 59849613.7049 | Cumulative Reward 0.2706\n",
    ">     Episode 444/500 | Average Loss 46461701.9016 | Cumulative Reward -0.2952\n",
    ">     Episode 445/500 | Average Loss 48389924.3934 | Cumulative Reward 1.0086\n",
    ">     Episode 446/500 | Average Loss 195580981.4754 | Cumulative Reward -0.2460\n",
    ">     Episode 447/500 | Average Loss 1722251685.6393 | Cumulative Reward 0.2214\n",
    ">     Episode 448/500 | Average Loss 3515392862.4262 | Cumulative Reward -0.0492\n",
    ">     Episode 449/500 | Average Loss 306165022.4836 | Cumulative Reward 1.4760\n",
    ">     Episode 450/500 | Average Loss 99212269.6230 | Cumulative Reward 0.0738\n",
    ">     Episode 451/500 | Average Loss 86367019.5410 | Cumulative Reward 2.5338\n",
    ">     Episode 452/500 | Average Loss 31133120.3689 | Cumulative Reward -0.6642\n",
    ">     Episode 453/500 | Average Loss 21015204.2172 | Cumulative Reward 1.8942\n",
    ">     Episode 454/500 | Average Loss 11878485.2541 | Cumulative Reward -0.9102\n",
    ">     Episode 455/500 | Average Loss 14406326.1680 | Cumulative Reward 0.1968\n",
    ">     Episode 456/500 | Average Loss 26474831.6639 | Cumulative Reward 0.0000\n",
    ">     Episode 457/500 | Average Loss 20445096.5492 | Cumulative Reward -0.1476\n",
    ">     Episode 458/500 | Average Loss 77507055.2131 | Cumulative Reward -0.9348\n",
    ">     Episode 459/500 | Average Loss 34715110.0984 | Cumulative Reward -0.1722\n",
    ">     Episode 460/500 | Average Loss 11601538.8443 | Cumulative Reward -0.1230\n",
    ">     Episode 461/500 | Average Loss 2247047.2961 | Cumulative Reward 0.1230\n",
    ">     Episode 462/500 | Average Loss 939256.9170 | Cumulative Reward -0.0246\n",
    ">     Episode 463/500 | Average Loss 808798.3960 | Cumulative Reward 1.6728\n",
    ">     Episode 464/500 | Average Loss 683601.6122 | Cumulative Reward -0.4182\n",
    ">     Episode 465/500 | Average Loss 1048500.5922 | Cumulative Reward 1.2792\n",
    ">     Episode 466/500 | Average Loss 996181.3012 | Cumulative Reward 0.9594\n",
    ">     Episode 467/500 | Average Loss 958677.9529 | Cumulative Reward -0.1230\n",
    ">     Episode 468/500 | Average Loss 640709.4234 | Cumulative Reward 0.2952\n",
    ">     Episode 469/500 | Average Loss 561099.1286 | Cumulative Reward -0.4182\n",
    ">     Episode 470/500 | Average Loss 858037.4851 | Cumulative Reward -0.0984\n",
    ">     Episode 471/500 | Average Loss 781562.1660 | Cumulative Reward -0.4428\n",
    ">     Episode 472/500 | Average Loss 585637.4631 | Cumulative Reward 0.2706\n",
    ">     Episode 473/500 | Average Loss 1691876.3504 | Cumulative Reward -0.0984\n",
    ">     Episode 474/500 | Average Loss 857916.0794 | Cumulative Reward 0.8364\n",
    ">     Episode 475/500 | Average Loss 697805.0133 | Cumulative Reward 1.2300\n",
    ">     Episode 476/500 | Average Loss 428738.8163 | Cumulative Reward -0.9102\n",
    ">     Episode 477/500 | Average Loss 412427.7167 | Cumulative Reward -0.1722\n",
    ">     Episode 478/500 | Average Loss 371060.8876 | Cumulative Reward 0.3198\n",
    ">     Episode 479/500 | Average Loss 290708.3730 | Cumulative Reward 0.0984\n",
    ">     Episode 480/500 | Average Loss 293991.3815 | Cumulative Reward 0.3690\n",
    ">     Episode 481/500 | Average Loss 271680.2395 | Cumulative Reward 0.1968\n",
    ">     Episode 482/500 | Average Loss 247069.2293 | Cumulative Reward 0.2706\n",
    ">     Episode 483/500 | Average Loss 298824.8330 | Cumulative Reward -0.0246\n",
    ">     Episode 484/500 | Average Loss 272823.3829 | Cumulative Reward 0.1476\n",
    ">     Episode 485/500 | Average Loss 221586.0561 | Cumulative Reward 0.2706\n",
    ">     Episode 486/500 | Average Loss 255791.8814 | Cumulative Reward -0.2952\n",
    ">     Episode 487/500 | Average Loss 508487.5830 | Cumulative Reward 0.0984\n",
    ">     Episode 488/500 | Average Loss 559980.2203 | Cumulative Reward -0.7134\n",
    ">     Episode 489/500 | Average Loss 378515.2789 | Cumulative Reward -0.7626\n",
    ">     Episode 490/500 | Average Loss 250586.9393 | Cumulative Reward -0.4182\n",
    ">     Episode 491/500 | Average Loss 251211.5902 | Cumulative Reward -0.5904\n",
    ">     Episode 492/500 | Average Loss 169529.8831 | Cumulative Reward -0.5904\n",
    ">     Episode 493/500 | Average Loss 242881.4125 | Cumulative Reward -0.1968\n",
    ">     Episode 494/500 | Average Loss 573974.6457 | Cumulative Reward 0.4182\n",
    ">     Episode 495/500 | Average Loss 200285.0907 | Cumulative Reward 0.7626\n",
    ">     Episode 496/500 | Average Loss 268848.6760 | Cumulative Reward 0.1968\n",
    ">     Episode 497/500 | Average Loss 235332.8327 | Cumulative Reward 0.0246\n",
    ">     Episode 498/500 | Average Loss 202253.1831 | Cumulative Reward 0.0492\n",
    ">     Episode 499/500 | Average Loss 149803.7602 | Cumulative Reward 0.1476"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training results\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.plot(returns)\n",
    "ax.set_ylabel(\"Return\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "We have trained our model for 500 episodes and the returns are plotted\n",
    "above. Note that the loss was still quite high at the end of training,\n",
    "which indicates that the algorithm hasn't converged. A possible\n",
    "explanation for this is that RL algorithms typically require\n",
    "significantly more steps to converge. Further, considering the size of\n",
    "the tranining dataset, the neural network used is very small. Besides\n",
    "that, DQN is known to be quite unstable and prone to diverge, which is\n",
    "why several new versions of this algorithm have been proposed since it\n",
    "was first introduced. A very common implementation consists of the\n",
    "Double DQN, which introduced a target Q-network used to compute the\n",
    "actions, which is updated at a lower rate than the main Q-network. In\n",
    "our implementation, the max operator uses the same network both to\n",
    "select and to evaluate an action. This may lead to wrongly selecting\n",
    "overestimated values. Having a separate target network can help prevent\n",
    "this, by decoupling the selection from the evaluation.\n",
    "\n",
    "Testing RL agent\n",
    "----------------\n",
    "\n",
    "In order to test our agent, we select the whole data from the 1st of\n",
    "January 2019, which wasn't included during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "reward_sum = 0.\n",
    "\n",
    "# Define testing interval, January 2019\n",
    "start = datetime.datetime(2019, 1, 1, 0, 0)\n",
    "end = datetime.datetime(2019, 1, 1, 23, 59)\n",
    "\n",
    "# Test learned model\n",
    "env = MarketEnv(oilDF_py, start, end, episode_size=np.inf, scope=sequence_scope)\n",
    "state = env.reset(random_starttime=False)\n",
    "input_t = state.reshape(1, sequence_scope, 1)\n",
    "while not done:    \n",
    "    states.append(state[-1])\n",
    "    q = model.predict(input_t)\n",
    "    action = np.argmax(q[0])\n",
    "    actions.append(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "    input_t = state.reshape(1, sequence_scope, 1)      \n",
    "print(\"Return = {}\".format(reward_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Return = 0.096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting testing results\n",
    "timesteps = np.linspace(1,len(states),len(states))\n",
    "longs = np.argwhere(np.asarray(actions) ==  0)\n",
    "shorts = np.argwhere(np.asarray(actions) ==  1)\n",
    "states = np.asarray(states)\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n",
    "ax[0].grid(True)\n",
    "ax[0].plot(timesteps, states, label='diff_close')\n",
    "ax[0].plot(timesteps[longs], states[longs].flatten(), '*g', markersize=12, label='long')\n",
    "ax[0].plot(timesteps[shorts], states[shorts].flatten(), '*r', markersize=12, label='short')\n",
    "ax[0].set_ylabel(\"(s,a)\")\n",
    "ax[0].set_xlabel(\"Timestep\")\n",
    "ax[0].set_xlim(1,len(states))\n",
    "ax[0].legend()\n",
    "ax[1].grid(True)\n",
    "ax[1].plot(timesteps, rewards, 'o-r')\n",
    "ax[1].set_ylabel(\"r\")\n",
    "ax[1].set_xlabel(\"Timestep\")\n",
    "ax[1].set_xlim(1,len(states))\n",
    "plt.tight_layout()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "We can see that the policy converged to always shorting, meaning that\n",
    "the agent never buys any stock. While abstaining from investments in\n",
    "fossil fuels may be good advice, the result is not very useful for our\n",
    "intended application. Nevertherless, reaching a successful automatic\n",
    "intraday trading bot in the short time we spent implementing this\n",
    "project would be a high bar. After all, this is more or less the holy\n",
    "grail of computational economy.\n",
    "\n",
    "Summary and Outlook\n",
    "-------------------\n",
    "\n",
    "In this project we have trained and tested an RL agent, using DQN for\n",
    "intraday trading. We started by processing the data and adding a\n",
    "\\*\\*diff\\_close\\*\\* column which contains the differece of the closing\n",
    "stock value between two timesteps. We then implemented our own Gym\n",
    "environment **MarketEnv**, to be able to read data from the BCUSD\n",
    "dataset and feed it to an agents, as weel as compute the reward given\n",
    "the agent's action. We used a DQN implementation, to train a\n",
    "convolutional Q-Network. Since we are using TensorFlow in the\n",
    "background, the training is automatically scaled to use all cpu cores\n",
    "available (see [here](https://www.xspdf.com/resolution/52582340.html)).\n",
    "Finally, we have tested our agent on new data, and concluded that more\n",
    "work needs to be put into making the algorithm convege.\n",
    "\n",
    "As future work we believe we can still improve the state and reward\n",
    "definitions. For the state, used a window of \\*\\*close\\_diff\\*\\* values\n",
    "as our state definiion. However, augmenting the state with longer term\n",
    "trends computed by the TrendCalculus algorithm could yield significant\n",
    "improvements. The TrendCalculus algorithm provides an analytical\n",
    "framework effective for identifying trends in historical price data,\n",
    "including [trend pattern\n",
    "analysis](https://lamastex.github.io/spark-trend-calculus-examples/notebooks/db/01trend-calculus-showcase.html)\n",
    "and [prediction of trend\n",
    "changes](https://lamastex.github.io/spark-trend-calculus-examples/notebooks/db/03streamable-trend-calculus-estimators.html).\n",
    "Below we present the idea behind TrendCalculus and how it relates to\n",
    "\\*\\*close\\_diff\\*\\*, as well as a few ideas on how it could be used for\n",
    "our application.\n",
    "\n",
    "### Trend Calculus\n",
    "\n",
    "Taken from: <https://github.com/lamastex/spark-trend-calculus-examples>\n",
    "\n",
    "Trend Calculus is an algorithm invented by Andrew Morgan that is used to\n",
    "find trend changes in a time series (see\n",
    "[here](https://github.com/bytesumo/TrendCalculus/blob/master/HowToStudyTrends_v1.03.pdf)).\n",
    "It works by grouping the observations in the time series into windows\n",
    "and defining a trend upwards as “higher highs and higher lows” compared\n",
    "to the previous window. A downwards trend is similarly defined as “lower\n",
    "highs and lower lows”.\n",
    "\n",
    "&lt;img\n",
    "src=https://lamastex.github.io/spark-trend-calculus-examples/notebooks/db/images/HigherHighHigherLow.png\n",
    "width=300&gt;\n",
    "\n",
    "If there is a higher high and lower low (or lower high and higher low),\n",
    "no trend is detected. This is solved by introducing intermediate windows\n",
    "that split the non-trend into two trends, ensuring that every point can\n",
    "be labeled with either an up or down trend.\n",
    "\n",
    "&lt;img\n",
    "src=https://lamastex.github.io/spark-trend-calculus-examples/notebooks/db/images/OuterInnerBars.png\n",
    "width=600&gt;\n",
    "\n",
    "When the trends have been calculated for all windows, the points where\n",
    "the trends change sign are labeled as reversals. If the reversal is from\n",
    "up to down, the previous high is the reversal point and if the reversal\n",
    "is from down to up, the previous low is the reversal. This means that\n",
    "the reversals always are the appropriate extrema (maximum for up to\n",
    "down, minimum for down to up).\n",
    "\n",
    "&lt;img\n",
    "src=https://lamastex.github.io/spark-trend-calculus-examples/notebooks/db/images/trendReversals.png\n",
    "width=600&gt;\n",
    "\n",
    "The output of the algorithm is a time series consisting of all the\n",
    "labelled reversal points. It is therefore possible to use this as the\n",
    "input for another run of the Trend Calculus algorithm, finding more long\n",
    "term trends. This can be seen when TrendCalculus is applied to the BCUSD\n",
    "datset, shown in column **reversal1** of the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val windowSize = 2\n",
    "val numReversals = 1 // we look at 1 iteration of the algorithm. \n",
    "\n",
    "val dfWithReversals = new TrendCalculus2(oilDS, windowSize, spark).nReversalsJoinedWithMaxRev(numReversals)\n",
    "display(dfWithReversals)\n",
    "\n",
    "val windowSpec = Window.orderBy(\"x\")\n",
    "val dfWithReversalsDiff = dfWithReversals \n",
    ".withColumn(\"diff_close\", $\"y\" - when((lag(\"y\", 1).over(windowSpec)).isNull, 0).otherwise(lag(\"y\", 1).over(windowSpec)))\n",
    "\n",
    "// Store loaded data as temp view, to be accessible in Python\n",
    "dfWithReversalsDiff.createOrReplaceTempView(\"temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "In conjunction with TrendCalculus, a complete automatic trading pipeline\n",
    "can be constructed, consisting of (i) trend analysis with TrendCalculus\n",
    "(ii) time series prediction and (iii) control, i.e. buy or sell.\n",
    "Implementing and evaluating a pipeline such as the one outlined in the\n",
    "aforementioned steps is left as a suggestion for future work, and it is\n",
    "of particular interest to compare the performance of such a method to a\n",
    "learning-based one.\n",
    "\n",
    "Below we show that sign(\\*\\*diff\\_close**) is equivalent to sign of the\n",
    "output of a single iteration of TrendCalculus with window size 2, over\n",
    "our **scope\\*\\*. A possible improvement of our algorithm would be to use\n",
    "TrendCalculus to compute long term trends from historical data and\n",
    "include it on our state definition. This way, if for example the agent\n",
    "observes a long term downward trend, then it can be encouraged to buy\n",
    "stock since it is bound to bounce up again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from: https://lamastex.github.io/spark-trend-calculus-examples/notebooks/db/01trend-calculus-showcase.html\n",
    "\n",
    "# Create Dataframe from temp data\n",
    "fullDS = spark.table(\"temp\")\n",
    "fullTS = fullDS.select(\"x\", \"y\", \"reversal1\", \"diff_close\").collect()\n",
    "\n",
    "startDate = datetime.datetime(2019, 1, 1, 1, 0) # first window used as scope\n",
    "endDate = datetime.datetime(2019, 1, 1, 23, 59)\n",
    "TS = [row for row in fullTS if startDate <= row['x'] and row['x'] <= endDate]\n",
    "\n",
    "allData = {'x': [row['x'] for row in TS], 'close': [row['y'] for row in TS], 'diff_close': [row['diff_close'] for row in TS], 'reversal1': [row['reversal1'] for row in TS]}\n",
    "\n",
    "# Plot reversals\n",
    "close = np.asarray(allData['close'])\n",
    "diff_close = np.asarray(allData['diff_close'])\n",
    "timesteps = np.linspace(1,len(diff_close),len(diff_close))\n",
    "revs = np.asarray(allData['reversal1'])\n",
    "pos_rev_ind = np.argwhere(revs ==  1)\n",
    "neg_rev_ind = np.argwhere(revs ==  -1)\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n",
    "ax[0].grid(True)\n",
    "ax[0].plot(timesteps, close, label='close')\n",
    "ax[0].plot(timesteps[pos_rev_ind], close[pos_rev_ind].flatten(), '*g', markersize=12, label='+ reversal')\n",
    "ax[0].plot(timesteps[neg_rev_ind], close[neg_rev_ind].flatten(), '*r', markersize=12, label='- reversal')\n",
    "ax[0].set_ylabel(\"close\")\n",
    "ax[0].set_xlabel(\"Timestep\")\n",
    "ax[0].set_xlim(1,len(close))\n",
    "ax[0].legend()\n",
    "ax[1].grid(True)\n",
    "ax[1].plot(timesteps, diff_close, label='diff_close')\n",
    "ax[1].plot(timesteps[pos_rev_ind], diff_close[pos_rev_ind].flatten(), '*g', markersize=12, label='+ reversal')\n",
    "ax[1].plot(timesteps[neg_rev_ind], diff_close[neg_rev_ind].flatten(), '*r', markersize=12, label='- reversal')\n",
    "ax[1].set_ylabel(\"diff_close\")\n",
    "ax[1].set_xlabel(\"Timestep\")\n",
    "ax[1].set_xlim(1,len(diff_close))\n",
    "plt.tight_layout()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
