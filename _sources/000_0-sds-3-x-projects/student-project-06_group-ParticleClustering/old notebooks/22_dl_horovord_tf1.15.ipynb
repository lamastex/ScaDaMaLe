{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is currently just a copy paste from the github repository of the\n",
    "file `train_kmean.py`. For distributed learning see the next notebook\n",
    "\n",
    "install correct version of tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall tensorflow\n",
    "#pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Name: tensorflow\n",
    ">     Version: 2.3.0\n",
    ">     Summary: TensorFlow is an open source machine learning framework for everyone.\n",
    ">     Home-page: https://www.tensorflow.org/\n",
    ">     Author: Google Inc.\n",
    ">     Author-email: packages@tensorflow.org\n",
    ">     License: Apache 2.0\n",
    ">     Location: /databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages\n",
    ">     Requires: astunparse, tensorflow-estimator, termcolor, wrapt, numpy, h5py, opt-einsum, protobuf, google-pasta, keras-preprocessing, six, scipy, gast, grpcio, absl-py, tensorboard, wheel\n",
    ">     Required-by: spark-tensorflow-distributor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install tensorflow-cpu==1.15.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Python interpreter will be restarted.\n",
    ">     Collecting tensorflow-cpu==1.15.*\n",
    ">       Using cached tensorflow_cpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (110.8 MB)\n",
    ">     Collecting tensorboard<1.16.0,>=1.15.0\n",
    ">       Using cached tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
    ">     Requirement already satisfied: termcolor>=1.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (1.1.0)\n",
    ">     Requirement already satisfied: wrapt>=1.11.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (1.11.2)\n",
    ">     Requirement already satisfied: keras-preprocessing>=1.0.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (1.1.2)\n",
    ">     Requirement already satisfied: astor>=0.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (0.8.0)\n",
    ">     Requirement already satisfied: numpy<2.0,>=1.16.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (1.18.1)\n",
    ">     Requirement already satisfied: opt-einsum>=2.3.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (3.3.0)\n",
    ">     Processing /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3/gast-0.2.2-py3-none-any.whl\n",
    ">     Requirement already satisfied: six>=1.10.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (1.14.0)\n",
    ">     Requirement already satisfied: protobuf>=3.6.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (3.11.4)\n",
    ">     Requirement already satisfied: grpcio>=1.8.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (1.27.2)\n",
    ">     Collecting tensorflow-estimator==1.15.1\n",
    ">       Using cached tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
    ">     Collecting keras-applications>=1.0.8\n",
    ">       Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
    ">     Requirement already satisfied: google-pasta>=0.1.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (0.2.0)\n",
    ">     Requirement already satisfied: absl-py>=0.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (0.9.0)\n",
    ">     Requirement already satisfied: wheel>=0.26 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (0.34.2)\n",
    ">     Requirement already satisfied: markdown>=2.6.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-cpu==1.15.*) (3.1.1)\n",
    ">     Requirement already satisfied: setuptools>=41.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-cpu==1.15.*) (45.2.0.post20200210)\n",
    ">     Requirement already satisfied: werkzeug>=0.11.15 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-cpu==1.15.*) (1.0.0)\n",
    ">     Requirement already satisfied: h5py in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow-cpu==1.15.*) (2.10.0)\n",
    ">     ERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\n",
    ">     ERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 1.15.0 which is incompatible.\n",
    ">     ERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\n",
    ">     Installing collected packages: tensorboard, gast, tensorflow-estimator, keras-applications, tensorflow-cpu\n",
    ">       Attempting uninstall: tensorboard\n",
    ">         Found existing installation: tensorboard 2.3.0\n",
    ">         Uninstalling tensorboard-2.3.0:\n",
    ">           Successfully uninstalled tensorboard-2.3.0\n",
    ">       Attempting uninstall: gast\n",
    ">         Found existing installation: gast 0.3.3\n",
    ">         Uninstalling gast-0.3.3:\n",
    ">           Successfully uninstalled gast-0.3.3\n",
    ">       Attempting uninstall: tensorflow-estimator\n",
    ">         Found existing installation: tensorflow-estimator 2.3.0\n",
    ">         Uninstalling tensorflow-estimator-2.3.0:\n",
    ">           Successfully uninstalled tensorflow-estimator-2.3.0\n",
    ">     Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-cpu-1.15.0 tensorflow-estimator-1.15.1\n",
    ">     Python interpreter will be restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install tensorflow-gpu==1.15.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Python interpreter will be restarted.\n",
    ">     Collecting tensorflow-gpu==1.15.*\n",
    ">       Downloading tensorflow_gpu-1.15.4-cp37-cp37m-manylinux2010_x86_64.whl (411.0 MB)\n",
    ">     Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-gpu==1.15.*) (1.15.0)\n",
    ">     Requirement already satisfied: termcolor>=1.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-gpu==1.15.*) (1.1.0)\n",
    ">     Requirement already satisfied: wrapt>=1.11.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-gpu==1.15.*) (1.11.2)\n",
    ">     Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-gpu==1.15.*) (0.34.2)\n",
    ">     Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-gpu==1.15.*) (1.18.1)\n",
    ">     Requirement already satisfied: keras-preprocessing>=1.0.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-gpu==1.15.*) (1.1.2)\n",
    ">     Requirement already satisfied: astor>=0.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-gpu==1.15.*) (0.8.0)\n",
    ">     Requirement already satisfied: opt-einsum>=2.3.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-gpu==1.15.*) (3.3.0)\n",
    ">     Requirement already satisfied: six>=1.10.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-gpu==1.15.*) (1.14.0)\n",
    ">     Requirement already satisfied: gast==0.2.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-gpu==1.15.*) (0.2.2)\n",
    ">     Requirement already satisfied: protobuf>=3.6.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-gpu==1.15.*) (3.11.4)\n",
    ">     Requirement already satisfied: grpcio>=1.8.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-gpu==1.15.*) (1.27.2)\n",
    ">     Requirement already satisfied: tensorflow-estimator==1.15.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-gpu==1.15.*) (1.15.1)\n",
    ">     Requirement already satisfied: keras-applications>=1.0.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-gpu==1.15.*) (1.0.8)\n",
    ">     Requirement already satisfied: google-pasta>=0.1.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-gpu==1.15.*) (0.2.0)\n",
    ">     Requirement already satisfied: absl-py>=0.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow-gpu==1.15.*) (0.9.0)\n",
    ">     Requirement already satisfied: markdown>=2.6.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.*) (3.1.1)\n",
    ">     Requirement already satisfied: setuptools>=41.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.*) (45.2.0.post20200210)\n",
    ">     Requirement already satisfied: werkzeug>=0.11.15 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.*) (1.0.0)\n",
    ">     Requirement already satisfied: h5py in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.*) (2.10.0)\n",
    ">     Installing collected packages: tensorflow-gpu\n",
    ">     Successfully installed tensorflow-gpu-1.15.4\n",
    ">     Python interpreter will be restarted.\n",
    "\n",
    "  \n",
    "\n",
    "This apparently the way to get tensorflow 1.15 gpu on databricks 7.2 ml\n",
    "(so might work for databricks 7.3 ml, which is what this cluster is\n",
    "using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install https://databricks-prod-cloudfront.cloud.databricks.com/artifacts/tensorflow/runtime-7.x/tensorflow-1.15.3-cp37-cp37m-linux_x86_64.whl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Python interpreter will be restarted.\n",
    ">     Collecting tensorflow==1.15.3\n",
    ">       Using cached https://databricks-prod-cloudfront.cloud.databricks.com/artifacts/tensorflow/runtime-7.x/tensorflow-1.15.3-cp37-cp37m-linux_x86_64.whl (310.3 MB)\n",
    ">     Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.15.0)\n",
    ">     Requirement already satisfied: termcolor>=1.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.1.0)\n",
    ">     Requirement already satisfied: wrapt>=1.11.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.11.2)\n",
    ">     Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow==1.15.3) (0.34.2)\n",
    ">     Requirement already satisfied: keras-preprocessing>=1.0.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.1.2)\n",
    ">     Requirement already satisfied: astor>=0.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow==1.15.3) (0.8.0)\n",
    ">     Requirement already satisfied: numpy<2.0,>=1.16.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.18.1)\n",
    ">     Requirement already satisfied: opt-einsum>=2.3.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow==1.15.3) (3.3.0)\n",
    ">     Requirement already satisfied: gast==0.2.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow==1.15.3) (0.2.2)\n",
    ">     Requirement already satisfied: six>=1.10.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.14.0)\n",
    ">     Requirement already satisfied: protobuf>=3.6.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow==1.15.3) (3.11.4)\n",
    ">     Requirement already satisfied: grpcio>=1.8.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.27.2)\n",
    ">     Requirement already satisfied: tensorflow-estimator==1.15.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.15.1)\n",
    ">     Requirement already satisfied: keras-applications>=1.0.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.0.8)\n",
    ">     Requirement already satisfied: google-pasta>=0.1.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow==1.15.3) (0.2.0)\n",
    ">     Requirement already satisfied: absl-py>=0.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorflow==1.15.3) (0.9.0)\n",
    ">     Requirement already satisfied: markdown>=2.6.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3) (3.1.1)\n",
    ">     Requirement already satisfied: setuptools>=41.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3) (45.2.0.post20200210)\n",
    ">     Requirement already satisfied: werkzeug>=0.11.15 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3) (1.0.0)\n",
    ">     Requirement already satisfied: h5py in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.3) (2.10.0)\n",
    ">     ERROR: spark-tensorflow-distributor 0.1.0 has requirement tensorflow>=2.1.0, but you'll have tensorflow 1.15.3 which is incompatible.\n",
    ">     Installing collected packages: tensorflow\n",
    ">       Attempting uninstall: tensorflow\n",
    ">         Found existing installation: tensorflow 2.3.0\n",
    ">         Uninstalling tensorflow-2.3.0:\n",
    ">           Successfully uninstalled tensorflow-2.3.0\n",
    ">     Successfully installed tensorflow-1.15.3\n",
    ">     Python interpreter will be restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls databricks/driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "  \n",
    "\n",
    "get the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, shlex\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import os\n",
    "import sys\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.set_printoptions(edgeitems=1000)\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "BASE_DIR = os.path.join(os.getcwd(), '06_LHC','scripts')  #os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, '..', 'models'))\n",
    "sys.path.append(os.path.join(BASE_DIR, '..', 'utils'))\n",
    "import provider\n",
    "import gapnet_classify as MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[2]: '1.15.3'\n",
    "\n",
    "  \n",
    "\n",
    "Get the arguments (we hvae to create the namespace like this with a\n",
    "dictionary since notebooks don't easily incorporate argument parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls databricks/driver/06_LHC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "parserdict = {'max_dim': 3, #help='Dimension of the encoding layer [Default: 3]')\n",
    "              'n_clusters': 3, #help='Number of clusters [Default: 3]')\n",
    "              'gpu': 0, #help='GPU to use [default: GPU 0]')\n",
    "              'model': 'gapnet_clasify', #help='Model name [default: gapnet_classify]')\n",
    "              'log_dir': 'log', #help='Log dir [default: log]')\n",
    "              'num_point': 100, #help='Point Number [default: 100]')\n",
    "              'max_epoch': 1, #help='Epoch to run [default: 200]')\n",
    "              'batch_size': 512, #help='Batch Size during training [default: 512]')\n",
    "              'learning_rate': 0.001, #help='Initial learning rate [default: 0.01]')\n",
    "\n",
    "              'momentum': 0.9, #help='Initial momentum [default: 0.9]')\n",
    "              'optimizer': 'adam', #help='adam or momentum [default: adam]')\n",
    "              'decay_step': 500000, #help='Decay step for lr decay [default: 500000]')\n",
    "              'wd': 0.0, #help='Weight Decay [Default: 0.0]')\n",
    "              'decay_rate': 0.5, #help='Decay rate for lr decay [default: 0.5]')\n",
    "              'output_dir': 'train_results', #help='Directory that stores all training logs and trained models')\n",
    "              'data_dir': os.path.join(os.getcwd(),'06_LHC', 'h5'), # '../h5', #help='directory with data [default: hdf5_data]')\n",
    "              'nfeat': 8, #help='Number of features [default: 8]')\n",
    "              'ncat': 20, #help='Number of categories [default: 20]')\n",
    "             }\n",
    "\n",
    "FLAGS = Namespace(**parserdict)\n",
    "\n",
    "H5_DIR = FLAGS.data_dir\n",
    "\n",
    "EPOCH_CNT = 0\n",
    "MAX_PRETRAIN = 10\n",
    "BATCH_SIZE = FLAGS.batch_size\n",
    "NUM_POINT = FLAGS.num_point\n",
    "NUM_FEAT = FLAGS.nfeat\n",
    "NUM_CLASSES = FLAGS.ncat\n",
    "MAX_EPOCH = FLAGS.max_epoch\n",
    "BASE_LEARNING_RATE = FLAGS.learning_rate\n",
    "GPU_INDEX = FLAGS.gpu\n",
    "MOMENTUM = FLAGS.momentum\n",
    "OPTIMIZER = FLAGS.optimizer\n",
    "DECAY_STEP = FLAGS.decay_step\n",
    "DECAY_RATE = FLAGS.decay_rate\n",
    "\n",
    "# MODEL = importlib.import_module(FLAGS.model) # import network module\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'models', FLAGS.model + '.py')\n",
    "LOG_DIR = os.path.join('..', 'logs', FLAGS.log_dir)\n",
    "\n",
    "if not os.path.exists(LOG_DIR): os.makedirs(LOG_DIR)\n",
    "os.system('cp %s.py %s' % (MODEL_FILE, LOG_DIR))  # bkp of model def\n",
    "os.system('cp train_kmeans.py %s' % (LOG_DIR))  # bkp of train procedure\n",
    "\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99\n",
    "\n",
    "LEARNING_RATE_CLIP = 1e-5\n",
    "HOSTNAME = socket.gethostname()\n",
    "\n",
    "TRAIN_FILES = provider.getDataFiles(os.path.join(H5_DIR, 'train_files_wztop.txt'))\n",
    "TEST_FILES = provider.getDataFiles(os.path.join(H5_DIR, 'test_files_wztop.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "define utilisation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate(batch):\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        BASE_LEARNING_RATE,  # Base learning rate.\n",
    "        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "        DECAY_STEP,  # Decay step.\n",
    "        DECAY_RATE,  # Decay rate.\n",
    "        staircase=True)\n",
    "    learning_rate = tf.maximum(learning_rate, LEARNING_RATE_CLIP)  # CLIP THE LEARNING RATE!\n",
    "    return learning_rate\n",
    "\n",
    "\n",
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.train.exponential_decay(\n",
    "        BN_INIT_DECAY,\n",
    "        batch * BATCH_SIZE,\n",
    "        BN_DECAY_DECAY_STEP,\n",
    "        BN_DECAY_DECAY_RATE,\n",
    "        staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "create checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    " \n",
    "checkpoint_dir = '/dbfs/databricks/driver/06_LHC/logs/train/{}/'.format(time.time())\n",
    " \n",
    "os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5_hvd(h5_filename, rank=0, size=1):\n",
    "    f = h5py.File(h5_filename, 'r')\n",
    "    data = f['data'][rank::size]\n",
    "    label = f['pid'][rank::size]\n",
    "    seg = f['label'][rank::size]\n",
    "    print(\"loaded {0} events\".format(len(data)))\n",
    "\n",
    "    return (data, label, seg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hvd():\n",
    "    import horovod.tensorflow as hvd\n",
    "    \n",
    "    \n",
    "    import argparse, shlex\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import socket\n",
    "    import os\n",
    "    import sys\n",
    "    from sklearn.cluster import KMeans\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    np.set_printoptions(edgeitems=1000)\n",
    "\n",
    "    from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "    BASE_DIR = os.path.join(os.getcwd(), '06_LHC','scripts')  #os.path.dirname(os.path.abspath(__file__))\n",
    "    sys.path.append(BASE_DIR)\n",
    "    sys.path.append(os.path.join(BASE_DIR, '..', 'models'))\n",
    "    sys.path.append(os.path.join(BASE_DIR, '..', 'utils'))\n",
    "    import provider\n",
    "    import gapnet_classify as MODEL\n",
    "    \n",
    "  \n",
    "    # Horovod: initialize Horovod.\n",
    "    hvd.init()\n",
    "        \n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:' + str(GPU_INDEX)):\n",
    "            pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT, NUM_FEAT)\n",
    "\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "\n",
    "            # Note the global_step=batch parameter to minimize.\n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            alpha = tf.placeholder(dtype=tf.float32, shape=())\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "            print(\"--- Get model and loss\")\n",
    "\n",
    "            pred, max_pool = MODEL.get_model(pointclouds_pl, is_training=is_training_pl,\n",
    "                                             bn_decay=bn_decay,\n",
    "                                             num_class=NUM_CLASSES, weight_decay=FLAGS.wd,\n",
    "                                             )\n",
    "\n",
    "            class_loss = MODEL.get_focal_loss(pred, labels_pl, NUM_CLASSES)\n",
    "            mu = tf.Variable(tf.zeros(shape=(FLAGS.n_clusters, FLAGS.max_dim)), name=\"mu\",\n",
    "                             trainable=True)  # k centroids\n",
    "            kmeans_loss, stack_dist = MODEL.get_loss_kmeans(max_pool, mu, FLAGS.max_dim,\n",
    "                                                            FLAGS.n_clusters, alpha)\n",
    "\n",
    "            full_loss = kmeans_loss + class_loss\n",
    "\n",
    "            print(\"--- Get training operator\")\n",
    "            # Get training operator\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            # scale learning rate from horovod dependent on number of processes (=hvd.size)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate * hvd.size(), momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate * hvd.size())\n",
    "            # Horovod: add Horovod Distributed Optimizer\n",
    "            optimizer = hvd.DistributedOptimizer(optimizer)\n",
    "\n",
    "            train_op_full = optimizer.minimize(full_loss, global_step=batch)\n",
    "            train_op = optimizer.minimize(class_loss, global_step=batch)\n",
    "\n",
    "            # Add ops to save and restore all the variables.\n",
    "            saver = tf.train.Saver()\n",
    "        \n",
    "        hooks = [\n",
    "          # Horovod: BroadcastGlobalVariablesHook broadcasts initial variable states\n",
    "          # from rank 0 to all other processes. This is necessary to ensure consistent\n",
    "          # initialization of all workers when training is started with random weights\n",
    "          # or restored from a checkpoint.\n",
    "          hvd.BroadcastGlobalVariablesHook(0),]\n",
    "  \n",
    "\n",
    "        # Create a session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = False\n",
    "        config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "        \n",
    "        # MonitoredTrainingSession\n",
    "        # takes care of session initialization,\n",
    "        # restoring from a checkpoint, saving to a checkpoint, and closing when done\n",
    "        # or an error occurs.\n",
    "        sess = tf.train.MonitoredSession(checkpoint_dir=checkpoint_dir,\n",
    "                                         hooks=hooks,\n",
    "                                         config=config)\n",
    "        \n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Add summary writers\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'), sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'), sess.graph)\n",
    "\n",
    "        # Init variables\n",
    "        print(\"Total number of weights for the model: \",\n",
    "              np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]))\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'labels_pl': labels_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'max_pool': max_pool,\n",
    "               'pred': pred,\n",
    "               'alpha': alpha,\n",
    "               'mu': mu,\n",
    "               'stack_dist': stack_dist,\n",
    "               'class_loss': class_loss,\n",
    "               'kmeans_loss': kmeans_loss,\n",
    "               'train_op': train_op,\n",
    "               'train_op_full': train_op_full,\n",
    "               'merged': merged,\n",
    "               'step': batch,\n",
    "               'learning_rate': learning_rate\n",
    "               }\n",
    "\n",
    "        for epoch in range(MAX_EPOCH):\n",
    "            print('**** EPOCH %03d ****' % (epoch))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_full_training = epoch > MAX_PRETRAIN\n",
    "            max_pool = train_one_epoch(sess, ops, train_writer, hvd.rank(), hvd.size(), is_full_training)\n",
    "            if epoch == MAX_PRETRAIN:\n",
    "                centers = KMeans(n_clusters=FLAGS.n_clusters).fit(np.squeeze(max_pool))\n",
    "                centers = centers.cluster_centers_\n",
    "                sess.run(tf.assign(mu, centers))\n",
    "\n",
    "            eval_one_epoch(sess, ops, test_writer, hvd.rank(), hvd.size(), is_full_training)\n",
    "            if is_full_training:\n",
    "                save_path = saver.save(sess, os.path.join(LOG_DIR, 'cluster.ckpt'))\n",
    "            else:\n",
    "                save_path = saver.save(sess, os.path.join(LOG_DIR, 'model.ckpt'))\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "\n",
    "def get_batch(data, label, start_idx, end_idx):\n",
    "    batch_label = label[start_idx:end_idx]\n",
    "    batch_data = data[start_idx:end_idx, :, :]\n",
    "    return batch_data, batch_label\n",
    "\n",
    "\n",
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = linear_sum_assignment(w.max() - w)\n",
    "    ind = np.asarray(ind)\n",
    "    ind = np.transpose(ind)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size\n",
    "\n",
    "\n",
    "def train_one_epoch(sess, ops, train_writer, hvd_rank, hvd_size, is_full_training):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = True\n",
    "\n",
    "    train_idxs = np.arange(0, len(TRAIN_FILES))\n",
    "\n",
    "    acc = loss_sum = 0\n",
    "    y_pool = []\n",
    "    for fn in range(len(TRAIN_FILES)):\n",
    "        # print('----' + str(fn) + '-----')\n",
    "        current_file = os.path.join(H5_DIR, TRAIN_FILES[train_idxs[fn]])\n",
    "        current_data, current_label, current_cluster = load_h5_hvd(current_file, hvd_rank, hvd_size)\n",
    "\n",
    "        current_label = np.squeeze(current_label)\n",
    "\n",
    "        file_size = current_data.shape[0]\n",
    "        num_batches = file_size // BATCH_SIZE\n",
    "        # num_batches = 5\n",
    "        print(str(datetime.now()))\n",
    "\n",
    "        # initialise progress bar\n",
    "        process_desc = \"Loss {:2.3e}\"\n",
    "        progress_bar = tqdm(initial=0, leave=True, total=num_batches,\n",
    "                            desc=process_desc.format(0),\n",
    "                            position=0)\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = (batch_idx + 1) * BATCH_SIZE\n",
    "            batch_data, batch_label = get_batch(current_data, current_label, start_idx, end_idx)\n",
    "            cur_batch_size = end_idx - start_idx\n",
    "\n",
    "            feed_dict = {ops['pointclouds_pl']: batch_data,\n",
    "                         ops['labels_pl']: batch_label,\n",
    "                         ops['is_training_pl']: is_training,\n",
    "                         ops['alpha']: 2 * (EPOCH_CNT - MAX_PRETRAIN + 1),}\n",
    "            if is_full_training:\n",
    "                summary, step, _, loss_val, dist, lr = sess.run([ops['merged'], ops['step'],\n",
    "                                                                 ops['train_op_full'], ops['kmeans_loss'],\n",
    "                                                                 ops['stack_dist'], ops['learning_rate']],\n",
    "                                                                feed_dict=feed_dict)\n",
    "\n",
    "                batch_cluster = np.array([np.where(r == 1)[0][0] for r in current_cluster[start_idx:end_idx]])\n",
    "                cluster_assign = np.zeros((cur_batch_size), dtype=int)\n",
    "\n",
    "                for i in range(cur_batch_size):\n",
    "                    index_closest_cluster = np.argmin(dist[:, i])\n",
    "                    cluster_assign[i] = index_closest_cluster\n",
    "\n",
    "                acc += cluster_acc(batch_cluster, cluster_assign)\n",
    "            else:\n",
    "                summary, step, _, loss_val, max_pool, lr = sess.run([ops['merged'], ops['step'],\n",
    "                                                                     ops['train_op'], ops['class_loss'],\n",
    "                                                                     ops['max_pool'], ops['learning_rate']],\n",
    "                                                                    feed_dict=feed_dict)\n",
    "\n",
    "                if len(y_pool) == 0:\n",
    "                    y_pool = np.squeeze(max_pool)\n",
    "                else:\n",
    "                    y_pool = np.concatenate((y_pool, np.squeeze(max_pool)), axis=0)\n",
    "\n",
    "            loss_sum += np.mean(loss_val)\n",
    "\n",
    "            train_writer.add_summary(summary, step)\n",
    "\n",
    "            # Update train bar\n",
    "            process_desc.format(loss_val)\n",
    "            progress_bar.update(1)\n",
    "        progress_bar.close()\n",
    "\n",
    "    print('learning rate: %f' % (lr))\n",
    "    print('train mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "    print('train clustering accuracy: %f' % (acc / float(num_batches)))\n",
    "    return y_pool\n",
    "\n",
    "\n",
    "def eval_one_epoch(sess, ops, test_writer, hvd_rank, hvd_size, is_full_training):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    global EPOCH_CNT\n",
    "    is_training = False\n",
    "    test_idxs = np.arange(0, len(TEST_FILES))\n",
    "    # Test on all data: last batch might be smaller than BATCH_SIZE\n",
    "    loss_sum = acc = 0\n",
    "    acc_kmeans = 0\n",
    "\n",
    "    for fn in range(len(TEST_FILES)):\n",
    "        # print('----' + str(fn) + '-----')\n",
    "        current_file = os.path.join(H5_DIR, TEST_FILES[test_idxs[fn]])\n",
    "        current_data, current_label, current_cluster = load_h5_hvd(current_file, hvd_rank, hvd_size)\n",
    "        current_label = np.squeeze(current_label)\n",
    "\n",
    "        file_size = current_data.shape[0]\n",
    "        num_batches = file_size // BATCH_SIZE\n",
    "        # num_batches = 5\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = (batch_idx + 1) * BATCH_SIZE\n",
    "            batch_data, batch_label = get_batch(current_data, current_label, start_idx, end_idx)\n",
    "            cur_batch_size = end_idx - start_idx\n",
    "\n",
    "            feed_dict = {ops['pointclouds_pl']: batch_data,\n",
    "                         ops['is_training_pl']: is_training,\n",
    "                         ops['labels_pl']: batch_label,\n",
    "                         ops['alpha']: 2 * (EPOCH_CNT - MAX_PRETRAIN + 1),}\n",
    "\n",
    "            if is_full_training:\n",
    "                summary, step, loss_val, max_pool, dist, mu = sess.run([ops['merged'], ops['step'],\n",
    "                                                                        ops['kmeans_loss'],\n",
    "                                                                        ops['max_pool'], ops['stack_dist'],\n",
    "                                                                        ops['mu']],\n",
    "                                                                       feed_dict=feed_dict)\n",
    "                if batch_idx == 0:\n",
    "                    print(\"mu: {}\".format(mu))\n",
    "                batch_cluster = np.array([np.where(r == 1)[0][0] for r in current_cluster[start_idx:end_idx]])\n",
    "                cluster_assign = np.zeros((cur_batch_size), dtype=int)\n",
    "                for i in range(cur_batch_size):\n",
    "                    index_closest_cluster = np.argmin(dist[:, i])\n",
    "                    cluster_assign[i] = index_closest_cluster\n",
    "\n",
    "                acc += cluster_acc(batch_cluster, cluster_assign)\n",
    "\n",
    "            else:\n",
    "                summary, step, loss_val = sess.run([ops['merged'], ops['step'],\n",
    "                                                    ops['class_loss']],\n",
    "                                                   feed_dict=feed_dict)\n",
    "\n",
    "            test_writer.add_summary(summary, step)\n",
    "\n",
    "            loss_sum += np.mean(loss_val)\n",
    "\n",
    "    total_loss = loss_sum * 1.0 / float(num_batches)\n",
    "    print('test mean loss: %f' % (total_loss))\n",
    "    print('testing clustering accuracy: %f' % (acc / float(num_batches)))\n",
    "\n",
    "    EPOCH_CNT += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkdl import HorovodRunner\n",
    " \n",
    "hr = HorovodRunner(np=2)\n",
    "hr.run(train_hvd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     HorovodRunner will stream all training logs to notebook cell output. If there are too many logs, you\n",
    ">     can adjust the log level in your train method. Or you can set driver_log_verbosity to\n",
    ">     'log_callback_only' and use a HorovodRunner log  callback on the first worker to get concise\n",
    ">     progress updates.\n",
    ">     The global names read or written to by the pickled function are {'get_bn_decay', 'MOMENTUM', 'range', 'NUM_CLASSES', 'OPTIMIZER', 'get_learning_rate', 'MAX_EPOCH', 'NUM_FEAT', 'GPU_INDEX', 'BATCH_SIZE', 'LOG_DIR', 'FLAGS', 'checkpoint_dir', 'eval_one_epoch', 'NUM_POINT', 'MAX_PRETRAIN', 'print', 'str', 'train_one_epoch'}.\n",
    ">     The pickled object size is 10641 bytes.\n",
    ">\n",
    ">     ### How to enable Horovod Timeline? ###\n",
    ">     HorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\n",
    ">     record a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\n",
    ">     timeline file to be created. You can then open the timeline file  using the chrome://tracing\n",
    ">     facility of the Chrome browser.\n",
    ">\n",
    ">     Start training.\n",
    ">     [1,0]<stderr>:Traceback (most recent call last):\n",
    ">     [1,0]<stderr>:  File \"<string>\", line 1, in <module>\n",
    ">     [1,0]<stderr>:  File \"/databricks/.python_edge_libs/sparkdl/horovod/runner.py\", line 222, in wrapped_main\n",
    ">     [1,0]<stderr>:    return_value = main(**kwargs)\n",
    ">     [1,0]<stderr>:  File \"<command-178059608683696>\", line 2, in train_hvd\n",
    ">     [1,0]<stderr>:  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages/horovod/tensorflow/__init__.py\", line 28, in <module>\n",
    ">     [1,0]<stderr>:    from horovod.tensorflow.mpi_ops import allgather, broadcast, _allreduce\n",
    ">     [1,0]<stderr>:  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages/horovod/tensorflow/mpi_ops.py\", line 49, in <module>\n",
    ">     [1,0]<stderr>:    MPI_LIB = _load_library('mpi_lib' + get_ext_suffix())\n",
    ">     [1,0]<stderr>:  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages/horovod/tensorflow/mpi_ops.py\", line 45, in _load_library\n",
    ">     [1,0]<stderr>:    library = load_library.load_op_library(filename)\n",
    ">     [1,0]<stderr>:  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages/tensorflow_core/python/framework/load_library.py\", line 61, in load_op_library\n",
    ">     [1,0]<stderr>:    lib_handle = py_tf.TF_LoadLibrary(library_filename)\n",
    ">     [1,0]<stderr>:tensorflow.python.framework.errors_impl.NotFoundError: libtensorflow_framework.so.2: cannot open shared object file: No such file or directory\n",
    ">     --------------------------------------------------------------------------\n",
    ">     Primary job  terminated normally, but 1 process returned\n",
    ">     a non-zero exit code. Per user-direction, the job has been aborted.\n",
    ">     --------------------------------------------------------------------------\n",
    ">     [1,1]<stderr>:Traceback (most recent call last):\n",
    ">     [1,1]<stderr>:  File \"<string>\", line 1, in <module>\n",
    ">     [1,1]<stderr>:  File \"/databricks/.python_edge_libs/sparkdl/horovod/runner.py\", line 222, in wrapped_main\n",
    ">     [1,1]<stderr>:    return_value = main(**kwargs)\n",
    ">     [1,1]<stderr>:  File \"<command-178059608683696>\", line 2, in train_hvd\n",
    ">     [1,1]<stderr>:  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages/horovod/tensorflow/__init__.py\", line 28, in <module>\n",
    ">     [1,1]<stderr>:    from horovod.tensorflow.mpi_ops import allgather, broadcast, _allreduce\n",
    ">     [1,1]<stderr>:  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages/horovod/tensorflow/mpi_ops.py\", line 49, in <module>\n",
    ">     [1,1]<stderr>:    MPI_LIB = _load_library('mpi_lib' + get_ext_suffix())\n",
    ">     [1,1]<stderr>:  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages/horovod/tensorflow/mpi_ops.py\", line 45, in _load_library\n",
    ">     [1,1]<stderr>:    library = load_library.load_op_library(filename)\n",
    ">     [1,1]<stderr>:  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-f326b9f3-c400-4f6e-be35-a40e35942671/lib/python3.7/site-packages/tensorflow_core/python/framework/load_library.py\", line 61, in load_op_library\n",
    ">     [1,1]<stderr>:    lib_handle = py_tf.TF_LoadLibrary(library_filename)\n",
    ">     [1,1]<stderr>:tensorflow.python.framework.errors_impl.NotFoundError: libtensorflow_framework.so.2: cannot open shared object file: No such file or directory\n",
    ">     --------------------------------------------------------------------------\n",
    ">     mpirun detected that one or more processes exited with non-zero status, thus causing\n",
    ">     the job to be terminated. The first process to do so was:\n",
    ">\n",
    ">       Process name: [[39501,1],0]\n",
    ">       Exit code:    1\n",
    ">     --------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hvd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
