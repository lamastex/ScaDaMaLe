{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is currently just a copy paste from the github repository of the\n",
    "file `train_kmean.py`. We have to do the following points: 1. Split\n",
    "reading file for each device 2. instead of session.run do model.compile\n",
    "and model.fit\n",
    "\n",
    "install correct version of tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall tensorflow\n",
    "#pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Name: tensorflow\n",
    ">     Version: 2.3.0\n",
    ">     Summary: TensorFlow is an open source machine learning framework for everyone.\n",
    ">     Home-page: https://www.tensorflow.org/\n",
    ">     Author: Google Inc.\n",
    ">     Author-email: packages@tensorflow.org\n",
    ">     License: Apache 2.0\n",
    ">     Location: /databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages\n",
    ">     Requires: astunparse, tensorflow-estimator, termcolor, wrapt, scipy, numpy, h5py, opt-einsum, protobuf, google-pasta, keras-preprocessing, six, gast, grpcio, absl-py, tensorboard, wheel\n",
    ">     Required-by: spark-tensorflow-distributor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install tensorflow-cpu==1.15.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Python interpreter will be restarted.\n",
    ">     Collecting tensorflow-cpu==1.15.*\n",
    ">       Downloading tensorflow_cpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (110.8 MB)\n",
    ">     Collecting tensorboard<1.16.0,>=1.15.0\n",
    ">       Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
    ">     Requirement already satisfied: termcolor>=1.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e09ee3ea-34b1-46b6-baae-5ccfa3b79d97/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (1.1.0)\n",
    ">     Requirement already satisfied: wrapt>=1.11.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e09ee3ea-34b1-46b6-baae-5ccfa3b79d97/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (1.11.2)\n",
    ">     Requirement already satisfied: keras-preprocessing>=1.0.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e09ee3ea-34b1-46b6-baae-5ccfa3b79d97/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (1.1.2)\n",
    ">     Requirement already satisfied: astor>=0.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e09ee3ea-34b1-46b6-baae-5ccfa3b79d97/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (0.8.0)\n",
    ">     Requirement already satisfied: numpy<2.0,>=1.16.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e09ee3ea-34b1-46b6-baae-5ccfa3b79d97/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (1.18.1)\n",
    ">     Requirement already satisfied: opt-einsum>=2.3.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e09ee3ea-34b1-46b6-baae-5ccfa3b79d97/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (3.3.0)\n",
    ">     Collecting gast==0.2.2\n",
    ">       Downloading gast-0.2.2.tar.gz (10 kB)\n",
    ">     Requirement already satisfied: six>=1.10.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e09ee3ea-34b1-46b6-baae-5ccfa3b79d97/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (1.14.0)\n",
    ">     Requirement already satisfied: protobuf>=3.6.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e09ee3ea-34b1-46b6-baae-5ccfa3b79d97/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (3.11.4)\n",
    ">     Requirement already satisfied: grpcio>=1.8.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e09ee3ea-34b1-46b6-baae-5ccfa3b79d97/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (1.27.2)\n",
    ">     Collecting tensorflow-estimator==1.15.1\n",
    ">       Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
    ">     Collecting keras-applications>=1.0.8\n",
    ">       Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
    ">     Requirement already satisfied: google-pasta>=0.1.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e09ee3ea-34b1-46b6-baae-5ccfa3b79d97/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (0.2.0)\n",
    ">     Requirement already satisfied: absl-py>=0.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e09ee3ea-34b1-46b6-baae-5ccfa3b79d97/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (0.9.0)\n",
    ">     Requirement already satisfied: wheel>=0.26 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e09ee3ea-34b1-46b6-baae-5ccfa3b79d97/lib/python3.7/site-packages (from tensorflow-cpu==1.15.*) (0.34.2)\n",
    ">     Requirement already satisfied: markdown>=2.6.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e09ee3ea-34b1-46b6-baae-5ccfa3b79d97/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-cpu==1.15.*) (3.1.1)\n",
    ">     Requirement already satisfied: setuptools>=41.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e09ee3ea-34b1-46b6-baae-5ccfa3b79d97/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-cpu==1.15.*) (45.2.0.post20200210)\n",
    ">     Requirement already satisfied: werkzeug>=0.11.15 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e09ee3ea-34b1-46b6-baae-5ccfa3b79d97/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-cpu==1.15.*) (1.0.0)\n",
    ">     Requirement already satisfied: h5py in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e09ee3ea-34b1-46b6-baae-5ccfa3b79d97/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow-cpu==1.15.*) (2.10.0)\n",
    ">     Building wheels for collected packages: gast\n",
    ">       Building wheel for gast (setup.py): started\n",
    ">       Building wheel for gast (setup.py): finished with status 'done'\n",
    ">       Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=823e1f48f27227247f35627f2e234d951d604375ed37d837dec5e95c5d967f26\n",
    ">       Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
    ">     Successfully built gast\n",
    ">     ERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\n",
    ">     ERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 1.15.0 which is incompatible.\n",
    ">     ERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\n",
    ">     Installing collected packages: tensorboard, gast, tensorflow-estimator, keras-applications, tensorflow-cpu\n",
    ">       Attempting uninstall: tensorboard\n",
    ">         Found existing installation: tensorboard 2.3.0\n",
    ">         Uninstalling tensorboard-2.3.0:\n",
    ">           Successfully uninstalled tensorboard-2.3.0\n",
    ">       Attempting uninstall: gast\n",
    ">         Found existing installation: gast 0.3.3\n",
    ">         Uninstalling gast-0.3.3:\n",
    ">           Successfully uninstalled gast-0.3.3\n",
    ">       Attempting uninstall: tensorflow-estimator\n",
    ">         Found existing installation: tensorflow-estimator 2.3.0\n",
    ">         Uninstalling tensorflow-estimator-2.3.0:\n",
    ">           Successfully uninstalled tensorflow-estimator-2.3.0\n",
    ">     Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-cpu-1.15.0 tensorflow-estimator-1.15.1\n",
    ">     Python interpreter will be restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install https://databricks-prod-cloudfront.cloud.databricks.com/artifacts/tensorflow/runtime-7.x/tensorflow-1.15.3-cp37-cp37m-linux_x86_64.whl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls databricks/driver/06_LHC/utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "  \n",
    "\n",
    "get the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, shlex\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import os\n",
    "import sys\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.set_printoptions(edgeitems=1000)\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "BASE_DIR = os.path.join(os.getcwd(), '06_LHC','scripts')  #os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, '..', 'models'))\n",
    "sys.path.append(os.path.join(BASE_DIR, '..', 'utils'))\n",
    "import provider\n",
    "import gapnet_classify as MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     conf\n",
    ">     derby.log\n",
    ">     eventlogs\n",
    ">     logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[13]: '1.15.0'\n",
    "\n",
    "  \n",
    "\n",
    "Get the arguments (we hvae to create the namespace like this with a\n",
    "dictionary since notebooks don't easily incorporate argument parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls databricks/driver/06_LHC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "parserdict = {'max_dim': 3, #help='Dimension of the encoding layer [Default: 3]')\n",
    "              'n_clusters': 3, #help='Number of clusters [Default: 3]')\n",
    "              'gpu': 0, #help='GPU to use [default: GPU 0]')\n",
    "              'model': 'gapnet_clasify', #help='Model name [default: gapnet_classify]')\n",
    "              'log_dir': 'log', #help='Log dir [default: log]')\n",
    "              'num_point': 100, #help='Point Number [default: 100]')\n",
    "              'max_epoch': 1, #help='Epoch to run [default: 200]')\n",
    "              'batch_size': 512, #help='Batch Size during training [default: 512]')\n",
    "              'learning_rate': 0.001, #help='Initial learning rate [default: 0.01]')\n",
    "\n",
    "              'momentum': 0.9, #help='Initial momentum [default: 0.9]')\n",
    "              'optimizer': 'adam', #help='adam or momentum [default: adam]')\n",
    "              'decay_step': 500000, #help='Decay step for lr decay [default: 500000]')\n",
    "              'wd': 0.0, #help='Weight Decay [Default: 0.0]')\n",
    "              'decay_rate': 0.5, #help='Decay rate for lr decay [default: 0.5]')\n",
    "              'output_dir': 'train_results', #help='Directory that stores all training logs and trained models')\n",
    "              'data_dir': os.path.join(os.getcwd(),'06_LHC', 'h5'), # '../h5', #help='directory with data [default: hdf5_data]')\n",
    "              'nfeat': 8, #help='Number of features [default: 8]')\n",
    "              'ncat': 20, #help='Number of categories [default: 20]')\n",
    "             }\n",
    "\n",
    "FLAGS = Namespace(**parserdict)\n",
    "\n",
    "H5_DIR = FLAGS.data_dir\n",
    "\n",
    "EPOCH_CNT = 0\n",
    "MAX_PRETRAIN = 10\n",
    "BATCH_SIZE = FLAGS.batch_size\n",
    "NUM_POINT = FLAGS.num_point\n",
    "NUM_FEAT = FLAGS.nfeat\n",
    "NUM_CLASSES = FLAGS.ncat\n",
    "MAX_EPOCH = FLAGS.max_epoch\n",
    "BASE_LEARNING_RATE = FLAGS.learning_rate\n",
    "GPU_INDEX = FLAGS.gpu\n",
    "MOMENTUM = FLAGS.momentum\n",
    "OPTIMIZER = FLAGS.optimizer\n",
    "DECAY_STEP = FLAGS.decay_step\n",
    "DECAY_RATE = FLAGS.decay_rate\n",
    "\n",
    "# MODEL = importlib.import_module(FLAGS.model) # import network module\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'models', FLAGS.model + '.py')\n",
    "LOG_DIR = os.path.join('..', 'logs', FLAGS.log_dir)\n",
    "\n",
    "if not os.path.exists(LOG_DIR): os.makedirs(LOG_DIR)\n",
    "os.system('cp %s.py %s' % (MODEL_FILE, LOG_DIR))  # bkp of model def\n",
    "os.system('cp train_kmeans.py %s' % (LOG_DIR))  # bkp of train procedure\n",
    "\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99\n",
    "\n",
    "LEARNING_RATE_CLIP = 1e-5\n",
    "HOSTNAME = socket.gethostname()\n",
    "\n",
    "TRAIN_FILES = provider.getDataFiles(os.path.join(H5_DIR, 'train_files_wztop.txt'))\n",
    "TEST_FILES = provider.getDataFiles(os.path.join(H5_DIR, 'test_files_wztop.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "define utilisation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate(batch):\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        BASE_LEARNING_RATE,  # Base learning rate.\n",
    "        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "        DECAY_STEP,  # Decay step.\n",
    "        DECAY_RATE,  # Decay rate.\n",
    "        staircase=True)\n",
    "    learning_rate = tf.maximum(learning_rate, LEARNING_RATE_CLIP)  # CLIP THE LEARNING RATE!\n",
    "    return learning_rate\n",
    "\n",
    "\n",
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.train.exponential_decay(\n",
    "        BN_INIT_DECAY,\n",
    "        batch * BATCH_SIZE,\n",
    "        BN_DECAY_DECAY_STEP,\n",
    "        BN_DECAY_DECAY_RATE,\n",
    "        staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:' + str(GPU_INDEX)):\n",
    "            pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT, NUM_FEAT)\n",
    "\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "\n",
    "            # Note the global_step=batch parameter to minimize.\n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            alpha = tf.placeholder(dtype=tf.float32, shape=())\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "            print(\"--- Get model and loss\")\n",
    "\n",
    "            pred, max_pool = MODEL.get_model(pointclouds_pl, is_training=is_training_pl,\n",
    "                                             bn_decay=bn_decay,\n",
    "                                             num_class=NUM_CLASSES, weight_decay=FLAGS.wd,\n",
    "                                             )\n",
    "\n",
    "            class_loss = MODEL.get_focal_loss(pred, labels_pl, NUM_CLASSES)\n",
    "            mu = tf.Variable(tf.zeros(shape=(FLAGS.n_clusters, FLAGS.max_dim)), name=\"mu\",\n",
    "                             trainable=True)  # k centroids\n",
    "            kmeans_loss, stack_dist = MODEL.get_loss_kmeans(max_pool, mu, FLAGS.max_dim,\n",
    "                                                            FLAGS.n_clusters, alpha)\n",
    "\n",
    "            full_loss = kmeans_loss + class_loss\n",
    "\n",
    "            print(\"--- Get training operator\")\n",
    "            # Get training operator\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "            train_op_full = optimizer.minimize(full_loss, global_step=batch)\n",
    "            train_op = optimizer.minimize(class_loss, global_step=batch)\n",
    "\n",
    "            # Add ops to save and restore all the variables.\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        # Create a session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = False\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Add summary writers\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'), sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'), sess.graph)\n",
    "\n",
    "        # Init variables\n",
    "        print(\"Total number of weights for the model: \",\n",
    "              np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]))\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'labels_pl': labels_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'max_pool': max_pool,\n",
    "               'pred': pred,\n",
    "               'alpha': alpha,\n",
    "               'mu': mu,\n",
    "               'stack_dist': stack_dist,\n",
    "               'class_loss': class_loss,\n",
    "               'kmeans_loss': kmeans_loss,\n",
    "               'train_op': train_op,\n",
    "               'train_op_full': train_op_full,\n",
    "               'merged': merged,\n",
    "               'step': batch,\n",
    "               'learning_rate': learning_rate\n",
    "               }\n",
    "\n",
    "        for epoch in range(MAX_EPOCH):\n",
    "            print('**** EPOCH %03d ****' % (epoch))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_full_training = epoch > MAX_PRETRAIN\n",
    "            max_pool = train_one_epoch(sess, ops, train_writer, is_full_training)\n",
    "            if epoch == MAX_PRETRAIN:\n",
    "                centers = KMeans(n_clusters=FLAGS.n_clusters).fit(np.squeeze(max_pool))\n",
    "                centers = centers.cluster_centers_\n",
    "                sess.run(tf.assign(mu, centers))\n",
    "\n",
    "            eval_one_epoch(sess, ops, test_writer, is_full_training)\n",
    "            if is_full_training:\n",
    "                save_path = saver.save(sess, os.path.join(LOG_DIR, 'cluster.ckpt'))\n",
    "            else:\n",
    "                save_path = saver.save(sess, os.path.join(LOG_DIR, 'model.ckpt'))\n",
    "            print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, label, start_idx, end_idx):\n",
    "    batch_label = label[start_idx:end_idx]\n",
    "    batch_data = data[start_idx:end_idx, :, :]\n",
    "    return batch_data, batch_label\n",
    "\n",
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = linear_sum_assignment(w.max() - w)\n",
    "    ind = np.asarray(ind)\n",
    "    ind = np.transpose(ind)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "one epoch training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(sess, ops, train_writer, is_full_training):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = True\n",
    "\n",
    "    train_idxs = np.arange(0, len(TRAIN_FILES))\n",
    "\n",
    "    acc = loss_sum = 0\n",
    "    y_pool = []\n",
    "    for fn in range(len(TRAIN_FILES)):\n",
    "        # print('----' + str(fn) + '-----')\n",
    "        current_file = os.path.join(H5_DIR, TRAIN_FILES[train_idxs[fn]])\n",
    "        current_data, current_label, current_cluster = provider.load_h5_data_label_seg(current_file)\n",
    "\n",
    "        current_label = np.squeeze(current_label)\n",
    "\n",
    "        file_size = current_data.shape[0]\n",
    "        num_batches = file_size // BATCH_SIZE\n",
    "        # num_batches = 5\n",
    "        print(str(datetime.now()))\n",
    "\n",
    "        # initialise progress bar\n",
    "        process_desc = \"Loss {:2.3e}\"\n",
    "        progress_bar = tqdm(initial=0, leave=True, total=num_batches,\n",
    "                            desc=process_desc.format(0),\n",
    "                            position=0)\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = (batch_idx + 1) * BATCH_SIZE\n",
    "            batch_data, batch_label = get_batch(current_data, current_label, start_idx, end_idx)\n",
    "            cur_batch_size = end_idx - start_idx\n",
    "            \n",
    "            # print(batch_weight)\n",
    "            feed_dict = {ops['pointclouds_pl']: batch_data,\n",
    "                         ops['labels_pl']: batch_label,\n",
    "                         ops['is_training_pl']: is_training,\n",
    "                         ops['alpha']: 2 * (EPOCH_CNT - MAX_PRETRAIN + 1),}\n",
    "            if is_full_training:\n",
    "                summary, step, _, loss_val, dist, lr = sess.run([ops['merged'], ops['step'],\n",
    "                                                                 ops['train_op_full'], ops['kmeans_loss'],\n",
    "                                                                 ops['stack_dist'], ops['learning_rate']],\n",
    "                                                                feed_dict=feed_dict)\n",
    "\n",
    "                batch_cluster = np.array([np.where(r == 1)[0][0] for r in current_cluster[start_idx:end_idx]])\n",
    "                cluster_assign = np.zeros((cur_batch_size), dtype=int)\n",
    "\n",
    "                for i in range(cur_batch_size):\n",
    "                    index_closest_cluster = np.argmin(dist[:, i])\n",
    "                    cluster_assign[i] = index_closest_cluster\n",
    "\n",
    "                acc += cluster_acc(batch_cluster, cluster_assign)\n",
    "            else:\n",
    "                summary, step, _, loss_val, max_pool, lr = sess.run([ops['merged'], ops['step'],\n",
    "                                                                     ops['train_op'], ops['class_loss'],\n",
    "                                                                     ops['max_pool'], ops['learning_rate']],\n",
    "                                                                    feed_dict=feed_dict)\n",
    "\n",
    "                if len(y_pool) == 0:\n",
    "                    y_pool = np.squeeze(max_pool)\n",
    "                else:\n",
    "                    y_pool = np.concatenate((y_pool, np.squeeze(max_pool)), axis=0)\n",
    "\n",
    "            loss_sum += np.mean(loss_val)\n",
    "\n",
    "            train_writer.add_summary(summary, step)\n",
    "\n",
    "            # Update train bar\n",
    "            process_desc.format(loss_val)\n",
    "            progress_bar.update(1)\n",
    "        progress_bar.close()\n",
    "\n",
    "    print('learning rate: %f' % (lr))\n",
    "    print('train mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "    print('train clustering accuracy: %f' % (acc / float(num_batches)))\n",
    "    return y_pool\n",
    "\n",
    "\n",
    "def eval_one_epoch(sess, ops, test_writer, is_full_training):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    global EPOCH_CNT\n",
    "    is_training = False\n",
    "    test_idxs = np.arange(0, len(TEST_FILES))\n",
    "    # Test on all data: last batch might be smaller than BATCH_SIZE\n",
    "    loss_sum = acc = 0\n",
    "    acc_kmeans = 0\n",
    "\n",
    "    for fn in range(len(TEST_FILES)):\n",
    "        # print('----' + str(fn) + '-----')\n",
    "        current_file = os.path.join(H5_DIR, TEST_FILES[test_idxs[fn]])\n",
    "        current_data, current_label, current_cluster = provider.load_h5_data_label_seg(current_file)\n",
    "\n",
    "        file_size = current_data.shape[0]\n",
    "        num_batches = file_size // BATCH_SIZE\n",
    "        # num_batches = 5\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = (batch_idx + 1) * BATCH_SIZE\n",
    "            batch_data, batch_label = get_batch(current_data, current_label, start_idx, end_idx)\n",
    "            cur_batch_size = end_idx - start_idx\n",
    "\n",
    "            feed_dict = {ops['pointclouds_pl']: batch_data,\n",
    "                         ops['is_training_pl']: is_training,\n",
    "                         ops['labels_pl']: batch_label,\n",
    "                         ops['alpha']: 2 * (EPOCH_CNT - MAX_PRETRAIN + 1),}\n",
    "\n",
    "            if is_full_training:\n",
    "                summary, step, loss_val, max_pool, dist, mu = sess.run([ops['merged'], ops['step'],\n",
    "                                                                        ops['kmeans_loss'],\n",
    "                                                                        ops['max_pool'], ops['stack_dist'],\n",
    "                                                                        ops['mu']],\n",
    "                                                                       feed_dict=feed_dict)\n",
    "                if batch_idx == 0:\n",
    "                    print(\"mu: {}\".format(mu))\n",
    "                batch_cluster = np.array([np.where(r == 1)[0][0] for r in current_cluster[start_idx:end_idx]])\n",
    "                cluster_assign = np.zeros((cur_batch_size), dtype=int)\n",
    "                for i in range(cur_batch_size):\n",
    "                    index_closest_cluster = np.argmin(dist[:, i])\n",
    "                    cluster_assign[i] = index_closest_cluster\n",
    "\n",
    "                acc += cluster_acc(batch_cluster, cluster_assign)\n",
    "\n",
    "            else:\n",
    "                summary, step, loss_val = sess.run([ops['merged'], ops['step'],\n",
    "                                                    ops['class_loss']],\n",
    "                                                   feed_dict=feed_dict)\n",
    "\n",
    "            test_writer.add_summary(summary, step)\n",
    "\n",
    "            loss_sum += np.mean(loss_val)\n",
    "\n",
    "    total_loss = loss_sum * 1.0 / float(num_batches)\n",
    "    print('test mean loss: %f' % (total_loss))\n",
    "    print('testing clustering accuracy: %f' % (acc / float(num_batches)))\n",
    "\n",
    "    EPOCH_CNT += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
