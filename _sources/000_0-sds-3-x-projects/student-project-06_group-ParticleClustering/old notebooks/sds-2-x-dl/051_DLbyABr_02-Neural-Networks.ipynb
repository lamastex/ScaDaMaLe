{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SDS-2.x, Scalable Data Engineering Science](https://lamastex.github.io/scalable-data-science/sds/2/x/)\n",
    "=======================================================================================================\n",
    "\n",
    "This is a 2019 augmentation and update of [Adam\n",
    "Breindel](https://www.linkedin.com/in/adbreind)'s initial notebooks.\n",
    "\n",
    "Artificial Neural Network - Perceptron\n",
    "======================================\n",
    "\n",
    "The field of artificial neural networks started out with an\n",
    "electromechanical binary unit called a perceptron.\n",
    "\n",
    "The perceptron took a weighted set of input signals and chose an ouput\n",
    "state (on/off or high/low) based on a threshold.\n",
    "\n",
    "&lt;img src=\"http://i.imgur.com/c4pBaaU.jpg\"&gt;\n",
    "\n",
    "(raaz) Thus, the perceptron is defined by:\n",
    "\n",
    "$$ f(1, x*1,x*2,\\\\ldots , x*n \\\\, ; \\\\, w*0,w*1,w*2,\\\\ldots , w*n) =\n",
    "\\\\begin{cases} 1 & \\\\text{if} \\\\quad \\\\sum*{i=0}^n w*i x*i &gt; 0 \\\\ 0 &\n",
    "\\\\text{otherwise} \\\\end{cases} $$ and implementable with the following\n",
    "arithmetical and logical unit (ALU) operations in a machine:\n",
    "\n",
    "-   n inputs from one $n$-dimensional data point: $x*1,x*2,\\\\ldots x\\_n\n",
    "    \\\\, \\\\in \\\\, \\\\mathbb{R}^n$\n",
    "-   arithmetic operations\n",
    "    -   n+1 multiplications\n",
    "    -   n additions\n",
    "-   boolean operations\n",
    "    -   one if-then on an inequality\n",
    "-   one output $o \\\\in {0,1}$, i.e., $o$ belongs to the set containing\n",
    "    $0$ and $1$\n",
    "-   n+1 parameters of interest\n",
    "\n",
    "This is just a hyperplane given by a dot product of $n+1$ known inputs\n",
    "and $n+1$ unknown parameters that can be estimated. This hyperplane can\n",
    "be used to define a hyperplane that partitions $\\\\mathbb{R}^{n+1}$, the\n",
    "real Euclidean space, into two parts labelled by the outputs $0$ and\n",
    "$1$.\n",
    "\n",
    "The problem of finding estimates of the parameters,\n",
    "$(\\\\hat{w}*0,\\\\hat{w}*1,\\\\hat{w}*2,\\\\ldots \\\\hat{w}*n) \\\\in\n",
    "\\\\mathbb{R}^{(n+1)}$, in some statistically meaningful manner for a\n",
    "predicting task by using the training data given by, say $k$ *labelled\n",
    "points*, where you know both the input and output: $$ \\\\left( ( \\\\, 1,\n",
    "x*1^{(1)},x*2^{(1)}, \\\\ldots x*n^{(1)}), (o^{(1)}) \\\\, ), \\\\, ( \\\\, 1,\n",
    "x*1^{(2)},x*2^{(2)}, \\\\ldots x*n^{(2)}), (o^{(2)}) \\\\, ), \\\\, \\\\ldots\n",
    "\\\\, , ( \\\\, 1, x*1^{(k)},x*2^{(k)}, \\\\ldots x\\_n^{(k)}), (o^{(k)}) \\\\, )\n",
    "\\\\right) \\\\, \\\\in \\\\, (\\\\mathbb{R}^{n+1} \\\\times { 0,1 } )^k $$ is the\n",
    "machine learning problem here.\n",
    "\n",
    "Succinctly, we are after a random mapping, denoted below by\n",
    "$\\\\mapsto*{\\\\rightsquigarrow}$, called the *estimator*: $$\n",
    "(\\\\mathbb{R}^{n+1} \\\\times {0,1})^k \\\\mapsto*{\\\\rightsquigarrow} \\\\,\n",
    "\\\\left( \\\\, \\\\mathtt{model}( (1,x*1,x*2,\\\\ldots,x*n) \\\\,;\\\\,\n",
    "(\\\\hat{w}*0,\\\\hat{w}*1,\\\\hat{w}*2,\\\\ldots \\\\hat{w}\\_n)) :\n",
    "\\\\mathbb{R}^{n+1} \\\\to {0,1} \\\\, \\\\right) $$ which takes *random*\n",
    "labelled dataset (to understand random here think of two scientists\n",
    "doing independent experiments to get their own training datasets) of\n",
    "size $k$ and returns a *model*. These mathematical notions correspond\n",
    "exactly to the `estimator` and `model` (which is a `transformer`) in the\n",
    "language of Apache Spark's Machine Learning Pipleines we have seen\n",
    "before.\n",
    "\n",
    "We can use this `transformer` for *prediction* of *unlabelled data*\n",
    "where we only observe the input and what to know the output under some\n",
    "reasonable assumptions.\n",
    "\n",
    "Of course we want to be able to generalize so we don't overfit to the\n",
    "training data using some *empirical risk minisation rule* such as\n",
    "cross-validation. Again, we have seen these in Apache Spark for other ML\n",
    "methods like linear regression and decision trees.\n",
    "\n",
    "If the output isn't right, we can adjust the weights, threshold, or bias\n",
    "($x\\_0$ above)\n",
    "\n",
    "The model was inspired by discoveries about the neurons of animals, so\n",
    "hopes were quite high that it could lead to a sophisticated machine.\n",
    "This model can be extended by adding multiple neurons in parallel. And\n",
    "we can use linear output instead of a threshold if we like for the\n",
    "output.\n",
    "\n",
    "If we were to do so, the output would look like ${x \\\\cdot w} + w\\_0$\n",
    "(this is where the vector multiplication and, eventually, matrix\n",
    "multiplication, comes in)\n",
    "\n",
    "When we look at the math this way, we see that despite this being an\n",
    "interesting model, it's really just a fancy linear calculation.\n",
    "\n",
    "And, in fact, the proof that this model -- being linear -- could not\n",
    "solve any problems whose solution was nonlinear ... led to the first of\n",
    "several \"AI / neural net winters\" when the excitement was quickly\n",
    "replaced by disappointment, and most research was abandoned.\n",
    "\n",
    "### Linear Perceptron\n",
    "\n",
    "We'll get to the non-linear part, but the linear perceptron model is a\n",
    "great way to warm up and bridge the gap from traditional linear\n",
    "regression to the neural-net flavor.\n",
    "\n",
    "Let's look at a problem -- the diamonds dataset from R -- and analyze it\n",
    "using two traditional methods in Scikit-Learn, and then we'll start\n",
    "attacking it with neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "input_file = \"/dbfs/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\"\n",
    "\n",
    "df = pd.read_csv(input_file, header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as disp\n",
    "pd.set_option('display.width', 200)\n",
    "disp.display(df[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">        Unnamed: 0  carat        cut color clarity  depth  table  price     x     y     z\n",
    ">     0           1   0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
    ">     1           2   0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
    ">     2           3   0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
    ">     3           4   0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
    ">     4           5   0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n",
    ">     5           6   0.24  Very Good     J    VVS2   62.8   57.0    336  3.94  3.96  2.48\n",
    ">     6           7   0.24  Very Good     I    VVS1   62.3   57.0    336  3.95  3.98  2.47\n",
    ">     7           8   0.26  Very Good     H     SI1   61.9   55.0    337  4.07  4.11  2.53\n",
    ">     8           9   0.22       Fair     E     VS2   65.1   61.0    337  3.87  3.78  2.49\n",
    ">     9          10   0.23  Very Good     H     VS1   59.4   61.0    338  4.00  4.05  2.39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "disp.display(df2[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">        carat      cut color clarity  depth  table  price     x     y     z\n",
    ">     0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
    ">     1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
    ">     2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.get_dummies(df2) # this gives a one-hot encoding of categorial variables\n",
    "\n",
    "disp.display(df3[range(7,18)][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">        cut_Fair  cut_Good  cut_Ideal  cut_Premium  cut_Very Good  color_D  color_E  color_F  color_G  color_H  color_I\n",
    ">     0         0         0          1            0              0        0        1        0        0        0        0\n",
    ">     1         0         0          0            1              0        0        1        0        0        0        0\n",
    ">     2         0         1          0            0              0        0        1        0        0        0        0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process to get y\n",
    "y = df3.iloc[:,3:4].as_matrix().flatten()\n",
    "y.flatten()\n",
    "\n",
    "# preprocess and reshape X as a matrix\n",
    "X = df3.drop(df3.columns[3], axis=1).as_matrix()\n",
    "np.shape(X)\n",
    "\n",
    "# break the dataset into training and test set with a 75% and 25% split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define a decisoin tree model with max depth 10\n",
    "dt = DecisionTreeRegressor(random_state=0, max_depth=10)\n",
    "\n",
    "# fit the decision tree to the training data to get a fitted model\n",
    "model = dt.fit(X_train, y_train)\n",
    "\n",
    "# predict the features or X values of the test data using the fitted model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# print the MSE performance measure of the fit by comparing the predicted versus the observed values of y \n",
    "print(\"RMSE %f\" % np.sqrt(mean_squared_error(y_test, y_pred)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     RMSE 726.921870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Do the same with linear regression and not a worse MSE\n",
    "lr = linear_model.LinearRegression()\n",
    "linear_model = lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linear_model.predict(X_test)\n",
    "print(\"RMSE %f\" % np.sqrt(mean_squared_error(y_test, y_pred)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     RMSE 1124.086095\n",
    "\n",
    "  \n",
    "\n",
    "Now that we have a baseline, let's build a neural network -- linear at\n",
    "first -- and go further.\n",
    "\n",
    "Neural Network with Keras\n",
    "-------------------------\n",
    "\n",
    "### [Keras](https://keras.io/) is a High-Level API for Neural Networks and Deep Learning\n",
    "\n",
    "&lt;img\n",
    "src=\"https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png\"\n",
    "width=600&gt;\n",
    "\n",
    "#### \"*Being able to go from idea to result with the least possible delay is key to doing good research.*\"\n",
    "\n",
    "Maintained by Francois Chollet at Google, it provides\n",
    "\n",
    "-   High level APIs\n",
    "-   Pluggable backends for Theano, TensorFlow, CNTK, MXNet\n",
    "-   CPU/GPU support\n",
    "-   The now-officially-endorsed high-level wrapper for TensorFlow; a\n",
    "    version ships in TF\n",
    "-   Model persistence and other niceties\n",
    "-   JavaScript, iOS, etc. deployment\n",
    "-   Interop with further frameworks, like DeepLearning4J, Spark DL\n",
    "    Pipelines ...\n",
    "\n",
    "Well, with all this, why would you ever *not* use Keras?\n",
    "\n",
    "As an API/Facade, Keras doesn't directly expose all of the internals you\n",
    "might need for something custom and low-level ... so you might need to\n",
    "implement at a lower level first, and then perhaps wrap it to make it\n",
    "easily usable in Keras.\n",
    "\n",
    "Mr. Chollet compiles stats (roughly quarterly) on \"\\[t\\]he state of the\n",
    "deep learning landscape: GitHub activity of major libraries over the\n",
    "past quarter (tickets, forks, and contributors).\"\n",
    "\n",
    "(October 2017: https://twitter.com/fchollet/status/915366704401719296;\n",
    "https://twitter.com/fchollet/status/915626952408436736)\n",
    "&lt;table&gt;&lt;tr&gt;&lt;td&gt;**GitHub**&lt;br&gt; &lt;img\n",
    "src=\"https://i.imgur.com/Dru8N9K.jpg\" width=600&gt;\n",
    "&lt;/td&gt;&lt;td&gt;**Research**&lt;br&gt; &lt;img\n",
    "src=\"https://i.imgur.com/i23TAwf.png\"\n",
    "width=600&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;\n",
    "\n",
    "Keras has wide adoption in industry\n",
    "-----------------------------------\n",
    "\n",
    "&lt;img\n",
    "src=\"https://s3.amazonaws.com/keras.io/img/dl*frameworks*power\\_scores.png\"\n",
    "width=600&gt;\n",
    "\n",
    "### We'll build a \"Dense Feed-Forward Shallow\" Network:\n",
    "\n",
    "(the number of units in the following diagram does not exactly match\n",
    "ours) &lt;img src=\"https://i.imgur.com/84fxFKa.png\"&gt;\n",
    "\n",
    "Grab a Keras API cheat sheet from\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog*assets/Keras*Cheat*Sheet*Python.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# we are going to add layers sequentially one after the other (feed-forward) to our neural network model\n",
    "model = Sequential()\n",
    "\n",
    "# the first layer has 30 nodes (or neurons) with input dimension 26 for our diamonds data\n",
    "# we will use Nomal or Guassian kernel to initialise the weights we want to estimate\n",
    "# our activation function is linear (to mimic linear regression)\n",
    "model.add(Dense(30, input_dim=26, kernel_initializer='normal', activation='linear'))\n",
    "# the next layer is for the response y and has only one node\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "# compile the model with other specifications for loss and type of gradient descent optimisation routine\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "# fit the model to the training data using stochastic gradient descent with a batch-size of 200 and 10% of data held out for validation\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=200, validation_split=0.1)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print()\n",
    "print(\"test set RMSE: %f\" % np.sqrt(scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Train on 36409 samples, validate on 4046 samples\n",
    ">     Epoch 1/10\n",
    ">       200/36409 [..............................] - ETA: 6s - loss: 31224094.0000 - mean_squared_error: 31224094.0000\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008 9400/36409 [======>.......................] - ETA: 0s - loss: 30944916.5957 - mean_squared_error: 30944916.5957\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000818800/36409 [==============>...............] - ETA: 0s - loss: 30824087.2340 - mean_squared_error: 30824087.2340\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000828400/36409 [======================>.......] - ETA: 0s - loss: 30608667.0986 - mean_squared_error: 30608667.0986\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000836409/36409 [==============================] - 0s - loss: 30675831.5550 - mean_squared_error: 30675831.5550 - val_loss: 30065817.1715 - val_mean_squared_error: 30065817.1715\n",
    ">     Epoch 2/10\n",
    ">       200/36409 [..............................] - ETA: 0s - loss: 30545272.0000 - mean_squared_error: 30545272.0000\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008 9600/36409 [======>.......................] - ETA: 0s - loss: 28146635.1042 - mean_squared_error: 28146635.1042\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000819200/36409 [==============>...............] - ETA: 0s - loss: 27564036.7396 - mean_squared_error: 27564036.7396\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000828800/36409 [======================>.......] - ETA: 0s - loss: 26721811.2708 - mean_squared_error: 26721811.2708\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000836409/36409 [==============================] - 0s - loss: 26152942.8957 - mean_squared_error: 26152942.8957 - val_loss: 23769236.3549 - val_mean_squared_error: 23769236.3549\n",
    ">     Epoch 3/10\n",
    ">       200/36409 [..............................] - ETA: 0s - loss: 20422714.0000 - mean_squared_error: 20422714.0000\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008 9800/36409 [=======>......................] - ETA: 0s - loss: 22692571.2857 - mean_squared_error: 22692571.2857\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000819200/36409 [==============>...............] - ETA: 0s - loss: 21344668.1875 - mean_squared_error: 21344668.1875\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000828800/36409 [======================>.......] - ETA: 0s - loss: 20461590.8403 - mean_squared_error: 20461590.8403\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000836409/36409 [==============================] - 0s - loss: 20010043.9827 - mean_squared_error: 20010043.9827 - val_loss: 18357240.2195 - val_mean_squared_error: 18357240.2195\n",
    ">     Epoch 4/10\n",
    ">       200/36409 [..............................] - ETA: 0s - loss: 14938532.0000 - mean_squared_error: 14938532.0000\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008 9800/36409 [=======>......................] - ETA: 0s - loss: 16965881.4286 - mean_squared_error: 16965881.4286\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000819000/36409 [==============>...............] - ETA: 0s - loss: 16703069.7895 - mean_squared_error: 16703069.7895\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000828600/36409 [======================>.......] - ETA: 0s - loss: 16424291.5734 - mean_squared_error: 16424291.5734\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000836409/36409 [==============================] - 0s - loss: 16346794.1603 - mean_squared_error: 16346794.1603 - val_loss: 16184870.8927 - val_mean_squared_error: 16184870.8927\n",
    ">     Epoch 5/10\n",
    ">       200/36409 [..............................] - ETA: 0s - loss: 15538066.0000 - mean_squared_error: 15538066.0000\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008 7600/36409 [=====>........................] - ETA: 0s - loss: 15490160.3947 - mean_squared_error: 15490160.3947\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000817200/36409 [=============>................] - ETA: 0s - loss: 15445834.4767 - mean_squared_error: 15445834.4767\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000826600/36409 [====================>.........] - ETA: 0s - loss: 15349497.5639 - mean_squared_error: 15349497.5639\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000836000/36409 [============================>.] - ETA: 0s - loss: 15268502.4000 - mean_squared_error: 15268502.4000\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000836409/36409 [==============================] - 0s - loss: 15265320.6092 - mean_squared_error: 15265320.6092 - val_loss: 15741750.1518 - val_mean_squared_error: 15741750.1518\n",
    ">     Epoch 6/10\n",
    ">       200/36409 [..............................] - ETA: 0s - loss: 14412805.0000 - mean_squared_error: 14412805.0000\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008 9800/36409 [=======>......................] - ETA: 0s - loss: 15223196.7551 - mean_squared_error: 15223196.7551\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000819400/36409 [==============>...............] - ETA: 0s - loss: 15234776.4639 - mean_squared_error: 15234776.4639\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000829000/36409 [======================>.......] - ETA: 0s - loss: 15098120.2828 - mean_squared_error: 15098120.2828\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000836409/36409 [==============================] - 0s - loss: 15076170.1684 - mean_squared_error: 15076170.1684 - val_loss: 15631488.4825 - val_mean_squared_error: 15631488.4825\n",
    ">     Epoch 7/10\n",
    ">       200/36409 [..............................] - ETA: 0s - loss: 18830928.0000 - mean_squared_error: 18830928.0000\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008 9400/36409 [======>.......................] - ETA: 0s - loss: 15545285.4681 - mean_squared_error: 15545285.4681\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000818600/36409 [==============>...............] - ETA: 0s - loss: 15304064.8387 - mean_squared_error: 15304064.8387\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000828000/36409 [======================>.......] - ETA: 0s - loss: 15159809.7929 - mean_squared_error: 15159809.7929\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000836409/36409 [==============================] - 0s - loss: 14990644.7577 - mean_squared_error: 14990644.7577 - val_loss: 15545448.1700 - val_mean_squared_error: 15545448.1700\n",
    ">     Epoch 8/10\n",
    ">       200/36409 [..............................] - ETA: 0s - loss: 13167877.0000 - mean_squared_error: 13167877.0000\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008 9600/36409 [======>.......................] - ETA: 0s - loss: 14997702.5625 - mean_squared_error: 14997702.5625\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000819200/36409 [==============>...............] - ETA: 0s - loss: 15045306.1667 - mean_squared_error: 15045306.1667\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000828800/36409 [======================>.......] - ETA: 0s - loss: 14895919.7847 - mean_squared_error: 14895919.7847\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000836409/36409 [==============================] - 0s - loss: 14905258.8482 - mean_squared_error: 14905258.8482 - val_loss: 15450581.2491 - val_mean_squared_error: 15450581.2491\n",
    ">     Epoch 9/10\n",
    ">       200/36409 [..............................] - ETA: 0s - loss: 13070927.0000 - mean_squared_error: 13070927.0000\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008 9800/36409 [=======>......................] - ETA: 0s - loss: 14927823.1633 - mean_squared_error: 14927823.1633\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000819400/36409 [==============>...............] - ETA: 0s - loss: 14795454.3918 - mean_squared_error: 14795454.3918\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000828800/36409 [======================>.......] - ETA: 0s - loss: 14833517.0347 - mean_squared_error: 14833517.0347\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000836409/36409 [==============================] - 0s - loss: 14810648.2165 - mean_squared_error: 14810648.2165 - val_loss: 15349156.2027 - val_mean_squared_error: 15349156.2027\n",
    ">     Epoch 10/10\n",
    ">       200/36409 [..............................] - ETA: 0s - loss: 13465992.0000 - mean_squared_error: 13465992.0000\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008 9600/36409 [======>.......................] - ETA: 0s - loss: 14671580.1458 - mean_squared_error: 14671580.1458\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000819000/36409 [==============>...............] - ETA: 0s - loss: 14810533.3053 - mean_squared_error: 14810533.3053\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000828600/36409 [======================>.......] - ETA: 0s - loss: 14778363.0350 - mean_squared_error: 14778363.0350\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000836409/36409 [==============================] - 0s - loss: 14708575.5622 - mean_squared_error: 14708575.5622 - val_loss: 15239347.9204 - val_mean_squared_error: 15239347.9204\n",
    ">        32/13485 [..............................] - ETA: 0s\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008 2944/13485 [=====>........................] - ETA: 0s\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008 5856/13485 [============>.................] - ETA: 0s\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008 8800/13485 [==================>...........] - ETA: 0s\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000811680/13485 [========================>.....] - ETA: 0s()\n",
    ">     test set RMSE: 3801.667197"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() # do you understand why the number of parameters in layer 1 is 810? 26*30+30=810"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     _________________________________________________________________\n",
    ">     Layer (type)                 Output Shape              Param #   \n",
    ">     =================================================================\n",
    ">     dense_3 (Dense)              (None, 30)                810       \n",
    ">     _________________________________________________________________\n",
    ">     dense_4 (Dense)              (None, 1)                 31        \n",
    ">     =================================================================\n",
    ">     Total params: 841\n",
    ">     Trainable params: 841\n",
    ">     Non-trainable params: 0\n",
    ">     _________________________________________________________________\n",
    "\n",
    "  \n",
    "\n",
    "Notes:\n",
    "\n",
    "-   We didn't have to explicitly write the \"input\" layer, courtesy of\n",
    "    the Keras API. We just said `input_dim=26` on the first (and only)\n",
    "    hidden layer.\n",
    "-   `kernel_initializer='normal'` is a simple (though not always\n",
    "    optimal) *weight initialization*\n",
    "-   Epoch: 1 pass over all of the training data\n",
    "-   Batch: Records processes together in a single training pass\n",
    "\n",
    "How is our RMSE vs. the std dev of the response?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[12]: 3989.4027576288736\n",
    "\n",
    "  \n",
    "\n",
    "Let's look at the error ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Let's set up a \"long-running\" training. This will take a few minutes to\n",
    "converge to the same performance we got more or less instantly with our\n",
    "sklearn linear regression :)\n",
    "\n",
    "While it's running, we can talk about the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "input_file = \"/dbfs/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\"\n",
    "\n",
    "df = pd.read_csv(input_file, header = 0)\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "df = pd.get_dummies(df, prefix=['cut_', 'color_', 'clarity_'])\n",
    "\n",
    "y = df.iloc[:,3:4].as_matrix().flatten()\n",
    "y.flatten()\n",
    "\n",
    "X = df.drop(df.columns[3], axis=1).as_matrix()\n",
    "np.shape(X)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=26, kernel_initializer='normal', activation='linear'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "history = model.fit(X_train, y_train, epochs=250, batch_size=100, validation_split=0.1, verbose=2)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\nroot %s: %f\" % (model.metrics_names[1], np.sqrt(scores[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Train on 36409 samples, validate on 4046 samples\n",
    ">     Epoch 1/250\n",
    ">     0s - loss: 28488681.4617 - mean_squared_error: 28488681.4617 - val_loss: 23874080.5418 - val_mean_squared_error: 23874080.5418\n",
    ">     Epoch 2/250\n",
    ">     0s - loss: 18229086.8179 - mean_squared_error: 18229086.8179 - val_loss: 16189728.0830 - val_mean_squared_error: 16189728.0830\n",
    ">     Epoch 3/250\n",
    ">     0s - loss: 15166499.5991 - mean_squared_error: 15166499.5991 - val_loss: 15623473.2966 - val_mean_squared_error: 15623473.2966\n",
    ">     Epoch 4/250\n",
    ">     0s - loss: 14946794.6502 - mean_squared_error: 14946794.6502 - val_loss: 15451932.3416 - val_mean_squared_error: 15451932.3416\n",
    ">     Epoch 5/250\n",
    ">     0s - loss: 14768661.4713 - mean_squared_error: 14768661.4713 - val_loss: 15255701.9491 - val_mean_squared_error: 15255701.9491\n",
    ">     Epoch 6/250\n",
    ">     0s - loss: 14561062.3305 - mean_squared_error: 14561062.3305 - val_loss: 15022190.2719 - val_mean_squared_error: 15022190.2719\n",
    ">     Epoch 7/250\n",
    ">     0s - loss: 14316009.8804 - mean_squared_error: 14316009.8804 - val_loss: 14745841.2986 - val_mean_squared_error: 14745841.2986\n",
    ">     Epoch 8/250\n",
    ">     0s - loss: 14022112.4315 - mean_squared_error: 14022112.4315 - val_loss: 14410444.0415 - val_mean_squared_error: 14410444.0415\n",
    ">     Epoch 9/250\n",
    ">     0s - loss: 13661721.1350 - mean_squared_error: 13661721.1350 - val_loss: 14000927.5487 - val_mean_squared_error: 14000927.5487\n",
    ">     Epoch 10/250\n",
    ">     0s - loss: 13215663.0982 - mean_squared_error: 13215663.0982 - val_loss: 13486251.6288 - val_mean_squared_error: 13486251.6288\n",
    ">     Epoch 11/250\n",
    ">     0s - loss: 12666634.4279 - mean_squared_error: 12666634.4279 - val_loss: 12865517.2798 - val_mean_squared_error: 12865517.2798\n",
    ">     Epoch 12/250\n",
    ">     0s - loss: 12006921.9485 - mean_squared_error: 12006921.9485 - val_loss: 12119694.9130 - val_mean_squared_error: 12119694.9130\n",
    ">     Epoch 13/250\n",
    ">     0s - loss: 11229263.1284 - mean_squared_error: 11229263.1284 - val_loss: 11260469.6698 - val_mean_squared_error: 11260469.6698\n",
    ">     Epoch 14/250\n",
    ">     0s - loss: 10353514.4225 - mean_squared_error: 10353514.4225 - val_loss: 10316194.0959 - val_mean_squared_error: 10316194.0959\n",
    ">     Epoch 15/250\n",
    ">     0s - loss: 9422868.6090 - mean_squared_error: 9422868.6090 - val_loss: 9340960.0435 - val_mean_squared_error: 9340960.0435\n",
    ">     Epoch 16/250\n",
    ">     0s - loss: 8487981.9583 - mean_squared_error: 8487981.9583 - val_loss: 8368759.9125 - val_mean_squared_error: 8368759.9125\n",
    ">     Epoch 17/250\n",
    ">     0s - loss: 7584754.6092 - mean_squared_error: 7584754.6092 - val_loss: 7457210.3542 - val_mean_squared_error: 7457210.3542\n",
    ">     Epoch 18/250\n",
    ">     0s - loss: 6746151.3189 - mean_squared_error: 6746151.3189 - val_loss: 6620543.4394 - val_mean_squared_error: 6620543.4394\n",
    ">     Epoch 19/250\n",
    ">     0s - loss: 6001624.5078 - mean_squared_error: 6001624.5078 - val_loss: 5876436.0129 - val_mean_squared_error: 5876436.0129\n",
    ">     Epoch 20/250\n",
    ">     0s - loss: 5358165.4339 - mean_squared_error: 5358165.4339 - val_loss: 5247298.6883 - val_mean_squared_error: 5247298.6883\n",
    ">     Epoch 21/250\n",
    ">     0s - loss: 4828392.9302 - mean_squared_error: 4828392.9302 - val_loss: 4730418.3690 - val_mean_squared_error: 4730418.3690\n",
    ">     Epoch 22/250\n",
    ">     0s - loss: 4399680.0461 - mean_squared_error: 4399680.0461 - val_loss: 4315441.9890 - val_mean_squared_error: 4315441.9890\n",
    ">     Epoch 23/250\n",
    ">     0s - loss: 4064137.3211 - mean_squared_error: 4064137.3211 - val_loss: 3987325.7475 - val_mean_squared_error: 3987325.7475\n",
    ">     Epoch 24/250\n",
    ">     0s - loss: 3804387.2306 - mean_squared_error: 3804387.2306 - val_loss: 3736058.7546 - val_mean_squared_error: 3736058.7546\n",
    ">     Epoch 25/250\n",
    ">     0s - loss: 3606198.7104 - mean_squared_error: 3606198.7104 - val_loss: 3537546.5429 - val_mean_squared_error: 3537546.5429\n",
    ">     Epoch 26/250\n",
    ">     0s - loss: 3452605.0296 - mean_squared_error: 3452605.0296 - val_loss: 3378940.3998 - val_mean_squared_error: 3378940.3998\n",
    ">     Epoch 27/250\n",
    ">     0s - loss: 3327355.0379 - mean_squared_error: 3327355.0379 - val_loss: 3250864.3769 - val_mean_squared_error: 3250864.3769\n",
    ">     Epoch 28/250\n",
    ">     0s - loss: 3227474.0017 - mean_squared_error: 3227474.0017 - val_loss: 3146721.7549 - val_mean_squared_error: 3146721.7549\n",
    ">     Epoch 29/250\n",
    ">     0s - loss: 3142451.5537 - mean_squared_error: 3142451.5537 - val_loss: 3057893.6639 - val_mean_squared_error: 3057893.6639\n",
    ">     Epoch 30/250\n",
    ">     0s - loss: 3069000.7258 - mean_squared_error: 3069000.7258 - val_loss: 2980783.2383 - val_mean_squared_error: 2980783.2383\n",
    ">     Epoch 31/250\n",
    ">     0s - loss: 3005016.5666 - mean_squared_error: 3005016.5666 - val_loss: 2912329.0091 - val_mean_squared_error: 2912329.0091\n",
    ">     Epoch 32/250\n",
    ">     0s - loss: 2945598.7436 - mean_squared_error: 2945598.7436 - val_loss: 2849878.2276 - val_mean_squared_error: 2849878.2276\n",
    ">     Epoch 33/250\n",
    ">     0s - loss: 2891473.2870 - mean_squared_error: 2891473.2870 - val_loss: 2793290.3015 - val_mean_squared_error: 2793290.3015\n",
    ">     Epoch 34/250\n",
    ">     0s - loss: 2843366.1829 - mean_squared_error: 2843366.1829 - val_loss: 2741582.8161 - val_mean_squared_error: 2741582.8161\n",
    ">     Epoch 35/250\n",
    ">     0s - loss: 2798384.1091 - mean_squared_error: 2798384.1091 - val_loss: 2698598.5163 - val_mean_squared_error: 2698598.5163\n",
    ">     Epoch 36/250\n",
    ">     0s - loss: 2754918.4916 - mean_squared_error: 2754918.4916 - val_loss: 2656071.9637 - val_mean_squared_error: 2656071.9637\n",
    ">     Epoch 37/250\n",
    ">     0s - loss: 2716886.0110 - mean_squared_error: 2716886.0110 - val_loss: 2612379.2228 - val_mean_squared_error: 2612379.2228\n",
    ">     Epoch 38/250\n",
    ">     0s - loss: 2680750.1550 - mean_squared_error: 2680750.1550 - val_loss: 2581572.0864 - val_mean_squared_error: 2581572.0864\n",
    ">     Epoch 39/250\n",
    ">     0s - loss: 2647750.9281 - mean_squared_error: 2647750.9281 - val_loss: 2547053.7806 - val_mean_squared_error: 2547053.7806\n",
    ">     Epoch 40/250\n",
    ">     0s - loss: 2614633.3723 - mean_squared_error: 2614633.3723 - val_loss: 2502690.5565 - val_mean_squared_error: 2502690.5565\n",
    ">     Epoch 41/250\n",
    ">     0s - loss: 2585038.8468 - mean_squared_error: 2585038.8468 - val_loss: 2474372.3826 - val_mean_squared_error: 2474372.3826\n",
    ">     Epoch 42/250\n",
    ">     0s - loss: 2559401.6082 - mean_squared_error: 2559401.6082 - val_loss: 2443888.3585 - val_mean_squared_error: 2443888.3585\n",
    ">     Epoch 43/250\n",
    ">     0s - loss: 2532813.3276 - mean_squared_error: 2532813.3276 - val_loss: 2418242.1923 - val_mean_squared_error: 2418242.1923\n",
    ">     Epoch 44/250\n",
    ">     0s - loss: 2510198.0725 - mean_squared_error: 2510198.0725 - val_loss: 2397035.2743 - val_mean_squared_error: 2397035.2743\n",
    ">     Epoch 45/250\n",
    ">     0s - loss: 2488252.0433 - mean_squared_error: 2488252.0433 - val_loss: 2374199.5779 - val_mean_squared_error: 2374199.5779\n",
    ">     Epoch 46/250\n",
    ">     0s - loss: 2467734.8528 - mean_squared_error: 2467734.8528 - val_loss: 2350500.9301 - val_mean_squared_error: 2350500.9301\n",
    ">     Epoch 47/250\n",
    ">     0s - loss: 2449345.1447 - mean_squared_error: 2449345.1447 - val_loss: 2331064.3030 - val_mean_squared_error: 2331064.3030\n",
    ">     Epoch 48/250\n",
    ">     0s - loss: 2432405.8400 - mean_squared_error: 2432405.8400 - val_loss: 2312961.1415 - val_mean_squared_error: 2312961.1415\n",
    ">     Epoch 49/250\n",
    ">     0s - loss: 2416510.3260 - mean_squared_error: 2416510.3260 - val_loss: 2297868.3544 - val_mean_squared_error: 2297868.3544\n",
    ">     Epoch 50/250\n",
    ">     0s - loss: 2400876.9122 - mean_squared_error: 2400876.9122 - val_loss: 2286882.4501 - val_mean_squared_error: 2286882.4501\n",
    ">     Epoch 51/250\n",
    ">     0s - loss: 2388083.7821 - mean_squared_error: 2388083.7821 - val_loss: 2267811.9818 - val_mean_squared_error: 2267811.9818\n",
    ">     Epoch 52/250\n",
    ">     0s - loss: 2374082.5599 - mean_squared_error: 2374082.5599 - val_loss: 2254034.9846 - val_mean_squared_error: 2254034.9846\n",
    ">     Epoch 53/250\n",
    ">     0s - loss: 2362793.5659 - mean_squared_error: 2362793.5659 - val_loss: 2243986.0586 - val_mean_squared_error: 2243986.0586\n",
    ">     Epoch 54/250\n",
    ">     0s - loss: 2350723.2258 - mean_squared_error: 2350723.2258 - val_loss: 2231136.9094 - val_mean_squared_error: 2231136.9094\n",
    ">     Epoch 55/250\n",
    ">     0s - loss: 2341111.2496 - mean_squared_error: 2341111.2496 - val_loss: 2222626.1373 - val_mean_squared_error: 2222626.1373\n",
    ">     Epoch 56/250\n",
    ">     0s - loss: 2330947.0380 - mean_squared_error: 2330947.0380 - val_loss: 2209991.0192 - val_mean_squared_error: 2209991.0192\n",
    ">     Epoch 57/250\n",
    ">     0s - loss: 2322436.9411 - mean_squared_error: 2322436.9411 - val_loss: 2200346.6637 - val_mean_squared_error: 2200346.6637\n",
    ">     Epoch 58/250\n",
    ">     0s - loss: 2312180.1612 - mean_squared_error: 2312180.1612 - val_loss: 2200348.3131 - val_mean_squared_error: 2200348.3131\n",
    ">     Epoch 59/250\n",
    ">     0s - loss: 2304027.6972 - mean_squared_error: 2304027.6972 - val_loss: 2185797.1092 - val_mean_squared_error: 2185797.1092\n",
    ">     Epoch 60/250\n",
    ">     0s - loss: 2295507.3035 - mean_squared_error: 2295507.3035 - val_loss: 2178808.8276 - val_mean_squared_error: 2178808.8276\n",
    ">     Epoch 61/250\n",
    ">     0s - loss: 2288186.5474 - mean_squared_error: 2288186.5474 - val_loss: 2172675.4569 - val_mean_squared_error: 2172675.4569\n",
    ">     Epoch 62/250\n",
    ">     0s - loss: 2280459.4387 - mean_squared_error: 2280459.4387 - val_loss: 2160138.4005 - val_mean_squared_error: 2160138.4005\n",
    ">     Epoch 63/250\n",
    ">     0s - loss: 2273555.1472 - mean_squared_error: 2273555.1472 - val_loss: 2154209.1761 - val_mean_squared_error: 2154209.1761\n",
    ">     Epoch 64/250\n",
    ">     0s - loss: 2267387.2401 - mean_squared_error: 2267387.2401 - val_loss: 2146368.0001 - val_mean_squared_error: 2146368.0001\n",
    ">     Epoch 65/250\n",
    ">     0s - loss: 2259540.6544 - mean_squared_error: 2259540.6544 - val_loss: 2139889.5100 - val_mean_squared_error: 2139889.5100\n",
    ">     Epoch 66/250\n",
    ">     0s - loss: 2252675.3133 - mean_squared_error: 2252675.3133 - val_loss: 2142529.6848 - val_mean_squared_error: 2142529.6848\n",
    ">     Epoch 67/250\n",
    ">     0s - loss: 2247124.1685 - mean_squared_error: 2247124.1685 - val_loss: 2133515.4996 - val_mean_squared_error: 2133515.4996\n",
    ">     Epoch 68/250\n",
    ">     0s - loss: 2239819.6204 - mean_squared_error: 2239819.6204 - val_loss: 2124088.7704 - val_mean_squared_error: 2124088.7704\n",
    ">     Epoch 69/250\n",
    ">     0s - loss: 2235481.4342 - mean_squared_error: 2235481.4342 - val_loss: 2118866.4429 - val_mean_squared_error: 2118866.4429\n",
    ">     Epoch 70/250\n",
    ">     0s - loss: 2227842.9432 - mean_squared_error: 2227842.9432 - val_loss: 2116489.2648 - val_mean_squared_error: 2116489.2648\n",
    ">     Epoch 71/250\n",
    ">     0s - loss: 2222389.5691 - mean_squared_error: 2222389.5691 - val_loss: 2109329.5252 - val_mean_squared_error: 2109329.5252\n",
    ">     Epoch 72/250\n",
    ">     0s - loss: 2217531.8429 - mean_squared_error: 2217531.8429 - val_loss: 2099498.1513 - val_mean_squared_error: 2099498.1513\n",
    ">     Epoch 73/250\n",
    ">     0s - loss: 2210847.0403 - mean_squared_error: 2210847.0403 - val_loss: 2093976.1300 - val_mean_squared_error: 2093976.1300\n",
    ">     Epoch 74/250\n",
    ">     0s - loss: 2205064.4717 - mean_squared_error: 2205064.4717 - val_loss: 2096019.6917 - val_mean_squared_error: 2096019.6917\n",
    ">     Epoch 75/250\n",
    ">     0s - loss: 2199245.1302 - mean_squared_error: 2199245.1302 - val_loss: 2085533.7033 - val_mean_squared_error: 2085533.7033\n",
    ">     Epoch 76/250\n",
    ">     0s - loss: 2193349.8692 - mean_squared_error: 2193349.8692 - val_loss: 2079436.7238 - val_mean_squared_error: 2079436.7238\n",
    ">     Epoch 77/250\n",
    ">     0s - loss: 2188263.9696 - mean_squared_error: 2188263.9696 - val_loss: 2078335.5267 - val_mean_squared_error: 2078335.5267\n",
    ">     Epoch 78/250\n",
    ">     0s - loss: 2182608.5343 - mean_squared_error: 2182608.5343 - val_loss: 2075093.3773 - val_mean_squared_error: 2075093.3773\n",
    ">     Epoch 79/250\n",
    ">     0s - loss: 2177324.1784 - mean_squared_error: 2177324.1784 - val_loss: 2065729.8246 - val_mean_squared_error: 2065729.8246\n",
    ">     Epoch 80/250\n",
    ">     0s - loss: 2172172.7067 - mean_squared_error: 2172172.7067 - val_loss: 2059672.8591 - val_mean_squared_error: 2059672.8591\n",
    ">     Epoch 81/250\n",
    ">     0s - loss: 2167139.5290 - mean_squared_error: 2167139.5290 - val_loss: 2058287.9642 - val_mean_squared_error: 2058287.9642\n",
    ">     Epoch 82/250\n",
    ">     0s - loss: 2161254.9477 - mean_squared_error: 2161254.9477 - val_loss: 2049552.9627 - val_mean_squared_error: 2049552.9627\n",
    ">     Epoch 83/250\n",
    ">     0s - loss: 2155459.7217 - mean_squared_error: 2155459.7217 - val_loss: 2047344.6170 - val_mean_squared_error: 2047344.6170\n",
    ">     Epoch 84/250\n",
    ">     0s - loss: 2151657.2357 - mean_squared_error: 2151657.2357 - val_loss: 2039571.8497 - val_mean_squared_error: 2039571.8497\n",
    ">     Epoch 85/250\n",
    ">     0s - loss: 2145618.5328 - mean_squared_error: 2145618.5328 - val_loss: 2035332.1645 - val_mean_squared_error: 2035332.1645\n",
    ">     Epoch 86/250\n",
    ">     0s - loss: 2140224.4930 - mean_squared_error: 2140224.4930 - val_loss: 2038711.6039 - val_mean_squared_error: 2038711.6039\n",
    ">     Epoch 87/250\n",
    ">     0s - loss: 2135980.4546 - mean_squared_error: 2135980.4546 - val_loss: 2025889.8646 - val_mean_squared_error: 2025889.8646\n",
    ">     Epoch 88/250\n",
    ">     0s - loss: 2130591.8489 - mean_squared_error: 2130591.8489 - val_loss: 2021677.8099 - val_mean_squared_error: 2021677.8099\n",
    ">     Epoch 89/250\n",
    ">     0s - loss: 2125141.5852 - mean_squared_error: 2125141.5852 - val_loss: 2019031.4621 - val_mean_squared_error: 2019031.4621\n",
    ">     Epoch 90/250\n",
    ">     0s - loss: 2120561.9612 - mean_squared_error: 2120561.9612 - val_loss: 2015348.2049 - val_mean_squared_error: 2015348.2049\n",
    ">     Epoch 91/250\n",
    ">     0s - loss: 2115254.3667 - mean_squared_error: 2115254.3667 - val_loss: 2009890.7876 - val_mean_squared_error: 2009890.7876\n",
    ">     Epoch 92/250\n",
    ">     0s - loss: 2111683.1785 - mean_squared_error: 2111683.1785 - val_loss: 2009529.4525 - val_mean_squared_error: 2009529.4525\n",
    ">     Epoch 93/250\n",
    ">     0s - loss: 2106094.0561 - mean_squared_error: 2106094.0561 - val_loss: 1999995.7113 - val_mean_squared_error: 1999995.7113\n",
    ">     Epoch 94/250\n",
    ">     0s - loss: 2101431.5341 - mean_squared_error: 2101431.5341 - val_loss: 1996198.3522 - val_mean_squared_error: 1996198.3522\n",
    ">     Epoch 95/250\n",
    ">     0s - loss: 2096510.5197 - mean_squared_error: 2096510.5197 - val_loss: 1994582.2173 - val_mean_squared_error: 1994582.2173\n",
    ">     Epoch 96/250\n",
    ">     0s - loss: 2091822.3899 - mean_squared_error: 2091822.3899 - val_loss: 1987896.7952 - val_mean_squared_error: 1987896.7952\n",
    ">     Epoch 97/250\n",
    ">     0s - loss: 2087355.8434 - mean_squared_error: 2087355.8434 - val_loss: 1983434.7677 - val_mean_squared_error: 1983434.7677\n",
    ">     Epoch 98/250\n",
    ">     0s - loss: 2083115.5390 - mean_squared_error: 2083115.5390 - val_loss: 1981595.0187 - val_mean_squared_error: 1981595.0187\n",
    ">     Epoch 99/250\n",
    ">     0s - loss: 2077102.0598 - mean_squared_error: 2077102.0598 - val_loss: 1980368.2116 - val_mean_squared_error: 1980368.2116\n",
    ">     Epoch 100/250\n",
    ">     0s - loss: 2073968.7934 - mean_squared_error: 2073968.7934 - val_loss: 1974796.1993 - val_mean_squared_error: 1974796.1993\n",
    ">     Epoch 101/250\n",
    ">     0s - loss: 2068667.0503 - mean_squared_error: 2068667.0503 - val_loss: 1969765.0780 - val_mean_squared_error: 1969765.0780\n",
    ">     Epoch 102/250\n",
    ">     0s - loss: 2064690.6578 - mean_squared_error: 2064690.6578 - val_loss: 1966384.0665 - val_mean_squared_error: 1966384.0665\n",
    ">     Epoch 103/250\n",
    ">     0s - loss: 2060186.1627 - mean_squared_error: 2060186.1627 - val_loss: 1965574.2973 - val_mean_squared_error: 1965574.2973\n",
    ">     Epoch 104/250\n",
    ">     0s - loss: 2054823.4142 - mean_squared_error: 2054823.4142 - val_loss: 1962934.2374 - val_mean_squared_error: 1962934.2374\n",
    ">     Epoch 105/250\n",
    ">     0s - loss: 2050762.5834 - mean_squared_error: 2050762.5834 - val_loss: 1963156.1925 - val_mean_squared_error: 1963156.1925\n",
    ">     Epoch 106/250\n",
    ">     0s - loss: 2045922.1364 - mean_squared_error: 2045922.1364 - val_loss: 1954789.0355 - val_mean_squared_error: 1954789.0355\n",
    ">     Epoch 107/250\n",
    ">     0s - loss: 2041155.5787 - mean_squared_error: 2041155.5787 - val_loss: 1947729.4626 - val_mean_squared_error: 1947729.4626\n",
    ">     Epoch 108/250\n",
    ">     0s - loss: 2037137.9055 - mean_squared_error: 2037137.9055 - val_loss: 1944769.3830 - val_mean_squared_error: 1944769.3830\n",
    ">     Epoch 109/250\n",
    ">     0s - loss: 2032794.9416 - mean_squared_error: 2032794.9416 - val_loss: 1941635.0489 - val_mean_squared_error: 1941635.0489\n",
    ">     Epoch 110/250\n",
    ">     0s - loss: 2028380.1420 - mean_squared_error: 2028380.1420 - val_loss: 1931580.4396 - val_mean_squared_error: 1931580.4396\n",
    ">     Epoch 111/250\n",
    ">     0s - loss: 2023606.8445 - mean_squared_error: 2023606.8445 - val_loss: 1928708.8721 - val_mean_squared_error: 1928708.8721\n",
    ">     Epoch 112/250\n",
    ">     0s - loss: 2019222.1547 - mean_squared_error: 2019222.1547 - val_loss: 1928838.3536 - val_mean_squared_error: 1928838.3536\n",
    ">     Epoch 113/250\n",
    ">     0s - loss: 2015556.0977 - mean_squared_error: 2015556.0977 - val_loss: 1922861.4312 - val_mean_squared_error: 1922861.4312\n",
    ">     Epoch 114/250\n",
    ">     0s - loss: 2010233.4348 - mean_squared_error: 2010233.4348 - val_loss: 1916408.8726 - val_mean_squared_error: 1916408.8726\n",
    ">     Epoch 115/250\n",
    ">     0s - loss: 2006104.0397 - mean_squared_error: 2006104.0397 - val_loss: 1922482.2547 - val_mean_squared_error: 1922482.2547\n",
    ">     Epoch 116/250\n",
    ">     0s - loss: 2001278.5629 - mean_squared_error: 2001278.5629 - val_loss: 1910170.1377 - val_mean_squared_error: 1910170.1377\n",
    ">     Epoch 117/250\n",
    ">     0s - loss: 1996985.5113 - mean_squared_error: 1996985.5113 - val_loss: 1905622.7374 - val_mean_squared_error: 1905622.7374\n",
    ">     Epoch 118/250\n",
    ">     0s - loss: 1993243.6008 - mean_squared_error: 1993243.6008 - val_loss: 1903218.0332 - val_mean_squared_error: 1903218.0332\n",
    ">     Epoch 119/250\n",
    ">     0s - loss: 1988322.9259 - mean_squared_error: 1988322.9259 - val_loss: 1897341.5341 - val_mean_squared_error: 1897341.5341\n",
    ">     Epoch 120/250\n",
    ">     0s - loss: 1984616.1014 - mean_squared_error: 1984616.1014 - val_loss: 1899326.0840 - val_mean_squared_error: 1899326.0840\n",
    ">     Epoch 121/250\n",
    ">     0s - loss: 1979608.8977 - mean_squared_error: 1979608.8977 - val_loss: 1897938.1212 - val_mean_squared_error: 1897938.1212\n",
    ">     Epoch 122/250\n",
    ">     0s - loss: 1976952.4138 - mean_squared_error: 1976952.4138 - val_loss: 1886597.4134 - val_mean_squared_error: 1886597.4134\n",
    ">     Epoch 123/250\n",
    ">     0s - loss: 1972243.2959 - mean_squared_error: 1972243.2959 - val_loss: 1886249.0314 - val_mean_squared_error: 1886249.0314\n",
    ">     Epoch 124/250\n",
    ">     0s - loss: 1967164.1162 - mean_squared_error: 1967164.1162 - val_loss: 1879247.9453 - val_mean_squared_error: 1879247.9453\n",
    ">     Epoch 125/250\n",
    ">     0s - loss: 1963242.1783 - mean_squared_error: 1963242.1783 - val_loss: 1876850.0489 - val_mean_squared_error: 1876850.0489\n",
    ">     Epoch 126/250\n",
    ">     0s - loss: 1958947.5973 - mean_squared_error: 1958947.5973 - val_loss: 1872630.0703 - val_mean_squared_error: 1872630.0703\n",
    ">     Epoch 127/250\n",
    ">     0s - loss: 1953799.0429 - mean_squared_error: 1953799.0429 - val_loss: 1873813.3243 - val_mean_squared_error: 1873813.3243\n",
    ">     Epoch 128/250\n",
    ">     0s - loss: 1950685.8404 - mean_squared_error: 1950685.8404 - val_loss: 1873086.9813 - val_mean_squared_error: 1873086.9813\n",
    ">     Epoch 129/250\n",
    ">     0s - loss: 1946082.8337 - mean_squared_error: 1946082.8337 - val_loss: 1865504.3232 - val_mean_squared_error: 1865504.3232\n",
    ">     Epoch 130/250\n",
    ">     0s - loss: 1941948.3297 - mean_squared_error: 1941948.3297 - val_loss: 1857014.4238 - val_mean_squared_error: 1857014.4238\n",
    ">     Epoch 131/250\n",
    ">     0s - loss: 1937903.8021 - mean_squared_error: 1937903.8021 - val_loss: 1889080.1796 - val_mean_squared_error: 1889080.1796\n",
    ">     Epoch 132/250\n",
    ">     0s - loss: 1933453.9736 - mean_squared_error: 1933453.9736 - val_loss: 1853606.2867 - val_mean_squared_error: 1853606.2867\n",
    ">     Epoch 133/250\n",
    ">     0s - loss: 1929543.1667 - mean_squared_error: 1929543.1667 - val_loss: 1845947.6530 - val_mean_squared_error: 1845947.6530\n",
    ">     Epoch 134/250\n",
    ">     0s - loss: 1924888.8127 - mean_squared_error: 1924888.8127 - val_loss: 1857894.5316 - val_mean_squared_error: 1857894.5316\n",
    ">     Epoch 135/250\n",
    ">     0s - loss: 1922789.7090 - mean_squared_error: 1922789.7090 - val_loss: 1840319.3262 - val_mean_squared_error: 1840319.3262\n",
    ">     Epoch 136/250\n",
    ">     0s - loss: 1916827.9438 - mean_squared_error: 1916827.9438 - val_loss: 1835055.8625 - val_mean_squared_error: 1835055.8625\n",
    ">     Epoch 137/250\n",
    ">     0s - loss: 1912334.4946 - mean_squared_error: 1912334.4946 - val_loss: 1832843.5136 - val_mean_squared_error: 1832843.5136\n",
    ">     Epoch 138/250\n",
    ">     0s - loss: 1908212.6282 - mean_squared_error: 1908212.6282 - val_loss: 1829784.9870 - val_mean_squared_error: 1829784.9870\n",
    ">     Epoch 139/250\n",
    ">     0s - loss: 1905601.5578 - mean_squared_error: 1905601.5578 - val_loss: 1824150.5903 - val_mean_squared_error: 1824150.5903\n",
    ">     Epoch 140/250\n",
    ">     0s - loss: 1901159.4617 - mean_squared_error: 1901159.4617 - val_loss: 1821092.4890 - val_mean_squared_error: 1821092.4890\n",
    ">     Epoch 141/250\n",
    ">     0s - loss: 1895819.1844 - mean_squared_error: 1895819.1844 - val_loss: 1816414.2478 - val_mean_squared_error: 1816414.2478\n",
    ">     Epoch 142/250\n",
    ">     0s - loss: 1892209.2715 - mean_squared_error: 1892209.2715 - val_loss: 1817700.9221 - val_mean_squared_error: 1817700.9221\n",
    ">     Epoch 143/250\n",
    ">     0s - loss: 1887862.7678 - mean_squared_error: 1887862.7678 - val_loss: 1809886.6367 - val_mean_squared_error: 1809886.6367\n",
    ">     Epoch 144/250\n",
    ">     0s - loss: 1883392.6189 - mean_squared_error: 1883392.6189 - val_loss: 1805897.7690 - val_mean_squared_error: 1805897.7690\n",
    ">     Epoch 145/250\n",
    ">     0s - loss: 1879688.2490 - mean_squared_error: 1879688.2490 - val_loss: 1808144.8508 - val_mean_squared_error: 1808144.8508\n",
    ">     Epoch 146/250\n",
    ">     0s - loss: 1874985.6462 - mean_squared_error: 1874985.6462 - val_loss: 1798752.8237 - val_mean_squared_error: 1798752.8237\n",
    ">     Epoch 147/250\n",
    ">     0s - loss: 1871220.9131 - mean_squared_error: 1871220.9131 - val_loss: 1797674.9889 - val_mean_squared_error: 1797674.9889\n",
    ">     Epoch 148/250\n",
    ">     0s - loss: 1868417.2108 - mean_squared_error: 1868417.2108 - val_loss: 1796638.2140 - val_mean_squared_error: 1796638.2140\n",
    ">     Epoch 149/250\n",
    ">     0s - loss: 1863316.5651 - mean_squared_error: 1863316.5651 - val_loss: 1788795.8538 - val_mean_squared_error: 1788795.8538\n",
    ">     Epoch 150/250\n",
    ">     0s - loss: 1858257.2962 - mean_squared_error: 1858257.2962 - val_loss: 1790632.6463 - val_mean_squared_error: 1790632.6463\n",
    ">     Epoch 151/250\n",
    ">     0s - loss: 1854766.9910 - mean_squared_error: 1854766.9910 - val_loss: 1789483.1170 - val_mean_squared_error: 1789483.1170\n",
    ">     Epoch 152/250\n",
    ">     0s - loss: 1849985.5356 - mean_squared_error: 1849985.5356 - val_loss: 1791846.7069 - val_mean_squared_error: 1791846.7069\n",
    ">     Epoch 153/250\n",
    ">     0s - loss: 1848028.1423 - mean_squared_error: 1848028.1423 - val_loss: 1773171.5716 - val_mean_squared_error: 1773171.5716\n",
    ">     Epoch 154/250\n",
    ">     0s - loss: 1843831.6297 - mean_squared_error: 1843831.6297 - val_loss: 1771350.8618 - val_mean_squared_error: 1771350.8618\n",
    ">     Epoch 155/250\n",
    ">     0s - loss: 1838917.1400 - mean_squared_error: 1838917.1400 - val_loss: 1767548.2048 - val_mean_squared_error: 1767548.2048\n",
    ">     Epoch 156/250\n",
    ">     0s - loss: 1833980.0406 - mean_squared_error: 1833980.0406 - val_loss: 1762363.3768 - val_mean_squared_error: 1762363.3768\n",
    ">     Epoch 157/250\n",
    ">     0s - loss: 1830403.1305 - mean_squared_error: 1830403.1305 - val_loss: 1763467.2515 - val_mean_squared_error: 1763467.2515\n",
    ">     Epoch 158/250\n",
    ">     0s - loss: 1826200.7238 - mean_squared_error: 1826200.7238 - val_loss: 1762496.3849 - val_mean_squared_error: 1762496.3849\n",
    ">     Epoch 159/250\n",
    ">     0s - loss: 1822537.7925 - mean_squared_error: 1822537.7925 - val_loss: 1755522.3327 - val_mean_squared_error: 1755522.3327\n",
    ">     Epoch 160/250\n",
    ">     0s - loss: 1818753.2099 - mean_squared_error: 1818753.2099 - val_loss: 1750551.0342 - val_mean_squared_error: 1750551.0342\n",
    ">     Epoch 161/250\n",
    ">     0s - loss: 1815868.0601 - mean_squared_error: 1815868.0601 - val_loss: 1744776.9519 - val_mean_squared_error: 1744776.9519\n",
    ">     Epoch 162/250\n",
    ">     0s - loss: 1810144.9616 - mean_squared_error: 1810144.9616 - val_loss: 1740964.0973 - val_mean_squared_error: 1740964.0973\n",
    ">     Epoch 163/250\n",
    ">     0s - loss: 1805573.3993 - mean_squared_error: 1805573.3993 - val_loss: 1739116.9242 - val_mean_squared_error: 1739116.9242\n",
    ">     Epoch 164/250\n",
    ">     0s - loss: 1801767.6498 - mean_squared_error: 1801767.6498 - val_loss: 1736558.6159 - val_mean_squared_error: 1736558.6159\n",
    ">     Epoch 165/250\n",
    ">     0s - loss: 1797356.5291 - mean_squared_error: 1797356.5291 - val_loss: 1731078.9323 - val_mean_squared_error: 1731078.9323\n",
    ">     Epoch 166/250\n",
    ">     0s - loss: 1793916.4087 - mean_squared_error: 1793916.4087 - val_loss: 1734475.7658 - val_mean_squared_error: 1734475.7658\n",
    ">     Epoch 167/250\n",
    ">     0s - loss: 1790518.4628 - mean_squared_error: 1790518.4628 - val_loss: 1730191.5828 - val_mean_squared_error: 1730191.5828\n",
    ">     Epoch 168/250\n",
    ">     0s - loss: 1785805.9124 - mean_squared_error: 1785805.9124 - val_loss: 1719618.1999 - val_mean_squared_error: 1719618.1999\n",
    ">     Epoch 169/250\n",
    ">     0s - loss: 1781726.6154 - mean_squared_error: 1781726.6154 - val_loss: 1726603.2285 - val_mean_squared_error: 1726603.2285\n",
    ">     Epoch 170/250\n",
    ">     0s - loss: 1778248.7883 - mean_squared_error: 1778248.7883 - val_loss: 1735455.0362 - val_mean_squared_error: 1735455.0362\n",
    ">     Epoch 171/250\n",
    ">     0s - loss: 1774466.9006 - mean_squared_error: 1774466.9006 - val_loss: 1721617.0734 - val_mean_squared_error: 1721617.0734\n",
    ">     Epoch 172/250\n",
    ">     0s - loss: 1770234.9116 - mean_squared_error: 1770234.9116 - val_loss: 1707746.7860 - val_mean_squared_error: 1707746.7860\n",
    ">     Epoch 173/250\n",
    ">     0s - loss: 1765879.3335 - mean_squared_error: 1765879.3335 - val_loss: 1701577.3759 - val_mean_squared_error: 1701577.3759\n",
    ">     Epoch 174/250\n",
    ">     0s - loss: 1761810.7733 - mean_squared_error: 1761810.7733 - val_loss: 1697890.8165 - val_mean_squared_error: 1697890.8165\n",
    ">     Epoch 175/250\n",
    ">     0s - loss: 1757112.3404 - mean_squared_error: 1757112.3404 - val_loss: 1695446.8824 - val_mean_squared_error: 1695446.8824\n",
    ">     Epoch 176/250\n",
    ">     0s - loss: 1754088.3442 - mean_squared_error: 1754088.3442 - val_loss: 1693044.2007 - val_mean_squared_error: 1693044.2007\n",
    ">     Epoch 177/250\n",
    ">     0s - loss: 1749726.2942 - mean_squared_error: 1749726.2942 - val_loss: 1687403.1916 - val_mean_squared_error: 1687403.1916\n",
    ">     Epoch 178/250\n",
    ">     0s - loss: 1745868.2238 - mean_squared_error: 1745868.2238 - val_loss: 1683885.3479 - val_mean_squared_error: 1683885.3479\n",
    ">     Epoch 179/250\n",
    ">     0s - loss: 1741785.8429 - mean_squared_error: 1741785.8429 - val_loss: 1687420.8096 - val_mean_squared_error: 1687420.8096\n",
    ">     Epoch 180/250\n",
    ">     0s - loss: 1738709.1977 - mean_squared_error: 1738709.1977 - val_loss: 1677222.8691 - val_mean_squared_error: 1677222.8691\n",
    ">     Epoch 181/250\n",
    ">     0s - loss: 1733345.2182 - mean_squared_error: 1733345.2182 - val_loss: 1680064.8729 - val_mean_squared_error: 1680064.8729\n",
    ">     Epoch 182/250\n",
    ">     0s - loss: 1730149.8998 - mean_squared_error: 1730149.8998 - val_loss: 1683411.6875 - val_mean_squared_error: 1683411.6875\n",
    ">     Epoch 183/250\n",
    ">     0s - loss: 1726408.1358 - mean_squared_error: 1726408.1358 - val_loss: 1682925.9373 - val_mean_squared_error: 1682925.9373\n",
    ">     Epoch 184/250\n",
    ">     0s - loss: 1722247.2033 - mean_squared_error: 1722247.2033 - val_loss: 1668686.2010 - val_mean_squared_error: 1668686.2010\n",
    ">     Epoch 185/250\n",
    ">     0s - loss: 1718507.5918 - mean_squared_error: 1718507.5918 - val_loss: 1666698.9611 - val_mean_squared_error: 1666698.9611\n",
    ">     Epoch 186/250\n",
    ">     0s - loss: 1713606.7246 - mean_squared_error: 1713606.7246 - val_loss: 1655468.5166 - val_mean_squared_error: 1655468.5166\n",
    ">     Epoch 187/250\n",
    ">     0s - loss: 1709152.5771 - mean_squared_error: 1709152.5771 - val_loss: 1652352.3302 - val_mean_squared_error: 1652352.3302\n",
    ">     Epoch 188/250\n",
    ">     0s - loss: 1705850.4658 - mean_squared_error: 1705850.4658 - val_loss: 1649543.0098 - val_mean_squared_error: 1649543.0098\n",
    ">     Epoch 189/250\n",
    ">     0s - loss: 1701942.4894 - mean_squared_error: 1701942.4894 - val_loss: 1644905.3089 - val_mean_squared_error: 1644905.3089\n",
    ">     Epoch 190/250\n",
    ">     0s - loss: 1697940.9734 - mean_squared_error: 1697940.9734 - val_loss: 1644603.7912 - val_mean_squared_error: 1644603.7912\n",
    ">     Epoch 191/250\n",
    ">     0s - loss: 1694759.5746 - mean_squared_error: 1694759.5746 - val_loss: 1645379.0333 - val_mean_squared_error: 1645379.0333\n",
    ">     Epoch 192/250\n",
    ">     0s - loss: 1689886.4290 - mean_squared_error: 1689886.4290 - val_loss: 1634460.9281 - val_mean_squared_error: 1634460.9281\n",
    ">     Epoch 193/250\n",
    ">     0s - loss: 1685948.3251 - mean_squared_error: 1685948.3251 - val_loss: 1641714.3667 - val_mean_squared_error: 1641714.3667\n",
    ">     Epoch 194/250\n",
    ">     0s - loss: 1683186.3515 - mean_squared_error: 1683186.3515 - val_loss: 1627595.1391 - val_mean_squared_error: 1627595.1391\n",
    ">     Epoch 195/250\n",
    ">     0s - loss: 1678192.9793 - mean_squared_error: 1678192.9793 - val_loss: 1625529.8073 - val_mean_squared_error: 1625529.8073\n",
    ">     Epoch 196/250\n",
    ">     0s - loss: 1674476.4529 - mean_squared_error: 1674476.4529 - val_loss: 1622107.9969 - val_mean_squared_error: 1622107.9969\n",
    ">     Epoch 197/250\n",
    ">     0s - loss: 1670724.4318 - mean_squared_error: 1670724.4318 - val_loss: 1616963.1277 - val_mean_squared_error: 1616963.1277\n",
    ">     Epoch 198/250\n",
    ">     0s - loss: 1667113.3325 - mean_squared_error: 1667113.3325 - val_loss: 1617531.1524 - val_mean_squared_error: 1617531.1524\n",
    ">     Epoch 199/250\n",
    ">     0s - loss: 1662485.1907 - mean_squared_error: 1662485.1907 - val_loss: 1611483.1937 - val_mean_squared_error: 1611483.1937\n",
    ">     Epoch 200/250\n",
    ">     0s - loss: 1659095.2644 - mean_squared_error: 1659095.2644 - val_loss: 1620385.3625 - val_mean_squared_error: 1620385.3625\n",
    ">     Epoch 201/250\n",
    ">     0s - loss: 1655968.4249 - mean_squared_error: 1655968.4249 - val_loss: 1609900.5784 - val_mean_squared_error: 1609900.5784\n",
    ">     Epoch 202/250\n",
    ">     0s - loss: 1651376.3636 - mean_squared_error: 1651376.3636 - val_loss: 1604755.1207 - val_mean_squared_error: 1604755.1207\n",
    ">     Epoch 203/250\n",
    ">     0s - loss: 1647407.6971 - mean_squared_error: 1647407.6971 - val_loss: 1596535.5395 - val_mean_squared_error: 1596535.5395\n",
    ">     Epoch 204/250\n",
    ">     0s - loss: 1643843.2520 - mean_squared_error: 1643843.2520 - val_loss: 1597280.3045 - val_mean_squared_error: 1597280.3045\n",
    ">     Epoch 205/250\n",
    ">     0s - loss: 1640501.1278 - mean_squared_error: 1640501.1278 - val_loss: 1589909.1643 - val_mean_squared_error: 1589909.1643\n",
    ">     Epoch 206/250\n",
    ">     0s - loss: 1637449.0752 - mean_squared_error: 1637449.0752 - val_loss: 1586960.2207 - val_mean_squared_error: 1586960.2207\n",
    ">     Epoch 207/250\n",
    ">     0s - loss: 1633022.4314 - mean_squared_error: 1633022.4314 - val_loss: 1589978.2194 - val_mean_squared_error: 1589978.2194\n",
    ">     Epoch 208/250\n",
    ">     0s - loss: 1629158.6126 - mean_squared_error: 1629158.6126 - val_loss: 1580997.8498 - val_mean_squared_error: 1580997.8498\n",
    ">     Epoch 209/250\n",
    ">     0s - loss: 1625007.0002 - mean_squared_error: 1625007.0002 - val_loss: 1581501.3130 - val_mean_squared_error: 1581501.3130\n",
    ">     Epoch 210/250\n",
    ">     0s - loss: 1621535.4290 - mean_squared_error: 1621535.4290 - val_loss: 1575202.3754 - val_mean_squared_error: 1575202.3754\n",
    ">     Epoch 211/250\n",
    ">     0s - loss: 1617952.2137 - mean_squared_error: 1617952.2137 - val_loss: 1569094.1986 - val_mean_squared_error: 1569094.1986\n",
    ">     Epoch 212/250\n",
    ">     0s - loss: 1613506.0875 - mean_squared_error: 1613506.0875 - val_loss: 1565731.9416 - val_mean_squared_error: 1565731.9416\n",
    ">     Epoch 213/250\n",
    ">     0s - loss: 1609531.5398 - mean_squared_error: 1609531.5398 - val_loss: 1562545.8748 - val_mean_squared_error: 1562545.8748\n",
    ">     Epoch 214/250\n",
    ">     0s - loss: 1606658.6721 - mean_squared_error: 1606658.6721 - val_loss: 1562381.3088 - val_mean_squared_error: 1562381.3088\n",
    ">     Epoch 215/250\n",
    ">     0s - loss: 1603549.7601 - mean_squared_error: 1603549.7601 - val_loss: 1557409.6149 - val_mean_squared_error: 1557409.6149\n",
    ">     Epoch 216/250\n",
    ">     0s - loss: 1598695.9285 - mean_squared_error: 1598695.9285 - val_loss: 1552052.4229 - val_mean_squared_error: 1552052.4229\n",
    ">     Epoch 217/250\n",
    ">     0s - loss: 1595564.0566 - mean_squared_error: 1595564.0566 - val_loss: 1550724.7394 - val_mean_squared_error: 1550724.7394\n",
    ">     Epoch 218/250\n",
    ">     0s - loss: 1591018.6489 - mean_squared_error: 1591018.6489 - val_loss: 1545397.2224 - val_mean_squared_error: 1545397.2224\n",
    ">     Epoch 219/250\n",
    ">     0s - loss: 1587767.1251 - mean_squared_error: 1587767.1251 - val_loss: 1546505.7617 - val_mean_squared_error: 1546505.7617\n",
    ">     Epoch 220/250\n",
    ">     0s - loss: 1584253.3639 - mean_squared_error: 1584253.3639 - val_loss: 1551424.2942 - val_mean_squared_error: 1551424.2942\n",
    ">     Epoch 221/250\n",
    ">     0s - loss: 1579802.2742 - mean_squared_error: 1579802.2742 - val_loss: 1535535.4573 - val_mean_squared_error: 1535535.4573\n",
    ">     Epoch 222/250\n",
    ">     0s - loss: 1576280.2625 - mean_squared_error: 1576280.2625 - val_loss: 1531791.3806 - val_mean_squared_error: 1531791.3806\n",
    ">     Epoch 223/250\n",
    ">     0s - loss: 1573347.9679 - mean_squared_error: 1573347.9679 - val_loss: 1537267.9145 - val_mean_squared_error: 1537267.9145\n",
    ">     Epoch 224/250\n",
    ">     0s - loss: 1569397.2130 - mean_squared_error: 1569397.2130 - val_loss: 1530229.1269 - val_mean_squared_error: 1530229.1269\n",
    ">     Epoch 225/250\n",
    ">     0s - loss: 1565550.6658 - mean_squared_error: 1565550.6658 - val_loss: 1522887.0238 - val_mean_squared_error: 1522887.0238\n",
    ">     Epoch 226/250\n",
    ">     0s - loss: 1561247.6334 - mean_squared_error: 1561247.6334 - val_loss: 1519037.9480 - val_mean_squared_error: 1519037.9480\n",
    ">     Epoch 227/250\n",
    ">     0s - loss: 1558774.2273 - mean_squared_error: 1558774.2273 - val_loss: 1515065.1012 - val_mean_squared_error: 1515065.1012\n",
    ">     Epoch 228/250\n",
    ">     0s - loss: 1554999.3584 - mean_squared_error: 1554999.3584 - val_loss: 1517494.8972 - val_mean_squared_error: 1517494.8972\n",
    ">     Epoch 229/250\n",
    ">     0s - loss: 1551283.4775 - mean_squared_error: 1551283.4775 - val_loss: 1514921.8493 - val_mean_squared_error: 1514921.8493\n",
    ">     Epoch 230/250\n",
    ">     0s - loss: 1548173.0661 - mean_squared_error: 1548173.0661 - val_loss: 1508863.5602 - val_mean_squared_error: 1508863.5602\n",
    ">     Epoch 231/250\n",
    ">     0s - loss: 1544465.2120 - mean_squared_error: 1544465.2120 - val_loss: 1504739.4705 - val_mean_squared_error: 1504739.4705\n",
    ">     Epoch 232/250\n",
    ">     0s - loss: 1540600.5711 - mean_squared_error: 1540600.5711 - val_loss: 1499019.3015 - val_mean_squared_error: 1499019.3015\n",
    ">     Epoch 233/250\n",
    ">     0s - loss: 1538577.3459 - mean_squared_error: 1538577.3459 - val_loss: 1506011.2035 - val_mean_squared_error: 1506011.2035\n",
    ">     Epoch 234/250\n",
    ">     0s - loss: 1533526.0004 - mean_squared_error: 1533526.0004 - val_loss: 1493005.9019 - val_mean_squared_error: 1493005.9019\n",
    ">     Epoch 235/250\n",
    ">     0s - loss: 1531700.7249 - mean_squared_error: 1531700.7249 - val_loss: 1489937.0910 - val_mean_squared_error: 1489937.0910\n",
    ">     Epoch 236/250\n",
    ">     0s - loss: 1527645.5102 - mean_squared_error: 1527645.5102 - val_loss: 1486564.5927 - val_mean_squared_error: 1486564.5927\n",
    ">     Epoch 237/250\n",
    ">     0s - loss: 1523949.8098 - mean_squared_error: 1523949.8098 - val_loss: 1483267.2962 - val_mean_squared_error: 1483267.2962\n",
    ">     Epoch 238/250\n",
    ">     0s - loss: 1521658.9500 - mean_squared_error: 1521658.9500 - val_loss: 1480662.7238 - val_mean_squared_error: 1480662.7238\n",
    ">     Epoch 239/250\n",
    ">     0s - loss: 1517222.4655 - mean_squared_error: 1517222.4655 - val_loss: 1478196.8099 - val_mean_squared_error: 1478196.8099\n",
    ">     Epoch 240/250\n",
    ">     0s - loss: 1514874.1922 - mean_squared_error: 1514874.1922 - val_loss: 1474388.9170 - val_mean_squared_error: 1474388.9170\n",
    ">     Epoch 241/250\n",
    ">     0s - loss: 1510850.0449 - mean_squared_error: 1510850.0449 - val_loss: 1471600.3893 - val_mean_squared_error: 1471600.3893\n",
    ">     Epoch 242/250\n",
    ">     0s - loss: 1507766.9016 - mean_squared_error: 1507766.9016 - val_loss: 1468082.8954 - val_mean_squared_error: 1468082.8954\n",
    ">     Epoch 243/250\n",
    ">     0s - loss: 1504710.6541 - mean_squared_error: 1504710.6541 - val_loss: 1467789.0764 - val_mean_squared_error: 1467789.0764\n",
    ">     Epoch 244/250\n",
    ">     0s - loss: 1501241.3013 - mean_squared_error: 1501241.3013 - val_loss: 1464039.5800 - val_mean_squared_error: 1464039.5800\n",
    ">     Epoch 245/250\n",
    ">     0s - loss: 1498228.6206 - mean_squared_error: 1498228.6206 - val_loss: 1459805.8586 - val_mean_squared_error: 1459805.8586\n",
    ">     Epoch 246/250\n",
    ">     0s - loss: 1494273.3504 - mean_squared_error: 1494273.3504 - val_loss: 1459658.6752 - val_mean_squared_error: 1459658.6752\n",
    ">     Epoch 247/250\n",
    ">     0s - loss: 1491648.6830 - mean_squared_error: 1491648.6830 - val_loss: 1465072.1720 - val_mean_squared_error: 1465072.1720\n",
    ">     Epoch 248/250\n",
    ">     0s - loss: 1489039.1954 - mean_squared_error: 1489039.1954 - val_loss: 1454388.2064 - val_mean_squared_error: 1454388.2064\n",
    ">     Epoch 249/250\n",
    ">     0s - loss: 1485836.2284 - mean_squared_error: 1485836.2284 - val_loss: 1447167.1258 - val_mean_squared_error: 1447167.1258\n",
    ">     Epoch 250/250\n",
    ">     0s - loss: 1482002.9176 - mean_squared_error: 1482002.9176 - val_loss: 1445375.0866 - val_mean_squared_error: 1445375.0866\n",
    ">        32/13485 [..............................] - ETA: 0s\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008 2848/13485 [=====>........................] - ETA: 0s\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008 5728/13485 [===========>..................] - ETA: 0s\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008 8608/13485 [==================>...........] - ETA: 0s\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u000811456/13485 [========================>.....] - ETA: 0s\n",
    ">     root mean_squared_error: 1207.738466\n",
    "\n",
    "  \n",
    "\n",
    "After all this hard work we are closer to the MSE we got from linear\n",
    "regression, but purely using a shallow feed-forward neural network.\n",
    "\n",
    "### Training: Gradient Descent\n",
    "\n",
    "A family of numeric optimization techniques, where we solve a problem\n",
    "with the following pattern:\n",
    "\n",
    "1.  Describe the error in the model output: this is usually some\n",
    "    difference between the the true values and the model's predicted\n",
    "    values, as a function of the model parameters (weights)\n",
    "\n",
    "2.  Compute the gradient, or directional derivative, of the error -- the\n",
    "    \"slope toward lower error\"\n",
    "\n",
    "3.  Adjust the parameters of the model variables in the indicated\n",
    "    direction\n",
    "\n",
    "4.  Repeat\n",
    "\n",
    "&lt;img src=\"https://i.imgur.com/HOYViqN.png\" width=500&gt;\n",
    "\n",
    "#### Some ideas to help build your intuition\n",
    "\n",
    "-   What happens if the variables (imagine just 2, to keep the mental\n",
    "    picture simple) are on wildly different scales ... like one ranges\n",
    "    from -1 to 1 while another from -1e6 to +1e6?\n",
    "\n",
    "-   What if some of the variables are correlated? I.e., a change in one\n",
    "    corresponds to, say, a linear change in another?\n",
    "\n",
    "-   Other things being equal, an approximate solution with fewer\n",
    "    variables is easier to work with than one with more -- how could we\n",
    "    get rid of some less valuable parameters? (e.g., L1 penalty)\n",
    "\n",
    "-   How do we know how far to \"adjust\" our parameters with each step?\n",
    "\n",
    "&lt;img src=\"http://i.imgur.com/AvM2TN6.png\" width=600&gt;\n",
    "\n",
    "What if we have billions of data points? Does it makes sense to use all\n",
    "of them for each update? Is there a shortcut?\n",
    "\n",
    "Yes: *Stochastic Gradient Descent*\n",
    "\n",
    "Stochastic gradient descent is an iterative learning algorithm that uses\n",
    "a training dataset to update a model. - The batch size is a\n",
    "hyperparameter of gradient descent that controls the number of training\n",
    "samples to work through before the model's internal parameters are\n",
    "updated. - The number of epochs is a hyperparameter of gradient descent\n",
    "that controls the number of complete passes through the training\n",
    "dataset.\n",
    "\n",
    "See\n",
    "<https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9>.\n",
    "\n",
    "But SGD has some shortcomings, so we typically use a \"smarter\" version\n",
    "of SGD, which has rules for adjusting the learning rate and even\n",
    "direction in order to avoid common problems.\n",
    "\n",
    "What about that \"Adam\" optimizer? Adam is short for \"adaptive moment\"\n",
    "and is a variant of SGD that includes momentum calculations that change\n",
    "over time. For more detail on optimizers, see the chapter \"Training Deep\n",
    "Neural Nets\" in Aurélien Géron's book: *Hands-On Machine Learning with\n",
    "Scikit-Learn and TensorFlow*\n",
    "(http://shop.oreilly.com/product/0636920052289.do)\n",
    "\n",
    "See <https://keras.io/optimizers/> and references therein.\n",
    "\n",
    "### Training: Backpropagation\n",
    "\n",
    "With a simple, flat model, we could use SGD or a related algorithm to\n",
    "derive the weights, since the error depends directly on those weights.\n",
    "\n",
    "With a deeper network, we have a couple of challenges:\n",
    "\n",
    "-   The error is computed from the final layer, so the gradient of the\n",
    "    error doesn't tell us immediately about problems in other-layer\n",
    "    weights\n",
    "-   Our tiny diamonds model has almost a thousand weights. Bigger models\n",
    "    can easily have millions of weights. Each of those weights may need\n",
    "    to move a little at a time, and we have to watch out for underflow\n",
    "    or undersignificance situations.\n",
    "\n",
    "**The insight is to iteratively calculate errors, one layer at a time,\n",
    "starting at the output. This is called backpropagation. It is neither\n",
    "magical nor surprising. The challenge is just doing it fast and not\n",
    "losing information.**\n",
    "\n",
    "&lt;img src=\"http://i.imgur.com/bjlYwjM.jpg\" width=800&gt;\n",
    "\n",
    "Ok so we've come up with a very slow way to perform a linear regression.\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### *Welcome to Neural Networks in the 1960s!*\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Watch closely now because this is where the magic happens...\n",
    "\n",
    "&lt;img src=\"https://media.giphy.com/media/Hw5LkPYy9yfVS/giphy.gif\"&gt;\n",
    "\n",
    "Non-Linearity + Perceptron = Universal Approximation\n",
    "====================================================\n",
    "\n",
    "### Where does the non-linearity fit in?\n",
    "\n",
    "-   We start with the inputs to a perceptron -- these could be from\n",
    "    source data, for example.\n",
    "-   We multiply each input by its respective weight, which gets us the\n",
    "    $x \\\\cdot w$\n",
    "-   Then add the \"bias\" -- an extra learnable parameter, to get ${x\n",
    "    \\\\cdot w} + b$\n",
    "    -   This value (so far) is sometimes called the \"pre-activation\"\n",
    "-   Now, apply a non-linear \"activation function\" to this value, such as\n",
    "    the logistic sigmoid\n",
    "\n",
    "&lt;img src=\"https://i.imgur.com/MhokAmo.gif\"&gt;\n",
    "\n",
    "### Now the network can \"learn\" non-linear functions\n",
    "\n",
    "To gain some intuition, consider that where the sigmoid is close to 1,\n",
    "we can think of that neuron as being \"on\" or activated, giving a\n",
    "specific output. When close to zero, it is \"off.\"\n",
    "\n",
    "So each neuron is a bit like a switch. If we have enough of them, we can\n",
    "theoretically express arbitrarily many different signals.\n",
    "\n",
    "In some ways this is like the original artificial neuron, with the\n",
    "thresholding output -- the main difference is that the sigmoid gives us\n",
    "a smooth (arbitrarily differentiable) output that we can optimize over\n",
    "using gradient descent to learn the weights.\n",
    "\n",
    "### Where does the signal \"go\" from these neurons?\n",
    "\n",
    "-   In a regression problem, like the diamonds dataset, the activations\n",
    "    from the hidden layer can feed into a single output neuron, with a\n",
    "    simple linear activation representing the final output of the\n",
    "    calculation.\n",
    "\n",
    "-   Frequently we want a classification output instead -- e.g., with\n",
    "    MNIST digits, where we need to choose from 10 classes. In that case,\n",
    "    we can feed the outputs from these hidden neurons forward into a\n",
    "    final layer of 10 neurons, and compare those final neurons'\n",
    "    activation levels.\n",
    "\n",
    "Ok, before we talk any more theory, let's run it and see if we can do\n",
    "better on our diamonds dataset adding this \"sigmoid activation.\"\n",
    "\n",
    "While that's running, let's look at the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "input_file = \"/dbfs/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\"\n",
    "\n",
    "df = pd.read_csv(input_file, header = 0)\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "df = pd.get_dummies(df, prefix=['cut_', 'color_', 'clarity_'])\n",
    "\n",
    "y = df.iloc[:,3:4].as_matrix().flatten()\n",
    "y.flatten()\n",
    "\n",
    "X = df.drop(df.columns[3], axis=1).as_matrix()\n",
    "np.shape(X)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=26, kernel_initializer='normal', activation='sigmoid')) # <- change to nonlinear activation\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='linear')) # <- activation is linear in output layer for this regression\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "history = model.fit(X_train, y_train, epochs=2000, batch_size=100, validation_split=0.1, verbose=2)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\nroot %s: %f\" % (model.metrics_names[1], np.sqrt(scores[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Train on 36409 samples, validate on 4046 samples\n",
    ">     Epoch 1/2000\n",
    ">      - 1s - loss: 31380780.6047 - mean_squared_error: 31380780.6047 - val_loss: 32358332.3638 - val_mean_squared_error: 32358332.3638\n",
    ">     Epoch 2/2000\n",
    ">      - 0s - loss: 31298065.4203 - mean_squared_error: 31298065.4203 - val_loss: 32276571.6461 - val_mean_squared_error: 32276571.6461\n",
    ">     Epoch 3/2000\n",
    ">      - 0s - loss: 31216004.9920 - mean_squared_error: 31216004.9920 - val_loss: 32191762.6911 - val_mean_squared_error: 32191762.6911\n",
    ">     Epoch 4/2000\n",
    ">      - 0s - loss: 31133911.1605 - mean_squared_error: 31133911.1605 - val_loss: 32109959.1181 - val_mean_squared_error: 32109959.1181\n",
    ">     Epoch 5/2000\n",
    ">      - 0s - loss: 31053780.8829 - mean_squared_error: 31053780.8829 - val_loss: 32029314.2709 - val_mean_squared_error: 32029314.2709\n",
    ">     Epoch 6/2000\n",
    ">      - 0s - loss: 30974261.1565 - mean_squared_error: 30974261.1565 - val_loss: 31949091.0203 - val_mean_squared_error: 31949091.0203\n",
    ">     Epoch 7/2000\n",
    ">      - 0s - loss: 30895358.6525 - mean_squared_error: 30895358.6525 - val_loss: 31869483.1992 - val_mean_squared_error: 31869483.1992\n",
    ">     Epoch 8/2000\n",
    ">      - 0s - loss: 30816933.3015 - mean_squared_error: 30816933.3015 - val_loss: 31790248.7850 - val_mean_squared_error: 31790248.7850\n",
    ">     Epoch 9/2000\n",
    ">      - 0s - loss: 30735031.6336 - mean_squared_error: 30735031.6336 - val_loss: 31703530.1018 - val_mean_squared_error: 31703530.1018\n",
    ">     Epoch 10/2000\n",
    ">      - 0s - loss: 30650979.2737 - mean_squared_error: 30650979.2737 - val_loss: 31620223.0746 - val_mean_squared_error: 31620223.0746\n",
    ">     Epoch 11/2000\n",
    ">      - 0s - loss: 30564208.1914 - mean_squared_error: 30564208.1914 - val_loss: 31527235.8487 - val_mean_squared_error: 31527235.8487\n",
    ">     Epoch 12/2000\n",
    ">      - 0s - loss: 30471671.6893 - mean_squared_error: 30471671.6893 - val_loss: 31434505.1438 - val_mean_squared_error: 31434505.1438\n",
    ">     Epoch 13/2000\n",
    ">      - 0s - loss: 30382774.6087 - mean_squared_error: 30382774.6087 - val_loss: 31345867.3376 - val_mean_squared_error: 31345867.3376\n",
    ">     Epoch 14/2000\n",
    ">      - 0s - loss: 30295878.0809 - mean_squared_error: 30295878.0809 - val_loss: 31258539.8992 - val_mean_squared_error: 31258539.8992\n",
    ">     Epoch 15/2000\n",
    ">      - 0s - loss: 30210245.2780 - mean_squared_error: 30210245.2780 - val_loss: 31172163.3673 - val_mean_squared_error: 31172163.3673\n",
    ">     Epoch 16/2000\n",
    ">      - 0s - loss: 30125167.6951 - mean_squared_error: 30125167.6951 - val_loss: 31086263.9644 - val_mean_squared_error: 31086263.9644\n",
    ">     Epoch 17/2000\n",
    ">      - 0s - loss: 30040693.1683 - mean_squared_error: 30040693.1683 - val_loss: 31000947.0559 - val_mean_squared_error: 31000947.0559\n",
    ">     Epoch 18/2000\n",
    ">      - 0s - loss: 29956545.2932 - mean_squared_error: 29956545.2932 - val_loss: 30915863.5709 - val_mean_squared_error: 30915863.5709\n",
    ">     Epoch 19/2000\n",
    ">      - 0s - loss: 29872689.6088 - mean_squared_error: 29872689.6088 - val_loss: 30831129.4968 - val_mean_squared_error: 30831129.4968\n",
    ">     Epoch 20/2000\n",
    ">      - 0s - loss: 29789302.8007 - mean_squared_error: 29789302.8007 - val_loss: 30746769.3732 - val_mean_squared_error: 30746769.3732\n",
    ">     Epoch 21/2000\n",
    ">      - 0s - loss: 29706222.5647 - mean_squared_error: 29706222.5647 - val_loss: 30662787.0954 - val_mean_squared_error: 30662787.0954\n",
    ">     Epoch 22/2000\n",
    ">      - 0s - loss: 29623479.2155 - mean_squared_error: 29623479.2155 - val_loss: 30579097.6026 - val_mean_squared_error: 30579097.6026\n",
    ">     Epoch 23/2000\n",
    ">      - 0s - loss: 29540912.2036 - mean_squared_error: 29540912.2036 - val_loss: 30495587.1537 - val_mean_squared_error: 30495587.1537\n",
    ">     Epoch 24/2000\n",
    ">      - 0s - loss: 29458598.1043 - mean_squared_error: 29458598.1043 - val_loss: 30412313.0509 - val_mean_squared_error: 30412313.0509\n",
    ">     Epoch 25/2000\n",
    ">      - 0s - loss: 29376574.1866 - mean_squared_error: 29376574.1866 - val_loss: 30329317.4652 - val_mean_squared_error: 30329317.4652\n",
    ">     Epoch 26/2000\n",
    ">      - 0s - loss: 29294745.7942 - mean_squared_error: 29294745.7942 - val_loss: 30246553.0677 - val_mean_squared_error: 30246553.0677\n",
    ">     Epoch 27/2000\n",
    ">      - 0s - loss: 29213295.1197 - mean_squared_error: 29213295.1197 - val_loss: 30164191.6738 - val_mean_squared_error: 30164191.6738\n",
    ">     Epoch 28/2000\n",
    ">      - 0s - loss: 29132082.6000 - mean_squared_error: 29132082.6000 - val_loss: 30082031.7647 - val_mean_squared_error: 30082031.7647\n",
    ">     Epoch 29/2000\n",
    ">      - 0s - loss: 29051069.7344 - mean_squared_error: 29051069.7344 - val_loss: 30000055.4869 - val_mean_squared_error: 30000055.4869\n",
    ">     Epoch 30/2000\n",
    ">      - 0s - loss: 28970459.9729 - mean_squared_error: 28970459.9729 - val_loss: 29918478.6387 - val_mean_squared_error: 29918478.6387\n",
    ">     Epoch 31/2000\n",
    ">      - 0s - loss: 28889962.9339 - mean_squared_error: 28889962.9339 - val_loss: 29836927.4503 - val_mean_squared_error: 29836927.4503\n",
    ">     Epoch 32/2000\n",
    ">      - 0s - loss: 28809790.9063 - mean_squared_error: 28809790.9063 - val_loss: 29755820.0830 - val_mean_squared_error: 29755820.0830\n",
    ">     Epoch 33/2000\n",
    ">      - 0s - loss: 28729963.0352 - mean_squared_error: 28729963.0352 - val_loss: 29675137.9496 - val_mean_squared_error: 29675137.9496\n",
    ">     Epoch 34/2000\n",
    ">      - 0s - loss: 28650360.5442 - mean_squared_error: 28650360.5442 - val_loss: 29594480.1928 - val_mean_squared_error: 29594480.1928\n",
    ">     Epoch 35/2000\n",
    ">      - 0s - loss: 28570924.5415 - mean_squared_error: 28570924.5415 - val_loss: 29514072.8730 - val_mean_squared_error: 29514072.8730\n",
    ">     Epoch 36/2000\n",
    ">      - 0s - loss: 28491709.2935 - mean_squared_error: 28491709.2935 - val_loss: 29433960.4943 - val_mean_squared_error: 29433960.4943\n",
    ">     Epoch 37/2000\n",
    ">      - 0s - loss: 28412798.1367 - mean_squared_error: 28412798.1367 - val_loss: 29354021.7696 - val_mean_squared_error: 29354021.7696\n",
    ">     Epoch 38/2000\n",
    ">      - 0s - loss: 28334104.4454 - mean_squared_error: 28334104.4454 - val_loss: 29274362.7504 - val_mean_squared_error: 29274362.7504\n",
    ">     Epoch 39/2000\n",
    ">      - 0s - loss: 28255585.5671 - mean_squared_error: 28255585.5671 - val_loss: 29194935.7578 - val_mean_squared_error: 29194935.7578\n",
    ">     Epoch 40/2000\n",
    ">      - 0s - loss: 28177311.3649 - mean_squared_error: 28177311.3649 - val_loss: 29115711.6283 - val_mean_squared_error: 29115711.6283\n",
    ">     Epoch 41/2000\n",
    ">      - 0s - loss: 28099405.9583 - mean_squared_error: 28099405.9583 - val_loss: 29036816.6812 - val_mean_squared_error: 29036816.6812\n",
    ">     Epoch 42/2000\n",
    ">      - 0s - loss: 28021745.1748 - mean_squared_error: 28021745.1748 - val_loss: 28958206.8423 - val_mean_squared_error: 28958206.8423\n",
    ">     Epoch 43/2000\n",
    ">      - 0s - loss: 27944321.2553 - mean_squared_error: 27944321.2553 - val_loss: 28879830.6901 - val_mean_squared_error: 28879830.6901\n",
    ">     Epoch 44/2000\n",
    ">      - 0s - loss: 27867160.2867 - mean_squared_error: 27867160.2867 - val_loss: 28801703.4691 - val_mean_squared_error: 28801703.4691\n",
    ">     Epoch 45/2000\n",
    ">      - 0s - loss: 27790252.0029 - mean_squared_error: 27790252.0029 - val_loss: 28723817.4879 - val_mean_squared_error: 28723817.4879\n",
    ">     Epoch 46/2000\n",
    ">      - 0s - loss: 27713554.5320 - mean_squared_error: 27713554.5320 - val_loss: 28646179.9268 - val_mean_squared_error: 28646179.9268\n",
    ">     Epoch 47/2000\n",
    ">      - 0s - loss: 27637164.8281 - mean_squared_error: 27637164.8281 - val_loss: 28568974.5803 - val_mean_squared_error: 28568974.5803\n",
    ">     Epoch 48/2000\n",
    ">      - 0s - loss: 27560923.5064 - mean_squared_error: 27560923.5064 - val_loss: 28491688.7721 - val_mean_squared_error: 28491688.7721\n",
    ">     Epoch 49/2000\n",
    ">      - 0s - loss: 27485063.1059 - mean_squared_error: 27485063.1059 - val_loss: 28414863.0114 - val_mean_squared_error: 28414863.0114\n",
    ">     Epoch 50/2000\n",
    ">      - 0s - loss: 27409439.2144 - mean_squared_error: 27409439.2144 - val_loss: 28338173.7677 - val_mean_squared_error: 28338173.7677\n",
    ">     Epoch 51/2000\n",
    ">      - 0s - loss: 27334131.0234 - mean_squared_error: 27334131.0234 - val_loss: 28262001.8013 - val_mean_squared_error: 28262001.8013\n",
    ">     Epoch 52/2000\n",
    ">      - 0s - loss: 27258972.5527 - mean_squared_error: 27258972.5527 - val_loss: 28185839.0064 - val_mean_squared_error: 28185839.0064\n",
    ">     Epoch 53/2000\n",
    ">      - 0s - loss: 27184161.5334 - mean_squared_error: 27184161.5334 - val_loss: 28110077.3406 - val_mean_squared_error: 28110077.3406\n",
    ">     Epoch 54/2000\n",
    ">      - 0s - loss: 27109518.9422 - mean_squared_error: 27109518.9422 - val_loss: 28034423.0133 - val_mean_squared_error: 28034423.0133\n",
    ">     Epoch 55/2000\n",
    ">      - 0s - loss: 27035128.6361 - mean_squared_error: 27035128.6361 - val_loss: 27959151.0756 - val_mean_squared_error: 27959151.0756\n",
    ">     Epoch 56/2000\n",
    ">      - 0s - loss: 26960971.6158 - mean_squared_error: 26960971.6158 - val_loss: 27884048.2521 - val_mean_squared_error: 27884048.2521\n",
    ">     Epoch 57/2000\n",
    ">      - 0s - loss: 26887081.3519 - mean_squared_error: 26887081.3519 - val_loss: 27809200.8601 - val_mean_squared_error: 27809200.8601\n",
    ">     Epoch 58/2000\n",
    ">      - 0s - loss: 26813503.5389 - mean_squared_error: 26813503.5389 - val_loss: 27734627.8863 - val_mean_squared_error: 27734627.8863\n",
    ">     Epoch 59/2000\n",
    ">      - 0s - loss: 26740185.8467 - mean_squared_error: 26740185.8467 - val_loss: 27660315.3821 - val_mean_squared_error: 27660315.3821\n",
    ">     Epoch 60/2000\n",
    ">      - 0s - loss: 26667075.5801 - mean_squared_error: 26667075.5801 - val_loss: 27586281.0489 - val_mean_squared_error: 27586281.0489\n",
    ">     Epoch 61/2000\n",
    ">      - 0s - loss: 26594314.0441 - mean_squared_error: 26594314.0441 - val_loss: 27512567.3356 - val_mean_squared_error: 27512567.3356\n",
    ">     Epoch 62/2000\n",
    ">      - 0s - loss: 26521670.2177 - mean_squared_error: 26521670.2177 - val_loss: 27438897.1527 - val_mean_squared_error: 27438897.1527\n",
    ">     Epoch 63/2000\n",
    ">      - 0s - loss: 26449391.6573 - mean_squared_error: 26449391.6573 - val_loss: 27365727.1676 - val_mean_squared_error: 27365727.1676\n",
    ">     Epoch 64/2000\n",
    ">      - 0s - loss: 26377206.7494 - mean_squared_error: 26377206.7494 - val_loss: 27292557.8497 - val_mean_squared_error: 27292557.8497\n",
    ">     Epoch 65/2000\n",
    ">      - 0s - loss: 26305377.2261 - mean_squared_error: 26305377.2261 - val_loss: 27219727.1705 - val_mean_squared_error: 27219727.1705\n",
    ">     Epoch 66/2000\n",
    ">      - 0s - loss: 26233774.5640 - mean_squared_error: 26233774.5640 - val_loss: 27147248.8393 - val_mean_squared_error: 27147248.8393\n",
    ">     Epoch 67/2000\n",
    ">      - 0s - loss: 26162426.6621 - mean_squared_error: 26162426.6621 - val_loss: 27074894.4360 - val_mean_squared_error: 27074894.4360\n",
    ">     Epoch 68/2000\n",
    ">      - 0s - loss: 26091317.9390 - mean_squared_error: 26091317.9390 - val_loss: 27002882.7682 - val_mean_squared_error: 27002882.7682\n",
    ">     Epoch 69/2000\n",
    ">      - 0s - loss: 26020459.9902 - mean_squared_error: 26020459.9902 - val_loss: 26931139.3267 - val_mean_squared_error: 26931139.3267\n",
    ">     Epoch 70/2000\n",
    ">      - 0s - loss: 25949811.0884 - mean_squared_error: 25949811.0884 - val_loss: 26859526.5754 - val_mean_squared_error: 26859526.5754\n",
    ">     Epoch 71/2000\n",
    ">      - 0s - loss: 25879355.5384 - mean_squared_error: 25879355.5384 - val_loss: 26788026.7138 - val_mean_squared_error: 26788026.7138\n",
    ">     Epoch 72/2000\n",
    ">      - 0s - loss: 25809385.0737 - mean_squared_error: 25809385.0737 - val_loss: 26717113.4187 - val_mean_squared_error: 26717113.4187\n",
    ">     Epoch 73/2000\n",
    ">      - 0s - loss: 25739624.7902 - mean_squared_error: 25739624.7902 - val_loss: 26646419.4276 - val_mean_squared_error: 26646419.4276\n",
    ">     Epoch 74/2000\n",
    ">      - 0s - loss: 25670110.1360 - mean_squared_error: 25670110.1360 - val_loss: 26576000.0336 - val_mean_squared_error: 26576000.0336\n",
    ">     Epoch 75/2000\n",
    ">      - 0s - loss: 25600809.1711 - mean_squared_error: 25600809.1711 - val_loss: 26505692.0465 - val_mean_squared_error: 26505692.0465\n",
    ">     Epoch 76/2000\n",
    ">      - 0s - loss: 25531733.8437 - mean_squared_error: 25531733.8437 - val_loss: 26435641.7113 - val_mean_squared_error: 26435641.7113\n",
    ">     Epoch 77/2000\n",
    ">      - 0s - loss: 25462972.3005 - mean_squared_error: 25462972.3005 - val_loss: 26365985.2813 - val_mean_squared_error: 26365985.2813\n",
    ">     Epoch 78/2000\n",
    ">      - 0s - loss: 25394526.8229 - mean_squared_error: 25394526.8229 - val_loss: 26296592.8700 - val_mean_squared_error: 26296592.8700\n",
    ">     Epoch 79/2000\n",
    ">      - 0s - loss: 25326216.6557 - mean_squared_error: 25326216.6557 - val_loss: 26227286.5368 - val_mean_squared_error: 26227286.5368\n",
    ">     Epoch 80/2000\n",
    ">      - 0s - loss: 25258160.7971 - mean_squared_error: 25258160.7971 - val_loss: 26158228.9204 - val_mean_squared_error: 26158228.9204\n",
    ">     Epoch 81/2000\n",
    ">      - 0s - loss: 25190418.3716 - mean_squared_error: 25190418.3716 - val_loss: 26089568.0910 - val_mean_squared_error: 26089568.0910\n",
    ">     Epoch 82/2000\n",
    ">      - 0s - loss: 25122887.6684 - mean_squared_error: 25122887.6684 - val_loss: 26021011.0905 - val_mean_squared_error: 26021011.0905\n",
    ">     Epoch 83/2000\n",
    ">      - 0s - loss: 25055646.1286 - mean_squared_error: 25055646.1286 - val_loss: 25952877.7914 - val_mean_squared_error: 25952877.7914\n",
    ">     Epoch 84/2000\n",
    ">      - 0s - loss: 24988565.3453 - mean_squared_error: 24988565.3453 - val_loss: 25884860.6960 - val_mean_squared_error: 25884860.6960\n",
    ">     Epoch 85/2000\n",
    ">      - 0s - loss: 24921778.5702 - mean_squared_error: 24921778.5702 - val_loss: 25817128.1157 - val_mean_squared_error: 25817128.1157\n",
    ">     Epoch 86/2000\n",
    ">      - 0s - loss: 24855278.4745 - mean_squared_error: 24855278.4745 - val_loss: 25749717.6303 - val_mean_squared_error: 25749717.6303\n",
    ">     Epoch 87/2000\n",
    ">      - 0s - loss: 24789066.9677 - mean_squared_error: 24789066.9677 - val_loss: 25682459.5175 - val_mean_squared_error: 25682459.5175\n",
    ">     Epoch 88/2000\n",
    ">      - 0s - loss: 24723090.3912 - mean_squared_error: 24723090.3912 - val_loss: 25615506.6901 - val_mean_squared_error: 25615506.6901\n",
    ">     Epoch 89/2000\n",
    ">      - 0s - loss: 24657334.3180 - mean_squared_error: 24657334.3180 - val_loss: 25548882.4024 - val_mean_squared_error: 25548882.4024\n",
    ">     Epoch 90/2000\n",
    ">      - 0s - loss: 24591838.8394 - mean_squared_error: 24591838.8394 - val_loss: 25482345.4513 - val_mean_squared_error: 25482345.4513\n",
    ">     Epoch 91/2000\n",
    ">      - 0s - loss: 24526544.8664 - mean_squared_error: 24526544.8664 - val_loss: 25416164.0761 - val_mean_squared_error: 25416164.0761\n",
    ">     Epoch 92/2000\n",
    ">      - 0s - loss: 24461648.8232 - mean_squared_error: 24461648.8232 - val_loss: 25350287.8626 - val_mean_squared_error: 25350287.8626\n",
    ">     Epoch 93/2000\n",
    ">      - 0s - loss: 24396903.4064 - mean_squared_error: 24396903.4064 - val_loss: 25284579.1340 - val_mean_squared_error: 25284579.1340\n",
    ">     Epoch 94/2000\n",
    ">      - 0s - loss: 24332436.7249 - mean_squared_error: 24332436.7249 - val_loss: 25219231.1409 - val_mean_squared_error: 25219231.1409\n",
    ">     Epoch 95/2000\n",
    ">      - 0s - loss: 24268330.4633 - mean_squared_error: 24268330.4633 - val_loss: 25154138.4330 - val_mean_squared_error: 25154138.4330\n",
    ">     Epoch 96/2000\n",
    ">      - 0s - loss: 24204372.0587 - mean_squared_error: 24204372.0587 - val_loss: 25089203.5255 - val_mean_squared_error: 25089203.5255\n",
    ">     Epoch 97/2000\n",
    ">      - 0s - loss: 24140610.1780 - mean_squared_error: 24140610.1780 - val_loss: 25024465.2664 - val_mean_squared_error: 25024465.2664\n",
    ">     Epoch 98/2000\n",
    ">      - 0s - loss: 24077252.2966 - mean_squared_error: 24077252.2966 - val_loss: 24960163.7430 - val_mean_squared_error: 24960163.7430\n",
    ">     Epoch 99/2000\n",
    ">      - 0s - loss: 24013917.8359 - mean_squared_error: 24013917.8359 - val_loss: 24895891.1112 - val_mean_squared_error: 24895891.1112\n",
    ">     Epoch 100/2000\n",
    ">      - 0s - loss: 23951020.9714 - mean_squared_error: 23951020.9714 - val_loss: 24831932.6505 - val_mean_squared_error: 24831932.6505\n",
    ">     Epoch 101/2000\n",
    ">      - 0s - loss: 23888220.1202 - mean_squared_error: 23888220.1202 - val_loss: 24768160.1671 - val_mean_squared_error: 24768160.1671\n",
    ">     Epoch 102/2000\n",
    ">      - 0s - loss: 23825687.9442 - mean_squared_error: 23825687.9442 - val_loss: 24704775.5087 - val_mean_squared_error: 24704775.5087\n",
    ">     Epoch 103/2000\n",
    ">      - 0s - loss: 23763591.7315 - mean_squared_error: 23763591.7315 - val_loss: 24641768.4014 - val_mean_squared_error: 24641768.4014\n",
    ">     Epoch 104/2000\n",
    ">      - 0s - loss: 23701595.4344 - mean_squared_error: 23701595.4344 - val_loss: 24578754.7355 - val_mean_squared_error: 24578754.7355\n",
    ">     Epoch 105/2000\n",
    ">      - 0s - loss: 23639821.5487 - mean_squared_error: 23639821.5487 - val_loss: 24516050.8166 - val_mean_squared_error: 24516050.8166\n",
    ">     Epoch 106/2000\n",
    ">      - 0s - loss: 23578443.7971 - mean_squared_error: 23578443.7971 - val_loss: 24453735.3900 - val_mean_squared_error: 24453735.3900\n",
    ">     Epoch 107/2000\n",
    ">      - 0s - loss: 23517176.9031 - mean_squared_error: 23517176.9031 - val_loss: 24391556.2126 - val_mean_squared_error: 24391556.2126\n",
    ">     Epoch 108/2000\n",
    ">      - 0s - loss: 23456172.7204 - mean_squared_error: 23456172.7204 - val_loss: 24329568.1493 - val_mean_squared_error: 24329568.1493\n",
    ">     Epoch 109/2000\n",
    ">      - 0s - loss: 23395525.0800 - mean_squared_error: 23395525.0800 - val_loss: 24267975.9090 - val_mean_squared_error: 24267975.9090\n",
    ">     Epoch 110/2000\n",
    ">      - 0s - loss: 23335126.7265 - mean_squared_error: 23335126.7265 - val_loss: 24206602.6041 - val_mean_squared_error: 24206602.6041\n",
    ">     Epoch 111/2000\n",
    ">      - 0s - loss: 23274969.8814 - mean_squared_error: 23274969.8814 - val_loss: 24145492.8828 - val_mean_squared_error: 24145492.8828\n",
    ">     Epoch 112/2000\n",
    ">      - 0s - loss: 23215052.1725 - mean_squared_error: 23215052.1725 - val_loss: 24084635.8517 - val_mean_squared_error: 24084635.8517\n",
    ">     Epoch 113/2000\n",
    ">      - 0s - loss: 23155380.1623 - mean_squared_error: 23155380.1623 - val_loss: 24023868.4518 - val_mean_squared_error: 24023868.4518\n",
    ">     Epoch 114/2000\n",
    ">      - 0s - loss: 23095990.8764 - mean_squared_error: 23095990.8764 - val_loss: 23963653.1221 - val_mean_squared_error: 23963653.1221\n",
    ">     Epoch 115/2000\n",
    ">      - 0s - loss: 23036852.2296 - mean_squared_error: 23036852.2296 - val_loss: 23903552.8304 - val_mean_squared_error: 23903552.8304\n",
    ">     Epoch 116/2000\n",
    ">      - 0s - loss: 22977854.2557 - mean_squared_error: 22977854.2557 - val_loss: 23843577.2842 - val_mean_squared_error: 23843577.2842\n",
    ">     Epoch 117/2000\n",
    ">      - 0s - loss: 22919132.7115 - mean_squared_error: 22919132.7115 - val_loss: 23783941.1903 - val_mean_squared_error: 23783941.1903\n",
    ">     Epoch 118/2000\n",
    ">      - 0s - loss: 22860675.2993 - mean_squared_error: 22860675.2993 - val_loss: 23724452.7316 - val_mean_squared_error: 23724452.7316\n",
    ">     Epoch 119/2000\n",
    ">      - 0s - loss: 22802480.1820 - mean_squared_error: 22802480.1820 - val_loss: 23665374.7593 - val_mean_squared_error: 23665374.7593\n",
    ">     Epoch 120/2000\n",
    ">      - 0s - loss: 22744486.7348 - mean_squared_error: 22744486.7348 - val_loss: 23606429.2111 - val_mean_squared_error: 23606429.2111\n",
    ">     Epoch 121/2000\n",
    ">      - 0s - loss: 22686857.5650 - mean_squared_error: 22686857.5650 - val_loss: 23547899.4839 - val_mean_squared_error: 23547899.4839\n",
    ">     Epoch 122/2000\n",
    ">      - 0s - loss: 22629439.9110 - mean_squared_error: 22629439.9110 - val_loss: 23489489.6391 - val_mean_squared_error: 23489489.6391\n",
    ">     Epoch 123/2000\n",
    ">      - 0s - loss: 22572283.1380 - mean_squared_error: 22572283.1380 - val_loss: 23431363.3495 - val_mean_squared_error: 23431363.3495\n",
    ">     Epoch 124/2000\n",
    ">      - 0s - loss: 22515383.5186 - mean_squared_error: 22515383.5186 - val_loss: 23373465.8695 - val_mean_squared_error: 23373465.8695\n",
    ">     Epoch 125/2000\n",
    ">      - 0s - loss: 22458715.0694 - mean_squared_error: 22458715.0694 - val_loss: 23315892.6387 - val_mean_squared_error: 23315892.6387\n",
    ">     Epoch 126/2000\n",
    ">      - 0s - loss: 22402235.9658 - mean_squared_error: 22402235.9658 - val_loss: 23258538.3223 - val_mean_squared_error: 23258538.3223\n",
    ">     Epoch 127/2000\n",
    ">      - 0s - loss: 22346055.1642 - mean_squared_error: 22346055.1642 - val_loss: 23201325.3880 - val_mean_squared_error: 23201325.3880\n",
    ">     Epoch 128/2000\n",
    ">      - 0s - loss: 22290144.3171 - mean_squared_error: 22290144.3171 - val_loss: 23144563.7865 - val_mean_squared_error: 23144563.7865\n",
    ">     Epoch 129/2000\n",
    ">      - 0s - loss: 22234343.6437 - mean_squared_error: 22234343.6437 - val_loss: 23087770.5348 - val_mean_squared_error: 23087770.5348\n",
    ">     Epoch 130/2000\n",
    ">      - 0s - loss: 22179189.1702 - mean_squared_error: 22179189.1702 - val_loss: 23031742.3094 - val_mean_squared_error: 23031742.3094\n",
    ">     Epoch 131/2000\n",
    ">      - 0s - loss: 22124210.8962 - mean_squared_error: 22124210.8962 - val_loss: 22975737.6411 - val_mean_squared_error: 22975737.6411\n",
    ">     Epoch 132/2000\n",
    ">      - 0s - loss: 22069474.0278 - mean_squared_error: 22069474.0278 - val_loss: 22920065.3208 - val_mean_squared_error: 22920065.3208\n",
    ">     Epoch 133/2000\n",
    ">      - 0s - loss: 22014981.8085 - mean_squared_error: 22014981.8085 - val_loss: 22864551.8389 - val_mean_squared_error: 22864551.8389\n",
    ">     Epoch 134/2000\n",
    ">      - 0s - loss: 21960659.8293 - mean_squared_error: 21960659.8293 - val_loss: 22809443.7350 - val_mean_squared_error: 22809443.7350\n",
    ">     Epoch 135/2000\n",
    ">      - 0s - loss: 21906667.1327 - mean_squared_error: 21906667.1327 - val_loss: 22754352.2323 - val_mean_squared_error: 22754352.2323\n",
    ">     Epoch 136/2000\n",
    ">      - 0s - loss: 21852773.1706 - mean_squared_error: 21852773.1706 - val_loss: 22699528.5279 - val_mean_squared_error: 22699528.5279\n",
    ">     Epoch 137/2000\n",
    ">      - 0s - loss: 21799218.2557 - mean_squared_error: 21799218.2557 - val_loss: 22645053.6391 - val_mean_squared_error: 22645053.6391\n",
    ">     Epoch 138/2000\n",
    ">      - 0s - loss: 21745988.5180 - mean_squared_error: 21745988.5180 - val_loss: 22590829.6362 - val_mean_squared_error: 22590829.6362\n",
    ">     Epoch 139/2000\n",
    ">      - 0s - loss: 21692884.7601 - mean_squared_error: 21692884.7601 - val_loss: 22536811.3139 - val_mean_squared_error: 22536811.3139\n",
    ">     Epoch 140/2000\n",
    ">      - 0s - loss: 21639922.0363 - mean_squared_error: 21639922.0363 - val_loss: 22482902.6268 - val_mean_squared_error: 22482902.6268\n",
    ">     Epoch 141/2000\n",
    ">      - 0s - loss: 21587277.8681 - mean_squared_error: 21587277.8681 - val_loss: 22429230.5803 - val_mean_squared_error: 22429230.5803\n",
    ">     Epoch 142/2000\n",
    ">      - 0s - loss: 21534855.1150 - mean_squared_error: 21534855.1150 - val_loss: 22375962.8720 - val_mean_squared_error: 22375962.8720\n",
    ">     Epoch 143/2000\n",
    ">      - 0s - loss: 21482826.3633 - mean_squared_error: 21482826.3633 - val_loss: 22323030.2056 - val_mean_squared_error: 22323030.2056\n",
    ">     Epoch 144/2000\n",
    ">      - 0s - loss: 21431066.9901 - mean_squared_error: 21431066.9901 - val_loss: 22270324.7177 - val_mean_squared_error: 22270324.7177\n",
    ">     Epoch 145/2000\n",
    ">      - 0s - loss: 21379457.4848 - mean_squared_error: 21379457.4848 - val_loss: 22217833.6293 - val_mean_squared_error: 22217833.6293\n",
    ">     Epoch 146/2000\n",
    ">      - 0s - loss: 21328267.0717 - mean_squared_error: 21328267.0717 - val_loss: 22165618.7365 - val_mean_squared_error: 22165618.7365\n",
    ">     Epoch 147/2000\n",
    ">      - 0s - loss: 21277279.8207 - mean_squared_error: 21277279.8207 - val_loss: 22113582.5645 - val_mean_squared_error: 22113582.5645\n",
    ">     Epoch 148/2000\n",
    ">      - 0s - loss: 21226358.6830 - mean_squared_error: 21226358.6830 - val_loss: 22061819.5957 - val_mean_squared_error: 22061819.5957\n",
    ">     Epoch 149/2000\n",
    ">      - 0s - loss: 21175828.1236 - mean_squared_error: 21175828.1236 - val_loss: 22010352.4785 - val_mean_squared_error: 22010352.4785\n",
    ">     Epoch 150/2000\n",
    ">      - 0s - loss: 21125455.0652 - mean_squared_error: 21125455.0652 - val_loss: 21958971.6164 - val_mean_squared_error: 21958971.6164\n",
    ">     Epoch 151/2000\n",
    ">      - 0s - loss: 21075471.0393 - mean_squared_error: 21075471.0393 - val_loss: 21908054.8433 - val_mean_squared_error: 21908054.8433\n",
    ">     Epoch 152/2000\n",
    ">      - 0s - loss: 21025684.3461 - mean_squared_error: 21025684.3461 - val_loss: 21857321.4810 - val_mean_squared_error: 21857321.4810\n",
    ">     Epoch 153/2000\n",
    ">      - 0s - loss: 20976130.8501 - mean_squared_error: 20976130.8501 - val_loss: 21806772.0712 - val_mean_squared_error: 21806772.0712\n",
    ">     Epoch 154/2000\n",
    ">      - 0s - loss: 20926800.1894 - mean_squared_error: 20926800.1894 - val_loss: 21756469.2773 - val_mean_squared_error: 21756469.2773\n",
    ">     Epoch 155/2000\n",
    ">      - 0s - loss: 20877648.5055 - mean_squared_error: 20877648.5055 - val_loss: 21706429.8903 - val_mean_squared_error: 21706429.8903\n",
    ">     Epoch 156/2000\n",
    ">      - 0s - loss: 20828803.0014 - mean_squared_error: 20828803.0014 - val_loss: 21656640.7276 - val_mean_squared_error: 21656640.7276\n",
    ">     Epoch 157/2000\n",
    ">      - 0s - loss: 20780237.4358 - mean_squared_error: 20780237.4358 - val_loss: 21607164.1770 - val_mean_squared_error: 21607164.1770\n",
    ">     Epoch 158/2000\n",
    ">      - 0s - loss: 20731964.5111 - mean_squared_error: 20731964.5111 - val_loss: 21557928.7899 - val_mean_squared_error: 21557928.7899\n",
    ">     Epoch 159/2000\n",
    ">      - 0s - loss: 20683983.8124 - mean_squared_error: 20683983.8124 - val_loss: 21508942.7563 - val_mean_squared_error: 21508942.7563\n",
    ">     Epoch 160/2000\n",
    ">      - 0s - loss: 20636215.6980 - mean_squared_error: 20636215.6980 - val_loss: 21460266.9946 - val_mean_squared_error: 21460266.9946\n",
    ">     Epoch 161/2000\n",
    ">      - 0s - loss: 20588576.9063 - mean_squared_error: 20588576.9063 - val_loss: 21411659.6817 - val_mean_squared_error: 21411659.6817\n",
    ">     Epoch 162/2000\n",
    ">      - 0s - loss: 20541391.1964 - mean_squared_error: 20541391.1964 - val_loss: 21363572.7583 - val_mean_squared_error: 21363572.7583\n",
    ">     Epoch 163/2000\n",
    ">      - 0s - loss: 20494382.0617 - mean_squared_error: 20494382.0617 - val_loss: 21315584.2096 - val_mean_squared_error: 21315584.2096\n",
    ">     Epoch 164/2000\n",
    ">      - 0s - loss: 20447582.2063 - mean_squared_error: 20447582.2063 - val_loss: 21267848.2007 - val_mean_squared_error: 21267848.2007\n",
    ">     Epoch 165/2000\n",
    ">      - 0s - loss: 20401091.7874 - mean_squared_error: 20401091.7874 - val_loss: 21220484.2867 - val_mean_squared_error: 21220484.2867\n",
    ">     Epoch 166/2000\n",
    ">      - 0s - loss: 20354782.9350 - mean_squared_error: 20354782.9350 - val_loss: 21173170.9224 - val_mean_squared_error: 21173170.9224\n",
    ">     Epoch 167/2000\n",
    ">      - 0s - loss: 20308683.8885 - mean_squared_error: 20308683.8885 - val_loss: 21126166.0732 - val_mean_squared_error: 21126166.0732\n",
    ">     Epoch 168/2000\n",
    ">      - 0s - loss: 20262880.0498 - mean_squared_error: 20262880.0498 - val_loss: 21079453.9427 - val_mean_squared_error: 21079453.9427\n",
    ">     Epoch 169/2000\n",
    ">      - 0s - loss: 20217302.2948 - mean_squared_error: 20217302.2948 - val_loss: 21032954.3272 - val_mean_squared_error: 21032954.3272\n",
    ">     Epoch 170/2000\n",
    ">      - 0s - loss: 20172039.3799 - mean_squared_error: 20172039.3799 - val_loss: 20986661.4800 - val_mean_squared_error: 20986661.4800\n",
    ">     Epoch 171/2000\n",
    ">      - 0s - loss: 20127028.1829 - mean_squared_error: 20127028.1829 - val_loss: 20940699.0222 - val_mean_squared_error: 20940699.0222\n",
    ">     Epoch 172/2000\n",
    ">      - 0s - loss: 20082127.6462 - mean_squared_error: 20082127.6462 - val_loss: 20894755.4909 - val_mean_squared_error: 20894755.4909\n",
    ">     Epoch 173/2000\n",
    ">      - 0s - loss: 20037422.4802 - mean_squared_error: 20037422.4802 - val_loss: 20849197.8527 - val_mean_squared_error: 20849197.8527\n",
    ">\n",
    ">     *** WARNING: skipped 241242 bytes of output ***\n",
    ">\n",
    ">     Epoch 1832/2000\n",
    ">      - 0s - loss: 15919839.0633 - mean_squared_error: 15919839.0633 - val_loss: 16556644.7647 - val_mean_squared_error: 16556644.7647\n",
    ">     Epoch 1833/2000\n",
    ">      - 0s - loss: 15919840.3255 - mean_squared_error: 15919840.3255 - val_loss: 16556635.6149 - val_mean_squared_error: 16556635.6149\n",
    ">     Epoch 1834/2000\n",
    ">      - 0s - loss: 15919841.6200 - mean_squared_error: 15919841.6200 - val_loss: 16556647.3777 - val_mean_squared_error: 16556647.3777\n",
    ">     Epoch 1835/2000\n",
    ">      - 0s - loss: 15919840.0938 - mean_squared_error: 15919840.0938 - val_loss: 16556649.3386 - val_mean_squared_error: 16556649.3386\n",
    ">     Epoch 1836/2000\n",
    ">      - 0s - loss: 15919840.6451 - mean_squared_error: 15919840.6451 - val_loss: 16556650.5734 - val_mean_squared_error: 16556650.5734\n",
    ">     Epoch 1837/2000\n",
    ">      - 0s - loss: 15919842.1420 - mean_squared_error: 15919842.1420 - val_loss: 16556663.7143 - val_mean_squared_error: 16556663.7143\n",
    ">     Epoch 1838/2000\n",
    ">      - 0s - loss: 15919842.1427 - mean_squared_error: 15919842.1427 - val_loss: 16556685.6446 - val_mean_squared_error: 16556685.6446\n",
    ">     Epoch 1839/2000\n",
    ">      - 0s - loss: 15919841.2475 - mean_squared_error: 15919841.2475 - val_loss: 16556689.8868 - val_mean_squared_error: 16556689.8868\n",
    ">     Epoch 1840/2000\n",
    ">      - 0s - loss: 15919837.4733 - mean_squared_error: 15919837.4733 - val_loss: 16556697.9367 - val_mean_squared_error: 16556697.9367\n",
    ">     Epoch 1841/2000\n",
    ">      - 0s - loss: 15919838.4133 - mean_squared_error: 15919838.4133 - val_loss: 16556688.7870 - val_mean_squared_error: 16556688.7870\n",
    ">     Epoch 1842/2000\n",
    ">      - 0s - loss: 15919840.9300 - mean_squared_error: 15919840.9300 - val_loss: 16556710.4123 - val_mean_squared_error: 16556710.4123\n",
    ">     Epoch 1843/2000\n",
    ">      - 0s - loss: 15919842.6381 - mean_squared_error: 15919842.6381 - val_loss: 16556705.8774 - val_mean_squared_error: 16556705.8774\n",
    ">     Epoch 1844/2000\n",
    ">      - 0s - loss: 15919837.4598 - mean_squared_error: 15919837.4598 - val_loss: 16556709.6480 - val_mean_squared_error: 16556709.6480\n",
    ">     Epoch 1845/2000\n",
    ">      - 0s - loss: 15919838.9912 - mean_squared_error: 15919838.9912 - val_loss: 16556699.7350 - val_mean_squared_error: 16556699.7350\n",
    ">     Epoch 1846/2000\n",
    ">      - 0s - loss: 15919842.1577 - mean_squared_error: 15919842.1577 - val_loss: 16556698.5793 - val_mean_squared_error: 16556698.5793\n",
    ">     Epoch 1847/2000\n",
    ">      - 0s - loss: 15919842.4977 - mean_squared_error: 15919842.4977 - val_loss: 16556697.8873 - val_mean_squared_error: 16556697.8873\n",
    ">     Epoch 1848/2000\n",
    ">      - 0s - loss: 15919841.3402 - mean_squared_error: 15919841.3402 - val_loss: 16556698.8255 - val_mean_squared_error: 16556698.8255\n",
    ">     Epoch 1849/2000\n",
    ">      - 0s - loss: 15919838.7708 - mean_squared_error: 15919838.7708 - val_loss: 16556703.6095 - val_mean_squared_error: 16556703.6095\n",
    ">     Epoch 1850/2000\n",
    ">      - 0s - loss: 15919837.3867 - mean_squared_error: 15919837.3867 - val_loss: 16556710.7069 - val_mean_squared_error: 16556710.7069\n",
    ">     Epoch 1851/2000\n",
    ">      - 0s - loss: 15919837.8497 - mean_squared_error: 15919837.8497 - val_loss: 16556712.7182 - val_mean_squared_error: 16556712.7182\n",
    ">     Epoch 1852/2000\n",
    ">      - 0s - loss: 15919836.2385 - mean_squared_error: 15919836.2385 - val_loss: 16556729.8883 - val_mean_squared_error: 16556729.8883\n",
    ">     Epoch 1853/2000\n",
    ">      - 0s - loss: 15919839.4165 - mean_squared_error: 15919839.4165 - val_loss: 16556707.3915 - val_mean_squared_error: 16556707.3915\n",
    ">     Epoch 1854/2000\n",
    ">      - 0s - loss: 15919839.0620 - mean_squared_error: 15919839.0620 - val_loss: 16556693.9283 - val_mean_squared_error: 16556693.9283\n",
    ">     Epoch 1855/2000\n",
    ">      - 0s - loss: 15919840.7434 - mean_squared_error: 15919840.7434 - val_loss: 16556700.7958 - val_mean_squared_error: 16556700.7958\n",
    ">     Epoch 1856/2000\n",
    ">      - 0s - loss: 15919839.6519 - mean_squared_error: 15919839.6519 - val_loss: 16556705.2462 - val_mean_squared_error: 16556705.2462\n",
    ">     Epoch 1857/2000\n",
    ">      - 0s - loss: 15919839.2747 - mean_squared_error: 15919839.2747 - val_loss: 16556678.6411 - val_mean_squared_error: 16556678.6411\n",
    ">     Epoch 1858/2000\n",
    ">      - 0s - loss: 15919837.8284 - mean_squared_error: 15919837.8284 - val_loss: 16556697.2106 - val_mean_squared_error: 16556697.2106\n",
    ">     Epoch 1859/2000\n",
    ">      - 0s - loss: 15919842.1135 - mean_squared_error: 15919842.1135 - val_loss: 16556700.1913 - val_mean_squared_error: 16556700.1913\n",
    ">     Epoch 1860/2000\n",
    ">      - 0s - loss: 15919840.7381 - mean_squared_error: 15919840.7381 - val_loss: 16556703.0657 - val_mean_squared_error: 16556703.0657\n",
    ">     Epoch 1861/2000\n",
    ">      - 0s - loss: 15919837.5396 - mean_squared_error: 15919837.5396 - val_loss: 16556699.3920 - val_mean_squared_error: 16556699.3920\n",
    ">     Epoch 1862/2000\n",
    ">      - 0s - loss: 15919838.4874 - mean_squared_error: 15919838.4874 - val_loss: 16556716.9224 - val_mean_squared_error: 16556716.9224\n",
    ">     Epoch 1863/2000\n",
    ">      - 0s - loss: 15919837.8046 - mean_squared_error: 15919837.8046 - val_loss: 16556725.6139 - val_mean_squared_error: 16556725.6139\n",
    ">     Epoch 1864/2000\n",
    ">      - 0s - loss: 15919840.9139 - mean_squared_error: 15919840.9139 - val_loss: 16556720.3747 - val_mean_squared_error: 16556720.3747\n",
    ">     Epoch 1865/2000\n",
    ">      - 0s - loss: 15919840.1795 - mean_squared_error: 15919840.1795 - val_loss: 16556722.3109 - val_mean_squared_error: 16556722.3109\n",
    ">     Epoch 1866/2000\n",
    ">      - 0s - loss: 15919843.0618 - mean_squared_error: 15919843.0618 - val_loss: 16556721.0391 - val_mean_squared_error: 16556721.0391\n",
    ">     Epoch 1867/2000\n",
    ">      - 0s - loss: 15919841.2889 - mean_squared_error: 15919841.2889 - val_loss: 16556744.9086 - val_mean_squared_error: 16556744.9086\n",
    ">     Epoch 1868/2000\n",
    ">      - 0s - loss: 15919839.5626 - mean_squared_error: 15919839.5626 - val_loss: 16556722.7899 - val_mean_squared_error: 16556722.7899\n",
    ">     Epoch 1869/2000\n",
    ">      - 0s - loss: 15919845.4279 - mean_squared_error: 15919845.4279 - val_loss: 16556720.3470 - val_mean_squared_error: 16556720.3470\n",
    ">     Epoch 1870/2000\n",
    ">      - 0s - loss: 15919837.2327 - mean_squared_error: 15919837.2327 - val_loss: 16556730.4474 - val_mean_squared_error: 16556730.4474\n",
    ">     Epoch 1871/2000\n",
    ">      - 0s - loss: 15919836.4915 - mean_squared_error: 15919836.4915 - val_loss: 16556728.8285 - val_mean_squared_error: 16556728.8285\n",
    ">     Epoch 1872/2000\n",
    ">      - 0s - loss: 15919848.0744 - mean_squared_error: 15919848.0744 - val_loss: 16556742.1928 - val_mean_squared_error: 16556742.1928\n",
    ">     Epoch 1873/2000\n",
    ">      - 0s - loss: 15919837.7825 - mean_squared_error: 15919837.7825 - val_loss: 16556739.9011 - val_mean_squared_error: 16556739.9011\n",
    ">     Epoch 1874/2000\n",
    ">      - 0s - loss: 15919839.3969 - mean_squared_error: 15919839.3969 - val_loss: 16556747.4587 - val_mean_squared_error: 16556747.4587\n",
    ">     Epoch 1875/2000\n",
    ">      - 0s - loss: 15919839.6557 - mean_squared_error: 15919839.6557 - val_loss: 16556750.2323 - val_mean_squared_error: 16556750.2323\n",
    ">     Epoch 1876/2000\n",
    ">      - 0s - loss: 15919835.1444 - mean_squared_error: 15919835.1444 - val_loss: 16556749.6050 - val_mean_squared_error: 16556749.6050\n",
    ">     Epoch 1877/2000\n",
    ">      - 0s - loss: 15919836.5018 - mean_squared_error: 15919836.5018 - val_loss: 16556753.1676 - val_mean_squared_error: 16556753.1676\n",
    ">     Epoch 1878/2000\n",
    ">      - 0s - loss: 15919839.9839 - mean_squared_error: 15919839.9839 - val_loss: 16556745.1547 - val_mean_squared_error: 16556745.1547\n",
    ">     Epoch 1879/2000\n",
    ">      - 0s - loss: 15919842.4734 - mean_squared_error: 15919842.4734 - val_loss: 16556743.0104 - val_mean_squared_error: 16556743.0104\n",
    ">     Epoch 1880/2000\n",
    ">      - 0s - loss: 15919841.0024 - mean_squared_error: 15919841.0024 - val_loss: 16556740.5314 - val_mean_squared_error: 16556740.5314\n",
    ">     Epoch 1881/2000\n",
    ">      - 0s - loss: 15919838.0398 - mean_squared_error: 15919838.0398 - val_loss: 16556748.0262 - val_mean_squared_error: 16556748.0262\n",
    ">     Epoch 1882/2000\n",
    ">      - 0s - loss: 15919840.2566 - mean_squared_error: 15919840.2566 - val_loss: 16556742.1710 - val_mean_squared_error: 16556742.1710\n",
    ">     Epoch 1883/2000\n",
    ">      - 0s - loss: 15919839.9691 - mean_squared_error: 15919839.9691 - val_loss: 16556754.4973 - val_mean_squared_error: 16556754.4973\n",
    ">     Epoch 1884/2000\n",
    ">      - 0s - loss: 15919842.2178 - mean_squared_error: 15919842.2178 - val_loss: 16556768.8739 - val_mean_squared_error: 16556768.8739\n",
    ">     Epoch 1885/2000\n",
    ">      - 0s - loss: 15919836.8102 - mean_squared_error: 15919836.8102 - val_loss: 16556762.7469 - val_mean_squared_error: 16556762.7469\n",
    ">     Epoch 1886/2000\n",
    ">      - 0s - loss: 15919842.2946 - mean_squared_error: 15919842.2946 - val_loss: 16556747.3361 - val_mean_squared_error: 16556747.3361\n",
    ">     Epoch 1887/2000\n",
    ">      - 0s - loss: 15919845.0852 - mean_squared_error: 15919845.0852 - val_loss: 16556757.1236 - val_mean_squared_error: 16556757.1236\n",
    ">     Epoch 1888/2000\n",
    ">      - 0s - loss: 15919842.2618 - mean_squared_error: 15919842.2618 - val_loss: 16556758.4439 - val_mean_squared_error: 16556758.4439\n",
    ">     Epoch 1889/2000\n",
    ">      - 0s - loss: 15919836.4589 - mean_squared_error: 15919836.4589 - val_loss: 16556775.5561 - val_mean_squared_error: 16556775.5561\n",
    ">     Epoch 1890/2000\n",
    ">      - 0s - loss: 15919837.9622 - mean_squared_error: 15919837.9622 - val_loss: 16556770.6382 - val_mean_squared_error: 16556770.6382\n",
    ">     Epoch 1891/2000\n",
    ">      - 0s - loss: 15919837.7282 - mean_squared_error: 15919837.7282 - val_loss: 16556775.9486 - val_mean_squared_error: 16556775.9486\n",
    ">     Epoch 1892/2000\n",
    ">      - 0s - loss: 15919838.4250 - mean_squared_error: 15919838.4250 - val_loss: 16556779.2289 - val_mean_squared_error: 16556779.2289\n",
    ">     Epoch 1893/2000\n",
    ">      - 0s - loss: 15919843.4984 - mean_squared_error: 15919843.4984 - val_loss: 16556777.0732 - val_mean_squared_error: 16556777.0732\n",
    ">     Epoch 1894/2000\n",
    ">      - 0s - loss: 15919843.3910 - mean_squared_error: 15919843.3910 - val_loss: 16556771.6846 - val_mean_squared_error: 16556771.6846\n",
    ">     Epoch 1895/2000\n",
    ">      - 0s - loss: 15919844.0152 - mean_squared_error: 15919844.0152 - val_loss: 16556793.1028 - val_mean_squared_error: 16556793.1028\n",
    ">     Epoch 1896/2000\n",
    ">      - 0s - loss: 15919838.8479 - mean_squared_error: 15919838.8479 - val_loss: 16556779.3030 - val_mean_squared_error: 16556779.3030\n",
    ">     Epoch 1897/2000\n",
    ">      - 0s - loss: 15919841.8588 - mean_squared_error: 15919841.8588 - val_loss: 16556779.0929 - val_mean_squared_error: 16556779.0929\n",
    ">     Epoch 1898/2000\n",
    ">      - 0s - loss: 15919841.9264 - mean_squared_error: 15919841.9264 - val_loss: 16556775.5067 - val_mean_squared_error: 16556775.5067\n",
    ">     Epoch 1899/2000\n",
    ">      - 0s - loss: 15919837.4295 - mean_squared_error: 15919837.4295 - val_loss: 16556756.5571 - val_mean_squared_error: 16556756.5571\n",
    ">     Epoch 1900/2000\n",
    ">      - 0s - loss: 15919842.6663 - mean_squared_error: 15919842.6663 - val_loss: 16556770.0306 - val_mean_squared_error: 16556770.0306\n",
    ">     Epoch 1901/2000\n",
    ">      - 0s - loss: 15919840.8584 - mean_squared_error: 15919840.8584 - val_loss: 16556772.1295 - val_mean_squared_error: 16556772.1295\n",
    ">     Epoch 1902/2000\n",
    ">      - 0s - loss: 15919838.3398 - mean_squared_error: 15919838.3398 - val_loss: 16556772.9174 - val_mean_squared_error: 16556772.9174\n",
    ">     Epoch 1903/2000\n",
    ">      - 0s - loss: 15919840.8919 - mean_squared_error: 15919840.8919 - val_loss: 16556779.0826 - val_mean_squared_error: 16556779.0826\n",
    ">     Epoch 1904/2000\n",
    ">      - 0s - loss: 15919840.6112 - mean_squared_error: 15919840.6112 - val_loss: 16556777.8384 - val_mean_squared_error: 16556777.8384\n",
    ">     Epoch 1905/2000\n",
    ">      - 0s - loss: 15919841.8981 - mean_squared_error: 15919841.8981 - val_loss: 16556751.7761 - val_mean_squared_error: 16556751.7761\n",
    ">     Epoch 1906/2000\n",
    ">      - 0s - loss: 15919839.4216 - mean_squared_error: 15919839.4216 - val_loss: 16556758.8156 - val_mean_squared_error: 16556758.8156\n",
    ">     Epoch 1907/2000\n",
    ">      - 0s - loss: 15919839.1625 - mean_squared_error: 15919839.1625 - val_loss: 16556743.3915 - val_mean_squared_error: 16556743.3915\n",
    ">     Epoch 1908/2000\n",
    ">      - 0s - loss: 15919844.9876 - mean_squared_error: 15919844.9876 - val_loss: 16556745.4009 - val_mean_squared_error: 16556745.4009\n",
    ">     Epoch 1909/2000\n",
    ">      - 0s - loss: 15919838.2083 - mean_squared_error: 15919838.2083 - val_loss: 16556738.0514 - val_mean_squared_error: 16556738.0514\n",
    ">     Epoch 1910/2000\n",
    ">      - 0s - loss: 15919838.9869 - mean_squared_error: 15919838.9869 - val_loss: 16556746.2289 - val_mean_squared_error: 16556746.2289\n",
    ">     Epoch 1911/2000\n",
    ">      - 0s - loss: 15919837.5660 - mean_squared_error: 15919837.5660 - val_loss: 16556758.8403 - val_mean_squared_error: 16556758.8403\n",
    ">     Epoch 1912/2000\n",
    ">      - 0s - loss: 15919839.5910 - mean_squared_error: 15919839.5910 - val_loss: 16556757.6569 - val_mean_squared_error: 16556757.6569\n",
    ">     Epoch 1913/2000\n",
    ">      - 0s - loss: 15919841.6379 - mean_squared_error: 15919841.6379 - val_loss: 16556757.2595 - val_mean_squared_error: 16556757.2595\n",
    ">     Epoch 1914/2000\n",
    ">      - 0s - loss: 15919838.3454 - mean_squared_error: 15919838.3454 - val_loss: 16556737.4622 - val_mean_squared_error: 16556737.4622\n",
    ">     Epoch 1915/2000\n",
    ">      - 0s - loss: 15919836.9584 - mean_squared_error: 15919836.9584 - val_loss: 16556750.6288 - val_mean_squared_error: 16556750.6288\n",
    ">     Epoch 1916/2000\n",
    ">      - 0s - loss: 15919841.6337 - mean_squared_error: 15919841.6337 - val_loss: 16556742.0465 - val_mean_squared_error: 16556742.0465\n",
    ">     Epoch 1917/2000\n",
    ">      - 0s - loss: 15919837.1959 - mean_squared_error: 15919837.1959 - val_loss: 16556732.6154 - val_mean_squared_error: 16556732.6154\n",
    ">     Epoch 1918/2000\n",
    ">      - 0s - loss: 15919839.7069 - mean_squared_error: 15919839.7069 - val_loss: 16556722.4315 - val_mean_squared_error: 16556722.4315\n",
    ">     Epoch 1919/2000\n",
    ">      - 0s - loss: 15919841.5924 - mean_squared_error: 15919841.5924 - val_loss: 16556710.8077 - val_mean_squared_error: 16556710.8077\n",
    ">     Epoch 1920/2000\n",
    ">      - 0s - loss: 15919838.2981 - mean_squared_error: 15919838.2981 - val_loss: 16556726.9560 - val_mean_squared_error: 16556726.9560\n",
    ">     Epoch 1921/2000\n",
    ">      - 0s - loss: 15919842.2630 - mean_squared_error: 15919842.2630 - val_loss: 16556718.1676 - val_mean_squared_error: 16556718.1676\n",
    ">     Epoch 1922/2000\n",
    ">      - 0s - loss: 15919841.9537 - mean_squared_error: 15919841.9537 - val_loss: 16556696.9728 - val_mean_squared_error: 16556696.9728\n",
    ">     Epoch 1923/2000\n",
    ">      - 0s - loss: 15919837.5424 - mean_squared_error: 15919837.5424 - val_loss: 16556700.0440 - val_mean_squared_error: 16556700.0440\n",
    ">     Epoch 1924/2000\n",
    ">      - 0s - loss: 15919838.7335 - mean_squared_error: 15919838.7335 - val_loss: 16556691.4266 - val_mean_squared_error: 16556691.4266\n",
    ">     Epoch 1925/2000\n",
    ">      - 0s - loss: 15919841.1641 - mean_squared_error: 15919841.1641 - val_loss: 16556709.9333 - val_mean_squared_error: 16556709.9333\n",
    ">     Epoch 1926/2000\n",
    ">      - 0s - loss: 15919839.5108 - mean_squared_error: 15919839.5108 - val_loss: 16556718.7815 - val_mean_squared_error: 16556718.7815\n",
    ">     Epoch 1927/2000\n",
    ">      - 0s - loss: 15919843.1280 - mean_squared_error: 15919843.1280 - val_loss: 16556725.6747 - val_mean_squared_error: 16556725.6747\n",
    ">     Epoch 1928/2000\n",
    ">      - 0s - loss: 15919839.0706 - mean_squared_error: 15919839.0706 - val_loss: 16556712.9263 - val_mean_squared_error: 16556712.9263\n",
    ">     Epoch 1929/2000\n",
    ">      - 0s - loss: 15919838.6063 - mean_squared_error: 15919838.6063 - val_loss: 16556722.3450 - val_mean_squared_error: 16556722.3450\n",
    ">     Epoch 1930/2000\n",
    ">      - 0s - loss: 15919834.3925 - mean_squared_error: 15919834.3925 - val_loss: 16556733.3292 - val_mean_squared_error: 16556733.3292\n",
    ">     Epoch 1931/2000\n",
    ">      - 0s - loss: 15919839.9176 - mean_squared_error: 15919839.9176 - val_loss: 16556740.5798 - val_mean_squared_error: 16556740.5798\n",
    ">     Epoch 1932/2000\n",
    ">      - 0s - loss: 15919837.0179 - mean_squared_error: 15919837.0179 - val_loss: 16556745.3638 - val_mean_squared_error: 16556745.3638\n",
    ">     Epoch 1933/2000\n",
    ">      - 0s - loss: 15919848.6947 - mean_squared_error: 15919848.6947 - val_loss: 16556746.5329 - val_mean_squared_error: 16556746.5329\n",
    ">     Epoch 1934/2000\n",
    ">      - 0s - loss: 15919844.9725 - mean_squared_error: 15919844.9725 - val_loss: 16556730.0910 - val_mean_squared_error: 16556730.0910\n",
    ">     Epoch 1935/2000\n",
    ">      - 0s - loss: 15919841.7440 - mean_squared_error: 15919841.7440 - val_loss: 16556734.9565 - val_mean_squared_error: 16556734.9565\n",
    ">     Epoch 1936/2000\n",
    ">      - 0s - loss: 15919842.5312 - mean_squared_error: 15919842.5312 - val_loss: 16556726.4142 - val_mean_squared_error: 16556726.4142\n",
    ">     Epoch 1937/2000\n",
    ">      - 0s - loss: 15919840.9410 - mean_squared_error: 15919840.9410 - val_loss: 16556731.3485 - val_mean_squared_error: 16556731.3485\n",
    ">     Epoch 1938/2000\n",
    ">      - 0s - loss: 15919840.0616 - mean_squared_error: 15919840.0616 - val_loss: 16556749.6298 - val_mean_squared_error: 16556749.6298\n",
    ">     Epoch 1939/2000\n",
    ">      - 0s - loss: 15919847.3593 - mean_squared_error: 15919847.3593 - val_loss: 16556732.9338 - val_mean_squared_error: 16556732.9338\n",
    ">     Epoch 1940/2000\n",
    ">      - 0s - loss: 15919842.1849 - mean_squared_error: 15919842.1849 - val_loss: 16556737.9916 - val_mean_squared_error: 16556737.9916\n",
    ">     Epoch 1941/2000\n",
    ">      - 0s - loss: 15919838.7040 - mean_squared_error: 15919838.7040 - val_loss: 16556734.5022 - val_mean_squared_error: 16556734.5022\n",
    ">     Epoch 1942/2000\n",
    ">      - 0s - loss: 15919838.1714 - mean_squared_error: 15919838.1714 - val_loss: 16556738.9278 - val_mean_squared_error: 16556738.9278\n",
    ">     Epoch 1943/2000\n",
    ">      - 0s - loss: 15919840.9966 - mean_squared_error: 15919840.9966 - val_loss: 16556731.4320 - val_mean_squared_error: 16556731.4320\n",
    ">     Epoch 1944/2000\n",
    ">      - 0s - loss: 15919839.2703 - mean_squared_error: 15919839.2703 - val_loss: 16556749.2343 - val_mean_squared_error: 16556749.2343\n",
    ">     Epoch 1945/2000\n",
    ">      - 0s - loss: 15919835.8730 - mean_squared_error: 15919835.8730 - val_loss: 16556757.6569 - val_mean_squared_error: 16556757.6569\n",
    ">     Epoch 1946/2000\n",
    ">      - 0s - loss: 15919843.3970 - mean_squared_error: 15919843.3970 - val_loss: 16556749.2126 - val_mean_squared_error: 16556749.2126\n",
    ">     Epoch 1947/2000\n",
    ">      - 0s - loss: 15919836.3736 - mean_squared_error: 15919836.3736 - val_loss: 16556777.0475 - val_mean_squared_error: 16556777.0475\n",
    ">     Epoch 1948/2000\n",
    ">      - 0s - loss: 15919838.4017 - mean_squared_error: 15919838.4017 - val_loss: 16556770.5734 - val_mean_squared_error: 16556770.5734\n",
    ">     Epoch 1949/2000\n",
    ">      - 0s - loss: 15919841.0001 - mean_squared_error: 15919841.0001 - val_loss: 16556777.0475 - val_mean_squared_error: 16556777.0475\n",
    ">     Epoch 1950/2000\n",
    ">      - 0s - loss: 15919840.6887 - mean_squared_error: 15919840.6887 - val_loss: 16556797.0094 - val_mean_squared_error: 16556797.0094\n",
    ">     Epoch 1951/2000\n",
    ">      - 0s - loss: 15919837.1316 - mean_squared_error: 15919837.1316 - val_loss: 16556769.6401 - val_mean_squared_error: 16556769.6401\n",
    ">     Epoch 1952/2000\n",
    ">      - 0s - loss: 15919839.7709 - mean_squared_error: 15919839.7709 - val_loss: 16556784.5185 - val_mean_squared_error: 16556784.5185\n",
    ">     Epoch 1953/2000\n",
    ">      - 0s - loss: 15919842.3838 - mean_squared_error: 15919842.3838 - val_loss: 16556778.0722 - val_mean_squared_error: 16556778.0722\n",
    ">     Epoch 1954/2000\n",
    ">      - 0s - loss: 15919838.7597 - mean_squared_error: 15919838.7597 - val_loss: 16556758.4439 - val_mean_squared_error: 16556758.4439\n",
    ">     Epoch 1955/2000\n",
    ">      - 0s - loss: 15919837.6232 - mean_squared_error: 15919837.6232 - val_loss: 16556769.7741 - val_mean_squared_error: 16556769.7741\n",
    ">     Epoch 1956/2000\n",
    ">      - 0s - loss: 15919841.1736 - mean_squared_error: 15919841.1736 - val_loss: 16556783.8428 - val_mean_squared_error: 16556783.8428\n",
    ">     Epoch 1957/2000\n",
    ">      - 0s - loss: 15919838.4189 - mean_squared_error: 15919838.4189 - val_loss: 16556789.1789 - val_mean_squared_error: 16556789.1789\n",
    ">     Epoch 1958/2000\n",
    ">      - 0s - loss: 15919844.6613 - mean_squared_error: 15919844.6613 - val_loss: 16556774.1008 - val_mean_squared_error: 16556774.1008\n",
    ">     Epoch 1959/2000\n",
    ">      - 0s - loss: 15919839.5645 - mean_squared_error: 15919839.5645 - val_loss: 16556772.2037 - val_mean_squared_error: 16556772.2037\n",
    ">     Epoch 1960/2000\n",
    ">      - 0s - loss: 15919837.8679 - mean_squared_error: 15919837.8679 - val_loss: 16556784.0519 - val_mean_squared_error: 16556784.0519\n",
    ">     Epoch 1961/2000\n",
    ">      - 0s - loss: 15919845.2541 - mean_squared_error: 15919845.2541 - val_loss: 16556773.5858 - val_mean_squared_error: 16556773.5858\n",
    ">     Epoch 1962/2000\n",
    ">      - 0s - loss: 15919843.2494 - mean_squared_error: 15919843.2494 - val_loss: 16556760.4770 - val_mean_squared_error: 16556760.4770\n",
    ">     Epoch 1963/2000\n",
    ">      - 0s - loss: 15919841.6358 - mean_squared_error: 15919841.6358 - val_loss: 16556767.1972 - val_mean_squared_error: 16556767.1972\n",
    ">     Epoch 1964/2000\n",
    ">      - 0s - loss: 15919845.4326 - mean_squared_error: 15919845.4326 - val_loss: 16556776.0381 - val_mean_squared_error: 16556776.0381\n",
    ">     Epoch 1965/2000\n",
    ">      - 0s - loss: 15919841.3805 - mean_squared_error: 15919841.3805 - val_loss: 16556780.6110 - val_mean_squared_error: 16556780.6110\n",
    ">     Epoch 1966/2000\n",
    ">      - 0s - loss: 15919840.0666 - mean_squared_error: 15919840.0666 - val_loss: 16556780.5225 - val_mean_squared_error: 16556780.5225\n",
    ">     Epoch 1967/2000\n",
    ">      - 0s - loss: 15919842.7959 - mean_squared_error: 15919842.7959 - val_loss: 16556774.6332 - val_mean_squared_error: 16556774.6332\n",
    ">     Epoch 1968/2000\n",
    ">      - 0s - loss: 15919840.4225 - mean_squared_error: 15919840.4225 - val_loss: 16556779.5625 - val_mean_squared_error: 16556779.5625\n",
    ">     Epoch 1969/2000\n",
    ">      - 0s - loss: 15919840.7085 - mean_squared_error: 15919840.7085 - val_loss: 16556764.1537 - val_mean_squared_error: 16556764.1537\n",
    ">     Epoch 1970/2000\n",
    ">      - 0s - loss: 15919842.7566 - mean_squared_error: 15919842.7566 - val_loss: 16556765.7059 - val_mean_squared_error: 16556765.7059\n",
    ">     Epoch 1971/2000\n",
    ">      - 0s - loss: 15919838.0659 - mean_squared_error: 15919838.0659 - val_loss: 16556773.7187 - val_mean_squared_error: 16556773.7187\n",
    ">     Epoch 1972/2000\n",
    ">      - 0s - loss: 15919840.6388 - mean_squared_error: 15919840.6388 - val_loss: 16556791.3376 - val_mean_squared_error: 16556791.3376\n",
    ">     Epoch 1973/2000\n",
    ">      - 0s - loss: 15919840.7910 - mean_squared_error: 15919840.7910 - val_loss: 16556788.7588 - val_mean_squared_error: 16556788.7588\n",
    ">     Epoch 1974/2000\n",
    ">      - 0s - loss: 15919847.2463 - mean_squared_error: 15919847.2463 - val_loss: 16556776.7909 - val_mean_squared_error: 16556776.7909\n",
    ">     Epoch 1975/2000\n",
    ">      - 0s - loss: 15919839.2122 - mean_squared_error: 15919839.2122 - val_loss: 16556773.5858 - val_mean_squared_error: 16556773.5858\n",
    ">     Epoch 1976/2000\n",
    ">      - 0s - loss: 15919842.3183 - mean_squared_error: 15919842.3183 - val_loss: 16556770.4745 - val_mean_squared_error: 16556770.4745\n",
    ">     Epoch 1977/2000\n",
    ">      - 0s - loss: 15919842.5022 - mean_squared_error: 15919842.5022 - val_loss: 16556780.1186 - val_mean_squared_error: 16556780.1186\n",
    ">     Epoch 1978/2000\n",
    ">      - 0s - loss: 15919842.7939 - mean_squared_error: 15919842.7939 - val_loss: 16556785.3589 - val_mean_squared_error: 16556785.3589\n",
    ">     Epoch 1979/2000\n",
    ">      - 0s - loss: 15919842.2156 - mean_squared_error: 15919842.2156 - val_loss: 16556791.3109 - val_mean_squared_error: 16556791.3109\n",
    ">     Epoch 1980/2000\n",
    ">      - 0s - loss: 15919837.1162 - mean_squared_error: 15919837.1162 - val_loss: 16556786.1241 - val_mean_squared_error: 16556786.1241\n",
    ">     Epoch 1981/2000\n",
    ">      - 0s - loss: 15919843.6045 - mean_squared_error: 15919843.6045 - val_loss: 16556763.5976 - val_mean_squared_error: 16556763.5976\n",
    ">     Epoch 1982/2000\n",
    ">      - 0s - loss: 15919840.3795 - mean_squared_error: 15919840.3795 - val_loss: 16556779.0465 - val_mean_squared_error: 16556779.0465\n",
    ">     Epoch 1983/2000\n",
    ">      - 0s - loss: 15919840.6743 - mean_squared_error: 15919840.6743 - val_loss: 16556755.6426 - val_mean_squared_error: 16556755.6426\n",
    ">     Epoch 1984/2000\n",
    ">      - 0s - loss: 15919839.0763 - mean_squared_error: 15919839.0763 - val_loss: 16556746.8809 - val_mean_squared_error: 16556746.8809\n",
    ">     Epoch 1985/2000\n",
    ">      - 0s - loss: 15919843.7900 - mean_squared_error: 15919843.7900 - val_loss: 16556765.8789 - val_mean_squared_error: 16556765.8789\n",
    ">     Epoch 1986/2000\n",
    ">      - 0s - loss: 15919839.8720 - mean_squared_error: 15919839.8720 - val_loss: 16556766.1518 - val_mean_squared_error: 16556766.1518\n",
    ">     Epoch 1987/2000\n",
    ">      - 0s - loss: 15919841.0103 - mean_squared_error: 15919841.0103 - val_loss: 16556761.3658 - val_mean_squared_error: 16556761.3658\n",
    ">     Epoch 1988/2000\n",
    ">      - 0s - loss: 15919844.0040 - mean_squared_error: 15919844.0040 - val_loss: 16556743.2175 - val_mean_squared_error: 16556743.2175\n",
    ">     Epoch 1989/2000\n",
    ">      - 0s - loss: 15919840.8275 - mean_squared_error: 15919840.8275 - val_loss: 16556755.9051 - val_mean_squared_error: 16556755.9051\n",
    ">     Epoch 1990/2000\n",
    ">      - 0s - loss: 15919839.0663 - mean_squared_error: 15919839.0663 - val_loss: 16556755.2264 - val_mean_squared_error: 16556755.2264\n",
    ">     Epoch 1991/2000\n",
    ">      - 0s - loss: 15919841.7517 - mean_squared_error: 15919841.7517 - val_loss: 16556744.1176 - val_mean_squared_error: 16556744.1176\n",
    ">     Epoch 1992/2000\n",
    ">      - 0s - loss: 15919839.2471 - mean_squared_error: 15919839.2471 - val_loss: 16556753.8700 - val_mean_squared_error: 16556753.8700\n",
    ">     Epoch 1993/2000\n",
    ">      - 0s - loss: 15919839.7289 - mean_squared_error: 15919839.7289 - val_loss: 16556742.0484 - val_mean_squared_error: 16556742.0484\n",
    ">     Epoch 1994/2000\n",
    ">      - 0s - loss: 15919842.0739 - mean_squared_error: 15919842.0739 - val_loss: 16556749.2343 - val_mean_squared_error: 16556749.2343\n",
    ">     Epoch 1995/2000\n",
    ">      - 0s - loss: 15919844.3030 - mean_squared_error: 15919844.3030 - val_loss: 16556739.4686 - val_mean_squared_error: 16556739.4686\n",
    ">     Epoch 1996/2000\n",
    ">      - 0s - loss: 15919837.5737 - mean_squared_error: 15919837.5737 - val_loss: 16556745.3144 - val_mean_squared_error: 16556745.3144\n",
    ">     Epoch 1997/2000\n",
    ">      - 0s - loss: 15919836.2656 - mean_squared_error: 15919836.2656 - val_loss: 16556755.5467 - val_mean_squared_error: 16556755.5467\n",
    ">     Epoch 1998/2000\n",
    ">      - 0s - loss: 15919835.8301 - mean_squared_error: 15919835.8301 - val_loss: 16556757.5057 - val_mean_squared_error: 16556757.5057\n",
    ">     Epoch 1999/2000\n",
    ">      - 0s - loss: 15919842.5290 - mean_squared_error: 15919842.5290 - val_loss: 16556768.8245 - val_mean_squared_error: 16556768.8245\n",
    ">     Epoch 2000/2000\n",
    ">      - 0s - loss: 15919839.9017 - mean_squared_error: 15919839.9017 - val_loss: 16556769.6495 - val_mean_squared_error: 16556769.6495\n",
    ">\n",
    ">        32/13485 [..............................] - ETA: 0s\n",
    ">      4384/13485 [========>.....................] - ETA: 0s\n",
    ">      8640/13485 [==================>...........] - ETA: 0s\n",
    ">     12960/13485 [===========================>..] - ETA: 0s\n",
    ">     13485/13485 [==============================] - 0s 12us/step\n",
    ">\n",
    ">     root mean_squared_error: 3963.688024\n",
    "\n",
    "  \n",
    "\n",
    "##### What is different here?\n",
    "\n",
    "-   We've changed the activation in the hidden layer to \"sigmoid\" per\n",
    "    our discussion.\n",
    "-   Next, notice that we're running 2000 training epochs!\n",
    "\n",
    "Even so, it takes a looooong time to converge. If you experiment a lot,\n",
    "you'll find that ... it still takes a long time to converge. Around the\n",
    "early part of the most recent deep learning renaissance, researchers\n",
    "started experimenting with other non-linearities.\n",
    "\n",
    "(Remember, we're talking about non-linear activations in the hidden\n",
    "layer. The output here is still using \"linear\" rather than \"softmax\"\n",
    "because we're performing regression, not classification.)\n",
    "\n",
    "In theory, any non-linearity should allow learning, and maybe we can use\n",
    "one that \"works better\"\n",
    "\n",
    "By \"works better\" we mean\n",
    "\n",
    "-   Simpler gradient - faster to compute\n",
    "-   Less prone to \"saturation\" -- where the neuron ends up way off in\n",
    "    the 0 or 1 territory of the sigmoid and can't easily learn anything\n",
    "-   Keeps gradients \"big\" -- avoiding the large, flat, near-zero\n",
    "    gradient areas of the sigmoid\n",
    "\n",
    "Turns out that a big breakthrough and popular solution is a very simple\n",
    "hack:\n",
    "\n",
    "### Rectified Linear Unit (ReLU)\n",
    "\n",
    "&lt;img src=\"http://i.imgur.com/oAYh9DN.png\" width=1000&gt;\n",
    "\n",
    "### Go change your hidden-layer activation from 'sigmoid' to 'relu'\n",
    "\n",
    "Start your script and watch the error for a bit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "input_file = \"/dbfs/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\"\n",
    "\n",
    "df = pd.read_csv(input_file, header = 0)\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "df = pd.get_dummies(df, prefix=['cut_', 'color_', 'clarity_'])\n",
    "\n",
    "y = df.iloc[:,3:4].as_matrix().flatten()\n",
    "y.flatten()\n",
    "\n",
    "X = df.drop(df.columns[3], axis=1).as_matrix()\n",
    "np.shape(X)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=26, kernel_initializer='normal', activation='relu')) # <--- CHANGE IS HERE\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "history = model.fit(X_train, y_train, epochs=2000, batch_size=100, validation_split=0.1, verbose=2)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\nroot %s: %f\" % (model.metrics_names[1], np.sqrt(scores[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Train on 36409 samples, validate on 4046 samples\n",
    ">     Epoch 1/2000\n",
    ">      - 1s - loss: 29938863.4237 - mean_squared_error: 29938863.4237 - val_loss: 27832422.8473 - val_mean_squared_error: 27832422.8473\n",
    ">     Epoch 2/2000\n",
    ">      - 0s - loss: 22388635.5017 - mean_squared_error: 22388635.5017 - val_loss: 19167540.5645 - val_mean_squared_error: 19167540.5645\n",
    ">     Epoch 3/2000\n",
    ">      - 0s - loss: 16431872.0007 - mean_squared_error: 16431872.0007 - val_loss: 16017428.7776 - val_mean_squared_error: 16017428.7776\n",
    ">     Epoch 4/2000\n",
    ">      - 0s - loss: 15169837.3108 - mean_squared_error: 15169837.3108 - val_loss: 15670100.1587 - val_mean_squared_error: 15670100.1587\n",
    ">     Epoch 5/2000\n",
    ">      - 0s - loss: 15005770.5474 - mean_squared_error: 15005770.5474 - val_loss: 15541303.1453 - val_mean_squared_error: 15541303.1453\n",
    ">     Epoch 6/2000\n",
    ">      - 0s - loss: 14886009.9103 - mean_squared_error: 14886009.9103 - val_loss: 15410182.4088 - val_mean_squared_error: 15410182.4088\n",
    ">     Epoch 7/2000\n",
    ">      - 0s - loss: 14750829.7501 - mean_squared_error: 14750829.7501 - val_loss: 15262493.8393 - val_mean_squared_error: 15262493.8393\n",
    ">     Epoch 8/2000\n",
    ">      - 0s - loss: 14598241.2764 - mean_squared_error: 14598241.2764 - val_loss: 15092539.8774 - val_mean_squared_error: 15092539.8774\n",
    ">     Epoch 9/2000\n",
    ">      - 0s - loss: 14426493.4177 - mean_squared_error: 14426493.4177 - val_loss: 14905501.4780 - val_mean_squared_error: 14905501.4780\n",
    ">     Epoch 10/2000\n",
    ">      - 0s - loss: 14228848.9103 - mean_squared_error: 14228848.9103 - val_loss: 14688015.9975 - val_mean_squared_error: 14688015.9975\n",
    ">     Epoch 11/2000\n",
    ">      - 1s - loss: 14001212.6608 - mean_squared_error: 14001212.6608 - val_loss: 14434330.9461 - val_mean_squared_error: 14434330.9461\n",
    ">     Epoch 12/2000\n",
    ">      - 0s - loss: 13736570.0820 - mean_squared_error: 13736570.0820 - val_loss: 14139074.0257 - val_mean_squared_error: 14139074.0257\n",
    ">     Epoch 13/2000\n",
    ">      - 0s - loss: 13428367.8534 - mean_squared_error: 13428367.8534 - val_loss: 13795523.7548 - val_mean_squared_error: 13795523.7548\n",
    ">     Epoch 14/2000\n",
    ">      - 0s - loss: 13070743.7831 - mean_squared_error: 13070743.7831 - val_loss: 13397306.3450 - val_mean_squared_error: 13397306.3450\n",
    ">     Epoch 15/2000\n",
    ">      - 0s - loss: 12656768.5956 - mean_squared_error: 12656768.5956 - val_loss: 12945219.0559 - val_mean_squared_error: 12945219.0559\n",
    ">     Epoch 16/2000\n",
    ">      - 0s - loss: 12185512.7982 - mean_squared_error: 12185512.7982 - val_loss: 12423358.2358 - val_mean_squared_error: 12423358.2358\n",
    ">     Epoch 17/2000\n",
    ">      - 0s - loss: 11655525.9851 - mean_squared_error: 11655525.9851 - val_loss: 11847318.5447 - val_mean_squared_error: 11847318.5447\n",
    ">     Epoch 18/2000\n",
    ">      - 0s - loss: 11071839.4945 - mean_squared_error: 11071839.4945 - val_loss: 11221099.8136 - val_mean_squared_error: 11221099.8136\n",
    ">     Epoch 19/2000\n",
    ">      - 0s - loss: 10444863.5801 - mean_squared_error: 10444863.5801 - val_loss: 10549720.4142 - val_mean_squared_error: 10549720.4142\n",
    ">     Epoch 20/2000\n",
    ">      - 0s - loss: 9788272.2497 - mean_squared_error: 9788272.2497 - val_loss: 9859712.9110 - val_mean_squared_error: 9859712.9110\n",
    ">     Epoch 21/2000\n",
    ">      - 0s - loss: 9115201.0716 - mean_squared_error: 9115201.0716 - val_loss: 9161731.1725 - val_mean_squared_error: 9161731.1725\n",
    ">     Epoch 22/2000\n",
    ">      - 0s - loss: 8446740.0727 - mean_squared_error: 8446740.0727 - val_loss: 8473166.5581 - val_mean_squared_error: 8473166.5581\n",
    ">     Epoch 23/2000\n",
    ">      - 0s - loss: 7793792.6583 - mean_squared_error: 7793792.6583 - val_loss: 7801613.9909 - val_mean_squared_error: 7801613.9909\n",
    ">     Epoch 24/2000\n",
    ">      - 0s - loss: 7166955.5462 - mean_squared_error: 7166955.5462 - val_loss: 7164038.4654 - val_mean_squared_error: 7164038.4654\n",
    ">     Epoch 25/2000\n",
    ">      - 0s - loss: 6582194.8524 - mean_squared_error: 6582194.8524 - val_loss: 6574832.5309 - val_mean_squared_error: 6574832.5309\n",
    ">     Epoch 26/2000\n",
    ">      - 0s - loss: 6047351.9016 - mean_squared_error: 6047351.9016 - val_loss: 6035904.0623 - val_mean_squared_error: 6035904.0623\n",
    ">     Epoch 27/2000\n",
    ">      - 0s - loss: 5566922.6957 - mean_squared_error: 5566922.6957 - val_loss: 5558418.2491 - val_mean_squared_error: 5558418.2491\n",
    ">     Epoch 28/2000\n",
    ">      - 0s - loss: 5143339.4477 - mean_squared_error: 5143339.4477 - val_loss: 5132038.7902 - val_mean_squared_error: 5132038.7902\n",
    ">     Epoch 29/2000\n",
    ">      - 0s - loss: 4767790.8131 - mean_squared_error: 4767790.8131 - val_loss: 4757215.1453 - val_mean_squared_error: 4757215.1453\n",
    ">     Epoch 30/2000\n",
    ">      - 0s - loss: 4439649.8817 - mean_squared_error: 4439649.8817 - val_loss: 4431028.5476 - val_mean_squared_error: 4431028.5476\n",
    ">     Epoch 31/2000\n",
    ">      - 0s - loss: 4152965.2396 - mean_squared_error: 4152965.2396 - val_loss: 4137767.6985 - val_mean_squared_error: 4137767.6985\n",
    ">     Epoch 32/2000\n",
    ">      - 0s - loss: 3901266.2654 - mean_squared_error: 3901266.2654 - val_loss: 3882356.7435 - val_mean_squared_error: 3882356.7435\n",
    ">     Epoch 33/2000\n",
    ">      - 0s - loss: 3677236.9365 - mean_squared_error: 3677236.9365 - val_loss: 3646491.6930 - val_mean_squared_error: 3646491.6930\n",
    ">     Epoch 34/2000\n",
    ">      - 0s - loss: 3477286.9178 - mean_squared_error: 3477286.9178 - val_loss: 3443949.0140 - val_mean_squared_error: 3443949.0140\n",
    ">     Epoch 35/2000\n",
    ">      - 0s - loss: 3302099.7013 - mean_squared_error: 3302099.7013 - val_loss: 3258397.3565 - val_mean_squared_error: 3258397.3565\n",
    ">     Epoch 36/2000\n",
    ">      - 0s - loss: 3144469.7298 - mean_squared_error: 3144469.7298 - val_loss: 3091707.3880 - val_mean_squared_error: 3091707.3880\n",
    ">     Epoch 37/2000\n",
    ">      - 0s - loss: 3001370.1311 - mean_squared_error: 3001370.1311 - val_loss: 2935768.6719 - val_mean_squared_error: 2935768.6719\n",
    ">     Epoch 38/2000\n",
    ">      - 0s - loss: 2869674.9387 - mean_squared_error: 2869674.9387 - val_loss: 2804373.1770 - val_mean_squared_error: 2804373.1770\n",
    ">     Epoch 39/2000\n",
    ">      - 0s - loss: 2750813.5957 - mean_squared_error: 2750813.5957 - val_loss: 2669017.5266 - val_mean_squared_error: 2669017.5266\n",
    ">     Epoch 40/2000\n",
    ">      - 0s - loss: 2639596.0706 - mean_squared_error: 2639596.0706 - val_loss: 2549130.5860 - val_mean_squared_error: 2549130.5860\n",
    ">     Epoch 41/2000\n",
    ">      - 0s - loss: 2539679.8545 - mean_squared_error: 2539679.8545 - val_loss: 2439497.7667 - val_mean_squared_error: 2439497.7667\n",
    ">     Epoch 42/2000\n",
    ">      - 0s - loss: 2446811.9564 - mean_squared_error: 2446811.9564 - val_loss: 2338626.5081 - val_mean_squared_error: 2338626.5081\n",
    ">     Epoch 43/2000\n",
    ">      - 0s - loss: 2362560.7893 - mean_squared_error: 2362560.7893 - val_loss: 2244730.6283 - val_mean_squared_error: 2244730.6283\n",
    ">     Epoch 44/2000\n",
    ">      - 0s - loss: 2283878.5586 - mean_squared_error: 2283878.5586 - val_loss: 2158127.9470 - val_mean_squared_error: 2158127.9470\n",
    ">     Epoch 45/2000\n",
    ">      - 0s - loss: 2210463.4408 - mean_squared_error: 2210463.4408 - val_loss: 2076556.6046 - val_mean_squared_error: 2076556.6046\n",
    ">     Epoch 46/2000\n",
    ">      - 0s - loss: 2141897.1993 - mean_squared_error: 2141897.1993 - val_loss: 2000310.3009 - val_mean_squared_error: 2000310.3009\n",
    ">     Epoch 47/2000\n",
    ">      - 0s - loss: 2078454.0809 - mean_squared_error: 2078454.0809 - val_loss: 1931987.7154 - val_mean_squared_error: 1931987.7154\n",
    ">     Epoch 48/2000\n",
    ">      - 0s - loss: 2021185.5238 - mean_squared_error: 2021185.5238 - val_loss: 1865060.8356 - val_mean_squared_error: 1865060.8356\n",
    ">     Epoch 49/2000\n",
    ">      - 0s - loss: 1966803.1820 - mean_squared_error: 1966803.1820 - val_loss: 1808340.0919 - val_mean_squared_error: 1808340.0919\n",
    ">     Epoch 50/2000\n",
    ">      - 0s - loss: 1915811.9821 - mean_squared_error: 1915811.9821 - val_loss: 1747630.3844 - val_mean_squared_error: 1747630.3844\n",
    ">     Epoch 51/2000\n",
    ">      - 0s - loss: 1866521.3634 - mean_squared_error: 1866521.3634 - val_loss: 1693315.2384 - val_mean_squared_error: 1693315.2384\n",
    ">     Epoch 52/2000\n",
    ">      - 0s - loss: 1821326.0610 - mean_squared_error: 1821326.0610 - val_loss: 1642927.1448 - val_mean_squared_error: 1642927.1448\n",
    ">     Epoch 53/2000\n",
    ">      - 0s - loss: 1778010.7790 - mean_squared_error: 1778010.7790 - val_loss: 1593469.0986 - val_mean_squared_error: 1593469.0986\n",
    ">     Epoch 54/2000\n",
    ">      - 0s - loss: 1737470.6897 - mean_squared_error: 1737470.6897 - val_loss: 1549439.4479 - val_mean_squared_error: 1549439.4479\n",
    ">     Epoch 55/2000\n",
    ">      - 0s - loss: 1700140.8353 - mean_squared_error: 1700140.8353 - val_loss: 1512758.0407 - val_mean_squared_error: 1512758.0407\n",
    ">     Epoch 56/2000\n",
    ">      - 0s - loss: 1664932.0321 - mean_squared_error: 1664932.0321 - val_loss: 1468464.1324 - val_mean_squared_error: 1468464.1324\n",
    ">     Epoch 57/2000\n",
    ">      - 0s - loss: 1631250.8124 - mean_squared_error: 1631250.8124 - val_loss: 1430306.0396 - val_mean_squared_error: 1430306.0396\n",
    ">     Epoch 58/2000\n",
    ">      - 0s - loss: 1599123.9408 - mean_squared_error: 1599123.9408 - val_loss: 1402570.5938 - val_mean_squared_error: 1402570.5938\n",
    ">     Epoch 59/2000\n",
    ">      - 0s - loss: 1569169.3397 - mean_squared_error: 1569169.3397 - val_loss: 1361781.7992 - val_mean_squared_error: 1361781.7992\n",
    ">     Epoch 60/2000\n",
    ">      - 0s - loss: 1540956.2998 - mean_squared_error: 1540956.2998 - val_loss: 1329689.5006 - val_mean_squared_error: 1329689.5006\n",
    ">     Epoch 61/2000\n",
    ">      - 0s - loss: 1513870.4650 - mean_squared_error: 1513870.4650 - val_loss: 1299788.0126 - val_mean_squared_error: 1299788.0126\n",
    ">     Epoch 62/2000\n",
    ">      - 0s - loss: 1488514.5449 - mean_squared_error: 1488514.5449 - val_loss: 1271596.4853 - val_mean_squared_error: 1271596.4853\n",
    ">     Epoch 63/2000\n",
    ">      - 0s - loss: 1464872.0181 - mean_squared_error: 1464872.0181 - val_loss: 1245587.1696 - val_mean_squared_error: 1245587.1696\n",
    ">     Epoch 64/2000\n",
    ">      - 0s - loss: 1441970.8339 - mean_squared_error: 1441970.8339 - val_loss: 1220000.9017 - val_mean_squared_error: 1220000.9017\n",
    ">     Epoch 65/2000\n",
    ">      - 0s - loss: 1419747.2820 - mean_squared_error: 1419747.2820 - val_loss: 1200149.5500 - val_mean_squared_error: 1200149.5500\n",
    ">     Epoch 66/2000\n",
    ">      - 0s - loss: 1399810.3150 - mean_squared_error: 1399810.3150 - val_loss: 1174179.1523 - val_mean_squared_error: 1174179.1523\n",
    ">     Epoch 67/2000\n",
    ">      - 0s - loss: 1379762.8659 - mean_squared_error: 1379762.8659 - val_loss: 1155421.8361 - val_mean_squared_error: 1155421.8361\n",
    ">     Epoch 68/2000\n",
    ">      - 0s - loss: 1361613.9023 - mean_squared_error: 1361613.9023 - val_loss: 1131801.8392 - val_mean_squared_error: 1131801.8392\n",
    ">     Epoch 69/2000\n",
    ">      - 0s - loss: 1344540.7886 - mean_squared_error: 1344540.7886 - val_loss: 1112408.5979 - val_mean_squared_error: 1112408.5979\n",
    ">     Epoch 70/2000\n",
    ">      - 0s - loss: 1327972.0930 - mean_squared_error: 1327972.0930 - val_loss: 1095669.5814 - val_mean_squared_error: 1095669.5814\n",
    ">     Epoch 71/2000\n",
    ">      - 0s - loss: 1311907.4601 - mean_squared_error: 1311907.4601 - val_loss: 1080797.0534 - val_mean_squared_error: 1080797.0534\n",
    ">     Epoch 72/2000\n",
    ">      - 0s - loss: 1297300.1515 - mean_squared_error: 1297300.1515 - val_loss: 1063274.4691 - val_mean_squared_error: 1063274.4691\n",
    ">     Epoch 73/2000\n",
    ">      - 0s - loss: 1282702.7680 - mean_squared_error: 1282702.7680 - val_loss: 1047914.8447 - val_mean_squared_error: 1047914.8447\n",
    ">     Epoch 74/2000\n",
    ">      - 0s - loss: 1269224.1888 - mean_squared_error: 1269224.1888 - val_loss: 1035664.7105 - val_mean_squared_error: 1035664.7105\n",
    ">     Epoch 75/2000\n",
    ">      - 0s - loss: 1255846.9290 - mean_squared_error: 1255846.9290 - val_loss: 1022400.9963 - val_mean_squared_error: 1022400.9963\n",
    ">     Epoch 76/2000\n",
    ">      - 0s - loss: 1243903.4871 - mean_squared_error: 1243903.4871 - val_loss: 1003828.2156 - val_mean_squared_error: 1003828.2156\n",
    ">     Epoch 77/2000\n",
    ">      - 0s - loss: 1232750.9959 - mean_squared_error: 1232750.9959 - val_loss: 991562.2552 - val_mean_squared_error: 991562.2552\n",
    ">     Epoch 78/2000\n",
    ">      - 0s - loss: 1221784.7819 - mean_squared_error: 1221784.7819 - val_loss: 988014.4801 - val_mean_squared_error: 988014.4801\n",
    ">     Epoch 79/2000\n",
    ">      - 0s - loss: 1210924.4235 - mean_squared_error: 1210924.4235 - val_loss: 966999.6418 - val_mean_squared_error: 966999.6418\n",
    ">     Epoch 80/2000\n",
    ">      - 0s - loss: 1201281.6524 - mean_squared_error: 1201281.6524 - val_loss: 957178.3798 - val_mean_squared_error: 957178.3798\n",
    ">     Epoch 81/2000\n",
    ">      - 0s - loss: 1191342.2884 - mean_squared_error: 1191342.2884 - val_loss: 945778.2848 - val_mean_squared_error: 945778.2848\n",
    ">     Epoch 82/2000\n",
    ">      - 0s - loss: 1182401.2386 - mean_squared_error: 1182401.2386 - val_loss: 941735.3018 - val_mean_squared_error: 941735.3018\n",
    ">     Epoch 83/2000\n",
    ">      - 0s - loss: 1173785.2414 - mean_squared_error: 1173785.2414 - val_loss: 926331.6598 - val_mean_squared_error: 926331.6598\n",
    ">     Epoch 84/2000\n",
    ">      - 0s - loss: 1165100.9749 - mean_squared_error: 1165100.9749 - val_loss: 922599.2534 - val_mean_squared_error: 922599.2534\n",
    ">     Epoch 85/2000\n",
    ">      - 0s - loss: 1157599.6469 - mean_squared_error: 1157599.6469 - val_loss: 909060.7537 - val_mean_squared_error: 909060.7537\n",
    ">     Epoch 86/2000\n",
    ">      - 0s - loss: 1149606.7023 - mean_squared_error: 1149606.7023 - val_loss: 905816.1376 - val_mean_squared_error: 905816.1376\n",
    ">     Epoch 87/2000\n",
    ">      - 0s - loss: 1142876.2513 - mean_squared_error: 1142876.2513 - val_loss: 894544.3443 - val_mean_squared_error: 894544.3443\n",
    ">     Epoch 88/2000\n",
    ">      - 0s - loss: 1135421.7432 - mean_squared_error: 1135421.7432 - val_loss: 885002.8672 - val_mean_squared_error: 885002.8672\n",
    ">     Epoch 89/2000\n",
    ">      - 0s - loss: 1128510.0997 - mean_squared_error: 1128510.0997 - val_loss: 885705.4161 - val_mean_squared_error: 885705.4161\n",
    ">     Epoch 90/2000\n",
    ">      - 0s - loss: 1122707.7963 - mean_squared_error: 1122707.7963 - val_loss: 869758.4723 - val_mean_squared_error: 869758.4723\n",
    ">     Epoch 91/2000\n",
    ">      - 0s - loss: 1115116.7629 - mean_squared_error: 1115116.7629 - val_loss: 870980.7851 - val_mean_squared_error: 870980.7851\n",
    ">     Epoch 92/2000\n",
    ">      - 0s - loss: 1108894.0348 - mean_squared_error: 1108894.0348 - val_loss: 855680.1749 - val_mean_squared_error: 855680.1749\n",
    ">     Epoch 93/2000\n",
    ">      - 0s - loss: 1103963.9562 - mean_squared_error: 1103963.9562 - val_loss: 852032.6139 - val_mean_squared_error: 852032.6139\n",
    ">     Epoch 94/2000\n",
    ">      - 0s - loss: 1098281.1880 - mean_squared_error: 1098281.1880 - val_loss: 845218.1591 - val_mean_squared_error: 845218.1591\n",
    ">     Epoch 95/2000\n",
    ">      - 0s - loss: 1092883.6761 - mean_squared_error: 1092883.6761 - val_loss: 836951.9199 - val_mean_squared_error: 836951.9199\n",
    ">     Epoch 96/2000\n",
    ">      - 0s - loss: 1087001.8999 - mean_squared_error: 1087001.8999 - val_loss: 831424.8668 - val_mean_squared_error: 831424.8668\n",
    ">     Epoch 97/2000\n",
    ">      - 0s - loss: 1081821.6931 - mean_squared_error: 1081821.6931 - val_loss: 826467.4629 - val_mean_squared_error: 826467.4629\n",
    ">     Epoch 98/2000\n",
    ">      - 0s - loss: 1076217.9142 - mean_squared_error: 1076217.9142 - val_loss: 819017.7581 - val_mean_squared_error: 819017.7581\n",
    ">     Epoch 99/2000\n",
    ">      - 0s - loss: 1072431.6157 - mean_squared_error: 1072431.6157 - val_loss: 817342.3566 - val_mean_squared_error: 817342.3566\n",
    ">     Epoch 100/2000\n",
    ">      - 0s - loss: 1066940.7825 - mean_squared_error: 1066940.7825 - val_loss: 808668.5299 - val_mean_squared_error: 808668.5299\n",
    ">     Epoch 101/2000\n",
    ">      - 0s - loss: 1062883.5691 - mean_squared_error: 1062883.5691 - val_loss: 804621.1654 - val_mean_squared_error: 804621.1654\n",
    ">     Epoch 102/2000\n",
    ">      - 0s - loss: 1058172.6794 - mean_squared_error: 1058172.6794 - val_loss: 798353.0681 - val_mean_squared_error: 798353.0681\n",
    ">     Epoch 103/2000\n",
    ">      - 0s - loss: 1053867.6362 - mean_squared_error: 1053867.6362 - val_loss: 804793.9548 - val_mean_squared_error: 804793.9548\n",
    ">     Epoch 104/2000\n",
    ">      - 0s - loss: 1049076.8368 - mean_squared_error: 1049076.8368 - val_loss: 789712.6155 - val_mean_squared_error: 789712.6155\n",
    ">     Epoch 105/2000\n",
    ">      - 0s - loss: 1045631.3630 - mean_squared_error: 1045631.3630 - val_loss: 785457.5816 - val_mean_squared_error: 785457.5816\n",
    ">     Epoch 106/2000\n",
    ">      - 0s - loss: 1040585.5933 - mean_squared_error: 1040585.5933 - val_loss: 780374.5012 - val_mean_squared_error: 780374.5012\n",
    ">     Epoch 107/2000\n",
    ">      - 0s - loss: 1037087.4549 - mean_squared_error: 1037087.4549 - val_loss: 783961.0112 - val_mean_squared_error: 783961.0112\n",
    ">     Epoch 108/2000\n",
    ">      - 0s - loss: 1033408.3128 - mean_squared_error: 1033408.3128 - val_loss: 773750.7693 - val_mean_squared_error: 773750.7693\n",
    ">     Epoch 109/2000\n",
    ">      - 0s - loss: 1029350.9100 - mean_squared_error: 1029350.9100 - val_loss: 771727.1558 - val_mean_squared_error: 771727.1558\n",
    ">     Epoch 110/2000\n",
    ">      - 0s - loss: 1025883.1703 - mean_squared_error: 1025883.1703 - val_loss: 768502.3288 - val_mean_squared_error: 768502.3288\n",
    ">     Epoch 111/2000\n",
    ">      - 0s - loss: 1021833.5502 - mean_squared_error: 1021833.5502 - val_loss: 760970.9669 - val_mean_squared_error: 760970.9669\n",
    ">     Epoch 112/2000\n",
    ">      - 0s - loss: 1018940.0217 - mean_squared_error: 1018940.0217 - val_loss: 760039.7225 - val_mean_squared_error: 760039.7225\n",
    ">     Epoch 113/2000\n",
    ">      - 0s - loss: 1014862.8041 - mean_squared_error: 1014862.8041 - val_loss: 764375.0141 - val_mean_squared_error: 764375.0141\n",
    ">     Epoch 114/2000\n",
    ">      - 0s - loss: 1011251.5824 - mean_squared_error: 1011251.5824 - val_loss: 754512.3164 - val_mean_squared_error: 754512.3164\n",
    ">     Epoch 115/2000\n",
    ">      - 0s - loss: 1008575.3374 - mean_squared_error: 1008575.3374 - val_loss: 748945.7377 - val_mean_squared_error: 748945.7377\n",
    ">     Epoch 116/2000\n",
    ">      - 0s - loss: 1005124.7456 - mean_squared_error: 1005124.7456 - val_loss: 749031.7273 - val_mean_squared_error: 749031.7273\n",
    ">     Epoch 117/2000\n",
    ">      - 0s - loss: 1001222.4575 - mean_squared_error: 1001222.4575 - val_loss: 739509.4394 - val_mean_squared_error: 739509.4394\n",
    ">     Epoch 118/2000\n",
    ">      - 0s - loss: 998861.1399 - mean_squared_error: 998861.1399 - val_loss: 745729.3119 - val_mean_squared_error: 745729.3119\n",
    ">     Epoch 119/2000\n",
    ">      - 0s - loss: 996182.4343 - mean_squared_error: 996182.4343 - val_loss: 757251.6217 - val_mean_squared_error: 757251.6217\n",
    ">     Epoch 120/2000\n",
    ">      - 0s - loss: 992934.4757 - mean_squared_error: 992934.4757 - val_loss: 733761.8970 - val_mean_squared_error: 733761.8970\n",
    ">     Epoch 121/2000\n",
    ">      - 0s - loss: 989464.3495 - mean_squared_error: 989464.3495 - val_loss: 728592.8324 - val_mean_squared_error: 728592.8324\n",
    ">     Epoch 122/2000\n",
    ">      - 0s - loss: 986923.2727 - mean_squared_error: 986923.2727 - val_loss: 731263.4120 - val_mean_squared_error: 731263.4120\n",
    ">     Epoch 123/2000\n",
    ">      - 0s - loss: 984284.1567 - mean_squared_error: 984284.1567 - val_loss: 734552.3397 - val_mean_squared_error: 734552.3397\n",
    ">     Epoch 124/2000\n",
    ">      - 0s - loss: 981964.4697 - mean_squared_error: 981964.4697 - val_loss: 721918.1604 - val_mean_squared_error: 721918.1604\n",
    ">     Epoch 125/2000\n",
    ">      - 0s - loss: 978881.0860 - mean_squared_error: 978881.0860 - val_loss: 718240.2593 - val_mean_squared_error: 718240.2593\n",
    ">     Epoch 126/2000\n",
    ">      - 0s - loss: 976116.4305 - mean_squared_error: 976116.4305 - val_loss: 716768.1848 - val_mean_squared_error: 716768.1848\n",
    ">     Epoch 127/2000\n",
    ">      - 0s - loss: 973458.0178 - mean_squared_error: 973458.0178 - val_loss: 715422.3851 - val_mean_squared_error: 715422.3851\n",
    ">     Epoch 128/2000\n",
    ">      - 0s - loss: 970484.2357 - mean_squared_error: 970484.2357 - val_loss: 709885.0327 - val_mean_squared_error: 709885.0327\n",
    ">     Epoch 129/2000\n",
    ">      - 0s - loss: 967710.4944 - mean_squared_error: 967710.4944 - val_loss: 706953.8875 - val_mean_squared_error: 706953.8875\n",
    ">     Epoch 130/2000\n",
    ">      - 0s - loss: 964926.7113 - mean_squared_error: 964926.7113 - val_loss: 704451.7275 - val_mean_squared_error: 704451.7275\n",
    ">     Epoch 131/2000\n",
    ">      - 0s - loss: 962205.7342 - mean_squared_error: 962205.7342 - val_loss: 703047.8709 - val_mean_squared_error: 703047.8709\n",
    ">     Epoch 132/2000\n",
    ">      - 0s - loss: 959855.5042 - mean_squared_error: 959855.5042 - val_loss: 699065.4193 - val_mean_squared_error: 699065.4193\n",
    ">     Epoch 133/2000\n",
    ">      - 0s - loss: 957806.8261 - mean_squared_error: 957806.8261 - val_loss: 709171.6166 - val_mean_squared_error: 709171.6166\n",
    ">     Epoch 134/2000\n",
    ">      - 0s - loss: 954564.9092 - mean_squared_error: 954564.9092 - val_loss: 696548.4695 - val_mean_squared_error: 696548.4695\n",
    ">     Epoch 135/2000\n",
    ">      - 0s - loss: 952602.2452 - mean_squared_error: 952602.2452 - val_loss: 697575.4079 - val_mean_squared_error: 697575.4079\n",
    ">     Epoch 136/2000\n",
    ">      - 0s - loss: 950352.9306 - mean_squared_error: 950352.9306 - val_loss: 689600.8201 - val_mean_squared_error: 689600.8201\n",
    ">     Epoch 137/2000\n",
    ">      - 0s - loss: 947697.6631 - mean_squared_error: 947697.6631 - val_loss: 689256.6092 - val_mean_squared_error: 689256.6092\n",
    ">     Epoch 138/2000\n",
    ">      - 0s - loss: 944707.3380 - mean_squared_error: 944707.3380 - val_loss: 697876.2123 - val_mean_squared_error: 697876.2123\n",
    ">     Epoch 139/2000\n",
    ">      - 0s - loss: 942931.2569 - mean_squared_error: 942931.2569 - val_loss: 688838.1186 - val_mean_squared_error: 688838.1186\n",
    ">     Epoch 140/2000\n",
    ">      - 0s - loss: 940727.0246 - mean_squared_error: 940727.0246 - val_loss: 683651.2944 - val_mean_squared_error: 683651.2944\n",
    ">     Epoch 141/2000\n",
    ">      - 0s - loss: 937611.2467 - mean_squared_error: 937611.2467 - val_loss: 679610.9674 - val_mean_squared_error: 679610.9674\n",
    ">     Epoch 142/2000\n",
    ">      - 0s - loss: 936145.2758 - mean_squared_error: 936145.2758 - val_loss: 679409.5040 - val_mean_squared_error: 679409.5040\n",
    ">     Epoch 143/2000\n",
    ">      - 0s - loss: 933791.8269 - mean_squared_error: 933791.8269 - val_loss: 674969.9004 - val_mean_squared_error: 674969.9004\n",
    ">     Epoch 144/2000\n",
    ">      - 0s - loss: 931934.5522 - mean_squared_error: 931934.5522 - val_loss: 673677.1087 - val_mean_squared_error: 673677.1087\n",
    ">     Epoch 145/2000\n",
    ">      - 0s - loss: 929419.1329 - mean_squared_error: 929419.1329 - val_loss: 672486.2596 - val_mean_squared_error: 672486.2596\n",
    ">     Epoch 146/2000\n",
    ">      - 0s - loss: 928259.8743 - mean_squared_error: 928259.8743 - val_loss: 679053.0771 - val_mean_squared_error: 679053.0771\n",
    ">     Epoch 147/2000\n",
    ">      - 0s - loss: 925840.3659 - mean_squared_error: 925840.3659 - val_loss: 670425.6315 - val_mean_squared_error: 670425.6315\n",
    ">     Epoch 148/2000\n",
    ">      - 0s - loss: 923053.3375 - mean_squared_error: 923053.3375 - val_loss: 666447.0378 - val_mean_squared_error: 666447.0378\n",
    ">     Epoch 149/2000\n",
    ">      - 0s - loss: 921570.8007 - mean_squared_error: 921570.8007 - val_loss: 671207.0466 - val_mean_squared_error: 671207.0466\n",
    ">     Epoch 150/2000\n",
    ">      - 0s - loss: 919778.3405 - mean_squared_error: 919778.3405 - val_loss: 663078.1736 - val_mean_squared_error: 663078.1736\n",
    ">     Epoch 151/2000\n",
    ">      - 0s - loss: 916954.4063 - mean_squared_error: 916954.4063 - val_loss: 661748.4101 - val_mean_squared_error: 661748.4101\n",
    ">     Epoch 152/2000\n",
    ">      - 0s - loss: 915275.2257 - mean_squared_error: 915275.2257 - val_loss: 660109.1333 - val_mean_squared_error: 660109.1333\n",
    ">     Epoch 153/2000\n",
    ">      - 0s - loss: 913935.4979 - mean_squared_error: 913935.4979 - val_loss: 660320.5198 - val_mean_squared_error: 660320.5198\n",
    ">     Epoch 154/2000\n",
    ">      - 0s - loss: 912403.0531 - mean_squared_error: 912403.0531 - val_loss: 658724.5849 - val_mean_squared_error: 658724.5849\n",
    ">     Epoch 155/2000\n",
    ">      - 0s - loss: 909915.3601 - mean_squared_error: 909915.3601 - val_loss: 655034.8768 - val_mean_squared_error: 655034.8768\n",
    ">     Epoch 156/2000\n",
    ">      - 0s - loss: 908297.1211 - mean_squared_error: 908297.1211 - val_loss: 654181.0143 - val_mean_squared_error: 654181.0143\n",
    ">     Epoch 157/2000\n",
    ">      - 0s - loss: 905624.7004 - mean_squared_error: 905624.7004 - val_loss: 656986.1485 - val_mean_squared_error: 656986.1485\n",
    ">     Epoch 158/2000\n",
    ">      - 0s - loss: 905111.6589 - mean_squared_error: 905111.6589 - val_loss: 655903.3115 - val_mean_squared_error: 655903.3115\n",
    ">     Epoch 159/2000\n",
    ">      - 0s - loss: 902944.8210 - mean_squared_error: 902944.8210 - val_loss: 654524.7966 - val_mean_squared_error: 654524.7966\n",
    ">     Epoch 160/2000\n",
    ">      - 0s - loss: 901144.1101 - mean_squared_error: 901144.1101 - val_loss: 651841.0205 - val_mean_squared_error: 651841.0205\n",
    ">     Epoch 161/2000\n",
    ">      - 0s - loss: 899314.5646 - mean_squared_error: 899314.5646 - val_loss: 646358.7536 - val_mean_squared_error: 646358.7536\n",
    ">     Epoch 162/2000\n",
    ">      - 0s - loss: 897382.2375 - mean_squared_error: 897382.2375 - val_loss: 646032.7151 - val_mean_squared_error: 646032.7151\n",
    ">     Epoch 163/2000\n",
    ">      - 0s - loss: 895482.0745 - mean_squared_error: 895482.0745 - val_loss: 647151.8484 - val_mean_squared_error: 647151.8484\n",
    ">     Epoch 164/2000\n",
    ">      - 0s - loss: 894577.8852 - mean_squared_error: 894577.8852 - val_loss: 643899.3744 - val_mean_squared_error: 643899.3744\n",
    ">     Epoch 165/2000\n",
    ">      - 0s - loss: 892187.8241 - mean_squared_error: 892187.8241 - val_loss: 641453.4793 - val_mean_squared_error: 641453.4793\n",
    ">     Epoch 166/2000\n",
    ">      - 0s - loss: 890396.7710 - mean_squared_error: 890396.7710 - val_loss: 639900.2524 - val_mean_squared_error: 639900.2524\n",
    ">     Epoch 167/2000\n",
    ">      - 0s - loss: 889097.9396 - mean_squared_error: 889097.9396 - val_loss: 640526.9566 - val_mean_squared_error: 640526.9566\n",
    ">     Epoch 168/2000\n",
    ">      - 0s - loss: 887303.5403 - mean_squared_error: 887303.5403 - val_loss: 637232.9263 - val_mean_squared_error: 637232.9263\n",
    ">     Epoch 169/2000\n",
    ">      - 0s - loss: 885714.2098 - mean_squared_error: 885714.2098 - val_loss: 638746.7358 - val_mean_squared_error: 638746.7358\n",
    ">     Epoch 170/2000\n",
    ">      - 0s - loss: 884497.2334 - mean_squared_error: 884497.2334 - val_loss: 634734.1812 - val_mean_squared_error: 634734.1812\n",
    ">     Epoch 171/2000\n",
    ">      - 0s - loss: 882821.3801 - mean_squared_error: 882821.3801 - val_loss: 634757.9038 - val_mean_squared_error: 634757.9038\n",
    ">     Epoch 172/2000\n",
    ">      - 0s - loss: 880668.8433 - mean_squared_error: 880668.8433 - val_loss: 631159.9473 - val_mean_squared_error: 631159.9473\n",
    ">     Epoch 173/2000\n",
    ">      - 0s - loss: 879786.0717 - mean_squared_error: 879786.0717 - val_loss: 632434.0280 - val_mean_squared_error: 632434.0280\n",
    ">     Epoch 174/2000\n",
    ">      - 0s - loss: 877921.8627 - mean_squared_error: 877921.8627 - val_loss: 629854.0848 - val_mean_squared_error: 629854.0848\n",
    ">     Epoch 175/2000\n",
    ">      - 0s - loss: 876936.1627 - mean_squared_error: 876936.1627 - val_loss: 639762.5728 - val_mean_squared_error: 639762.5728\n",
    ">     Epoch 176/2000\n",
    ">      - 0s - loss: 874990.3023 - mean_squared_error: 874990.3023 - val_loss: 627614.5739 - val_mean_squared_error: 627614.5739\n",
    ">     Epoch 177/2000\n",
    ">      - 0s - loss: 873407.4634 - mean_squared_error: 873407.4634 - val_loss: 625926.6324 - val_mean_squared_error: 625926.6324\n",
    ">     Epoch 178/2000\n",
    ">      - 0s - loss: 871298.3328 - mean_squared_error: 871298.3328 - val_loss: 654647.0849 - val_mean_squared_error: 654647.0849\n",
    ">     Epoch 179/2000\n",
    ">      - 0s - loss: 871422.8988 - mean_squared_error: 871422.8988 - val_loss: 624784.2562 - val_mean_squared_error: 624784.2562\n",
    ">     Epoch 180/2000\n",
    ">      - 0s - loss: 868830.5286 - mean_squared_error: 868830.5286 - val_loss: 623332.0319 - val_mean_squared_error: 623332.0319\n",
    ">\n",
    ">     *** WARNING: skipped 225655 bytes of output ***\n",
    ">\n",
    ">     Epoch 1823/2000\n",
    ">      - 0s - loss: 397390.2962 - mean_squared_error: 397390.2962 - val_loss: 353209.6269 - val_mean_squared_error: 353209.6269\n",
    ">     Epoch 1824/2000\n",
    ">      - 0s - loss: 397583.4764 - mean_squared_error: 397583.4764 - val_loss: 353313.4139 - val_mean_squared_error: 353313.4139\n",
    ">     Epoch 1825/2000\n",
    ">      - 0s - loss: 396889.4295 - mean_squared_error: 396889.4295 - val_loss: 353050.2891 - val_mean_squared_error: 353050.2891\n",
    ">     Epoch 1826/2000\n",
    ">      - 0s - loss: 396688.1741 - mean_squared_error: 396688.1741 - val_loss: 352968.9833 - val_mean_squared_error: 352968.9833\n",
    ">     Epoch 1827/2000\n",
    ">      - 0s - loss: 396879.9892 - mean_squared_error: 396879.9892 - val_loss: 353596.4543 - val_mean_squared_error: 353596.4543\n",
    ">     Epoch 1828/2000\n",
    ">      - 0s - loss: 397163.0979 - mean_squared_error: 397163.0979 - val_loss: 355002.6447 - val_mean_squared_error: 355002.6447\n",
    ">     Epoch 1829/2000\n",
    ">      - 0s - loss: 397371.9557 - mean_squared_error: 397371.9557 - val_loss: 360991.2768 - val_mean_squared_error: 360991.2768\n",
    ">     Epoch 1830/2000\n",
    ">      - 0s - loss: 397330.9307 - mean_squared_error: 397330.9307 - val_loss: 352858.5714 - val_mean_squared_error: 352858.5714\n",
    ">     Epoch 1831/2000\n",
    ">      - 0s - loss: 397069.0619 - mean_squared_error: 397069.0619 - val_loss: 356316.7071 - val_mean_squared_error: 356316.7071\n",
    ">     Epoch 1832/2000\n",
    ">      - 0s - loss: 397248.6073 - mean_squared_error: 397248.6073 - val_loss: 352828.9623 - val_mean_squared_error: 352828.9623\n",
    ">     Epoch 1833/2000\n",
    ">      - 0s - loss: 396082.9898 - mean_squared_error: 396082.9898 - val_loss: 352698.9154 - val_mean_squared_error: 352698.9154\n",
    ">     Epoch 1834/2000\n",
    ">      - 0s - loss: 397319.9301 - mean_squared_error: 397319.9301 - val_loss: 352547.3161 - val_mean_squared_error: 352547.3161\n",
    ">     Epoch 1835/2000\n",
    ">      - 0s - loss: 396171.3512 - mean_squared_error: 396171.3512 - val_loss: 352890.7365 - val_mean_squared_error: 352890.7365\n",
    ">     Epoch 1836/2000\n",
    ">      - 0s - loss: 397558.3849 - mean_squared_error: 397558.3849 - val_loss: 352797.1587 - val_mean_squared_error: 352797.1587\n",
    ">     Epoch 1837/2000\n",
    ">      - 0s - loss: 396650.8045 - mean_squared_error: 396650.8045 - val_loss: 352551.9260 - val_mean_squared_error: 352551.9260\n",
    ">     Epoch 1838/2000\n",
    ">      - 0s - loss: 396322.6335 - mean_squared_error: 396322.6335 - val_loss: 352522.2167 - val_mean_squared_error: 352522.2167\n",
    ">     Epoch 1839/2000\n",
    ">      - 0s - loss: 396564.7714 - mean_squared_error: 396564.7714 - val_loss: 356043.7555 - val_mean_squared_error: 356043.7555\n",
    ">     Epoch 1840/2000\n",
    ">      - 0s - loss: 396659.8503 - mean_squared_error: 396659.8503 - val_loss: 353330.3860 - val_mean_squared_error: 353330.3860\n",
    ">     Epoch 1841/2000\n",
    ">      - 0s - loss: 396947.4321 - mean_squared_error: 396947.4321 - val_loss: 352941.9958 - val_mean_squared_error: 352941.9958\n",
    ">     Epoch 1842/2000\n",
    ">      - 0s - loss: 396686.2913 - mean_squared_error: 396686.2913 - val_loss: 353136.8284 - val_mean_squared_error: 353136.8284\n",
    ">     Epoch 1843/2000\n",
    ">      - 0s - loss: 397013.0222 - mean_squared_error: 397013.0222 - val_loss: 357842.3895 - val_mean_squared_error: 357842.3895\n",
    ">     Epoch 1844/2000\n",
    ">      - 0s - loss: 396760.1421 - mean_squared_error: 396760.1421 - val_loss: 353699.3172 - val_mean_squared_error: 353699.3172\n",
    ">     Epoch 1845/2000\n",
    ">      - 0s - loss: 396413.3329 - mean_squared_error: 396413.3329 - val_loss: 353588.4062 - val_mean_squared_error: 353588.4062\n",
    ">     Epoch 1846/2000\n",
    ">      - 0s - loss: 396626.7565 - mean_squared_error: 396626.7565 - val_loss: 352466.3480 - val_mean_squared_error: 352466.3480\n",
    ">     Epoch 1847/2000\n",
    ">      - 0s - loss: 396830.8680 - mean_squared_error: 396830.8680 - val_loss: 353063.0210 - val_mean_squared_error: 353063.0210\n",
    ">     Epoch 1848/2000\n",
    ">      - 0s - loss: 396382.9396 - mean_squared_error: 396382.9396 - val_loss: 352376.7488 - val_mean_squared_error: 352376.7488\n",
    ">     Epoch 1849/2000\n",
    ">      - 0s - loss: 396371.6686 - mean_squared_error: 396371.6686 - val_loss: 353424.5889 - val_mean_squared_error: 353424.5889\n",
    ">     Epoch 1850/2000\n",
    ">      - 0s - loss: 395971.4266 - mean_squared_error: 395971.4266 - val_loss: 353351.6123 - val_mean_squared_error: 353351.6123\n",
    ">     Epoch 1851/2000\n",
    ">      - 0s - loss: 396122.5219 - mean_squared_error: 396122.5219 - val_loss: 352835.1150 - val_mean_squared_error: 352835.1150\n",
    ">     Epoch 1852/2000\n",
    ">      - 0s - loss: 395927.3674 - mean_squared_error: 395927.3674 - val_loss: 358838.3104 - val_mean_squared_error: 358838.3104\n",
    ">     Epoch 1853/2000\n",
    ">      - 0s - loss: 396239.7226 - mean_squared_error: 396239.7226 - val_loss: 353089.9473 - val_mean_squared_error: 353089.9473\n",
    ">     Epoch 1854/2000\n",
    ">      - 0s - loss: 396131.3741 - mean_squared_error: 396131.3741 - val_loss: 352670.2379 - val_mean_squared_error: 352670.2379\n",
    ">     Epoch 1855/2000\n",
    ">      - 0s - loss: 396213.9961 - mean_squared_error: 396213.9961 - val_loss: 355889.2487 - val_mean_squared_error: 355889.2487\n",
    ">     Epoch 1856/2000\n",
    ">      - 0s - loss: 395907.4523 - mean_squared_error: 395907.4523 - val_loss: 353334.4587 - val_mean_squared_error: 353334.4587\n",
    ">     Epoch 1857/2000\n",
    ">      - 0s - loss: 396093.9572 - mean_squared_error: 396093.9572 - val_loss: 355890.8466 - val_mean_squared_error: 355890.8466\n",
    ">     Epoch 1858/2000\n",
    ">      - 0s - loss: 396148.8290 - mean_squared_error: 396148.8290 - val_loss: 352772.3574 - val_mean_squared_error: 352772.3574\n",
    ">     Epoch 1859/2000\n",
    ">      - 0s - loss: 396374.1615 - mean_squared_error: 396374.1615 - val_loss: 352408.7239 - val_mean_squared_error: 352408.7239\n",
    ">     Epoch 1860/2000\n",
    ">      - 0s - loss: 396224.3341 - mean_squared_error: 396224.3341 - val_loss: 352462.1340 - val_mean_squared_error: 352462.1340\n",
    ">     Epoch 1861/2000\n",
    ">      - 0s - loss: 395775.2315 - mean_squared_error: 395775.2315 - val_loss: 354116.2042 - val_mean_squared_error: 354116.2042\n",
    ">     Epoch 1862/2000\n",
    ">      - 0s - loss: 396024.3581 - mean_squared_error: 396024.3581 - val_loss: 352205.7111 - val_mean_squared_error: 352205.7111\n",
    ">     Epoch 1863/2000\n",
    ">      - 0s - loss: 395613.7328 - mean_squared_error: 395613.7328 - val_loss: 352385.5322 - val_mean_squared_error: 352385.5322\n",
    ">     Epoch 1864/2000\n",
    ">      - 0s - loss: 396227.5467 - mean_squared_error: 396227.5467 - val_loss: 352645.4930 - val_mean_squared_error: 352645.4930\n",
    ">     Epoch 1865/2000\n",
    ">      - 0s - loss: 396004.5046 - mean_squared_error: 396004.5046 - val_loss: 353968.2366 - val_mean_squared_error: 353968.2366\n",
    ">     Epoch 1866/2000\n",
    ">      - 0s - loss: 396020.5718 - mean_squared_error: 396020.5718 - val_loss: 354020.5083 - val_mean_squared_error: 354020.5083\n",
    ">     Epoch 1867/2000\n",
    ">      - 0s - loss: 395672.2817 - mean_squared_error: 395672.2817 - val_loss: 353314.8828 - val_mean_squared_error: 353314.8828\n",
    ">     Epoch 1868/2000\n",
    ">      - 0s - loss: 395795.3118 - mean_squared_error: 395795.3118 - val_loss: 352232.4819 - val_mean_squared_error: 352232.4819\n",
    ">     Epoch 1869/2000\n",
    ">      - 0s - loss: 395839.6442 - mean_squared_error: 395839.6442 - val_loss: 352895.0211 - val_mean_squared_error: 352895.0211\n",
    ">     Epoch 1870/2000\n",
    ">      - 0s - loss: 396112.3882 - mean_squared_error: 396112.3882 - val_loss: 353443.6297 - val_mean_squared_error: 353443.6297\n",
    ">     Epoch 1871/2000\n",
    ">      - 0s - loss: 395928.2209 - mean_squared_error: 395928.2209 - val_loss: 352084.1819 - val_mean_squared_error: 352084.1819\n",
    ">     Epoch 1872/2000\n",
    ">      - 0s - loss: 395532.7622 - mean_squared_error: 395532.7622 - val_loss: 351922.8059 - val_mean_squared_error: 351922.8059\n",
    ">     Epoch 1873/2000\n",
    ">      - 0s - loss: 394991.3888 - mean_squared_error: 394991.3888 - val_loss: 352354.4650 - val_mean_squared_error: 352354.4650\n",
    ">     Epoch 1874/2000\n",
    ">      - 0s - loss: 395868.8979 - mean_squared_error: 395868.8979 - val_loss: 353307.8581 - val_mean_squared_error: 353307.8581\n",
    ">     Epoch 1875/2000\n",
    ">      - 0s - loss: 395731.7098 - mean_squared_error: 395731.7098 - val_loss: 351873.0492 - val_mean_squared_error: 351873.0492\n",
    ">     Epoch 1876/2000\n",
    ">      - 0s - loss: 395132.8965 - mean_squared_error: 395132.8965 - val_loss: 351852.0628 - val_mean_squared_error: 351852.0628\n",
    ">     Epoch 1877/2000\n",
    ">      - 0s - loss: 395518.6568 - mean_squared_error: 395518.6568 - val_loss: 351841.3919 - val_mean_squared_error: 351841.3919\n",
    ">     Epoch 1878/2000\n",
    ">      - 0s - loss: 395575.1640 - mean_squared_error: 395575.1640 - val_loss: 351899.8707 - val_mean_squared_error: 351899.8707\n",
    ">     Epoch 1879/2000\n",
    ">      - 0s - loss: 395166.7193 - mean_squared_error: 395166.7193 - val_loss: 352113.9540 - val_mean_squared_error: 352113.9540\n",
    ">     Epoch 1880/2000\n",
    ">      - 0s - loss: 394905.8654 - mean_squared_error: 394905.8654 - val_loss: 354147.1539 - val_mean_squared_error: 354147.1539\n",
    ">     Epoch 1881/2000\n",
    ">      - 0s - loss: 395029.2658 - mean_squared_error: 395029.2658 - val_loss: 354190.9711 - val_mean_squared_error: 354190.9711\n",
    ">     Epoch 1882/2000\n",
    ">      - 0s - loss: 395449.4634 - mean_squared_error: 395449.4634 - val_loss: 352587.5978 - val_mean_squared_error: 352587.5978\n",
    ">     Epoch 1883/2000\n",
    ">      - 0s - loss: 395276.2936 - mean_squared_error: 395276.2936 - val_loss: 352548.3871 - val_mean_squared_error: 352548.3871\n",
    ">     Epoch 1884/2000\n",
    ">      - 0s - loss: 394877.4458 - mean_squared_error: 394877.4458 - val_loss: 352523.8919 - val_mean_squared_error: 352523.8919\n",
    ">     Epoch 1885/2000\n",
    ">      - 0s - loss: 395268.0316 - mean_squared_error: 395268.0316 - val_loss: 353429.1411 - val_mean_squared_error: 353429.1411\n",
    ">     Epoch 1886/2000\n",
    ">      - 0s - loss: 395224.4547 - mean_squared_error: 395224.4547 - val_loss: 351589.5054 - val_mean_squared_error: 351589.5054\n",
    ">     Epoch 1887/2000\n",
    ">      - 0s - loss: 394930.0051 - mean_squared_error: 394930.0051 - val_loss: 351562.0556 - val_mean_squared_error: 351562.0556\n",
    ">     Epoch 1888/2000\n",
    ">      - 0s - loss: 395516.0361 - mean_squared_error: 395516.0361 - val_loss: 355380.2538 - val_mean_squared_error: 355380.2538\n",
    ">     Epoch 1889/2000\n",
    ">      - 0s - loss: 395076.7389 - mean_squared_error: 395076.7389 - val_loss: 351812.1470 - val_mean_squared_error: 351812.1470\n",
    ">     Epoch 1890/2000\n",
    ">      - 0s - loss: 394752.7494 - mean_squared_error: 394752.7494 - val_loss: 352221.3691 - val_mean_squared_error: 352221.3691\n",
    ">     Epoch 1891/2000\n",
    ">      - 0s - loss: 395093.2258 - mean_squared_error: 395093.2258 - val_loss: 351600.1716 - val_mean_squared_error: 351600.1716\n",
    ">     Epoch 1892/2000\n",
    ">      - 0s - loss: 395046.2364 - mean_squared_error: 395046.2364 - val_loss: 352985.1909 - val_mean_squared_error: 352985.1909\n",
    ">     Epoch 1893/2000\n",
    ">      - 0s - loss: 394981.4052 - mean_squared_error: 394981.4052 - val_loss: 355643.5620 - val_mean_squared_error: 355643.5620\n",
    ">     Epoch 1894/2000\n",
    ">      - 0s - loss: 394896.0592 - mean_squared_error: 394896.0592 - val_loss: 352217.3145 - val_mean_squared_error: 352217.3145\n",
    ">     Epoch 1895/2000\n",
    ">      - 0s - loss: 394552.8456 - mean_squared_error: 394552.8456 - val_loss: 354756.0459 - val_mean_squared_error: 354756.0459\n",
    ">     Epoch 1896/2000\n",
    ">      - 0s - loss: 395051.9754 - mean_squared_error: 395051.9754 - val_loss: 353615.7922 - val_mean_squared_error: 353615.7922\n",
    ">     Epoch 1897/2000\n",
    ">      - 0s - loss: 394813.7603 - mean_squared_error: 394813.7603 - val_loss: 351627.1015 - val_mean_squared_error: 351627.1015\n",
    ">     Epoch 1898/2000\n",
    ">      - 0s - loss: 394762.4370 - mean_squared_error: 394762.4370 - val_loss: 351538.6374 - val_mean_squared_error: 351538.6374\n",
    ">     Epoch 1899/2000\n",
    ">      - 0s - loss: 394675.4384 - mean_squared_error: 394675.4384 - val_loss: 354470.4357 - val_mean_squared_error: 354470.4357\n",
    ">     Epoch 1900/2000\n",
    ">      - 0s - loss: 395040.6355 - mean_squared_error: 395040.6355 - val_loss: 351296.9343 - val_mean_squared_error: 351296.9343\n",
    ">     Epoch 1901/2000\n",
    ">      - 0s - loss: 394279.4941 - mean_squared_error: 394279.4941 - val_loss: 351612.3258 - val_mean_squared_error: 351612.3258\n",
    ">     Epoch 1902/2000\n",
    ">      - 0s - loss: 394407.4887 - mean_squared_error: 394407.4887 - val_loss: 353701.2704 - val_mean_squared_error: 353701.2704\n",
    ">     Epoch 1903/2000\n",
    ">      - 0s - loss: 394554.4996 - mean_squared_error: 394554.4996 - val_loss: 357333.1414 - val_mean_squared_error: 357333.1414\n",
    ">     Epoch 1904/2000\n",
    ">      - 0s - loss: 394564.6796 - mean_squared_error: 394564.6796 - val_loss: 352524.9584 - val_mean_squared_error: 352524.9584\n",
    ">     Epoch 1905/2000\n",
    ">      - 0s - loss: 394802.8757 - mean_squared_error: 394802.8757 - val_loss: 351848.0285 - val_mean_squared_error: 351848.0285\n",
    ">     Epoch 1906/2000\n",
    ">      - 0s - loss: 394504.0665 - mean_squared_error: 394504.0665 - val_loss: 352623.8804 - val_mean_squared_error: 352623.8804\n",
    ">     Epoch 1907/2000\n",
    ">      - 0s - loss: 394084.1085 - mean_squared_error: 394084.1085 - val_loss: 352172.3271 - val_mean_squared_error: 352172.3271\n",
    ">     Epoch 1908/2000\n",
    ">      - 0s - loss: 394395.0822 - mean_squared_error: 394395.0822 - val_loss: 353779.5877 - val_mean_squared_error: 353779.5877\n",
    ">     Epoch 1909/2000\n",
    ">      - 0s - loss: 394750.7526 - mean_squared_error: 394750.7526 - val_loss: 351798.7503 - val_mean_squared_error: 351798.7503\n",
    ">     Epoch 1910/2000\n",
    ">      - 0s - loss: 394402.4697 - mean_squared_error: 394402.4697 - val_loss: 353139.7217 - val_mean_squared_error: 353139.7217\n",
    ">     Epoch 1911/2000\n",
    ">      - 0s - loss: 395000.8836 - mean_squared_error: 395000.8836 - val_loss: 351335.7235 - val_mean_squared_error: 351335.7235\n",
    ">     Epoch 1912/2000\n",
    ">      - 0s - loss: 394422.7909 - mean_squared_error: 394422.7909 - val_loss: 351607.3362 - val_mean_squared_error: 351607.3362\n",
    ">     Epoch 1913/2000\n",
    ">      - 0s - loss: 394186.5859 - mean_squared_error: 394186.5859 - val_loss: 351026.2007 - val_mean_squared_error: 351026.2007\n",
    ">     Epoch 1914/2000\n",
    ">      - 0s - loss: 394105.2153 - mean_squared_error: 394105.2153 - val_loss: 351883.1954 - val_mean_squared_error: 351883.1954\n",
    ">     Epoch 1915/2000\n",
    ">      - 0s - loss: 394142.5565 - mean_squared_error: 394142.5565 - val_loss: 351992.0573 - val_mean_squared_error: 351992.0573\n",
    ">     Epoch 1916/2000\n",
    ">      - 0s - loss: 394209.5957 - mean_squared_error: 394209.5957 - val_loss: 352018.4708 - val_mean_squared_error: 352018.4708\n",
    ">     Epoch 1917/2000\n",
    ">      - 0s - loss: 393973.0199 - mean_squared_error: 393973.0199 - val_loss: 352887.3921 - val_mean_squared_error: 352887.3921\n",
    ">     Epoch 1918/2000\n",
    ">      - 0s - loss: 394573.7631 - mean_squared_error: 394573.7631 - val_loss: 351478.7113 - val_mean_squared_error: 351478.7113\n",
    ">     Epoch 1919/2000\n",
    ">      - 0s - loss: 394233.9014 - mean_squared_error: 394233.9014 - val_loss: 351782.2848 - val_mean_squared_error: 351782.2848\n",
    ">     Epoch 1920/2000\n",
    ">      - 0s - loss: 393708.4802 - mean_squared_error: 393708.4802 - val_loss: 350981.9167 - val_mean_squared_error: 350981.9167\n",
    ">     Epoch 1921/2000\n",
    ">      - 0s - loss: 394551.6854 - mean_squared_error: 394551.6854 - val_loss: 351259.1100 - val_mean_squared_error: 351259.1100\n",
    ">     Epoch 1922/2000\n",
    ">      - 0s - loss: 394300.5748 - mean_squared_error: 394300.5748 - val_loss: 352688.1585 - val_mean_squared_error: 352688.1585\n",
    ">     Epoch 1923/2000\n",
    ">      - 0s - loss: 394623.9563 - mean_squared_error: 394623.9563 - val_loss: 354501.3561 - val_mean_squared_error: 354501.3561\n",
    ">     Epoch 1924/2000\n",
    ">      - 0s - loss: 394134.7472 - mean_squared_error: 394134.7472 - val_loss: 351192.2701 - val_mean_squared_error: 351192.2701\n",
    ">     Epoch 1925/2000\n",
    ">      - 0s - loss: 394189.8784 - mean_squared_error: 394189.8784 - val_loss: 351355.5768 - val_mean_squared_error: 351355.5768\n",
    ">     Epoch 1926/2000\n",
    ">      - 0s - loss: 393910.6115 - mean_squared_error: 393910.6115 - val_loss: 350905.4286 - val_mean_squared_error: 350905.4286\n",
    ">     Epoch 1927/2000\n",
    ">      - 0s - loss: 394152.6381 - mean_squared_error: 394152.6381 - val_loss: 351523.0727 - val_mean_squared_error: 351523.0727\n",
    ">     Epoch 1928/2000\n",
    ">      - 0s - loss: 394294.2439 - mean_squared_error: 394294.2439 - val_loss: 355596.1872 - val_mean_squared_error: 355596.1872\n",
    ">     Epoch 1929/2000\n",
    ">      - 0s - loss: 394115.4967 - mean_squared_error: 394115.4967 - val_loss: 355812.5067 - val_mean_squared_error: 355812.5067\n",
    ">     Epoch 1930/2000\n",
    ">      - 0s - loss: 393797.4838 - mean_squared_error: 393797.4838 - val_loss: 351328.4472 - val_mean_squared_error: 351328.4472\n",
    ">     Epoch 1931/2000\n",
    ">      - 0s - loss: 394132.0738 - mean_squared_error: 394132.0738 - val_loss: 351409.6900 - val_mean_squared_error: 351409.6900\n",
    ">     Epoch 1932/2000\n",
    ">      - 0s - loss: 393748.0739 - mean_squared_error: 393748.0739 - val_loss: 351585.4802 - val_mean_squared_error: 351585.4802\n",
    ">     Epoch 1933/2000\n",
    ">      - 0s - loss: 393903.1115 - mean_squared_error: 393903.1115 - val_loss: 353536.1962 - val_mean_squared_error: 353536.1962\n",
    ">     Epoch 1934/2000\n",
    ">      - 0s - loss: 393744.9373 - mean_squared_error: 393744.9373 - val_loss: 351350.9460 - val_mean_squared_error: 351350.9460\n",
    ">     Epoch 1935/2000\n",
    ">      - 0s - loss: 393876.7852 - mean_squared_error: 393876.7852 - val_loss: 351608.3271 - val_mean_squared_error: 351608.3271\n",
    ">     Epoch 1936/2000\n",
    ">      - 0s - loss: 394187.8004 - mean_squared_error: 394187.8004 - val_loss: 351772.0344 - val_mean_squared_error: 351772.0344\n",
    ">     Epoch 1937/2000\n",
    ">      - 0s - loss: 393540.9881 - mean_squared_error: 393540.9881 - val_loss: 353472.4013 - val_mean_squared_error: 353472.4013\n",
    ">     Epoch 1938/2000\n",
    ">      - 0s - loss: 393782.9584 - mean_squared_error: 393782.9584 - val_loss: 355712.6185 - val_mean_squared_error: 355712.6185\n",
    ">     Epoch 1939/2000\n",
    ">      - 0s - loss: 393449.4543 - mean_squared_error: 393449.4543 - val_loss: 352500.5792 - val_mean_squared_error: 352500.5792\n",
    ">     Epoch 1940/2000\n",
    ">      - 0s - loss: 393696.1645 - mean_squared_error: 393696.1645 - val_loss: 350963.5166 - val_mean_squared_error: 350963.5166\n",
    ">     Epoch 1941/2000\n",
    ">      - 0s - loss: 393587.2240 - mean_squared_error: 393587.2240 - val_loss: 351407.7796 - val_mean_squared_error: 351407.7796\n",
    ">     Epoch 1942/2000\n",
    ">      - 0s - loss: 394481.9304 - mean_squared_error: 394481.9304 - val_loss: 351683.8144 - val_mean_squared_error: 351683.8144\n",
    ">     Epoch 1943/2000\n",
    ">      - 0s - loss: 393125.3010 - mean_squared_error: 393125.3010 - val_loss: 351086.1665 - val_mean_squared_error: 351086.1665\n",
    ">     Epoch 1944/2000\n",
    ">      - 0s - loss: 393339.4577 - mean_squared_error: 393339.4577 - val_loss: 351448.7661 - val_mean_squared_error: 351448.7661\n",
    ">     Epoch 1945/2000\n",
    ">      - 0s - loss: 393831.5202 - mean_squared_error: 393831.5202 - val_loss: 350619.9688 - val_mean_squared_error: 350619.9688\n",
    ">     Epoch 1946/2000\n",
    ">      - 0s - loss: 393500.0341 - mean_squared_error: 393500.0341 - val_loss: 350897.6872 - val_mean_squared_error: 350897.6872\n",
    ">     Epoch 1947/2000\n",
    ">      - 0s - loss: 393520.8761 - mean_squared_error: 393520.8761 - val_loss: 350720.5475 - val_mean_squared_error: 350720.5475\n",
    ">     Epoch 1948/2000\n",
    ">      - 0s - loss: 393573.5040 - mean_squared_error: 393573.5040 - val_loss: 350986.0485 - val_mean_squared_error: 350986.0485\n",
    ">     Epoch 1949/2000\n",
    ">      - 0s - loss: 393692.4018 - mean_squared_error: 393692.4018 - val_loss: 351082.4832 - val_mean_squared_error: 351082.4832\n",
    ">     Epoch 1950/2000\n",
    ">      - 0s - loss: 393331.1218 - mean_squared_error: 393331.1218 - val_loss: 351067.0518 - val_mean_squared_error: 351067.0518\n",
    ">     Epoch 1951/2000\n",
    ">      - 0s - loss: 394190.3323 - mean_squared_error: 394190.3323 - val_loss: 351539.0065 - val_mean_squared_error: 351539.0065\n",
    ">     Epoch 1952/2000\n",
    ">      - 0s - loss: 393509.6535 - mean_squared_error: 393509.6535 - val_loss: 350568.9273 - val_mean_squared_error: 350568.9273\n",
    ">     Epoch 1953/2000\n",
    ">      - 0s - loss: 393606.4257 - mean_squared_error: 393606.4257 - val_loss: 350894.8561 - val_mean_squared_error: 350894.8561\n",
    ">     Epoch 1954/2000\n",
    ">      - 0s - loss: 392888.9072 - mean_squared_error: 392888.9072 - val_loss: 354381.4223 - val_mean_squared_error: 354381.4223\n",
    ">     Epoch 1955/2000\n",
    ">      - 0s - loss: 393230.7114 - mean_squared_error: 393230.7114 - val_loss: 351739.0441 - val_mean_squared_error: 351739.0441\n",
    ">     Epoch 1956/2000\n",
    ">      - 0s - loss: 393856.4126 - mean_squared_error: 393856.4126 - val_loss: 350882.9445 - val_mean_squared_error: 350882.9445\n",
    ">     Epoch 1957/2000\n",
    ">      - 0s - loss: 393106.8604 - mean_squared_error: 393106.8604 - val_loss: 350710.3429 - val_mean_squared_error: 350710.3429\n",
    ">     Epoch 1958/2000\n",
    ">      - 0s - loss: 393181.0624 - mean_squared_error: 393181.0624 - val_loss: 352552.5458 - val_mean_squared_error: 352552.5458\n",
    ">     Epoch 1959/2000\n",
    ">      - 0s - loss: 393266.2823 - mean_squared_error: 393266.2823 - val_loss: 351487.5482 - val_mean_squared_error: 351487.5482\n",
    ">     Epoch 1960/2000\n",
    ">      - 0s - loss: 393295.5574 - mean_squared_error: 393295.5574 - val_loss: 350987.4455 - val_mean_squared_error: 350987.4455\n",
    ">     Epoch 1961/2000\n",
    ">      - 0s - loss: 393692.0431 - mean_squared_error: 393692.0431 - val_loss: 350439.3957 - val_mean_squared_error: 350439.3957\n",
    ">     Epoch 1962/2000\n",
    ">      - 0s - loss: 393166.8479 - mean_squared_error: 393166.8479 - val_loss: 350663.6741 - val_mean_squared_error: 350663.6741\n",
    ">     Epoch 1963/2000\n",
    ">      - 0s - loss: 393099.1775 - mean_squared_error: 393099.1775 - val_loss: 350883.2008 - val_mean_squared_error: 350883.2008\n",
    ">     Epoch 1964/2000\n",
    ">      - 0s - loss: 393037.7546 - mean_squared_error: 393037.7546 - val_loss: 353033.8329 - val_mean_squared_error: 353033.8329\n",
    ">     Epoch 1965/2000\n",
    ">      - 0s - loss: 393300.0515 - mean_squared_error: 393300.0515 - val_loss: 350703.8575 - val_mean_squared_error: 350703.8575\n",
    ">     Epoch 1966/2000\n",
    ">      - 0s - loss: 392879.9891 - mean_squared_error: 392879.9891 - val_loss: 350489.7307 - val_mean_squared_error: 350489.7307\n",
    ">     Epoch 1967/2000\n",
    ">      - 0s - loss: 393060.9444 - mean_squared_error: 393060.9444 - val_loss: 350217.0181 - val_mean_squared_error: 350217.0181\n",
    ">     Epoch 1968/2000\n",
    ">      - 0s - loss: 392965.4809 - mean_squared_error: 392965.4809 - val_loss: 351338.5765 - val_mean_squared_error: 351338.5765\n",
    ">     Epoch 1969/2000\n",
    ">      - 0s - loss: 393158.6968 - mean_squared_error: 393158.6968 - val_loss: 350496.7281 - val_mean_squared_error: 350496.7281\n",
    ">     Epoch 1970/2000\n",
    ">      - 0s - loss: 393017.0785 - mean_squared_error: 393017.0785 - val_loss: 351737.2013 - val_mean_squared_error: 351737.2013\n",
    ">     Epoch 1971/2000\n",
    ">      - 0s - loss: 392864.5516 - mean_squared_error: 392864.5516 - val_loss: 351419.1868 - val_mean_squared_error: 351419.1868\n",
    ">     Epoch 1972/2000\n",
    ">      - 0s - loss: 392802.1848 - mean_squared_error: 392802.1848 - val_loss: 350596.9279 - val_mean_squared_error: 350596.9279\n",
    ">     Epoch 1973/2000\n",
    ">      - 0s - loss: 393229.4714 - mean_squared_error: 393229.4714 - val_loss: 352161.0665 - val_mean_squared_error: 352161.0665\n",
    ">     Epoch 1974/2000\n",
    ">      - 0s - loss: 393905.6125 - mean_squared_error: 393905.6125 - val_loss: 351650.0413 - val_mean_squared_error: 351650.0413\n",
    ">     Epoch 1975/2000\n",
    ">      - 0s - loss: 393232.5136 - mean_squared_error: 393232.5136 - val_loss: 350512.8810 - val_mean_squared_error: 350512.8810\n",
    ">     Epoch 1976/2000\n",
    ">      - 0s - loss: 392996.3353 - mean_squared_error: 392996.3353 - val_loss: 350257.6995 - val_mean_squared_error: 350257.6995\n",
    ">     Epoch 1977/2000\n",
    ">      - 0s - loss: 392572.7001 - mean_squared_error: 392572.7001 - val_loss: 351083.8819 - val_mean_squared_error: 351083.8819\n",
    ">     Epoch 1978/2000\n",
    ">      - 0s - loss: 393093.1763 - mean_squared_error: 393093.1763 - val_loss: 350253.7080 - val_mean_squared_error: 350253.7080\n",
    ">     Epoch 1979/2000\n",
    ">      - 0s - loss: 392868.2256 - mean_squared_error: 392868.2256 - val_loss: 353122.8400 - val_mean_squared_error: 353122.8400\n",
    ">     Epoch 1980/2000\n",
    ">      - 0s - loss: 392905.3495 - mean_squared_error: 392905.3495 - val_loss: 350134.9386 - val_mean_squared_error: 350134.9386\n",
    ">     Epoch 1981/2000\n",
    ">      - 0s - loss: 392815.0487 - mean_squared_error: 392815.0487 - val_loss: 350536.9625 - val_mean_squared_error: 350536.9625\n",
    ">     Epoch 1982/2000\n",
    ">      - 0s - loss: 392264.1909 - mean_squared_error: 392264.1909 - val_loss: 351010.5383 - val_mean_squared_error: 351010.5383\n",
    ">     Epoch 1983/2000\n",
    ">      - 0s - loss: 392464.8971 - mean_squared_error: 392464.8971 - val_loss: 352511.0336 - val_mean_squared_error: 352511.0336\n",
    ">     Epoch 1984/2000\n",
    ">      - 0s - loss: 392728.0114 - mean_squared_error: 392728.0114 - val_loss: 350361.5722 - val_mean_squared_error: 350361.5722\n",
    ">     Epoch 1985/2000\n",
    ">      - 0s - loss: 392867.1127 - mean_squared_error: 392867.1127 - val_loss: 350956.7653 - val_mean_squared_error: 350956.7653\n",
    ">     Epoch 1986/2000\n",
    ">      - 0s - loss: 393184.4572 - mean_squared_error: 393184.4572 - val_loss: 350048.0675 - val_mean_squared_error: 350048.0675\n",
    ">     Epoch 1987/2000\n",
    ">      - 0s - loss: 392299.4790 - mean_squared_error: 392299.4790 - val_loss: 350371.2309 - val_mean_squared_error: 350371.2309\n",
    ">     Epoch 1988/2000\n",
    ">      - 0s - loss: 392543.2559 - mean_squared_error: 392543.2559 - val_loss: 349955.0816 - val_mean_squared_error: 349955.0816\n",
    ">     Epoch 1989/2000\n",
    ">      - 0s - loss: 392207.9921 - mean_squared_error: 392207.9921 - val_loss: 350564.9174 - val_mean_squared_error: 350564.9174\n",
    ">     Epoch 1990/2000\n",
    ">      - 0s - loss: 392585.1471 - mean_squared_error: 392585.1471 - val_loss: 351939.7183 - val_mean_squared_error: 351939.7183\n",
    ">     Epoch 1991/2000\n",
    ">      - 0s - loss: 392636.6855 - mean_squared_error: 392636.6855 - val_loss: 350742.1343 - val_mean_squared_error: 350742.1343\n",
    ">     Epoch 1992/2000\n",
    ">      - 0s - loss: 392685.5213 - mean_squared_error: 392685.5213 - val_loss: 350790.6480 - val_mean_squared_error: 350790.6480\n",
    ">     Epoch 1993/2000\n",
    ">      - 0s - loss: 392365.0846 - mean_squared_error: 392365.0846 - val_loss: 350923.8519 - val_mean_squared_error: 350923.8519\n",
    ">     Epoch 1994/2000\n",
    ">      - 0s - loss: 392777.8883 - mean_squared_error: 392777.8883 - val_loss: 352346.1241 - val_mean_squared_error: 352346.1241\n",
    ">     Epoch 1995/2000\n",
    ">      - 0s - loss: 392558.7267 - mean_squared_error: 392558.7267 - val_loss: 350589.9146 - val_mean_squared_error: 350589.9146\n",
    ">     Epoch 1996/2000\n",
    ">      - 0s - loss: 392280.0797 - mean_squared_error: 392280.0797 - val_loss: 350947.3711 - val_mean_squared_error: 350947.3711\n",
    ">     Epoch 1997/2000\n",
    ">      - 0s - loss: 392389.0843 - mean_squared_error: 392389.0843 - val_loss: 351116.2743 - val_mean_squared_error: 351116.2743\n",
    ">     Epoch 1998/2000\n",
    ">      - 0s - loss: 392736.0409 - mean_squared_error: 392736.0409 - val_loss: 350774.5216 - val_mean_squared_error: 350774.5216\n",
    ">     Epoch 1999/2000\n",
    ">      - 0s - loss: 392251.0854 - mean_squared_error: 392251.0854 - val_loss: 349920.3269 - val_mean_squared_error: 349920.3269\n",
    ">     Epoch 2000/2000\n",
    ">      - 0s - loss: 392027.3671 - mean_squared_error: 392027.3671 - val_loss: 349922.0056 - val_mean_squared_error: 349922.0056\n",
    ">\n",
    ">        32/13485 [..............................] - ETA: 0s\n",
    ">      4544/13485 [=========>....................] - ETA: 0s\n",
    ">      8928/13485 [==================>...........] - ETA: 0s\n",
    ">     13472/13485 [============================>.] - ETA: 0s\n",
    ">     13485/13485 [==============================] - 0s 11us/step\n",
    ">\n",
    ">     root mean_squared_error: 619.680206\n",
    "\n",
    "  \n",
    "\n",
    "Would you look at that?!\n",
    "\n",
    "-   We break $1000 RMSE around epoch 112\n",
    "-   $900 around epoch 220\n",
    "-   $800 around epoch 450\n",
    "-   By around epoch 2000, my RMSE is &lt; $600\n",
    "\n",
    "...\n",
    "\n",
    "**Same theory; different activation function. Huge difference**\n",
    "\n",
    "Multilayer Networks\n",
    "===================\n",
    "\n",
    "If a single-layer perceptron network learns the importance of different\n",
    "combinations of features in the data...\n",
    "\n",
    "What would another network learn if it had a second (hidden) layer of\n",
    "neurons?\n",
    "\n",
    "It depends on how we train the network. We'll talk in the next section\n",
    "about how this training works, but the general idea is that we still\n",
    "work backward from the error gradient.\n",
    "\n",
    "That is, the last layer learns from error in the output; the\n",
    "second-to-last layer learns from error transmitted through that last\n",
    "layer, etc. It's a touch hand-wavy for now, but we'll make it more\n",
    "concrete later.\n",
    "\n",
    "Given this approach, we can say that:\n",
    "\n",
    "1.  The second (hidden) layer is learning features composed of\n",
    "    activations in the first (hidden) layer\n",
    "2.  The first (hidden) layer is learning feature weights that enable the\n",
    "    second layer to perform best\n",
    "    -   Why? Earlier, the first hidden layer just learned feature\n",
    "        weights because that's how it was judged\n",
    "    -   Now, the first hidden layer is judged on the error in the second\n",
    "        layer, so it learns to contribute to that second layer\n",
    "3.  The second layer is learning new features that aren't explicit in\n",
    "    the data, and is teaching the first layer to supply it with the\n",
    "    necessary information to compose these new features\n",
    "\n",
    "### So instead of just feature weighting and combining, we have new feature learning!\n",
    "\n",
    "This concept is the foundation of the \"Deep Feed-Forward Network\"\n",
    "\n",
    "&lt;img src=\"http://i.imgur.com/fHGrs4X.png\"&gt;\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Let's try it!\n",
    "\n",
    "**Add a layer to your Keras network, perhaps another 20 neurons, and see\n",
    "how the training goes.**\n",
    "\n",
    "if you get stuck, there is a solution in the Keras-DFFN notebook\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "I'm getting RMSE &lt; $1000 by epoch 35 or so\n",
    "\n",
    "&lt; $800 by epoch 90\n",
    "\n",
    "In this configuration, mine makes progress to around 700 epochs or so\n",
    "and then stalls with RMSE around $560\n",
    "\n",
    "### Our network has \"gone meta\"\n",
    "\n",
    "It's now able to exceed where a simple decision tree can go, because it\n",
    "can create new features and then split on those\n",
    "\n",
    "Congrats! You have built your first deep-learning model!\n",
    "--------------------------------------------------------\n",
    "\n",
    "So does that mean we can just keep adding more layers and solve\n",
    "anything?\n",
    "\n",
    "Well, theoretically maybe ... try reconfiguring your network, watch the\n",
    "training, and see what happens.\n",
    "\n",
    "&lt;img src=\"http://i.imgur.com/BumsXgL.jpg\" width=500&gt;"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
