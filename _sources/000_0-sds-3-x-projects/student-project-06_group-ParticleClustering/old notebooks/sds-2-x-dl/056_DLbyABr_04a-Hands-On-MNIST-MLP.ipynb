{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SDS-2.x, Scalable Data Engineering Science](https://lamastex.github.io/scalable-data-science/sds/2/x/)\n",
    "=======================================================================================================\n",
    "\n",
    "This is a 2019 augmentation and update of [Adam\n",
    "Breindel](https://www.linkedin.com/in/adbreind)'s initial notebooks.\n",
    "\n",
    "#### As we dive into more hands-on works, let's recap some basic guidelines:\n",
    "\n",
    "1.  Structure of your network is the first thing to work with, before\n",
    "    worrying about the precise number of neurons, size of convolution\n",
    "    filters etc.\n",
    "\n",
    "2.  \"Business records\" or fairly (ideally?) uncorrelated predictors --\n",
    "    use Dense Perceptron Layer(s)\n",
    "\n",
    "3.  Data that has 2-D patterns: 2D Convolution layer(s)\n",
    "\n",
    "4.  For activation of hidden layers, when in doubt, use ReLU\n",
    "\n",
    "5.  Output:\n",
    "\n",
    "-   Regression: 1 neuron with linear activation\n",
    "-   For k-way classification: k neurons with softmax activation\n",
    "\n",
    "1.  Deeper networks are \"smarter\" than wider networks (in terms of\n",
    "    abstraction)\n",
    "\n",
    "2.  More neurons & layers $\\\\to$ more capacity $\\\\to$ more data $\\\\to$\n",
    "    more regularization (to prevent overfitting)\n",
    "\n",
    "3.  If you don't have any specific reason not to use the \"adam\"\n",
    "    optimizer, use that one\n",
    "\n",
    "4.  Errors:\n",
    "\n",
    "-   For regression or \"wide\" content matching (e.g., large image\n",
    "    similarity), use mean-square-error;\n",
    "-   For classification or narrow content matching, use cross-entropy\n",
    "\n",
    "1.  As you simplify and abstract from your raw data, you should need\n",
    "    less features/parameters, so your layers probably become smaller and\n",
    "    simpler.\n",
    "\n",
    "As a baseline, let's start a lab running with what we already know.\n",
    "\n",
    "We'll take our deep feed-forward multilayer perceptron network, with\n",
    "ReLU activations and reasonable initializations, and apply it to\n",
    "learning the MNIST digits.\n",
    "\n",
    "The main part of the code looks like the following (full code you can\n",
    "run is in the next cell):\n",
    "\n",
    "\\`\\`\\`\n",
    "\n",
    "imports, setup, load data sets\n",
    "==============================\n",
    "\n",
    "model = Sequential() model.add(Dense(20, input*dim=784,\n",
    "kernel*initializer='normal', activation='relu')) model.add(Dense(15,\n",
    "kernel*initializer='normal', activation='relu')) model.add(Dense(10,\n",
    "kernel*initializer='normal', activation='softmax'))\n",
    "model.compile(loss='categorical*crossentropy', optimizer='adam',\n",
    "metrics=\\['categorical*accuracy'\\])\n",
    "\n",
    "categorical*labels = to*categorical(y*train, num*classes=10)\n",
    "\n",
    "history = model.fit(X*train, categorical*labels, epochs=100,\n",
    "batch\\_size=100)\n",
    "\n",
    "print metrics, plot errors\n",
    "==========================\n",
    "\n",
    "\\`\\`\\`\n",
    "\n",
    "Note the changes, which are largely about building a classifier instead\n",
    "of a regression model:\n",
    "\n",
    "-   Output layer has one neuron per category, with softmax activation\n",
    "-   **Loss function is cross-entropy loss**\n",
    "-   Accuracy metric is categorical accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "import sklearn.datasets\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train_libsvm = \"/dbfs/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt\"\n",
    "test_libsvm = \"/dbfs/databricks-datasets/mnist-digits/data-001/mnist-digits-test.txt\"\n",
    "\n",
    "X_train, y_train = sklearn.datasets.load_svmlight_file(train_libsvm, n_features=784)\n",
    "X_train = X_train.toarray()\n",
    "\n",
    "X_test, y_test = sklearn.datasets.load_svmlight_file(test_libsvm, n_features=784)\n",
    "X_test = X_test.toarray()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=784, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(15, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, kernel_initializer='normal', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "categorical_labels = to_categorical(y_train, num_classes=10)\n",
    "start = datetime.datetime.today()\n",
    "\n",
    "history = model.fit(X_train, categorical_labels, epochs=40, batch_size=100, validation_split=0.1, verbose=2)\n",
    "\n",
    "scores = model.evaluate(X_test, to_categorical(y_test, num_classes=10))\n",
    "\n",
    "print\n",
    "for i in range(len(model.metrics_names)):\n",
    "\tprint(\"%s: %f\" % (model.metrics_names[i], scores[i]))\n",
    "\n",
    "print (\"Start: \" + str(start))\n",
    "end = datetime.datetime.today()\n",
    "print (\"End: \" + str(end))\n",
    "print (\"Elapse: \" + str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Using TensorFlow backend.\n",
    ">     WARNING:tensorflow:From /databricks/python/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
    ">     Instructions for updating:\n",
    ">     Colocations handled automatically by placer.\n",
    ">     WARNING:tensorflow:From /databricks/python/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
    ">     Instructions for updating:\n",
    ">     Use tf.cast instead.\n",
    ">     Train on 54000 samples, validate on 6000 samples\n",
    ">     Epoch 1/40\n",
    ">      - 1s - loss: 0.6256 - categorical_accuracy: 0.8013 - val_loss: 0.2426 - val_categorical_accuracy: 0.9318\n",
    ">     Epoch 2/40\n",
    ">      - 1s - loss: 0.2634 - categorical_accuracy: 0.9227 - val_loss: 0.2033 - val_categorical_accuracy: 0.9437\n",
    ">     Epoch 3/40\n",
    ">      - 1s - loss: 0.2136 - categorical_accuracy: 0.9376 - val_loss: 0.1813 - val_categorical_accuracy: 0.9515\n",
    ">     Epoch 4/40\n",
    ">      - 1s - loss: 0.1829 - categorical_accuracy: 0.9462 - val_loss: 0.1691 - val_categorical_accuracy: 0.9522\n",
    ">     Epoch 5/40\n",
    ">      - 1s - loss: 0.1646 - categorical_accuracy: 0.9517 - val_loss: 0.1693 - val_categorical_accuracy: 0.9520\n",
    ">     Epoch 6/40\n",
    ">      - 1s - loss: 0.1478 - categorical_accuracy: 0.9558 - val_loss: 0.1525 - val_categorical_accuracy: 0.9598\n",
    ">     Epoch 7/40\n",
    ">      - 1s - loss: 0.1370 - categorical_accuracy: 0.9600 - val_loss: 0.1405 - val_categorical_accuracy: 0.9610\n",
    ">     Epoch 8/40\n",
    ">      - 1s - loss: 0.1280 - categorical_accuracy: 0.9625 - val_loss: 0.1371 - val_categorical_accuracy: 0.9615\n",
    ">     Epoch 9/40\n",
    ">      - 1s - loss: 0.1211 - categorical_accuracy: 0.9639 - val_loss: 0.1721 - val_categorical_accuracy: 0.9503\n",
    ">     Epoch 10/40\n",
    ">      - 1s - loss: 0.1177 - categorical_accuracy: 0.9645 - val_loss: 0.1383 - val_categorical_accuracy: 0.9648\n",
    ">     Epoch 11/40\n",
    ">      - 1s - loss: 0.1101 - categorical_accuracy: 0.9672 - val_loss: 0.1323 - val_categorical_accuracy: 0.9650\n",
    ">     Epoch 12/40\n",
    ">      - 1s - loss: 0.1037 - categorical_accuracy: 0.9693 - val_loss: 0.1647 - val_categorical_accuracy: 0.9585\n",
    ">     Epoch 13/40\n",
    ">      - 1s - loss: 0.1002 - categorical_accuracy: 0.9702 - val_loss: 0.1459 - val_categorical_accuracy: 0.9603\n",
    ">     Epoch 14/40\n",
    ">      - 1s - loss: 0.0990 - categorical_accuracy: 0.9697 - val_loss: 0.1997 - val_categorical_accuracy: 0.9448\n",
    ">     Epoch 15/40\n",
    ">      - 1s - loss: 0.0963 - categorical_accuracy: 0.9705 - val_loss: 0.1397 - val_categorical_accuracy: 0.9640\n",
    ">     Epoch 16/40\n",
    ">      - 1s - loss: 0.0911 - categorical_accuracy: 0.9718 - val_loss: 0.1362 - val_categorical_accuracy: 0.9670\n",
    ">     Epoch 17/40\n",
    ">      - 1s - loss: 0.0881 - categorical_accuracy: 0.9731 - val_loss: 0.1592 - val_categorical_accuracy: 0.9568\n",
    ">     Epoch 18/40\n",
    ">      - 2s - loss: 0.0856 - categorical_accuracy: 0.9739 - val_loss: 0.1480 - val_categorical_accuracy: 0.9632\n",
    ">     Epoch 19/40\n",
    ">      - 1s - loss: 0.0850 - categorical_accuracy: 0.9740 - val_loss: 0.1585 - val_categorical_accuracy: 0.9625\n",
    ">     Epoch 20/40\n",
    ">      - 2s - loss: 0.0802 - categorical_accuracy: 0.9755 - val_loss: 0.1500 - val_categorical_accuracy: 0.9630\n",
    ">     Epoch 21/40\n",
    ">      - 1s - loss: 0.0798 - categorical_accuracy: 0.9751 - val_loss: 0.1451 - val_categorical_accuracy: 0.9665\n",
    ">     Epoch 22/40\n",
    ">      - 2s - loss: 0.0780 - categorical_accuracy: 0.9762 - val_loss: 0.1499 - val_categorical_accuracy: 0.9642\n",
    ">     Epoch 23/40\n",
    ">      - 2s - loss: 0.0772 - categorical_accuracy: 0.9761 - val_loss: 0.1521 - val_categorical_accuracy: 0.9607\n",
    ">     Epoch 24/40\n",
    ">      - 2s - loss: 0.0746 - categorical_accuracy: 0.9772 - val_loss: 0.1568 - val_categorical_accuracy: 0.9630\n",
    ">     Epoch 25/40\n",
    ">      - 2s - loss: 0.0725 - categorical_accuracy: 0.9773 - val_loss: 0.1655 - val_categorical_accuracy: 0.9603\n",
    ">     Epoch 26/40\n",
    ">      - 2s - loss: 0.0723 - categorical_accuracy: 0.9776 - val_loss: 0.1782 - val_categorical_accuracy: 0.9583\n",
    ">     Epoch 27/40\n",
    ">      - 2s - loss: 0.0737 - categorical_accuracy: 0.9774 - val_loss: 0.1672 - val_categorical_accuracy: 0.9597\n",
    ">     Epoch 28/40\n",
    ">      - 1s - loss: 0.0700 - categorical_accuracy: 0.9789 - val_loss: 0.1846 - val_categorical_accuracy: 0.9562\n",
    ">     Epoch 29/40\n",
    ">      - 1s - loss: 0.0679 - categorical_accuracy: 0.9793 - val_loss: 0.1665 - val_categorical_accuracy: 0.9615\n",
    ">     Epoch 30/40\n",
    ">      - 1s - loss: 0.0711 - categorical_accuracy: 0.9782 - val_loss: 0.1678 - val_categorical_accuracy: 0.9602\n",
    ">     Epoch 31/40\n",
    ">      - 1s - loss: 0.0681 - categorical_accuracy: 0.9790 - val_loss: 0.1656 - val_categorical_accuracy: 0.9612\n",
    ">     Epoch 32/40\n",
    ">      - 1s - loss: 0.0647 - categorical_accuracy: 0.9793 - val_loss: 0.1738 - val_categorical_accuracy: 0.9608\n",
    ">     Epoch 33/40\n",
    ">      - 1s - loss: 0.0681 - categorical_accuracy: 0.9794 - val_loss: 0.1838 - val_categorical_accuracy: 0.9578\n",
    ">     Epoch 34/40\n",
    ">      - 1s - loss: 0.0664 - categorical_accuracy: 0.9797 - val_loss: 0.1817 - val_categorical_accuracy: 0.9593\n",
    ">     Epoch 35/40\n",
    ">      - 1s - loss: 0.0631 - categorical_accuracy: 0.9808 - val_loss: 0.1737 - val_categorical_accuracy: 0.9632\n",
    ">     Epoch 36/40\n",
    ">      - 1s - loss: 0.0638 - categorical_accuracy: 0.9802 - val_loss: 0.1842 - val_categorical_accuracy: 0.9607\n",
    ">     Epoch 37/40\n",
    ">      - 1s - loss: 0.0617 - categorical_accuracy: 0.9808 - val_loss: 0.1834 - val_categorical_accuracy: 0.9602\n",
    ">     Epoch 38/40\n",
    ">      - 1s - loss: 0.0630 - categorical_accuracy: 0.9803 - val_loss: 0.2150 - val_categorical_accuracy: 0.9570\n",
    ">     Epoch 39/40\n",
    ">      - 1s - loss: 0.0619 - categorical_accuracy: 0.9806 - val_loss: 0.1846 - val_categorical_accuracy: 0.9615\n",
    ">     Epoch 40/40\n",
    ">      - 1s - loss: 0.0564 - categorical_accuracy: 0.9830 - val_loss: 0.2025 - val_categorical_accuracy: 0.9608\n",
    ">\n",
    ">        32/10000 [..............................] - ETA: 0s\n",
    ">      2496/10000 [======>.......................] - ETA: 0s\n",
    ">      4800/10000 [=============>................] - ETA: 0s\n",
    ">      7168/10000 [====================>.........] - ETA: 0s\n",
    ">      9696/10000 [============================>.] - ETA: 0s\n",
    ">     10000/10000 [==============================] - 0s 21us/step\n",
    ">\n",
    ">     loss: 0.222367\n",
    ">     categorical_accuracy: 0.956200\n",
    ">     Start: 2019-06-08 20:47:25.065766\n",
    ">     End: 2019-06-08 20:48:12.525942\n",
    ">     Elapse: 0:00:47.460176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((5,5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "What are the big takeaways from this experiment?\n",
    "\n",
    "1.  We get pretty impressive \"apparent error\" accuracy right from the\n",
    "    start! A small network gets us to training accuracy 97% by epoch 20\n",
    "2.  The model *appears* to continue to learn if we let it run, although\n",
    "    it does slow down and oscillate a bit.\n",
    "3.  Our test accuracy is about 95% after 5 epochs and never gets better\n",
    "    ... it gets worse!\n",
    "4.  Therefore, we are overfitting very quickly... most of the \"training\"\n",
    "    turns out to be a waste.\n",
    "5.  For what it's worth, we get 95% accuracy without much work.\n",
    "\n",
    "This is not terrible compared to other, non-neural-network approaches to\n",
    "the problem. After all, we could probably tweak this a bit and do even\n",
    "better.\n",
    "\n",
    "But we talked about using deep learning to solve \"95%\" problems or \"98%\"\n",
    "problems ... where one error in 20, or 50 simply won't work. If we can\n",
    "get to \"multiple nines\" of accuracy, then we can do things like automate\n",
    "mail sorting and translation, create cars that react properly (all the\n",
    "time) to street signs, and control systems for robots or drones that\n",
    "function autonomously.\n",
    "\n",
    "You Try Now!\n",
    "------------\n",
    "\n",
    "Try two more experiments (try them separately):\n",
    "\n",
    "1.  Add a third, hidden layer.\n",
    "2.  Increase the size of the hidden layers.\n",
    "\n",
    "Adding another layer slows things down a little (why?) but doesn't seem\n",
    "to make a difference in accuracy.\n",
    "\n",
    "Adding a lot more neurons into the first topology slows things down\n",
    "significantly -- 10x as many neurons, and only a marginal increase in\n",
    "accuracy. Notice also (in the plot) that the learning clearly degrades\n",
    "after epoch 50 or so."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
