{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "#import tf_util\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def attn_feature(input_feature, output_dim, neighbors_idx, activation, in_dropout=0.0, coef_dropout=0.0, is_training=None, bn_decay=None, layer='', k=20, i=0, is_dist=False):\n",
    "    batch_size = input_feature.get_shape()[0]\n",
    "    num_dim = input_feature.get_shape()[-1]\n",
    "\n",
    "    input_feature = tf.squeeze(input_feature)\n",
    "    if batch_size == 1:\n",
    "        input_feature = tf.expand_dims(input_feature, 0)\n",
    "\n",
    "    input_feature = tf.expand_dims(input_feature, axis=-2)\n",
    "\n",
    "\n",
    "    # if in_dropout != 0.0:\n",
    "    #     input = tf.nn.dropout(input, 1.0 - in_dropout)\n",
    "\n",
    "    new_feature = conv2d_nobias(input_feature, output_dim, [1, 1], padding='VALID', stride=[1, 1], bn=True,\n",
    "                                        is_training=is_training, scope=layer + '_newfea_conv_head_' + str(i),\n",
    "                                        bn_decay=bn_decay, is_dist=is_dist)\n",
    "    #Encode the high level features in a 1 layer CNN, the weights are learnable parameters of this filter\n",
    "\n",
    "    neighbors = get_neighbors(input_feature, nn_idx=neighbors_idx, k=k) #Group up the neighbors using the index passed on the arguments\n",
    "    input_feature_tiled = tf.tile(input_feature, [1, 1, k, 1])\n",
    "    edge_feature = input_feature_tiled - neighbors #Make the edge features yij\n",
    "    #edge_feature = tf.concat([input_feature_tiled, input_feature_tiled-neighbors], axis=-1)\n",
    "    edge_feature = conv2d(edge_feature, output_dim, [1, 1], padding='VALID', stride=[1, 1],\n",
    "                               bn=True, is_training=is_training, scope=layer + '_edgefea_' + str(i), bn_decay=bn_decay, is_dist=is_dist)\n",
    "    #Enconde that as well\n",
    "\n",
    "    self_attention = conv2d(new_feature, 1, [1, 1], padding='VALID', stride=[1, 1], bn=True,\n",
    "                                  is_training=is_training, scope=layer+'_self_att_conv_head_'+str(i), bn_decay=bn_decay, is_dist=is_dist)\n",
    "    \n",
    "    neibor_attention = conv2d(edge_feature, 1, [1, 1], padding='VALID', stride=[1, 1], bn=True,\n",
    "                                    is_training=is_training, scope=layer+'_neib_att_conv_head_'+str(i), bn_decay=bn_decay, is_dist=is_dist)\n",
    "    #To merge both contributions, pass them to a 1 layer, size 1 output\n",
    "\n",
    "    logits = self_attention + neibor_attention\n",
    "    logits = tf.transpose(a=logits, perm=[0, 1, 3, 2])\n",
    "\n",
    "    coefs = tf.nn.softmax(tf.nn.leaky_relu(logits))\n",
    "    # zero_tf =  tf.fill(tf.shape(coefs), tf.constant(0.0, dtype=coefs.dtype))\n",
    "    # coefs = tf.where(tf.less_equal(coefs,0.1),zero_tf,coefs ) #Keep only att. > 0.1\n",
    "    #coefs = tf.nn.softmax(tf.nn.relu(logits))\n",
    "    # coefs = tf.ones_like(coefs)\n",
    "    #\n",
    "    # if coef_dropout != 0.0:\n",
    "    #     coefs = tf.nn.dropout(coefs, 1.0 - coef_dropout)\n",
    "\n",
    "\n",
    "    vals = tf.matmul(coefs, edge_feature)\n",
    "\n",
    "    if is_dist:\n",
    "        ret = activation(vals)\n",
    "    else:\n",
    "        # ret = tf.contrib.layers.bias_add(vals)\n",
    "        ret = activation(vals)\n",
    "\n",
    "\n",
    "    return ret, self_attention, edge_feature\n",
    "    #return ret, coefs, edge_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_util.py\n",
    "  \n",
    "\"\"\" Wrapper functions for TensorFlow layers.\n",
    "Author: Charles R. Qi\n",
    "Date: November 2016\n",
    "Upadted by Yue Wang and Yongbin Sun\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import lorentz\n",
    "from math import *\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "# from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
    "# import tensorflow.contrib.seq2seq as seq2seq\n",
    "\n",
    "def _variable_on_cpu(name, shape, initializer, use_fp16=False, trainable=True):\n",
    "    \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    initializer: initializer for Variable\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        dtype = tf.float16 if use_fp16 else tf.float32\n",
    "        var = tf.compat.v1.get_variable(name, shape, initializer=initializer, dtype=dtype, trainable=trainable,\n",
    "                                        use_resource=False)\n",
    "    return var\n",
    "\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd, use_xavier=True):\n",
    "    \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "  Note that the Variable is initialized with a truncated normal distribution.\n",
    "  A weight decay is added only if one is specified.\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    stddev: standard deviation of a truncated Gaussian\n",
    "    wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "        decay is not added for this Variable.\n",
    "    use_xavier: bool, whether to use xavier initializer\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "    if use_xavier:\n",
    "        initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\")\n",
    "    else:\n",
    "        initializer = tf.compat.v1.truncated_normal_initializer(stddev=stddev)\n",
    "    var = _variable_on_cpu(name, shape, initializer)\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.compat.v1.add_to_collection('losses', weight_decay)\n",
    "    return var\n",
    "\n",
    "\n",
    "def conv1d(inputs,\n",
    "           num_output_channels,\n",
    "           kernel_size,\n",
    "           scope,\n",
    "           stride=1,\n",
    "           padding='SAME',\n",
    "           use_xavier=True,\n",
    "           stddev=1e-3,\n",
    "           weight_decay=0.0,\n",
    "           activation_fn=tf.nn.relu,\n",
    "           bn=False,\n",
    "           bn_decay=None,\n",
    "           is_training=None,\n",
    "           is_dist=False):\n",
    "    \"\"\" 1D convolution with non-linear operation.\n",
    "  Args:\n",
    "    inputs: 3-D tensor variable BxLxC\n",
    "    num_output_channels: int\n",
    "    kernel_size: int\n",
    "    scope: string\n",
    "    stride: int\n",
    "    padding: 'SAME' or 'VALID'\n",
    "    use_xavier: bool, use xavier_initializer if true\n",
    "    stddev: float, stddev for truncated_normal init\n",
    "    weight_decay: float\n",
    "    activation_fn: function\n",
    "    bn: bool, whether to use batch norm\n",
    "    bn_decay: float or float tensor variable in [0,1]\n",
    "    is_training: bool Tensor variable\n",
    "  Returns:\n",
    "    Variable tensor\n",
    "  \"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope) as sc:\n",
    "        num_in_channels = inputs.get_shape()[-1].value\n",
    "        kernel_shape = [kernel_size,\n",
    "                        num_in_channels, num_output_channels]\n",
    "        kernel = _variable_with_weight_decay('weights',\n",
    "                                             shape=kernel_shape,\n",
    "                                             use_xavier=use_xavier,\n",
    "                                             stddev=stddev,\n",
    "                                             wd=weight_decay)\n",
    "        outputs = tf.nn.conv1d(input=inputs, filters=kernel,\n",
    "                               stride=stride,\n",
    "                               padding=padding)\n",
    "        biases = _variable_on_cpu('biases', [num_output_channels],\n",
    "                                  tf.compat.v1.constant_initializer(0.0))\n",
    "        outputs = tf.nn.bias_add(outputs, biases)\n",
    "\n",
    "        if bn:\n",
    "            outputs = batch_norm_for_conv1d(outputs, is_training,\n",
    "                                            bn_decay=bn_decay, scope='bn', is_dist=is_dist)\n",
    "\n",
    "        if activation_fn is not None:\n",
    "            outputs = activation_fn(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def conv2d_nobias(inputs,\n",
    "                  num_output_channels,\n",
    "                  kernel_size,\n",
    "                  scope,\n",
    "                  stride=[1, 1],\n",
    "                  padding='SAME',\n",
    "                  use_xavier=True,\n",
    "                  stddev=1e-3,\n",
    "                  weight_decay=0.0,\n",
    "                  activation_fn=tf.nn.relu,\n",
    "                  bn=False,\n",
    "                  bn_decay=None,\n",
    "                  is_training=None,\n",
    "                  is_dist=False):\n",
    "    \"\"\" 2D convolution with non-linear operation.\n",
    "      Args:\n",
    "        inputs: 4-D tensor variable BxHxWxC\n",
    "        num_output_channels: int\n",
    "        kernel_size: a list of 2 ints\n",
    "        scope: string\n",
    "        stride: a list of 2 ints\n",
    "        padding: 'SAME' or 'VALID'\n",
    "        use_xavier: bool, use xavier_initializer if true\n",
    "        stddev: float, stddev for truncated_normal init\n",
    "        weight_decay: float\n",
    "        activation_fn: function\n",
    "        bn: bool, whether to use batch norm\n",
    "        bn_decay: float or float tensor variable in [0,1]\n",
    "        is_training: bool Tensor variable\n",
    "      Returns:\n",
    "        Variable tensor\n",
    "      \"\"\"\n",
    "\n",
    "    with tf.compat.v1.variable_scope(scope) as sc:\n",
    "        kernel_h, kernel_w = kernel_size\n",
    "        num_in_channels = inputs.get_shape()[-1]\n",
    "        kernel_shape = [kernel_h, kernel_w,\n",
    "                        num_in_channels, num_output_channels]\n",
    "        kernel = _variable_with_weight_decay('weights',\n",
    "                                             shape=kernel_shape,\n",
    "                                             use_xavier=use_xavier,\n",
    "                                             stddev=stddev,\n",
    "                                             wd=weight_decay)\n",
    "        stride_h, stride_w = stride\n",
    "        outputs = tf.nn.conv2d(input=inputs, filters=kernel,\n",
    "                               strides=[1, stride_h, stride_w, 1],\n",
    "                               padding=padding)\n",
    "\n",
    "        if bn:\n",
    "            outputs = batch_norm_for_conv2d(outputs, is_training,\n",
    "                                            bn_decay=bn_decay, scope='bn', is_dist=is_dist)\n",
    "\n",
    "        if activation_fn is not None:\n",
    "            outputs = activation_fn(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def get_neighbors(point_cloud, nn_idx, k=20):\n",
    "    \"\"\"Construct neighbors feature for each point\n",
    "      Args:\n",
    "        point_cloud: (batch_size, num_points, 1, num_dims)\n",
    "        nn_idx: (batch_size, num_points, k)\n",
    "        k: int\n",
    "      Returns:\n",
    "        neighbors features: (batch_size, num_points, k, num_dims)\n",
    "      \"\"\"\n",
    "    og_batch_size = point_cloud.get_shape().as_list()[0]\n",
    "    og_num_dims = point_cloud.get_shape().as_list()[-1]\n",
    "    point_cloud = tf.squeeze(point_cloud)\n",
    "    if og_batch_size == 1:\n",
    "        point_cloud = tf.expand_dims(point_cloud, 0)\n",
    "    if og_num_dims == 1:\n",
    "        point_cloud = tf.expand_dims(point_cloud, -1)\n",
    "\n",
    "    point_cloud_shape = point_cloud.get_shape()\n",
    "    batch_size = point_cloud_shape[0]\n",
    "    num_points = point_cloud_shape[1]\n",
    "    num_dims = point_cloud_shape[2]\n",
    "\n",
    "    idx_ = tf.range(batch_size) * num_points\n",
    "    idx_ = tf.reshape(idx_, [batch_size, 1, 1])\n",
    "\n",
    "    point_cloud_flat = tf.reshape(point_cloud, [-1, num_dims])\n",
    "    point_cloud_neighbors = tf.gather(point_cloud_flat, nn_idx + idx_)\n",
    "\n",
    "    return point_cloud_neighbors\n",
    "\n",
    "\n",
    "def conv2d(inputs,\n",
    "           num_output_channels,\n",
    "           kernel_size,\n",
    "           scope,\n",
    "           stride=[1, 1],\n",
    "           padding='SAME',\n",
    "           use_xavier=True,\n",
    "           stddev=1e-3,\n",
    "           weight_decay=0.0,\n",
    "           activation_fn=tf.nn.relu,\n",
    "           bn=False,\n",
    "           bn_decay=None,\n",
    "           is_training=None,\n",
    "           is_dist=False):\n",
    "    \"\"\" 2D convolution with non-linear operation.\n",
    "  Args:\n",
    "    inputs: 4-D tensor variable BxHxWxC\n",
    "    num_output_channels: int\n",
    "    kernel_size: a list of 2 ints\n",
    "    scope: string\n",
    "    stride: a list of 2 ints\n",
    "    padding: 'SAME' or 'VALID'\n",
    "    use_xavier: bool, use xavier_initializer if true\n",
    "    stddev: float, stddev for truncated_normal init\n",
    "    weight_decay: float\n",
    "    activation_fn: function\n",
    "    bn: bool, whether to use batch norm\n",
    "    bn_decay: float or float tensor variable in [0,1]\n",
    "    is_training: bool Tensor variable\n",
    "  Returns:\n",
    "    Variable tensor\n",
    "  \"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope) as sc:\n",
    "        kernel_h, kernel_w = kernel_size\n",
    "        num_in_channels = inputs.get_shape()[-1]\n",
    "        kernel_shape = [kernel_h, kernel_w,\n",
    "                        num_in_channels, num_output_channels]\n",
    "        kernel = _variable_with_weight_decay('weights',\n",
    "                                             shape=kernel_shape,\n",
    "                                             use_xavier=use_xavier,\n",
    "                                             stddev=stddev,\n",
    "                                             wd=weight_decay)\n",
    "        stride_h, stride_w = stride\n",
    "        outputs = tf.nn.conv2d(input=inputs, filters=kernel,\n",
    "                               strides=[1, stride_h, stride_w, 1],\n",
    "                               padding=padding)\n",
    "        biases = _variable_on_cpu('biases', [num_output_channels],\n",
    "                                  tf.compat.v1.constant_initializer(0.0))\n",
    "        outputs = tf.nn.bias_add(outputs, biases)\n",
    "\n",
    "        if bn:\n",
    "            outputs = batch_norm_for_conv2d(outputs, is_training,\n",
    "                                            bn_decay=bn_decay, scope='bn', is_dist=is_dist)\n",
    "\n",
    "        if activation_fn is not None:\n",
    "            outputs = activation_fn(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def conv2d_transpose(inputs,\n",
    "                     num_output_channels,\n",
    "                     kernel_size,\n",
    "                     scope,\n",
    "                     stride=[1, 1],\n",
    "                     padding='SAME',\n",
    "                     use_xavier=True,\n",
    "                     stddev=1e-3,\n",
    "                     weight_decay=0.0,\n",
    "                     activation_fn=tf.nn.relu,\n",
    "                     bn=False,\n",
    "                     bn_decay=None,\n",
    "                     is_training=None,\n",
    "                     is_dist=False):\n",
    "    \"\"\" 2D convolution transpose with non-linear operation.\n",
    "  Args:\n",
    "    inputs: 4-D tensor variable BxHxWxC\n",
    "    num_output_channels: int\n",
    "    kernel_size: a list of 2 ints\n",
    "    scope: string\n",
    "    stride: a list of 2 ints\n",
    "    padding: 'SAME' or 'VALID'\n",
    "    use_xavier: bool, use xavier_initializer if true\n",
    "    stddev: float, stddev for truncated_normal init\n",
    "    weight_decay: float\n",
    "    activation_fn: function\n",
    "    bn: bool, whether to use batch norm\n",
    "    bn_decay: float or float tensor variable in [0,1]\n",
    "    is_training: bool Tensor variable\n",
    "  Returns:\n",
    "    Variable tensor\n",
    "  Note: conv2d(conv2d_transpose(a, num_out, ksize, stride), a.shape[-1], ksize, stride) == a\n",
    "  \"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope) as sc:\n",
    "        kernel_h, kernel_w = kernel_size\n",
    "        num_in_channels = inputs.get_shape()[-1]\n",
    "        kernel_shape = [kernel_h, kernel_w,\n",
    "                        num_output_channels, num_in_channels]  # reversed to conv2d\n",
    "        kernel = _variable_with_weight_decay('weights',\n",
    "                                             shape=kernel_shape,\n",
    "                                             use_xavier=use_xavier,\n",
    "                                             stddev=stddev,\n",
    "                                             wd=weight_decay)\n",
    "        stride_h, stride_w = stride\n",
    "\n",
    "        # from slim.convolution2d_transpose\n",
    "        def get_deconv_dim(dim_size, stride_size, kernel_size, padding):\n",
    "            dim_size *= stride_size\n",
    "\n",
    "            if padding == 'VALID' and dim_size is not None:\n",
    "                dim_size += max(kernel_size - stride_size, 0)\n",
    "            return dim_size\n",
    "\n",
    "        # caculate output shape\n",
    "        batch_size = inputs.get_shape()[0]\n",
    "        height = inputs.get_shape()[1]\n",
    "        width = inputs.get_shape()[2]\n",
    "        out_height = get_deconv_dim(height, stride_h, kernel_h, padding)\n",
    "        out_width = get_deconv_dim(width, stride_w, kernel_w, padding)\n",
    "        output_shape = [batch_size, out_height, out_width, num_output_channels]\n",
    "\n",
    "        outputs = tf.nn.conv2d_transpose(inputs, kernel, output_shape,\n",
    "                                         [1, stride_h, stride_w, 1],\n",
    "                                         padding=padding)\n",
    "        biases = _variable_on_cpu('biases', [num_output_channels],\n",
    "                                  tf.compat.v1.constant_initializer(0.0))\n",
    "        outputs = tf.nn.bias_add(outputs, biases)\n",
    "\n",
    "        if bn:\n",
    "            outputs = batch_norm_for_conv2d(outputs, is_training,\n",
    "                                            bn_decay=bn_decay, scope='bn', is_dist=is_dist)\n",
    "\n",
    "        if activation_fn is not None:\n",
    "            outputs = activation_fn(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def conv3d(inputs,\n",
    "           num_output_channels,\n",
    "           kernel_size,\n",
    "           scope,\n",
    "           stride=[1, 1, 1],\n",
    "           padding='SAME',\n",
    "           use_xavier=True,\n",
    "           stddev=1e-3,\n",
    "           weight_decay=0.0,\n",
    "           activation_fn=tf.nn.relu,\n",
    "           bn=False,\n",
    "           bn_decay=None,\n",
    "           is_training=None,\n",
    "           is_dist=False):\n",
    "    \"\"\" 3D convolution with non-linear operation.\n",
    "  Args:\n",
    "    inputs: 5-D tensor variable BxDxHxWxC\n",
    "    num_output_channels: int\n",
    "    kernel_size: a list of 3 ints\n",
    "    scope: string\n",
    "    stride: a list of 3 ints\n",
    "    padding: 'SAME' or 'VALID'\n",
    "    use_xavier: bool, use xavier_initializer if true\n",
    "    stddev: float, stddev for truncated_normal init\n",
    "    weight_decay: float\n",
    "    activation_fn: function\n",
    "    bn: bool, whether to use batch norm\n",
    "    bn_decay: float or float tensor variable in [0,1]\n",
    "    is_training: bool Tensor variable\n",
    "  Returns:\n",
    "    Variable tensor\n",
    "  \"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope) as sc:\n",
    "        kernel_d, kernel_h, kernel_w = kernel_size\n",
    "        num_in_channels = inputs.get_shape()[-1]\n",
    "        kernel_shape = [kernel_d, kernel_h, kernel_w,\n",
    "                        num_in_channels, num_output_channels]\n",
    "        kernel = _variable_with_weight_decay('weights',\n",
    "                                             shape=kernel_shape,\n",
    "                                             use_xavier=use_xavier,\n",
    "                                             stddev=stddev,\n",
    "                                             wd=weight_decay)\n",
    "        stride_d, stride_h, stride_w = stride\n",
    "        outputs = tf.nn.conv3d(inputs, kernel,\n",
    "                               [1, stride_d, stride_h, stride_w, 1],\n",
    "                               padding=padding)\n",
    "        biases = _variable_on_cpu('biases', [num_output_channels],\n",
    "                                  tf.compat.v1.constant_initializer(0.0))\n",
    "        outputs = tf.nn.bias_add(outputs, biases)\n",
    "\n",
    "        if bn:\n",
    "            outputs = batch_norm_for_conv3d(outputs, is_training,\n",
    "                                            bn_decay=bn_decay, scope='bn', is_dist=is_dist)\n",
    "\n",
    "        if activation_fn is not None:\n",
    "            outputs = activation_fn(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def fully_connected(inputs,\n",
    "                    num_outputs,\n",
    "                    scope,\n",
    "                    use_xavier=True,\n",
    "                    stddev=1e-3,\n",
    "                    weight_decay=0.0,\n",
    "                    activation_fn=tf.nn.relu,\n",
    "                    bn=False,\n",
    "                    bn_decay=None,\n",
    "                    is_training=None,\n",
    "                    is_dist=False):\n",
    "    \"\"\" Fully connected layer with non-linear operation.\n",
    "  \n",
    "  Args:\n",
    "    inputs: 2-D tensor BxN\n",
    "    num_outputs: int\n",
    "  \n",
    "  Returns:\n",
    "    Variable tensor of size B x num_outputs.\n",
    "  \"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope) as sc:\n",
    "        num_input_units = inputs.get_shape()[-1]\n",
    "        weights = _variable_with_weight_decay('weights',\n",
    "                                              shape=[num_input_units, num_outputs],\n",
    "                                              use_xavier=use_xavier,\n",
    "                                              stddev=stddev,\n",
    "                                              wd=weight_decay)\n",
    "        outputs = tf.matmul(inputs, weights)\n",
    "        biases = _variable_on_cpu('biases', [num_outputs],\n",
    "                                  tf.compat.v1.constant_initializer(0.0))\n",
    "        outputs = tf.nn.bias_add(outputs, biases)\n",
    "\n",
    "        if bn:\n",
    "            outputs = batch_norm_for_fc(outputs, is_training, bn_decay, 'bn', is_dist=is_dist)\n",
    "\n",
    "        if activation_fn is not None:\n",
    "            if activation_fn == tf.nn.softmax:\n",
    "                outputs = activation_fn(outputs - tf.reduce_max(input_tensor=outputs, axis=1, keepdims=True))\n",
    "            else:\n",
    "                outputs = activation_fn(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def max_pool2d(inputs,\n",
    "               kernel_size,\n",
    "               scope,\n",
    "               stride=[2, 2],\n",
    "               padding='VALID'):\n",
    "    \"\"\" 2D max pooling.\n",
    "  Args:\n",
    "    inputs: 4-D tensor BxHxWxC\n",
    "    kernel_size: a list of 2 ints\n",
    "    stride: a list of 2 ints\n",
    "  \n",
    "  Returns:\n",
    "    Variable tensor\n",
    "  \"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope) as sc:\n",
    "        kernel_h, kernel_w = kernel_size\n",
    "        stride_h, stride_w = stride\n",
    "        outputs = tf.nn.max_pool2d(input=inputs,\n",
    "                                 ksize=[1, kernel_h, kernel_w, 1],\n",
    "                                 strides=[1, stride_h, stride_w, 1],\n",
    "                                 padding=padding,\n",
    "                                 name=sc.name)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def avg_pool2d(inputs,\n",
    "               kernel_size,\n",
    "               scope,\n",
    "               stride=[2, 2],\n",
    "               padding='VALID'):\n",
    "    \"\"\" 2D avg pooling.\n",
    "  Args:\n",
    "    inputs: 4-D tensor BxHxWxC\n",
    "    kernel_size: a list of 2 ints\n",
    "    stride: a list of 2 ints\n",
    "  \n",
    "  Returns:\n",
    "    Variable tensor\n",
    "  \"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope) as sc:\n",
    "        kernel_h, kernel_w = kernel_size\n",
    "        stride_h, stride_w = stride\n",
    "        outputs = tf.nn.avg_pool2d(input=inputs,\n",
    "                                 ksize=[1, kernel_h, kernel_w, 1],\n",
    "                                 strides=[1, stride_h, stride_w, 1],\n",
    "                                 padding=padding,\n",
    "                                 name=sc.name)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def max_pool3d(inputs,\n",
    "               kernel_size,\n",
    "               scope,\n",
    "               stride=[2, 2, 2],\n",
    "               padding='VALID'):\n",
    "    \"\"\" 3D max pooling.\n",
    "  Args:\n",
    "    inputs: 5-D tensor BxDxHxWxC\n",
    "    kernel_size: a list of 3 ints\n",
    "    stride: a list of 3 ints\n",
    "  \n",
    "  Returns:\n",
    "    Variable tensor\n",
    "  \"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope) as sc:\n",
    "        kernel_d, kernel_h, kernel_w = kernel_size\n",
    "        stride_d, stride_h, stride_w = stride\n",
    "        outputs = tf.nn.max_pool3d(inputs,\n",
    "                                   ksize=[1, kernel_d, kernel_h, kernel_w, 1],\n",
    "                                   strides=[1, stride_d, stride_h, stride_w, 1],\n",
    "                                   padding=padding,\n",
    "                                   name=sc.name)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def avg_pool3d(inputs,\n",
    "               kernel_size,\n",
    "               scope,\n",
    "               stride=[2, 2, 2],\n",
    "               padding='VALID'):\n",
    "    \"\"\" 3D avg pooling.\n",
    "  Args:\n",
    "    inputs: 5-D tensor BxDxHxWxC\n",
    "    kernel_size: a list of 3 ints\n",
    "    stride: a list of 3 ints\n",
    "  \n",
    "  Returns:\n",
    "    Variable tensor\n",
    "  \"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope) as sc:\n",
    "        kernel_d, kernel_h, kernel_w = kernel_size\n",
    "        stride_d, stride_h, stride_w = stride\n",
    "        outputs = tf.nn.avg_pool3d(inputs,\n",
    "                                   ksize=[1, kernel_d, kernel_h, kernel_w, 1],\n",
    "                                   strides=[1, stride_d, stride_h, stride_w, 1],\n",
    "                                   padding=padding,\n",
    "                                   name=sc.name)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def batch_norm_template(inputs, is_training, scope, moments_dims, bn_decay):\n",
    "    \"\"\" Batch normalization on convolutional maps and beyond...\n",
    "  Ref.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n",
    "  \n",
    "  Args:\n",
    "      inputs:        Tensor, k-D input ... x C could be BC or BHWC or BDHWC\n",
    "      is_training:   boolean tf.Varialbe, true indicates training phase\n",
    "      scope:         string, variable scope\n",
    "      moments_dims:  a list of ints, indicating dimensions for moments calculation\n",
    "      bn_decay:      float or float tensor variable, controling moving average weight\n",
    "  Return:\n",
    "      normed:        batch-normalized maps\n",
    "  \"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope) as sc:\n",
    "        num_channels = inputs.get_shape()[-1]\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[num_channels]),\n",
    "                           name='beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[num_channels]),\n",
    "                            name='gamma', trainable=True)\n",
    "        batch_mean, batch_var = tf.nn.moments(x=inputs, axes=moments_dims, name='moments')\n",
    "        decay = bn_decay if bn_decay is not None else 0.9\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=decay)\n",
    "        # Operator that maintains moving averages of variables.\n",
    "        ema_apply_op = tf.cond(pred=is_training,\n",
    "                               true_fn=lambda: ema.apply([batch_mean, batch_var]),\n",
    "                               false_fn=lambda: tf.no_op())\n",
    "\n",
    "        # Update moving average and return current batch's avg and var.\n",
    "        def mean_var_with_update():\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "        # ema.average returns the Variable holding the average of var.\n",
    "        mean, var = tf.cond(pred=is_training,\n",
    "                            true_fn=mean_var_with_update,\n",
    "                            false_fn=lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "        # print('types again', inputs.dtype, mean.dtype, var.dtype, beta.dtype, gamma.dtype)\n",
    "        # normed = tf.nn.batch_normalization(inputs, mean, var, tf.cast(beta,tf.float64), tf.cast(gamma,tf.float64), 1e-3)\n",
    "        normed = tf.nn.batch_normalization(inputs, mean, var, beta, gamma, 1e-3)\n",
    "    return normed\n",
    "\n",
    "\n",
    "def batch_norm_dist_template(inputs, is_training, scope, moments_dims, bn_decay):\n",
    "    \"\"\" The batch normalization for distributed training.\n",
    "  Args:\n",
    "      inputs:        Tensor, k-D input ... x C could be BC or BHWC or BDHWC\n",
    "      is_training:   boolean tf.Varialbe, true indicates training phase\n",
    "      scope:         string, variable scope\n",
    "      moments_dims:  a list of ints, indicating dimensions for moments calculation\n",
    "      bn_decay:      float or float tensor variable, controling moving average weight\n",
    "  Return:\n",
    "      normed:        batch-normalized maps\n",
    "  \"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope) as sc:\n",
    "        num_channels = inputs.get_shape()[-1]\n",
    "        beta = _variable_on_cpu('beta', [num_channels], initializer=tf.compat.v1.zeros_initializer())\n",
    "        gamma = _variable_on_cpu('gamma', [num_channels], initializer=tf.compat.v1.ones_initializer())\n",
    "\n",
    "        pop_mean = _variable_on_cpu('pop_mean', [num_channels], initializer=tf.compat.v1.zeros_initializer(), trainable=False)\n",
    "        pop_var = _variable_on_cpu('pop_var', [num_channels], initializer=tf.compat.v1.ones_initializer(), trainable=False)\n",
    "\n",
    "        def train_bn_op():\n",
    "            batch_mean, batch_var = tf.nn.moments(x=inputs, axes=moments_dims, name='moments')\n",
    "            decay = bn_decay if bn_decay is not None else 0.9\n",
    "            # decay = tf.cast(decay,tf.float64)\n",
    "            # print('types:', pop_mean.dtype,  decay.dtype,batch_mean.dtype)\n",
    "            train_mean = tf.compat.v1.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n",
    "            train_var = tf.compat.v1.assign(pop_var, pop_var * decay + batch_var * (1 - decay))\n",
    "            with tf.control_dependencies([train_mean, train_var]):\n",
    "                return tf.nn.batch_normalization(inputs, batch_mean, batch_var, beta, gamma, 1e-3)\n",
    "\n",
    "        def test_bn_op():\n",
    "            return tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, gamma, 1e-3)\n",
    "\n",
    "        normed = tf.cond(pred=is_training,\n",
    "                         true_fn=train_bn_op,\n",
    "                         false_fn=test_bn_op)\n",
    "        return normed\n",
    "\n",
    "\n",
    "def batch_norm_for_fc(inputs, is_training, bn_decay, scope, is_dist=False):\n",
    "    \"\"\" Batch normalization on FC data.\n",
    "  \n",
    "  Args:\n",
    "      inputs:      Tensor, 2D BxC input\n",
    "      is_training: boolean tf.Varialbe, true indicates training phase\n",
    "      bn_decay:    float or float tensor variable, controling moving average weight\n",
    "      scope:       string, variable scope\n",
    "      is_dist:     true indicating distributed training scheme\n",
    "  Return:\n",
    "      normed:      batch-normalized maps\n",
    "  \"\"\"\n",
    "    if is_dist:\n",
    "        return batch_norm_dist_template(inputs, is_training, scope, [0, ], bn_decay)\n",
    "    else:\n",
    "        return batch_norm_template(inputs, is_training, scope, [0, ], bn_decay)\n",
    "\n",
    "\n",
    "def batch_norm_for_conv1d(inputs, is_training, bn_decay, scope, is_dist=False):\n",
    "    \"\"\" Batch normalization on 1D convolutional maps.\n",
    "  \n",
    "  Args:\n",
    "      inputs:      Tensor, 3D BLC input maps\n",
    "      is_training: boolean tf.Varialbe, true indicates training phase\n",
    "      bn_decay:    float or float tensor variable, controling moving average weight\n",
    "      scope:       string, variable scope\n",
    "      is_dist:     true indicating distributed training scheme\n",
    "  Return:\n",
    "      normed:      batch-normalized maps\n",
    "  \"\"\"\n",
    "    if is_dist:\n",
    "        return batch_norm_dist_template(inputs, is_training, scope, [0, 1], bn_decay)\n",
    "    else:\n",
    "        return batch_norm_template(inputs, is_training, scope, [0, 1], bn_decay)\n",
    "\n",
    "\n",
    "def batch_norm_for_conv2d(inputs, is_training, bn_decay, scope, is_dist=False):\n",
    "    \"\"\" Batch normalization on 2D convolutional maps.\n",
    "  \n",
    "  Args:\n",
    "      inputs:      Tensor, 4D BHWC input maps\n",
    "      is_training: boolean tf.Varialbe, true indicates training phase\n",
    "      bn_decay:    float or float tensor variable, controling moving average weight\n",
    "      scope:       string, variable scope\n",
    "      is_dist:     true indicating distributed training scheme\n",
    "  Return:\n",
    "      normed:      batch-normalized maps\n",
    "  \"\"\"\n",
    "    if is_dist:\n",
    "        return batch_norm_dist_template(inputs, is_training, scope, [0, 1, 2], bn_decay)\n",
    "    else:\n",
    "        return batch_norm_template(inputs, is_training, scope, [0, 1, 2], bn_decay)\n",
    "\n",
    "\n",
    "def batch_norm_for_conv3d(inputs, is_training, bn_decay, scope, is_dist=False):\n",
    "    \"\"\" Batch normalization on 3D convolutional maps.\n",
    "  \n",
    "  Args:\n",
    "      inputs:      Tensor, 5D BDHWC input maps\n",
    "      is_training: boolean tf.Varialbe, true indicates training phase\n",
    "      bn_decay:    float or float tensor variable, controling moving average weight\n",
    "      scope:       string, variable scope\n",
    "      is_dist:     true indicating distributed training scheme\n",
    "  Return:\n",
    "      normed:      batch-normalized maps\n",
    "  \"\"\"\n",
    "    if is_dist:\n",
    "        return batch_norm_dist_template(inputs, is_training, scope, [0, 1, 2, 3], bn_decay)\n",
    "    else:\n",
    "        return batch_norm_template(inputs, is_training, scope, [0, 1, 2, 3], bn_decay)\n",
    "\n",
    "\n",
    "def dropout(inputs,\n",
    "            is_training,\n",
    "            scope,\n",
    "            keep_prob=0.5,\n",
    "            noise_shape=None):\n",
    "    \"\"\" Dropout layer.\n",
    "  Args:\n",
    "    inputs: tensor\n",
    "    is_training: boolean tf.Variable\n",
    "    scope: string\n",
    "    keep_prob: float in [0,1]\n",
    "    noise_shape: list of ints\n",
    "  Returns:\n",
    "    tensor variable\n",
    "  \"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope) as sc:\n",
    "        outputs = tf.cond(pred=is_training,\n",
    "                          true_fn=lambda: tf.nn.dropout(inputs, 1 - (keep_prob), noise_shape),\n",
    "                          false_fn=lambda: inputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def pairwise_distance(point_cloud):\n",
    "    \"\"\"Compute pairwise distance of a point cloud.\n",
    "  Args:\n",
    "    point_cloud: tensor (batch_size, num_points, num_dims)\n",
    "  Returns:\n",
    "    pairwise distance: (batch_size, num_points, num_points)\n",
    "  \"\"\"\n",
    "    og_batch_size = point_cloud.get_shape().as_list()[0]\n",
    "    point_cloud = tf.squeeze(point_cloud)\n",
    "    if og_batch_size == 1:\n",
    "        point_cloud = tf.expand_dims(point_cloud, 0)  # first dim is batch size\n",
    "\n",
    "    point_cloud_transpose = tf.transpose(a=point_cloud, perm=[0, 2, 1])\n",
    "    point_cloud_inner = tf.matmul(point_cloud, point_cloud_transpose)  # x.x + y.y + z.z shape: NxN\n",
    "    point_cloud_inner = -2 * point_cloud_inner\n",
    "    point_cloud_square = tf.reduce_sum(input_tensor=tf.square(point_cloud), axis=-1,\n",
    "                                       keepdims=True)  # from x.x, y.y, z.z to x.x + y.y + z.z\n",
    "    point_cloud_square_tranpose = tf.transpose(a=point_cloud_square, perm=[0, 2, 1])\n",
    "    return point_cloud_square + point_cloud_inner + point_cloud_square_tranpose\n",
    "\n",
    "\n",
    "def pairwise_distanceR(point_cloud):\n",
    "    \"\"\"Compute pairwise distance in the eta-phi plane for the point cloud.\n",
    "  Uses the third dimension to find the zero-padded terms\n",
    "  Args:\n",
    "    point_cloud: tensor (batch_size, num_points, 2)\n",
    "    IMPORTANT: The order should be (eta, phi) \n",
    "  Returns:\n",
    "    pairwise distance: (batch_size, num_points, num_points)\n",
    "  \"\"\"\n",
    "    og_batch_size = point_cloud.get_shape().as_list()[0]\n",
    "    point_cloud = tf.squeeze(point_cloud)\n",
    "    if og_batch_size == 1:\n",
    "        point_cloud = tf.expand_dims(point_cloud, 0)  # first dim is batch size\n",
    "\n",
    "    pt = point_cloud[:, :, 2]\n",
    "    pt = tf.expand_dims(pt, -1)\n",
    "    is_zero = point_cloud[:, :, ]\n",
    "    point_shift = 1000 * tf.compat.v1.where(tf.equal(pt, 0), tf.ones_like(pt),\n",
    "                                  tf.fill(tf.shape(input=pt), tf.constant(0.0, dtype=pt.dtype)))\n",
    "    point_shift_transpose = tf.transpose(a=point_shift, perm=[0, 2, 1])\n",
    "    # pt = tf.exp(pt)\n",
    "    point_cloud = point_cloud[:, :, :2]\n",
    "\n",
    "    point_cloud_transpose = tf.transpose(a=point_cloud, perm=[0, 2, 1])\n",
    "    point_cloud_phi = point_cloud_transpose[:, 1:, :]\n",
    "    point_cloud_phi = tf.tile(point_cloud_phi, [1, point_cloud_phi.get_shape()[2], 1])\n",
    "    point_cloud_phi_transpose = tf.transpose(a=point_cloud_phi, perm=[0, 2, 1])\n",
    "    point_cloud_phi = tf.abs(point_cloud_phi - point_cloud_phi_transpose)\n",
    "    is_bigger2pi = tf.greater_equal(tf.abs(point_cloud_phi), 2 * np.pi)\n",
    "    point_cloud_phi_corr = tf.compat.v1.where(is_bigger2pi, 4 * np.pi ** 2 - 4 * np.pi * point_cloud_phi,\n",
    "                                    point_cloud_phi - point_cloud_phi)\n",
    "    point_cloud_inner = tf.matmul(point_cloud, point_cloud_transpose)  # x.x + y.y + z.z shape: NxN\n",
    "    point_cloud_inner = -2 * point_cloud_inner\n",
    "    point_cloud_square = tf.reduce_sum(input_tensor=tf.square(point_cloud), axis=-1,\n",
    "                                       keepdims=True)  # from x.x, y.y, z.z to x.x + y.y + z.z\n",
    "    point_cloud_square_tranpose = tf.transpose(a=point_cloud_square, perm=[0, 2, 1])\n",
    "\n",
    "    # print(\"shape\",point_cloud_square.shape)\n",
    "    return point_cloud_square + point_cloud_inner + point_cloud_square_tranpose + point_cloud_phi_corr + point_shift + point_shift_transpose\n",
    "    # return point_shift\n",
    "\n",
    "\n",
    "def knn(adj_matrix, k=20):\n",
    "    \"\"\"Get KNN based on the pairwise distance.\n",
    "  Args:\n",
    "    pairwise distance: (batch_size, num_points, num_points)\n",
    "    k: int\n",
    "  Returns:\n",
    "    nearest neighbors: (batch_size, num_points, k)\n",
    "  \"\"\"\n",
    "    neg_adj = -adj_matrix\n",
    "    _, nn_idx = tf.nn.top_k(neg_adj, k=k)  # values, indices\n",
    "    return nn_idx\n",
    "\n",
    "\n",
    "def get_edge_feature(point_cloud, nn_idx, k=20, edge_type='dgcnn'):\n",
    "    \"\"\"Construct edge feature for each point\n",
    "  Args:\n",
    "    point_cloud: (batch_size, num_points, 1, num_dims) \n",
    "    nn_idx: (batch_size, num_points, k)\n",
    "    k: int\n",
    "  Returns:\n",
    "    edge features: (batch_size, num_points, k, 2*num_dims)\n",
    "  \"\"\"\n",
    "    og_batch_size = point_cloud.get_shape().as_list()[0]\n",
    "    point_cloud = tf.squeeze(point_cloud)\n",
    "    if og_batch_size == 1:\n",
    "        point_cloud = tf.expand_dims(point_cloud, 0)\n",
    "\n",
    "    point_cloud_central = point_cloud\n",
    "\n",
    "    point_cloud_shape = point_cloud.get_shape()\n",
    "    batch_size = point_cloud_shape[0]\n",
    "    num_points = point_cloud_shape[1]\n",
    "    num_dims = point_cloud_shape[2]\n",
    "\n",
    "    idx_ = tf.range(batch_size) * num_points\n",
    "    idx_ = tf.reshape(idx_, [batch_size, 1, 1])\n",
    "    point_cloud_flat = tf.reshape(point_cloud, [-1, num_dims])\n",
    "    point_cloud_neighbors = tf.gather(point_cloud_flat, nn_idx + idx_)\n",
    "\n",
    "    point_cloud_central = tf.expand_dims(point_cloud_central, axis=-2)\n",
    "\n",
    "    if edge_type == \"dgcnn\":\n",
    "        point_cloud_central = tf.tile(point_cloud_central, [1, 1, k, 1])\n",
    "        edge_feature = tf.concat([point_cloud_central, point_cloud_neighbors - point_cloud_central], axis=-1)\n",
    "    elif edge_type == \"sub\":\n",
    "        point_cloud_central = tf.tile(point_cloud_central, [1, 1, k, 1])\n",
    "        # 4 vector difference with invariant mass sum\n",
    "        edge_feature = Sub_Cloud(point_cloud_central, point_cloud_neighbors)\n",
    "        edge_feature = tf.concat([point_cloud_central, edge_feature], axis=-1)\n",
    "    elif edge_type == \"add\":\n",
    "        # 4 vector difference with invariant mass sum\n",
    "        edge_feature = Add_Cloud(point_cloud_central, point_cloud_neighbors)\n",
    "        edge_feature = tf.concat([point_cloud_central, edge_feature], axis=-1)\n",
    "        # edge_feature= tf.reduce_max(edge_feature, axis=-2, keep_dims=True)\n",
    "    return edge_feature\n",
    "\n",
    "\n",
    "def Sub_Cloud(central, neighbors):\n",
    "    \"\"\" Input: BxPxKxF for central and K-neighbors\n",
    "    Returns: BxPxKx(F+7), 7 = eta, phi, pt, px, py, pz differences + invariant mass sum \"\"\"\n",
    "    num_batch = central.get_shape()[0]\n",
    "    num_point = central.get_shape()[1]\n",
    "    num_k = central.get_shape()[2]\n",
    "    num_dims = central.get_shape()[3]\n",
    "    point_diff = central - neighbors\n",
    "\n",
    "    identity = -np.identity(4, dtype=np.float32)  # 4vector\n",
    "    identity[0][0] = 1\n",
    "    identity = tf.expand_dims(identity, 0)\n",
    "    identity = tf.expand_dims(identity, 0)\n",
    "    identity = tf.expand_dims(identity, 0)\n",
    "    identity = tf.tile(identity, [num_batch, num_point, num_k, 1, 1])\n",
    "\n",
    "    sum_vec = neighbors + central\n",
    "    sum_vec = tf.concat([point_diff[:, :, 0:1, :], sum_vec[:, :, 1:, :]], -2)  # first neighbor is the point itself\n",
    "    sum_vec = sum_vec[:, :, :, 4:8]\n",
    "    sum_vec = tf.expand_dims(sum_vec, -2)\n",
    "    sum_vec_T = tf.transpose(a=sum_vec, perm=[0, 1, 2, 4, 3])\n",
    "    mult = tf.matmul(sum_vec, identity)\n",
    "    mult = tf.matmul(mult, sum_vec_T)\n",
    "    mult = tf.sqrt(tf.abs(mult))\n",
    "    # return tf.squeeze(mult,axis=-2)\n",
    "    phi = point_diff[:, :, :, 1:2]  # Correct phi for 2pi bound\n",
    "    is_bigger2pi = tf.greater_equal(tf.abs(phi), 2 * np.pi)\n",
    "    phi_corr = tf.compat.v1.where(is_bigger2pi, phi - 2 * np.pi, phi)\n",
    "\n",
    "    diff_update = point_diff[:, :, :, 0:1]\n",
    "    diff_update = tf.concat([diff_update, phi_corr], axis=-1)\n",
    "    diff_update = tf.concat([diff_update, point_diff[:, :, :, 2:3]], axis=-1)\n",
    "    diff_update = tf.concat([diff_update, tf.squeeze(mult, axis=-2)], axis=-1)\n",
    "    diff_update = tf.concat([diff_update, point_diff[:, :, :, 4:]], axis=-1)\n",
    "    return diff_update\n",
    "\n",
    "\n",
    "def Add_Cloud(central, neighbors):\n",
    "    \"\"\" Input: BxPxKxF for central and K-neighbors\n",
    "    Returns: BxPxKx(F+7), 5 = E, px, py, pz sum + invariant mass sum \"\"\"\n",
    "    num_batch = central.get_shape()[0]\n",
    "    num_point = central.get_shape()[1]\n",
    "    num_k = central.get_shape()[2]\n",
    "    num_dims = central.get_shape()[3]\n",
    "\n",
    "    identity = -np.identity(4, dtype=np.float32)  # 4vector\n",
    "    identity[0][0] = 1\n",
    "    identity = tf.expand_dims(identity, 0)\n",
    "    identity = tf.expand_dims(identity, 0)\n",
    "    identity = tf.expand_dims(identity, 0)\n",
    "    identity = tf.tile(identity, [num_batch, num_point, num_k, 1, 1])\n",
    "\n",
    "    sum_vec = neighbors + central\n",
    "    sum_vec = tf.concat([point_diff[:, :, 0:1, :], sum_vec[:, :, 1:, :]], -2)  # first neighbor is the point itself\n",
    "    sum_vec = sum_vec[:, :, :, 4:8]\n",
    "    point_sum = sum_vec\n",
    "    sum_vec = tf.expand_dims(sum_vec, -2)\n",
    "    sum_vec_T = tf.transpose(a=sum_vec, perm=[0, 1, 2, 4, 3])\n",
    "    mult = tf.matmul(sum_vec, identity)\n",
    "    mult = tf.matmul(mult, sum_vec_T)\n",
    "    mult = tf.sqrt(tf.abs(mult))\n",
    "\n",
    "    diff_update = tf.squeeze(mult, axis=-2)\n",
    "    diff_update = tf.concat([diff_update, point_sum], axis=-1)\n",
    "    return diff_update\n",
    "\n",
    "\n",
    "def Add_3VecCloud(central, neighbors):\n",
    "    \"\"\" Will add the 4vectors for the sum of 3 4-vectors with the invariant mass sum \"\"\"\n",
    "    num_batch = central.get_shape()[0]\n",
    "    num_point = central.get_shape()[1]\n",
    "    num_k = central.get_shape()[2]\n",
    "    num_dims = central.get_shape()[3]\n",
    "    point_diff = central - neighbors\n",
    "\n",
    "    identity = -np.identity(4, dtype=np.float32)  # 4vector\n",
    "    identity[0][0] = 1\n",
    "    identity = tf.expand_dims(identity, 0)\n",
    "    identity = tf.expand_dims(identity, 0)\n",
    "    identity = tf.expand_dims(identity, 0)\n",
    "    identity = tf.tile(identity, [num_batch, num_point, num_k, 1, 1])\n",
    "\n",
    "    sum_vec = neighbors + central\n",
    "    sum_vec = tf.concat([point_diff[:, :, 0:1, :], sum_vec[:, :, 1:, :]], -2)\n",
    "    sum_vec = sum_vec[:, :, :, 4:8]\n",
    "    point_sum = sum_vec\n",
    "    sum_vec = tf.expand_dims(sum_vec, -2)\n",
    "    sum_vec_T = tf.transpose(a=sum_vec, perm=[0, 1, 2, 4, 3])\n",
    "    mult = tf.matmul(sum_vec, identity)\n",
    "    mult = tf.matmul(mult, sum_vec_T)\n",
    "    mult = tf.sqrt(tf.abs(mult))\n",
    "\n",
    "    diff_update = tf.squeeze(mult, axis=-2)\n",
    "    diff_update = tf.concat([diff_update, point_sum], axis=-1)\n",
    "    return diff_update\n",
    "\n",
    "\n",
    "# def seq2seq_with_attention(inputs,\n",
    "#         hidden_size,\n",
    "#         scope,\n",
    "#         activation_fn=tf.nn.relu,\n",
    "#         bn=False,\n",
    "#         bn_decay=None,\n",
    "#         is_training=None):\n",
    "#     \"\"\" sequence model with attention.\n",
    "#        Args:\n",
    "#          inputs: 4-D tensor variable BxNxTxD\n",
    "#          hidden_size: int\n",
    "#          scope: encoder\n",
    "#          activation_fn: function\n",
    "#          bn: bool, whether to use batch norm\n",
    "#          bn_decay: float or float tensor variable in [0,1]\n",
    "#          is_training: bool Tensor variable\n",
    "#        Return:\n",
    "#          Variable Tensor BxNxD\n",
    "#        \"\"\"\n",
    "#     with tf.variable_scope(scope) as sc:\n",
    "#         batch_size = inputs.get_shape()[0]\n",
    "#         npoint = inputs.get_shape()[1]\n",
    "#         nstep = inputs.get_shape()[2]\n",
    "#         in_size = inputs.get_shape()[3]\n",
    "#         reshaped_inputs = tf.reshape(inputs, (-1, nstep, in_size))\n",
    "\n",
    "#         with tf.variable_scope('encoder'):\n",
    "#             #build encoder\n",
    "#             encoder_cell = tf.nn.rnn_cell.LSTMCell(hidden_size)\n",
    "#             encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell, reshaped_inputs,\n",
    "#                                                                sequence_length=tf.fill([batch_size*npoint], 4),\n",
    "#                                                                dtype=tf.float64, time_major=False)\n",
    "#         with tf.variable_scope('decoder'):\n",
    "#             #build decoder\n",
    "#             decoder_cell = tf.nn.rnn_cell.LSTMCell(hidden_size)\n",
    "#             decoder_inputs = tf.reshape(encoder_state.h, [batch_size*npoint, 1, hidden_size])\n",
    "\n",
    "#             # building attention mechanism: default Bahdanau\n",
    "#             # 'Bahdanau' style attention: https://arxiv.org/abs/1409.0473\n",
    "#             attention_mechanism = seq2seq.BahdanauAttention(num_units=hidden_size, memory=encoder_outputs)\n",
    "#             # 'Luong' style attention: https://arxiv.org/abs/1508.04025\n",
    "#             # attention_mechanism = seq2seq.LuongAttention(num_units=hidden_size, memory=encoder_outputs)\n",
    "\n",
    "#             # AttentionWrapper wraps RNNCell with the attention_mechanism\n",
    "#             decoder_cell = seq2seq.AttentionWrapper(cell=decoder_cell, attention_mechanism=attention_mechanism,\n",
    "#                                                               attention_layer_size=hidden_size)\n",
    "\n",
    "#             # Helper to feed inputs for training: read inputs from dense ground truth vectors\n",
    "#             train_helper = seq2seq.TrainingHelper(inputs=decoder_inputs, sequence_length=tf.fill([batch_size*npoint], 1),\n",
    "#                                                   time_major=False)\n",
    "#             decoder_initial_state = decoder_cell.zero_state(batch_size=batch_size*npoint, dtype=tf.float64)\n",
    "#             train_decoder = seq2seq.BasicDecoder(cell=decoder_cell, helper=train_helper, initial_state=decoder_initial_state, output_layer=None)\n",
    "#             decoder_outputs_train, decoder_last_state_train, decoder_outputs_length_train = seq2seq.dynamic_decode(\n",
    "#                 decoder=train_decoder, output_time_major=False, impute_finished=True)\n",
    "\n",
    "#         outputs = tf.reshape(decoder_last_state_train[0].h, (-1, npoint, hidden_size))\n",
    "#         if bn:\n",
    "#           outputs = batch_norm_for_fc(outputs, is_training, bn_decay, 'bn')\n",
    "\n",
    "#         if activation_fn is not None:\n",
    "#           outputs = activation_fn(outputs)\n",
    "#         return outputs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #import provider\n",
    "    import numpy as np\n",
    "\n",
    "    batch_size = 1\n",
    "    num_pt = 3\n",
    "    pos_dim = 5\n",
    "    k = 2\n",
    "    # wmass = ROOT.TH1D(\"wmass\",\"wmass\",100,0,6)\n",
    "    # test_feed = np.random.rand(batch_size, num_pt, pos_dim)\n",
    "    # pairwise_distanceR(pointclouds_pl)\n",
    "    a = np.array(\n",
    "        [\n",
    "            [\n",
    "                [-5.0, 5, 1, 2, 3],\n",
    "                [0, 0, 0, 0, 0],\n",
    "                [3, -3, 10, 11, 12],\n",
    "\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # a, b, c = provider.load_h5_data_label_seg(\"../data/ttbb/h5/test_files_ttbar.h5\")\n",
    "\n",
    "    batch_size = a.shape[0]\n",
    "    with tf.Graph().as_default():\n",
    "        pointclouds_pl = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, num_pt, pos_dim))\n",
    "        # nn_idx = tf.placeholder(tf.int32, shape=(batch_size, num_pt, k))\n",
    "        pair = pairwise_distanceR(pointclouds_pl[:, :, :3])\n",
    "        nn_idx = knn(pair, k=k)\n",
    "\n",
    "        # edge = get_edge_feature(pointclouds_pl, nn_idx, k=k,edge_type='sub')\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            sess.run(tf.compat.v1.global_variables_initializer())\n",
    "            feed_dict = {\n",
    "                pointclouds_pl: a,\n",
    "                # nn_idx: idx\n",
    "            }\n",
    "            # edges = sess.run([pair], feed_dict=feed_dict)\n",
    "            idxs, pairs = sess.run([nn_idx, pair], feed_dict=feed_dict)\n",
    "            print(idxs, pairs)\n",
    "            # for batch in edges:\n",
    "            #  for point in batch:\n",
    "            #    for mass in point:\n",
    "            #       wmass.Fill(mass[1][0])\n",
    "\n",
    "    # wmass.Draw()\n",
    "    # raw_input()\n",
    "    # pair = pairwise_distanceR(a)\n",
    "    # r = tf.greater_equal(tf.abs(a),2*np.pi)\n",
    "    # re = tf.where(r,4*np.pi**2-4*np.pi*tf.abs(a),a-a)\n",
    "    # print(a.get_shape())\n",
    "    # print(a.get_shape(),'a')\n",
    "    # at = tf.transpose(a,[0,2,1])\n",
    "    # print(at.get_shape(),'at')\n",
    "    # phi = at[:,1:,:]\n",
    "    # print(phi.get_shape()[2],'phi')\n",
    "    # phi5 = tf.tile(phi,[1,5,1])\n",
    "    # b =  tf.constant([[1],[2]])\n",
    "    # c = a + b\n",
    "    # print(pair.eval())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     [[[0 2]\n",
    ">       [2 0]\n",
    ">       [2 0]]] [[[   0.      1050.        66.94745]\n",
    ">       [1050.      2000.      1018.     ]\n",
    ">       [  66.94745 1018.         0.     ]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gapnet_classify.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "BASE_DIR = './'\n",
    "sys.path.append(os.path.dirname(BASE_DIR))\n",
    "sys.path.append(os.path.join(BASE_DIR, '../utils'))\n",
    "sys.path.append(os.path.join(BASE_DIR, '../models'))\n",
    "#import tf_util\n",
    "\n",
    "#from gat_layers import attn_feature\n",
    "\n",
    "\n",
    "def placeholder_inputs(batch_size, num_point, num_features):\n",
    "    pointclouds_pl = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, num_point, num_features))\n",
    "    labels_pl = tf.compat.v1.placeholder(tf.int32, shape=(batch_size))\n",
    "    return pointclouds_pl, labels_pl\n",
    "\n",
    "\n",
    "def gap_block(k, n_heads, nn_idx, net, point_cloud, edge_size, bn_decay, weight_decay, is_training, scname):\n",
    "    attns = []\n",
    "    local_features = []\n",
    "    for i in range(n_heads):\n",
    "        edge_feature, coefs, locals = attn_feature(net, edge_size[1], nn_idx, activation=tf.nn.relu,\n",
    "                                                   in_dropout=0.6,\n",
    "                                                   coef_dropout=0.6, is_training=is_training, bn_decay=bn_decay,\n",
    "                                                   layer='layer{0}'.format(edge_size[0]) + scname, k=k, i=i)\n",
    "        attns.append(edge_feature)  # This is the edge feature * att. coeff. activated by RELU, one per particle\n",
    "        local_features.append(locals)  # Those are the yij\n",
    "\n",
    "    neighbors_features = tf.concat(attns, axis=-1)\n",
    "    net = tf.squeeze(net)\n",
    "    neighbors_features = tf.concat([tf.expand_dims(point_cloud, -2), neighbors_features], axis=-1)\n",
    "\n",
    "    locals_transform = tf.reduce_max(input_tensor=tf.concat(local_features, axis=-1), axis=-2, keepdims=True)\n",
    "\n",
    "    return neighbors_features, locals_transform, coefs\n",
    "\n",
    "\n",
    "def get_model(point_cloud, is_training, num_class,\n",
    "              weight_decay=None, bn_decay=None, scname=''):\n",
    "    ''' input: BxNxF\n",
    "    output:BxNx(cats*segms)  '''\n",
    "    batch_size = point_cloud.get_shape()[0]\n",
    "    num_point = point_cloud.get_shape()[1]\n",
    "    num_feat = point_cloud.get_shape()[2]\n",
    "\n",
    "    k = 10\n",
    "    adj = pairwise_distanceR(point_cloud[:, :, :3])\n",
    "    n_heads = 1\n",
    "    nn_idx = knn(adj, k=k)\n",
    "\n",
    "    net, locals_transform, coefs = gap_block(k, n_heads, nn_idx, point_cloud, point_cloud, ('filter0', 16), bn_decay,\n",
    "                                             weight_decay, is_training, scname)\n",
    "\n",
    "    net = conv2d(net, 64, [1, 1], padding='VALID', stride=[1, 1], activation_fn=tf.nn.relu,\n",
    "                         bn=True, is_training=is_training, scope='gapnet01' + scname, bn_decay=bn_decay)\n",
    "    net01 = net\n",
    "\n",
    "    net = conv2d(net, 128, [1, 1], padding='VALID', stride=[1, 1], activation_fn=tf.nn.relu,\n",
    "                         bn=True, is_training=is_training, scope='gapnet02' + scname, bn_decay=bn_decay)\n",
    "\n",
    "    net02 = net\n",
    "    adj_matrix = pairwise_distance(net)\n",
    "    nn_idx = knn(adj_matrix, k=k)\n",
    "    adj_conv = nn_idx\n",
    "    n_heads = 1\n",
    "\n",
    "    net, locals_transform1, coefs2 = gap_block(k, n_heads, nn_idx, net, point_cloud, ('filter1', 128), bn_decay,\n",
    "                                               weight_decay, is_training, scname)\n",
    "\n",
    "    net = conv2d(net, 256, [1, 1], padding='VALID', stride=[1, 1], activation_fn=tf.nn.relu,\n",
    "                         bn=True, is_training=is_training, scope='gapnet11' + scname, bn_decay=bn_decay)\n",
    "    net11 = net\n",
    "\n",
    "    net = conv2d(net, 256, [1, 1], padding='VALID', stride=[1, 1], activation_fn=tf.nn.relu,\n",
    "                         bn=True, is_training=is_training, scope='gapnet12' + scname, bn_decay=bn_decay)\n",
    "\n",
    "    net12 = net\n",
    "\n",
    "    net = tf.concat([\n",
    "        net01,\n",
    "        net02,\n",
    "        net11,\n",
    "        net12,\n",
    "        locals_transform,\n",
    "        locals_transform1\n",
    "    ], axis=-1)\n",
    "\n",
    "    net = conv2d(net, 3, [1, 1], padding='VALID', stride=[1, 1],\n",
    "                         activation_fn=tf.nn.relu,\n",
    "                         bn=True, is_training=is_training, scope='agg' + scname, bn_decay=bn_decay)\n",
    "\n",
    "    net = avg_pool2d(net, [num_point, 1], padding='VALID', scope='avgpool' + scname)\n",
    "    max_pool = net\n",
    "\n",
    "    net = tf.reshape(net, [batch_size, -1])\n",
    "    net = fully_connected(net, 256, bn=True, is_training=is_training, activation_fn=tf.nn.relu,\n",
    "                                  scope='fc1' + scname, bn_decay=bn_decay)\n",
    "    net = fully_connected(net, 128, bn=True, is_training=is_training, activation_fn=tf.nn.relu,\n",
    "                                  scope='fc2' + scname, bn_decay=bn_decay)\n",
    "    net = fully_connected(net, num_class, activation_fn=None, scope='fc3' + scname)\n",
    "\n",
    "    net = tf.squeeze(net)\n",
    "\n",
    "    return net, max_pool\n",
    "\n",
    "\n",
    "def get_focal_loss(y_pred, label, num_class, gamma=4., alpha=10):\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "    epsilon = 1.e-9\n",
    "\n",
    "    labels = tf.one_hot(indices=label, depth=num_class)\n",
    "    y_true = tf.convert_to_tensor(value=labels, dtype=tf.float32)\n",
    "    y_pred = tf.convert_to_tensor(value=y_pred, dtype=tf.float32)\n",
    "\n",
    "    y_pred = tf.nn.softmax(y_pred - tf.reduce_max(input_tensor=y_pred, axis=1, keepdims=True))\n",
    "\n",
    "    model_out = tf.add(y_pred, epsilon)\n",
    "    ce = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "    weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))\n",
    "    fl = tf.multiply(alpha, tf.multiply(weight, ce))\n",
    "    reduced_fl = tf.reduce_max(input_tensor=fl, axis=1)\n",
    "    return tf.reduce_mean(input_tensor=reduced_fl)\n",
    "\n",
    "\n",
    "def get_loss_kmeans(max_pool, mu, max_dim, n_clusters, alpha=100):\n",
    "    list_dist = []\n",
    "    for i in range(0, n_clusters):\n",
    "        dist = f_func(tf.squeeze(max_pool), tf.reshape(mu[i, :], (1, max_dim)))\n",
    "        list_dist.append(dist)\n",
    "    stack_dist = tf.stack(list_dist)\n",
    "    min_dist = tf.reduce_min(input_tensor=list_dist, axis=0)\n",
    "\n",
    "    list_exp = []\n",
    "    for i in range(n_clusters):\n",
    "        exp = tf.exp(-alpha * (stack_dist[i] - min_dist))\n",
    "        list_exp.append(exp)\n",
    "\n",
    "    stack_exp = tf.stack(list_exp)\n",
    "    sum_exponentials = tf.reduce_sum(input_tensor=stack_exp, axis=0)\n",
    "\n",
    "    list_weighted_dist = []\n",
    "    for j in range(n_clusters):\n",
    "        softmax = stack_exp[j] / sum_exponentials\n",
    "        weighted_dist = stack_dist[j] * softmax\n",
    "\n",
    "        list_weighted_dist.append(weighted_dist)\n",
    "\n",
    "    stack_weighted_dist = tf.stack(list_weighted_dist)\n",
    "    kmeans_loss = tf.reduce_mean(input_tensor=tf.reduce_sum(input_tensor=stack_weighted_dist, axis=0))\n",
    "\n",
    "    return kmeans_loss, stack_dist\n",
    "\n",
    "\n",
    "def f_func(x, y):\n",
    "    dists = tf.square(x - y)\n",
    "    return tf.reduce_sum(input_tensor=dists, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provider.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "#BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "BASE_DIR = './'\n",
    "sys.path.append(BASE_DIR)\n",
    "\n",
    "\n",
    "# Download dataset for point cloud classification\n",
    "\n",
    "\n",
    "def shuffle_data(data, labels, global_pl=[], weights=[]):\n",
    "    \"\"\" Shuffle data and labels.\n",
    "    Input:\n",
    "      data: B,N,... numpy array\n",
    "      label: B,N, numpy array\n",
    "    Return:\n",
    "      shuffled data, label and shuffle indices\n",
    "  \"\"\"\n",
    "    idx = np.arange(len(labels))\n",
    "    np.random.shuffle(idx)\n",
    "    # return data[idx,:], labels[idx,:], idx\n",
    "    if global_pl != []:\n",
    "        return data[idx, :], labels[idx], global_pl[idx, :], idx\n",
    "    elif weights == []:\n",
    "        return data[idx, :], labels[idx], idx\n",
    "    else:\n",
    "        return data[idx, :], labels[idx], weights[idx], idx\n",
    "\n",
    "\n",
    "def rotate_point_cloud(batch_data):\n",
    "    \"\"\" Randomly rotate the point clouds to augument the dataset\n",
    "    rotation is per shape based along up direction\n",
    "    Input:\n",
    "      BxNx3 array, original batch of point clouds\n",
    "    Return:\n",
    "      BxNx3 array, rotated batch of point clouds\n",
    "  \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in xrange(batch_data.shape[0]):\n",
    "        rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, 0, sinval],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-sinval, 0, cosval]])\n",
    "        shape_pc = batch_data[k, ...]\n",
    "        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n",
    "    return rotated_data\n",
    "\n",
    "\n",
    "def rotate_point_cloud_by_angle(batch_data, rotation_angle):\n",
    "    \"\"\" Rotate the point cloud along up direction with certain angle.\n",
    "    Input:\n",
    "      BxNx3 array, original batch of point clouds\n",
    "    Return:\n",
    "      BxNx3 array, rotated batch of point clouds\n",
    "  \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in xrange(batch_data.shape[0]):\n",
    "        # rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, 0, sinval],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-sinval, 0, cosval]])\n",
    "        shape_pc = batch_data[k, ...]\n",
    "        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n",
    "    return rotated_data\n",
    "\n",
    "\n",
    "def rotate_perturbation_point_cloud(batch_data, angle_sigma=0.06, angle_clip=0.18):\n",
    "    \"\"\" Randomly perturb the point clouds by small rotations\n",
    "    Input:\n",
    "      BxNx3 array, original batch of point clouds\n",
    "    Return:\n",
    "      BxNx3 array, rotated batch of point clouds\n",
    "  \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in xrange(batch_data.shape[0]):\n",
    "        angles = np.clip(angle_sigma * np.random.randn(3), -angle_clip, angle_clip)\n",
    "        Rx = np.array([[1, 0, 0],\n",
    "                       [0, np.cos(angles[0]), -np.sin(angles[0])],\n",
    "                       [0, np.sin(angles[0]), np.cos(angles[0])]])\n",
    "        Ry = np.array([[np.cos(angles[1]), 0, np.sin(angles[1])],\n",
    "                       [0, 1, 0],\n",
    "                       [-np.sin(angles[1]), 0, np.cos(angles[1])]])\n",
    "        Rz = np.array([[np.cos(angles[2]), -np.sin(angles[2]), 0],\n",
    "                       [np.sin(angles[2]), np.cos(angles[2]), 0],\n",
    "                       [0, 0, 1]])\n",
    "        R = np.dot(Rz, np.dot(Ry, Rx))\n",
    "        shape_pc = batch_data[k, ...]\n",
    "        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), R)\n",
    "    return rotated_data\n",
    "\n",
    "\n",
    "def jitter_point_cloud(batch_data, sigma=0.01, clip=0.05):\n",
    "    \"\"\" Randomly jitter points. jittering is per point.\n",
    "    Input:\n",
    "      BxNx3 array, original batch of point clouds\n",
    "    Return:\n",
    "      BxNx3 array, jittered batch of point clouds\n",
    "  \"\"\"\n",
    "    B, N, C = batch_data.shape\n",
    "    assert (clip > 0)\n",
    "    jittered_data = np.clip(sigma * np.random.randn(B, N, C), -1 * clip, clip)\n",
    "    jittered_data += batch_data\n",
    "    return jittered_data\n",
    "\n",
    "\n",
    "def shift_point_cloud(batch_data, shift_range=0.1):\n",
    "    \"\"\" Randomly shift point cloud. Shift is per point cloud.\n",
    "    Input:\n",
    "      BxNx3 array, original batch of point clouds\n",
    "    Return:\n",
    "      BxNx3 array, shifted batch of point clouds\n",
    "  \"\"\"\n",
    "    B, N, C = batch_data.shape\n",
    "    shifts = np.random.uniform(-shift_range, shift_range, (B, 3))\n",
    "    for batch_index in range(B):\n",
    "        batch_data[batch_index, :, :] += shifts[batch_index, :]\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "def random_scale_point_cloud(batch_data, scale_low=0.8, scale_high=1.25):\n",
    "    \"\"\" Randomly scale the point cloud. Scale is per point cloud.\n",
    "    Input:\n",
    "      BxNx3 array, original batch of point clouds\n",
    "    Return:\n",
    "      BxNx3 array, scaled batch of point clouds\n",
    "  \"\"\"\n",
    "    B, N, C = batch_data.shape\n",
    "    scales = np.random.uniform(scale_low, scale_high, B)\n",
    "    for batch_index in range(B):\n",
    "        batch_data[batch_index, :, :] *= scales[batch_index]\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "def norm_inputs_point_cloud(data, cloud=True):\n",
    "    \"\"\" Normalize the input data by the mean of the distribution\"\"\"\n",
    "    if cloud:\n",
    "        NPOINTS = data.shape[1]\n",
    "        NFEATURES = data.shape[2]\n",
    "    else:\n",
    "        NPOINTS = data.shape[0]\n",
    "        NFEATURES = data.shape[1]\n",
    "    reshape = np.reshape(data, (-1, NFEATURES))\n",
    "    # scaler = StandardScaler()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(reshape[0::NPOINTS])\n",
    "    # print(scaler.mean_)\n",
    "    zero_arr = [0] * NFEATURES\n",
    "    zero_arr = scaler.transform([zero_arr])\n",
    "    reshape = scaler.transform(reshape)\n",
    "    for i in range(NFEATURES):\n",
    "        reshape[reshape == zero_arr[0][i]] = 0\n",
    "    if cloud:\n",
    "        reshape = np.reshape(reshape, (-1, NPOINTS, NFEATURES))\n",
    "        # print(reshape)\n",
    "    else:\n",
    "        reshape = np.reshape(reshape, (-1, NFEATURES))\n",
    "\n",
    "    print(\"Normalized the data\")\n",
    "    return reshape\n",
    "\n",
    "\n",
    "def getDataFiles(list_filename):\n",
    "    return [line.rstrip() for line in open(list_filename)]\n",
    "\n",
    "\n",
    "def load_add(h5_filename, names=[]):\n",
    "    f = h5py.File(h5_filename, 'r')\n",
    "    if len(names) == 0:\n",
    "        names = list(f.keys())\n",
    "        print(names)\n",
    "        names.remove('data')\n",
    "        names.remove('pid')\n",
    "        names.remove('label')\n",
    "\n",
    "    datasets = {}\n",
    "    for data in names:\n",
    "        datasets[data] = f[data][:]\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def load_h5(h5_filename, mode='seg', unsup=False, glob=False):\n",
    "    f = h5py.File(h5_filename, 'r')\n",
    "    data = f['data'][:]\n",
    "    # data = norm_inputs_point_cloud(data)\n",
    "    if mode == 'class':\n",
    "        label = f['pid'][:].astype(int)\n",
    "    elif mode == 'seg':\n",
    "        label = f['label'][:].astype(int)\n",
    "    else:\n",
    "        print('No mode found')\n",
    "    print(\"loaded {0} events\".format(len(data)))\n",
    "    if glob:\n",
    "        global_pl = f['global'][:]\n",
    "        return (data, label, global_pl)\n",
    "    else:\n",
    "        return (data, label)\n",
    "        # global_pl = norm_inputs_point_cloud(global_pl,cloud=False)\n",
    "\n",
    "\n",
    "def load_h5_weights(h5_filename):\n",
    "    f = h5py.File(h5_filename, 'r')\n",
    "    data = f['data'][:]\n",
    "    label = f['pid'][:]\n",
    "    weights = {}\n",
    "    for var in f.keys():\n",
    "        if 'w' in var:\n",
    "            weights[var] = f[var][:]\n",
    "\n",
    "    return (data, label, weights)\n",
    "\n",
    "\n",
    "def load_h5_eval(h5_filename):\n",
    "    f = h5py.File(h5_filename, 'r')\n",
    "    data = f['data'][:]\n",
    "    label = f['pid'][:]\n",
    "    weight_nom = np.abs(f['weight_nom'][:])\n",
    "    weight_up = np.abs(f['weight_up'][:])\n",
    "    weight_down = np.abs(f['weight_down'][:])\n",
    "    return (data, label, weight_nom, weight_up, weight_down)\n",
    "\n",
    "\n",
    "def loadDataFile(filename):\n",
    "    return load_h5(filename)\n",
    "\n",
    "\n",
    "def load_h5_data_label_seg(h5_filename):\n",
    "    f = h5py.File(h5_filename, 'r')\n",
    "    data = f['data'][:]  # (2048, 2048, 3)\n",
    "    # data = norm_inputs_point_cloud(data)\n",
    "    label = f['pid'][:]  # (2048, 1)\n",
    "    seg = f['label'][:]  # (2048, 2048)\n",
    "    print(\"loaded {0} events\".format(len(data)))\n",
    "\n",
    "    return (data, label, seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls 06_LHC/models\n",
    "ls 06_LHC/utils\n",
    "ls 06_LHC/h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     __pycache__\n",
    ">     gapnet_classify.py\n",
    ">     gapnet_seg.py\n",
    ">     gat_layers.py\n",
    ">     __pycache__\n",
    ">     provider.py\n",
    ">     tf_util.py\n",
    ">     evaluate_files_RD.txt\n",
    ">     evaluate_files_b1.txt\n",
    ">     evaluate_files_b2.txt\n",
    ">     evaluate_files_b3.txt\n",
    ">     evaluate_files_gwztop.txt\n",
    ">     evaluate_files_wztop.txt\n",
    ">     test_files_RD.txt\n",
    ">     test_files_b1.txt\n",
    ">     test_files_b2.txt\n",
    ">     test_files_b3.txt\n",
    ">     test_files_gwztop.txt\n",
    ">     test_files_wztop.txt\n",
    ">     test_multi_20v_100P.h5\n",
    ">     train_files_RD.txt\n",
    ">     train_files_b1.txt\n",
    ">     train_files_b2.txt\n",
    ">     train_files_b3.txt\n",
    ">     train_files_gwztop.txt\n",
    ">     train_files_wztop.txt\n",
    ">     train_multi_20v_100P.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "  \n",
    "  \n",
    "import argparse\n",
    "import h5py\n",
    "from math import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os, ast\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT SETTINGS\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('--gpu', type=int, default=0, help='GPUs to use [default: 0]')\n",
    "#parser.add_argument('--n_clusters', type=int, default=3, help='Number of clusters [Default: 3]')\n",
    "#parser.add_argument('--max_dim', type=int, default=3, help='Dimension of the encoding layer [Default: 3]')\n",
    "#parser.add_argument('--log_dir', default='log', help='Log dir [default: log]')\n",
    "#parser.add_argument('--batch', type=int, default=512, help='Batch Size  during training [default: 512]')\n",
    "#parser.add_argument('--num_point', type=int, default=100, help='Point Number [default: 100]')\n",
    "#parser.add_argument('--data_dir', default='../h5', help='directory with data [default: ../h5]')\n",
    "#parser.add_argument('--nfeat', type=int, default=8, help='Number of features [default: 8]')\n",
    "#parser.add_argument('--ncat', type=int, default=20, help='Number of categories [default: 20]')\n",
    "#parser.add_argument('--name', default=\"\", help='name of the output file')\n",
    "#parser.add_argument('--h5_folder', default=\"../h5/\", help='folder to store output files')\n",
    "#parser.add_argument('--full_train', default=False, action='store_true',\n",
    "#                    help='load full training results [default: False]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval():\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:' + str(FLAGSGPU)):\n",
    "            pointclouds_pl, labels_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT, NFEATURES)\n",
    "            batch = tf.Variable(0, trainable=False)\n",
    "            alpha = tf.compat.v1.placeholder(tf.float32, shape=())\n",
    "            is_training_pl = tf.compat.v1.placeholder(tf.bool, shape=())\n",
    "            pred, max_pool = get_model(pointclouds_pl, is_training=is_training_pl, num_class=NUM_CATEGORIES)\n",
    "            mu = tf.Variable(tf.zeros(shape=(N_CLUSTERS, MAX_DIM)), name=\"mu\",\n",
    "                             trainable=False)  # k centroids\n",
    "\n",
    "            classify_loss = get_focal_loss(pred, labels_pl, NUM_CATEGORIES)\n",
    "            kmeans_loss, stack_dist = get_loss_kmeans(max_pool, mu, MAX_DIM,\n",
    "                                                            N_CLUSTERS, alpha)\n",
    "\n",
    "            saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "        if FULL_TRAINING:\n",
    "            saver.restore(sess, os.path.join(LOG_DIR, 'cluster.ckpt-27000'))\n",
    "        else:\n",
    "            saver.restore(sess, os.path.join(LOG_DIR, 'model.ckpt'))\n",
    "        print('model restored')\n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'labels_pl': labels_pl,\n",
    "               'stack_dist': stack_dist,\n",
    "               'kmeans_loss': kmeans_loss,\n",
    "               'pred': pred,\n",
    "               'alpha': alpha,\n",
    "               'max_pool': max_pool,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'classify_loss': classify_loss, }\n",
    "\n",
    "        eval_one_epoch(sess, ops)\n",
    "\n",
    "def get_batch(data, label, start_idx, end_idx):\n",
    "    batch_label = label[start_idx:end_idx]\n",
    "    batch_data = data[start_idx:end_idx, :, :]\n",
    "    return batch_data, batch_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_epoch(sess, ops):\n",
    "    is_training = False\n",
    "\n",
    "    eval_idxs = np.arange(0, len(EVALUATE_FILES))\n",
    "    y_val = []\n",
    "    for fn in range(len(EVALUATE_FILES)):\n",
    "        current_file = os.path.join(H5_DIR, EVALUATE_FILES[eval_idxs[fn]])\n",
    "        current_data, current_label, current_cluster = load_h5_data_label_seg(current_file)\n",
    "        adds = load_add(current_file, ['masses'])\n",
    "\n",
    "        current_label = np.squeeze(current_label)\n",
    "\n",
    "        file_size = current_data.shape[0]\n",
    "        num_batches = file_size // BATCH_SIZE\n",
    "        num_batches = 5\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = (batch_idx + 1) * BATCH_SIZE\n",
    "\n",
    "            batch_data, batch_label = get_batch(current_data, current_label, start_idx, end_idx)\n",
    "            batch_cluster = current_cluster[start_idx:end_idx]\n",
    "            cur_batch_size = end_idx - start_idx\n",
    "\n",
    "            feed_dict = {ops['pointclouds_pl']: batch_data,\n",
    "                         ops['labels_pl']: batch_label,\n",
    "                         ops['alpha']: 1,  # No impact on evaluation,\n",
    "                         ops['is_training_pl']: is_training,\n",
    "                         }\n",
    "\n",
    "            loss, dist, max_pool = sess.run([ops['kmeans_loss'], ops['stack_dist'],\n",
    "                                             ops['max_pool']], feed_dict=feed_dict)\n",
    "            cluster_assign = np.zeros((cur_batch_size), dtype=int)\n",
    "            for i in range(cur_batch_size):\n",
    "                index_closest_cluster = np.argmin(dist[:, i])\n",
    "                cluster_assign[i] = index_closest_cluster\n",
    "\n",
    "            batch_cluster = np.array([np.where(r == 1)[0][0] for r in current_cluster[start_idx:end_idx]])\n",
    "\n",
    "            if len(y_val) == 0:\n",
    "                y_val = batch_cluster\n",
    "                y_assign = cluster_assign\n",
    "                y_pool = np.squeeze(max_pool)\n",
    "                y_mass = adds['masses'][start_idx:end_idx]\n",
    "            else:\n",
    "                y_val = np.concatenate((y_val, batch_cluster), axis=0)\n",
    "                y_assign = np.concatenate((y_assign, cluster_assign), axis=0)\n",
    "                y_pool = np.concatenate((y_pool, np.squeeze(max_pool)), axis=0)\n",
    "                y_mass = np.concatenate((y_mass, adds['masses'][start_idx:end_idx]), axis=0)\n",
    "\n",
    "    with h5py.File(os.path.join(H5_OUT, '{0}.h5'.format(NAME)), \"w\") as fh5:\n",
    "        dset = fh5.create_dataset(\"pid\", data=y_val)  # Real jet categories\n",
    "        dset = fh5.create_dataset(\"label\", data=y_assign)  # Cluster labeling\n",
    "        dset = fh5.create_dataset(\"max_pool\", data=y_pool)\n",
    "        dset = fh5.create_dataset(\"masses\", data=y_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "#BASE_DIR = os.path.dirname(os.path.abspath(__file__)) #FILE SHOULD BE SPECIFIED - H5 MODEL FILE?\n",
    "BASE_DIR = './'\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.dirname(BASE_DIR))\n",
    "sys.path.append(os.path.join(BASE_DIR, '..', 'models'))\n",
    "sys.path.append(os.path.join(BASE_DIR, '..', 'utils'))\n",
    "# from MVA_cfg import *\n",
    "#import provider\n",
    "#import gapnet_classify as MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLAGS = parser.parse_args()\n",
    "#LOG_DIR = os.path.join('..', 'logs', FLAGS.log_dir)\n",
    "#DATA_DIR = FLAGS.data_dir\n",
    "#H5_DIR = os.path.join(BASE_DIR, DATA_DIR)\n",
    "#H5_OUT = FLAGS.h5_folder\n",
    "#if not os.path.exists(H5_OUT): os.mkdir(H5_OUT)\n",
    "\n",
    "LOG_DIR = 'log'\n",
    "DATA_DIR = '../h5'\n",
    "H5_DIR = DATA_DIR\n",
    "H5_OUT = '../h5/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MAIN SCRIPT\n",
    "#NUM_POINT = FLAGS.num_point\n",
    "#BATCH_SIZE = FLAGS.batch\n",
    "#NFEATURES = FLAGS.nfeat\n",
    "#FULL_TRAINING = FLAGS.full_train\n",
    "\n",
    "NUM_POINT = 100\n",
    "BATCH_SIZE = 512\n",
    "NFEATURES = 8\n",
    "FULL_TRAINING = 'FALSE'\n",
    "N_CLUSTERS = 3\n",
    "MAX_DIM = 3\n",
    "NAME = 'OUTPUT.TXT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE_DIR = os.path.join(os.getcwd(), '06_LHC','scripts')  \n",
    "#os.path.dirname(os.path.abspath(__file__))\n",
    "#sys.path.append(BASE_DIR)\n",
    "#sys.path.append(os.path.join(BASE_DIR, '..', 'models'))\n",
    "\n",
    "#RUN THE ACTUAL EVALUATION HERE!\n",
    "H5_MODEL_PATH = '06_LHC/h5'\n",
    "\n",
    "#NUM_CATEGORIES = FLAGS.ncat\n",
    "NUM_CATEGORIES = 20\n",
    "FLAGSGPU = 0\n",
    "# Only used to get how many parts per category\n",
    "\n",
    "print('#### Batch Size : {0}'.format(BATCH_SIZE))\n",
    "print('#### Point Number: {0}'.format(NUM_POINT))\n",
    "print('#### Using GPUs: {0}'.format(FLAGSGPU))\n",
    "#print('#### Using GPUs: {0}'.format(FLAGS.gpu))\n",
    "\n",
    "print('### Starting evaluation')\n",
    "\n",
    "EVALUATE_FILES = getDataFiles(os.path.join(H5_MODEL_PATH, 'test_files_wztop.txt'))\n",
    "#EVALUATE_FILES = getDataFiles(os.path.join(H5_DIR, 'evaluate_files_wztop.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     #### Batch Size : 512\n",
    ">     #### Point Number: 100\n",
    ">     #### Using GPUs: 0\n",
    ">     ### Starting evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR='/dbfs/databricks/driver/06_LHC/logs/train/1608545228.287947/'\n",
    "#os.path.join(os.getcwd(), LOG_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[97]: '/dbfs/databricks/driver/06_LHC/logs/train/1608545228.287947/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%sh \n",
    "#ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     06_LHC\n",
    ">     conf\n",
    ">     derby.log\n",
    ">     eventlogs\n",
    ">     ganglia\n",
    ">     log\n",
    ">     logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists('log'):\n",
    "      os.makedirs('log')\n",
    "    eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     INFO:tensorflow:Restoring parameters from /dbfs/databricks/driver/06_LHC/logs/train/1608545228.287947/cluster.ckpt-27000\n",
    ">     model restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /dbfs/databricks/driver/06_LHC/logs/train/1608545228.287947/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     checkpoint\n",
    ">     cluster.ckpt-27000.data-00000-of-00002\n",
    ">     cluster.ckpt-27000.data-00001-of-00002\n",
    ">     cluster.ckpt-27000.index\n",
    ">     cluster.ckpt-27000.meta\n",
    ">     cluster.ckpt-28000.data-00000-of-00002\n",
    ">     cluster.ckpt-28000.data-00001-of-00002\n",
    ">     cluster.ckpt-28000.index\n",
    ">     cluster.ckpt-28000.meta\n",
    ">     cluster.ckpt-29000.data-00000-of-00002\n",
    ">     cluster.ckpt-29000.data-00001-of-00002\n",
    ">     cluster.ckpt-29000.index\n",
    ">     cluster.ckpt-29000.meta\n",
    ">     events.out.tfevents.1608545270.1120-144117-apses921-10-149-235-171\n",
    ">     events.out.tfevents.1608545270.1120-144117-apses921-10-149-242-155\n",
    ">     graph.pbtxt\n",
    ">     model.ckpt-27339.data-00000-of-00002\n",
    ">     model.ckpt-27339.data-00001-of-00002\n",
    ">     model.ckpt-27339.index\n",
    ">     model.ckpt-27339.meta\n",
    ">     model.ckpt-28778.data-00000-of-00002\n",
    ">     model.ckpt-28778.data-00001-of-00002\n",
    ">     model.ckpt-28778.index\n",
    ">     model.ckpt-28778.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls 06_LHC/h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     evaluate_files_RD.txt\n",
    ">     evaluate_files_b1.txt\n",
    ">     evaluate_files_b2.txt\n",
    ">     evaluate_files_b3.txt\n",
    ">     evaluate_files_gwztop.txt\n",
    ">     evaluate_files_wztop.txt\n",
    ">     test_files_RD.txt\n",
    ">     test_files_b1.txt\n",
    ">     test_files_b2.txt\n",
    ">     test_files_b3.txt\n",
    ">     test_files_gwztop.txt\n",
    ">     test_files_wztop.txt\n",
    ">     test_multi_20v_100P.h5\n",
    ">     train_files_RD.txt\n",
    ">     train_files_b1.txt\n",
    ">     train_files_b2.txt\n",
    ">     train_files_b3.txt\n",
    ">     train_files_gwztop.txt\n",
    ">     train_files_wztop.txt\n",
    ">     train_multi_20v_100P.h5"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
