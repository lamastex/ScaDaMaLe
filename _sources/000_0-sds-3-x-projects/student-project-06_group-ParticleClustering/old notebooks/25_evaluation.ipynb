{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls 06_LHC/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     __pycache__\n",
    ">     gapnet_classify.py\n",
    ">     gapnet_seg.py\n",
    ">     gat_layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import Namespace\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import os, ast\n",
    "import sys\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import json\n",
    "from math import *\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "#BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "BASE_DIR = os.path.join(os.getcwd(), '06_LHC','scripts') \n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.dirname(BASE_DIR))\n",
    "sys.path.append(os.path.join(BASE_DIR,'..', 'models'))\n",
    "sys.path.append(os.path.join(BASE_DIR,'..' ,'utils'))\n",
    "#from MVA_cfg import *\n",
    "import provider\n",
    "import gapnet_classify as MODEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DEFAULT SETTINGS\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu', type=int, default=0, help='GPUs to use [default: 0]')\n",
    "parser.add_argument('--n_clusters', type=int, default=3, help='Number of clusters [Default: 3]')\n",
    "parser.add_argument('--max_dim', type=int, default=3, help='Dimension of the encoding layer [Default: 3]')\n",
    "parser.add_argument('--log_dir', default='log', help='Log dir [default: log]')\n",
    "parser.add_argument('--batch', type=int, default=512, help='Batch Size  during training [default: 512]')\n",
    "parser.add_argument('--num_point', type=int, default=100, help='Point Number [default: 100]')\n",
    "parser.add_argument('--data_dir', default='../h5', help='directory with data [default: ../h5]')\n",
    "parser.add_argument('--nfeat', type=int, default=8, help='Number of features [default: 8]')\n",
    "parser.add_argument('--ncat', type=int, default=20, help='Number of categories [default: 20]')\n",
    "parser.add_argument('--name', default=\"\", help='name of the output file')\n",
    "parser.add_argument('--h5_folder', default=\"../h5/\", help='folder to store output files')\n",
    "parser.add_argument('--full_train',  default=False, action='store_true',help='load full training results [default: False]')\n",
    "FLAGS = parser.parse_args()\n",
    "print('args12')\n",
    "\n",
    "LOG_DIR = os.path.join('..','logs',FLAGS.log_dir)\n",
    "DATA_DIR = FLAGS.data_dir\n",
    "H5_DIR = os.path.join(BASE_DIR, DATA_DIR)\n",
    "H5_OUT = FLAGS.h5_folder\n",
    "if not os.path.exists(H5_OUT): os.mkdir(H5_OUT)  \n",
    "\n",
    "# MAIN SCRIPT\n",
    "NUM_POINT = FLAGS.num_point\n",
    "BATCH_SIZE = FLAGS.batch\n",
    "NFEATURES = FLAGS.nfeat\n",
    "FULL_TRAINING = FLAGS.full_train\n",
    "\n",
    "\n",
    "NUM_CATEGORIES = FLAGS.ncat\n",
    "#Only used to get how many parts per category\n",
    "\n",
    "print('#### Batch Size : {0}'.format(BATCH_SIZE))\n",
    "print('#### Point Number: {0}'.format(NUM_POINT))\n",
    "print('#### Using GPUs: {0}'.format(FLAGS.gpu))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "print('### Starting evaluation')\n",
    "\n",
    "\n",
    "EVALUATE_FILES = provider.getDataFiles(os.path.join(H5_DIR, 'evaluate_files_wztop.txt')) \n",
    "\n",
    "  \n",
    "def eval():\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:'+str(FLAGS.gpu)):\n",
    "            pointclouds_pl,  labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT,NFEATURES) \n",
    "            batch = tf.Variable(0, trainable=False)\n",
    "            alpha = tf.placeholder(tf.float32, shape=())\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "            pred,max_pool = MODEL.get_model(pointclouds_pl, is_training=is_training_pl,num_class=NUM_CATEGORIES)\n",
    "            mu = tf.Variable(tf.zeros(shape=(FLAGS.n_clusters,FLAGS.max_dim)),name=\"mu\",trainable=False) #k centroids\n",
    "            \n",
    "\n",
    "            classify_loss = MODEL.get_focal_loss(pred, labels_pl,NUM_CATEGORIES)\n",
    "            kmeans_loss, stack_dist= MODEL.get_loss_kmeans(max_pool,mu, FLAGS.max_dim,\n",
    "                                                           FLAGS.n_clusters,alpha)\n",
    "\n",
    "            \n",
    "            saver = tf.train.Saver()\n",
    "          \n",
    "\n",
    "    \n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        if FULL_TRAINING:\n",
    "            saver.restore(sess,os.path.join(LOG_DIR,'cluster.ckpt'))\n",
    "        else:\n",
    "            saver.restore(sess,os.path.join(LOG_DIR,'model.ckpt'))\n",
    "        print('model restored')\n",
    "        \n",
    "        \n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'labels_pl': labels_pl,\n",
    "               'stack_dist':stack_dist,\n",
    "               'kmeans_loss':kmeans_loss,\n",
    "               'pred': pred,\n",
    "               'alpha': alpha,\n",
    "               'max_pool': max_pool,\n",
    "               'is_training_pl':is_training_pl,\n",
    "               'classify_loss': classify_loss,}\n",
    "            \n",
    "        eval_one_epoch(sess,ops)\n",
    "\n",
    "def get_batch(data,label,start_idx, end_idx):\n",
    "    batch_label = label[start_idx:end_idx]\n",
    "    batch_data = data[start_idx:end_idx,:,:]\n",
    "    return batch_data, batch_label\n",
    "\n",
    "        \n",
    "def eval_one_epoch(sess,ops):\n",
    "    is_training = False\n",
    "\n",
    "    eval_idxs = np.arange(0, len(EVALUATE_FILES))\n",
    "    y_val = []\n",
    "    for fn in range(len(EVALUATE_FILES)):\n",
    "        current_file = os.path.join(H5_DIR,EVALUATE_FILES[eval_idxs[fn]])\n",
    "        current_data, current_label, current_cluster = provider.load_h5_data_label_seg(current_file)\n",
    "        adds = provider.load_add(current_file,['masses'])\n",
    "\n",
    "        current_label = np.squeeze(current_label)\n",
    "        \n",
    "        file_size = current_data.shape[0]\n",
    "        num_batches = file_size // BATCH_SIZE\n",
    "        num_batches = 5\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "\n",
    "            batch_data, batch_label = get_batch(current_data, current_label,start_idx, end_idx)\n",
    "            batch_cluster = current_cluster[start_idx:end_idx]\n",
    "            cur_batch_size = end_idx-start_idx\n",
    "\n",
    "\n",
    "            feed_dict = {ops['pointclouds_pl']: batch_data,\n",
    "                         ops['labels_pl']: batch_label,\n",
    "                         ops['alpha']: 1, #No impact on evaluation,\n",
    "                         ops['is_training_pl']: is_training,\n",
    "            }\n",
    "\n",
    "            loss, dist,max_pool = sess.run([ops['kmeans_loss'], ops['stack_dist'],\n",
    "                                            ops['max_pool']],feed_dict=feed_dict)\n",
    "            cluster_assign = np.zeros((cur_batch_size), dtype=int)\n",
    "            for i in range(cur_batch_size):\n",
    "                index_closest_cluster = np.argmin(dist[:, i])\n",
    "                cluster_assign[i] = index_closest_cluster\n",
    "\n",
    "            batch_cluster = np.array([np.where(r==1)[0][0] for r in current_cluster[start_idx:end_idx]])\n",
    "\n",
    "            if len(y_val)==0:      \n",
    "                y_val=batch_cluster\n",
    "                y_assign=cluster_assign\n",
    "                y_pool=np.squeeze(max_pool)\n",
    "                y_mass = adds['masses'][start_idx:end_idx]\n",
    "            else:\n",
    "                y_val=np.concatenate((y_val,batch_cluster),axis=0)\n",
    "                y_assign=np.concatenate((y_assign,cluster_assign),axis=0)\n",
    "                y_pool=np.concatenate((y_pool,np.squeeze(max_pool)),axis=0)\n",
    "                y_mass=np.concatenate((y_mass,adds['masses'][start_idx:end_idx]),axis=0)\n",
    "                            \n",
    "    with h5py.File(os.path.join(H5_OUT,'{0}.h5'.format(FLAGS.name)), \"w\") as fh5:\n",
    "        dset = fh5.create_dataset(\"pid\", data=y_val) #Real jet categories\n",
    "        dset = fh5.create_dataset(\"label\", data=y_assign) #Cluster labeling\n",
    "        dset = fh5.create_dataset(\"max_pool\", data=y_pool)        \n",
    "        dset = fh5.create_dataset(\"masses\", data=y_mass)\n",
    "\n",
    "################################################          \n",
    "    \n",
    "\n",
    "if __name__=='__main__':\n",
    "  eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     args\n",
    ">     args1\n",
    ">     args2\n",
    ">     args3\n",
    ">     args4\n",
    ">     args5\n",
    ">     args6\n",
    ">     args7\n",
    ">     args8\n",
    ">     args9\n",
    ">     args10\n",
    ">     args11\n",
    ">     usage: PythonShell.py [-h] [--gpu GPU] [--n_clusters N_CLUSTERS]\n",
    ">                           [--max_dim MAX_DIM] [--log_dir LOG_DIR] [--batch BATCH]\n",
    ">                           [--num_point NUM_POINT] [--data_dir DATA_DIR]\n",
    ">                           [--nfeat NFEAT] [--ncat NCAT] [--name NAME]\n",
    ">                           [--h5_folder H5_FOLDER] [--full_train]\n",
    ">     PythonShell.py: error: unrecognized arguments: 33093 0 50000 1139 b02935da423b4a2ea2408dacb76e2946 3.0.1 8265200fc2a0af85ce368cc256f464c6be291e8744bb5ea6fcfc8dfea267cd42 unpinned"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
