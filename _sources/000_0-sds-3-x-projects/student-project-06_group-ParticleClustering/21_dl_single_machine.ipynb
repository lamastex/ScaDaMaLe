{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will modified the files for TF v2.x of our code and\n",
    "run them on a single machine using cells.\n",
    "\n",
    "First: Check if the data is in local. If not, go to notebook\n",
    "`1_data_and_preprocessing` and download the data from dbfs to local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls 06_LHC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     LICENSE\n",
    ">     README.md\n",
    ">     data\n",
    ">     h5\n",
    ">     models\n",
    ">     scripts\n",
    ">     utils\n",
    "\n",
    "  \n",
    "\n",
    "Get the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import Namespace\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import os\n",
    "import sys\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "#Added matplot for accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(edgeitems=1000)\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "BASE_DIR = os.path.join(os.getcwd(), '06_LHC','scripts')  \n",
    "#os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, '..', 'models'))\n",
    "sys.path.append(os.path.join(BASE_DIR, '..', 'utils'))\n",
    "import provider\n",
    "import gapnet_classify as MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Get the input parameters. Necessary modifications: - import arguements\n",
    "by a namespace; not by an argument parser - change the data directories\n",
    "appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parserdict = {'max_dim': 3, #help='Dimension of the encoding layer [Default: 3]')\n",
    "              'n_clusters': 3, #help='Number of clusters [Default: 3]')\n",
    "              'gpu': 0, #help='GPU to use [default: GPU 0]')\n",
    "              'model': 'gapnet_clasify', #help='Model name [default: gapnet_classify]')\n",
    "              'log_dir': 'log', #help='Log dir [default: log]')\n",
    "              'num_point': 100, #help='Point Number [default: 100]')\n",
    "              'max_epoch': 100, #help='Epoch to run [default: 200]')\n",
    "              'epochs_pretrain': 10, #help='Epochs to for pretraining [default: 10]')\n",
    "              'batch_size': 512, #help='Batch Size during training [default: 512]')\n",
    "              'learning_rate': 0.001, #help='Initial learning rate [default: 0.01]')\n",
    "\n",
    "              'momentum': 0.9, #help='Initial momentum [default: 0.9]')\n",
    "              'optimizer': 'adam', #help='adam or momentum [default: adam]')\n",
    "              'decay_step': 500000, #help='Decay step for lr decay [default: 500000]')\n",
    "              'wd': 0.0, #help='Weight Decay [Default: 0.0]')\n",
    "              'decay_rate': 0.5, #help='Decay rate for lr decay [default: 0.5]')\n",
    "              'output_dir': 'train_results', #help='Directory that stores all training logs and trained models')\n",
    "              'data_dir': os.path.join(os.getcwd(),'06_LHC', 'h5'), # '../h5', #help='directory with data [default: hdf5_data]')\n",
    "              'nfeat': 8, #help='Number of features [default: 8]')\n",
    "              'ncat': 20, #help='Number of categories [default: 20]')\n",
    "             }\n",
    "\n",
    "FLAGS = Namespace(**parserdict)\n",
    "H5_DIR = FLAGS.data_dir\n",
    "\n",
    "EPOCH_CNT = 0\n",
    "MAX_PRETRAIN = FLAGS.epochs_pretrain\n",
    "BATCH_SIZE = FLAGS.batch_size\n",
    "NUM_POINT = FLAGS.num_point\n",
    "NUM_FEAT = FLAGS.nfeat\n",
    "NUM_CLASSES = FLAGS.ncat\n",
    "MAX_EPOCH = FLAGS.max_epoch\n",
    "BASE_LEARNING_RATE = FLAGS.learning_rate\n",
    "GPU_INDEX = FLAGS.gpu\n",
    "MOMENTUM = FLAGS.momentum\n",
    "OPTIMIZER = FLAGS.optimizer\n",
    "DECAY_STEP = FLAGS.decay_step\n",
    "DECAY_RATE = FLAGS.decay_rate\n",
    "\n",
    "# MODEL = importlib.import_module(FLAGS.model) # import network module\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'models', FLAGS.model + '.py')\n",
    "LOG_DIR = os.path.join('..', 'logs', FLAGS.log_dir)\n",
    "\n",
    "if not os.path.exists(LOG_DIR): os.makedirs(LOG_DIR)\n",
    "os.system('cp %s.py %s' % (MODEL_FILE, LOG_DIR))  # bkp of model def\n",
    "os.system('cp train_kmeans.py %s' % (LOG_DIR))  # bkp of train procedure\n",
    "\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99\n",
    "\n",
    "LEARNING_RATE_CLIP = 1e-5\n",
    "HOSTNAME = socket.gethostname()\n",
    "\n",
    "TRAIN_FILES = provider.getDataFiles(os.path.join(H5_DIR, 'train_files_wztop.txt'))\n",
    "TEST_FILES = provider.getDataFiles(os.path.join(H5_DIR, 'test_files_wztop.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Define the utils functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate(batch):\n",
    "    learning_rate = tf.compat.v1.train.exponential_decay(\n",
    "        BASE_LEARNING_RATE,  # Base learning rate.\n",
    "        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "        DECAY_STEP,  # Decay step.\n",
    "        DECAY_RATE,  # Decay rate.\n",
    "        staircase=True)\n",
    "    learning_rate = tf.maximum(learning_rate, LEARNING_RATE_CLIP)  # CLIP THE LEARNING RATE!\n",
    "    return learning_rate\n",
    "\n",
    "\n",
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.compat.v1.train.exponential_decay(\n",
    "        BN_INIT_DECAY,\n",
    "        batch * BATCH_SIZE,\n",
    "        BN_DECAY_DECAY_STEP,\n",
    "        BN_DECAY_DECAY_RATE,\n",
    "        staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Main training function: - calls the training one epoch - calls the\n",
    "evaluation one epoch - selects the loss and optimizer - save the final\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:' + str(GPU_INDEX)):\n",
    "            #ADDED THIS TO RECORD ACCURACY\n",
    "            epochs_acc = []\n",
    "            pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT, NUM_FEAT)\n",
    "\n",
    "            is_training_pl = tf.compat.v1.placeholder(tf.bool, shape=())\n",
    "\n",
    "            # Note the global_step=batch parameter to minimize.\n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            alpha = tf.compat.v1.placeholder(dtype=tf.float32, shape=())\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.compat.v1.summary.scalar('bn_decay', bn_decay)\n",
    "            print(\"--- Get model and loss\")\n",
    "\n",
    "            pred, max_pool = MODEL.get_model(pointclouds_pl, is_training=is_training_pl,\n",
    "                                             bn_decay=bn_decay,\n",
    "                                             num_class=NUM_CLASSES, weight_decay=FLAGS.wd,\n",
    "                                             )\n",
    "\n",
    "            class_loss = MODEL.get_focal_loss(pred, labels_pl, NUM_CLASSES)\n",
    "            mu = tf.Variable(tf.zeros(shape=(FLAGS.n_clusters, FLAGS.max_dim)), name=\"mu\",\n",
    "                             trainable=True)  # k centroids\n",
    "            kmeans_loss, stack_dist = MODEL.get_loss_kmeans(max_pool, mu, FLAGS.max_dim,\n",
    "                                                            FLAGS.n_clusters, alpha)\n",
    "\n",
    "            full_loss = kmeans_loss + class_loss\n",
    "\n",
    "            print(\"--- Get training operator\")\n",
    "            # Get training operator\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.compat.v1.summary.scalar('learning_rate', learning_rate)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "            train_op_full = optimizer.minimize(full_loss, global_step=batch)\n",
    "            train_op = optimizer.minimize(class_loss, global_step=batch)\n",
    "\n",
    "            # Add ops to save and restore all the variables.\n",
    "            saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "        # Create a session\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = False\n",
    "        sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        # Add summary writers\n",
    "        merged = tf.compat.v1.summary.merge_all()\n",
    "        train_writer = tf.compat.v1.summary.FileWriter(os.path.join(LOG_DIR, 'train'), sess.graph)\n",
    "        test_writer = tf.compat.v1.summary.FileWriter(os.path.join(LOG_DIR, 'test'), sess.graph)\n",
    "\n",
    "        # Init variables\n",
    "        print(\"Total number of weights for the model: \",\n",
    "              np.sum([np.prod(v.get_shape().as_list()) for v in tf.compat.v1.trainable_variables()]))\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'labels_pl': labels_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'max_pool': max_pool,\n",
    "               'pred': pred,\n",
    "               'alpha': alpha,\n",
    "               'mu': mu,\n",
    "               'stack_dist': stack_dist,\n",
    "               'class_loss': class_loss,\n",
    "               'kmeans_loss': kmeans_loss,\n",
    "               'train_op': train_op,\n",
    "               'train_op_full': train_op_full,\n",
    "               'merged': merged,\n",
    "               'step': batch,\n",
    "               'learning_rate': learning_rate\n",
    "               }\n",
    "\n",
    "        for epoch in range(MAX_EPOCH):\n",
    "            print('\\n**** EPOCH %03d ****' % (epoch))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_full_training = epoch > MAX_PRETRAIN\n",
    "            max_pool = train_one_epoch(sess, ops, train_writer, is_full_training)\n",
    "            if epoch == MAX_PRETRAIN:\n",
    "                centers = KMeans(n_clusters=FLAGS.n_clusters).fit(np.squeeze(max_pool))\n",
    "                centers = centers.cluster_centers_\n",
    "                sess.run(tf.compat.v1.assign(mu, centers))\n",
    "\n",
    "            #eval_one_epoch(sess, ops, test_writer, is_full_training)\n",
    "            #Added this line to record accuracy\n",
    "            epoch_acc = eval_one_epoch(sess, ops, test_writer, is_full_training)\n",
    "            epochs_acc.append(epoch_acc)\n",
    "            if is_full_training:\n",
    "                save_path = saver.save(sess, os.path.join(LOG_DIR, 'cluster.ckpt'))\n",
    "            else:\n",
    "                save_path = saver.save(sess, os.path.join(LOG_DIR, 'model.ckpt'))\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "        plt.plot(epochs_acc)\n",
    "        plt.ylabel('Validation accuracy')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.show()\n",
    "        plt.savefig('single_maching.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Training utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, label, start_idx, end_idx):\n",
    "    batch_label = label[start_idx:end_idx]\n",
    "    batch_data = data[start_idx:end_idx, :, :]\n",
    "    return batch_data, batch_label\n",
    "\n",
    "\n",
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = linear_sum_assignment(w.max() - w)\n",
    "    ind = np.asarray(ind)\n",
    "    ind = np.transpose(ind)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "One epoch training and evaluation functions. - loop over all batches in\n",
    "the training (or evaluation) dataloader. - prints training (and\n",
    "evaluation) loss. - switches loss after n=10 epochs. Reason is that the\n",
    "full loss requires an initial guess for the position of the cluster\n",
    "centroids. - prints training (and evaluation) cluster accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(sess, ops, train_writer, is_full_training):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = True\n",
    "\n",
    "    train_idxs = np.arange(0, len(TRAIN_FILES))\n",
    "\n",
    "    acc = loss_sum = 0\n",
    "    y_pool = []\n",
    "    for fn in range(len(TRAIN_FILES)):\n",
    "        # print('----' + str(fn) + '-----')\n",
    "        current_file = os.path.join(H5_DIR, TRAIN_FILES[train_idxs[fn]])\n",
    "        current_data, current_label, current_cluster = provider.load_h5_data_label_seg(current_file)\n",
    "\n",
    "        current_label = np.squeeze(current_label)\n",
    "\n",
    "        file_size = current_data.shape[0]\n",
    "        num_batches = file_size // BATCH_SIZE\n",
    "        # num_batches = 5\n",
    "        print(str(datetime.now()))\n",
    "\n",
    "        # initialise progress bar\n",
    "        process_desc = \"TRAINING: Loss {:2.3e}\"\n",
    "        progress_bar = tqdm(initial=0, leave=True, total=num_batches,\n",
    "                            desc=process_desc.format(0),\n",
    "                            position=0)\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = (batch_idx + 1) * BATCH_SIZE\n",
    "            batch_data, batch_label = get_batch(current_data, current_label, start_idx, end_idx)\n",
    "            cur_batch_size = end_idx - start_idx\n",
    "\n",
    "            # print(batch_weight)\n",
    "            feed_dict = {ops['pointclouds_pl']: batch_data,\n",
    "                         ops['labels_pl']: batch_label,\n",
    "                         ops['is_training_pl']: is_training,\n",
    "                         ops['alpha']: 2 * (EPOCH_CNT - MAX_PRETRAIN + 1),}\n",
    "            if is_full_training:\n",
    "                summary, step, _, loss_val, dist, lr = sess.run([ops['merged'], ops['step'],\n",
    "                                                                 ops['train_op_full'], ops['kmeans_loss'],\n",
    "                                                                 ops['stack_dist'], ops['learning_rate']],\n",
    "                                                                feed_dict=feed_dict)\n",
    "\n",
    "                batch_cluster = np.array([np.where(r == 1)[0][0] for r in current_cluster[start_idx:end_idx]])\n",
    "                cluster_assign = np.zeros((cur_batch_size), dtype=int)\n",
    "\n",
    "                for i in range(cur_batch_size):\n",
    "                    index_closest_cluster = np.argmin(dist[:, i])\n",
    "                    cluster_assign[i] = index_closest_cluster\n",
    "\n",
    "                acc += cluster_acc(batch_cluster, cluster_assign)\n",
    "            else:\n",
    "                summary, step, _, loss_val, max_pool, lr = sess.run([ops['merged'], ops['step'],\n",
    "                                                                     ops['train_op'], ops['class_loss'],\n",
    "                                                                     ops['max_pool'], ops['learning_rate']],\n",
    "                                                                    feed_dict=feed_dict)\n",
    "\n",
    "                if len(y_pool) == 0:\n",
    "                    y_pool = np.squeeze(max_pool)\n",
    "                else:\n",
    "                    y_pool = np.concatenate((y_pool, np.squeeze(max_pool)), axis=0)\n",
    "\n",
    "            loss_sum += np.mean(loss_val)\n",
    "\n",
    "            train_writer.add_summary(summary, step)\n",
    "\n",
    "            # Update train bar\n",
    "            process_desc.format(loss_val)\n",
    "            progress_bar.update(1)\n",
    "        progress_bar.close()\n",
    "\n",
    "    print('learning rate: %f' % (lr))\n",
    "    print('train mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "    print('train clustering accuracy: %f' % (acc / float(num_batches)))\n",
    "    return y_pool\n",
    "\n",
    "\n",
    "def eval_one_epoch(sess, ops, test_writer, is_full_training):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    global EPOCH_CNT\n",
    "    is_training = False\n",
    "    test_idxs = np.arange(0, len(TEST_FILES))\n",
    "    # Test on all data: last batch might be smaller than BATCH_SIZE\n",
    "    loss_sum = acc = 0\n",
    "    acc_kmeans = 0\n",
    "\n",
    "    for fn in range(len(TEST_FILES)):\n",
    "        # print('----' + str(fn) + '-----')\n",
    "        current_file = os.path.join(H5_DIR, TEST_FILES[test_idxs[fn]])\n",
    "        current_data, current_label, current_cluster = provider.load_h5_data_label_seg(current_file)\n",
    "        current_label = np.squeeze(current_label)\n",
    "\n",
    "        file_size = current_data.shape[0]\n",
    "        num_batches = file_size // BATCH_SIZE\n",
    "        \n",
    "        process_desc = \"VALIDATION: Loss {:2.3e}\"\n",
    "        progress_bar = tqdm(initial=0, leave=True, total=num_batches,\n",
    "                        desc=process_desc.format(0),\n",
    "                        position=0)\n",
    "        # num_batches = 5\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = (batch_idx + 1) * BATCH_SIZE\n",
    "            batch_data, batch_label = get_batch(current_data, current_label, start_idx, end_idx)\n",
    "            cur_batch_size = end_idx - start_idx\n",
    "\n",
    "            feed_dict = {ops['pointclouds_pl']: batch_data,\n",
    "                         ops['is_training_pl']: is_training,\n",
    "                         ops['labels_pl']: batch_label,\n",
    "                         ops['alpha']: 2 * (EPOCH_CNT - MAX_PRETRAIN + 1),}\n",
    "\n",
    "            if is_full_training:\n",
    "                summary, step, loss_val, max_pool, dist, mu = sess.run([ops['merged'], ops['step'],\n",
    "                                                                        ops['kmeans_loss'],\n",
    "                                                                        ops['max_pool'], ops['stack_dist'],\n",
    "                                                                        ops['mu']],\n",
    "                                                                       feed_dict=feed_dict)\n",
    "                if batch_idx == 0:\n",
    "                    print(\"mu: {}\".format(mu))\n",
    "                batch_cluster = np.array([np.where(r == 1)[0][0] for r in current_cluster[start_idx:end_idx]])\n",
    "                cluster_assign = np.zeros((cur_batch_size), dtype=int)\n",
    "                for i in range(cur_batch_size):\n",
    "                    index_closest_cluster = np.argmin(dist[:, i])\n",
    "                    cluster_assign[i] = index_closest_cluster\n",
    "\n",
    "                acc += cluster_acc(batch_cluster, cluster_assign)\n",
    "\n",
    "            else:\n",
    "                summary, step, loss_val = sess.run([ops['merged'], ops['step'],\n",
    "                                                    ops['class_loss']],\n",
    "                                                   feed_dict=feed_dict)\n",
    "\n",
    "            test_writer.add_summary(summary, step)\n",
    "\n",
    "            loss_sum += np.mean(loss_val)\n",
    "            \n",
    "            # Update train bar\n",
    "            process_desc.format(loss_val)\n",
    "            progress_bar.update(1)\n",
    "        progress_bar.close()\n",
    "\n",
    "    total_loss = loss_sum * 1.0 / float(num_batches)\n",
    "    print('test mean loss: %f' % (total_loss))\n",
    "    print('testing clustering accuracy: %f' % (acc / float(num_batches)))\n",
    "    \n",
    "    #Added this to save accuracy\n",
    "    return acc/float(num_batches)\n",
    "    \n",
    "    \n",
    "    EPOCH_CNT += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Result: - We see that the learning on a single machine works. - One\n",
    "epoch takes about XXXXXXX - Resulting accuracy lies at about XXXXX"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
