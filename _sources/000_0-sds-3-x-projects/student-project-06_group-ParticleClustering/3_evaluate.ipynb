{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "In this notebook we evaluate the trained model on new data. The data has\n",
    "already been downloaded in notebook 11 and is stored in the h5 directory\n",
    "together with the training data. At the end of this notebook we will\n",
    "have the clustering accuracy of the model on this new data.\n",
    "\n",
    "Import packages needed for the script and set the correct paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import Namespace\n",
    "from math import *\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os, ast\n",
    "import sys\n",
    "import socket\n",
    "from sklearn.cluster import KMeans\n",
    "np.set_printoptions(edgeitems=1000)\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "BASE_DIR = os.path.join(os.getcwd(), '06_LHC','scripts')  \n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, '..', 'models'))\n",
    "sys.path.append(os.path.join(BASE_DIR, '..', 'utils'))\n",
    "import provider\n",
    "import gapnet_classify as MODEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parserdict = {'gpu':0, #help='GPUs to use [default: 0]')\n",
    "              'n_clusters':3,# type=int, default=3, #help='Number of clusters [Default: 3]')\n",
    "              'max_dim':3, #type=int, default=3, #help='Dimension of the encoding layer [Default: 3]')\n",
    "              'log_dir': 'log',#default='log', #help='Log dir [default: log]')\n",
    "              'batch':512,# type=int, default=512, #help='Batch Size  during training [default: 512]')\n",
    "              'num_point':100, # type=int, default=100, #help='Point Number [default: 100]')\n",
    "              'data_dir':'../h5/', #default='../h5', #help='directory with data [default: ../h5]')\n",
    "              'nfeat':8,# type=int, default=8, #help='Number of features [default: 8]')\n",
    "              'ncat':20, # type=int, default=20, #help='Number of categories [default: 20]')\n",
    "              'name': \"evaluation\", #default=\"\", #help='name of the output file')\n",
    "              'h5_folder':'../h5/', #default=\"../h5/\", #help='folder to store output files')\n",
    "              'full_train':True,# default=False, action='store_true',\n",
    "                    #help='load full training results [default: False]')\n",
    "              'checkpoint_folder':'/dbfs/databricks/driver/06_LHC/logs/train/', #help: The folder where the checkpoint is saved. The script\n",
    "                   #will retrieved the latest checkpoint created here.\n",
    "             }\n",
    "\n",
    "FLAGS = Namespace(**parserdict)\n",
    "#LOG_DIR = os.path.join('..', 'logs', FLAGS.log_dir)\n",
    "LOG_DIR = os.path.join(os.getcwd(), '06_LHC', 'logs', FLAGS.log_dir)\n",
    "DATA_DIR = FLAGS.data_dir\n",
    "H5_DIR = os.path.join(BASE_DIR, DATA_DIR)\n",
    "H5_OUT = FLAGS.h5_folder\n",
    "CHECKPOINT_PATH = FLAGS.checkpoint_folder\n",
    "if not os.path.exists(H5_OUT): os.mkdir(H5_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the clustering accuracy\n",
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = linear_sum_assignment(w.max() - w)\n",
    "    ind = np.asarray(ind)\n",
    "    ind = np.transpose(ind)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest checkpoint (the training script saves one every 1000th step)\n",
    "def find_ckpt(path,base):\n",
    "  files = os.listdir(os.path.join(path,os.listdir(path)[-1]))\n",
    "  s=base+\".ckpt-\"\n",
    "  ckpts = [r for r in files if s in r]\n",
    "  numbers = [int(r.split('.')[1].split('-')[1]) for r in ckpts]\n",
    "  ckpt = base+'.ckpt-'+str(np.max(numbers))\n",
    "  return os.path.join(CHECKPOINT_PATH,os.listdir(CHECKPOINT_PATH)[-1],ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Run the evaluation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_POINT = FLAGS.num_point\n",
    "BATCH_SIZE = FLAGS.batch\n",
    "NFEATURES = FLAGS.nfeat\n",
    "FULL_TRAINING = FLAGS.full_train\n",
    "\n",
    "NUM_CATEGORIES = FLAGS.ncat\n",
    "# Only used to get how many parts per categor\n",
    "print('#### Batch Size : {0}'.format(BATCH_SIZE))\n",
    "print('#### Point Number: {0}'.format(NUM_POINT))\n",
    "print('#### Using GPUs: {0}'.format(FLAGS.gpu))\n",
    "\n",
    "print('### Starting evaluation')\n",
    "\n",
    "EVALUATE_FILES = provider.getDataFiles(os.path.join(H5_DIR, 'evaluate_files_wztop.txt'))\n",
    "\n",
    "\n",
    "def eval():\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:' + str(FLAGS.gpu)):\n",
    "            pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT, NFEATURES)\n",
    "            batch = tf.Variable(0, trainable=False)\n",
    "            alpha = tf.compat.v1.placeholder(tf.float32, shape=())\n",
    "            is_training_pl = tf.compat.v1.placeholder(tf.bool, shape=())\n",
    "            pred, max_pool = MODEL.get_model(pointclouds_pl, is_training=is_training_pl, num_class=NUM_CATEGORIES)\n",
    "            mu = tf.Variable(tf.zeros(shape=(FLAGS.n_clusters, FLAGS.max_dim)), name=\"mu\",\n",
    "                             trainable=False)  # k centroids\n",
    "\n",
    "            classify_loss = MODEL.get_focal_loss(pred, labels_pl, NUM_CATEGORIES)\n",
    "            kmeans_loss, stack_dist = MODEL.get_loss_kmeans(max_pool, mu, FLAGS.max_dim,\n",
    "                                                            FLAGS.n_clusters, alpha)\n",
    "\n",
    "            saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "        if FULL_TRAINING:\n",
    "            #saver.restore(sess, os.path.join(LOG_DIR, 'cluster.ckpt'))\n",
    "            saver.restore(sess, find_ckpt(CHECKPOINT_PATH,'cluster'))\n",
    "        else:\n",
    "            saver.restore(sess, find_ckpt(CHECKPOINT_PATH,'model'))\n",
    "            #saver.restore(sess, os.path.join(LOG_DIR, 'model.ckpt'))\n",
    "        print('model restored')\n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'labels_pl': labels_pl,\n",
    "               'stack_dist': stack_dist,\n",
    "               'kmeans_loss': kmeans_loss,\n",
    "               'pred': pred,\n",
    "               'alpha': alpha,\n",
    "               'max_pool': max_pool,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'classify_loss': classify_loss, }\n",
    "\n",
    "        eval_one_epoch(sess, ops)\n",
    "\n",
    "\n",
    "def get_batch(data, label, start_idx, end_idx):\n",
    "    batch_label = label[start_idx:end_idx]\n",
    "    batch_data = data[start_idx:end_idx, :, :]\n",
    "    return batch_data, batch_label\n",
    "\n",
    "\n",
    "def eval_one_epoch(sess, ops):\n",
    "    is_training = False\n",
    "\n",
    "    eval_idxs = np.arange(0, len(EVALUATE_FILES))\n",
    "    y_val = []\n",
    "    acc = 0\n",
    "    \n",
    "    for fn in range(len(EVALUATE_FILES)):\n",
    "        current_file = os.path.join(H5_DIR, EVALUATE_FILES[eval_idxs[fn]])\n",
    "        current_data, current_label, current_cluster = provider.load_h5_data_label_seg(current_file)\n",
    "        adds = provider.load_add(current_file, ['masses'])\n",
    "\n",
    "        current_label = np.squeeze(current_label)\n",
    "\n",
    "        file_size = current_data.shape[0]\n",
    "        num_batches = file_size // BATCH_SIZE\n",
    "       # num_batches = 5\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = (batch_idx + 1) * BATCH_SIZE\n",
    "\n",
    "            batch_data, batch_label = get_batch(current_data, current_label, start_idx, end_idx)\n",
    "            batch_cluster = current_cluster[start_idx:end_idx]\n",
    "            cur_batch_size = end_idx - start_idx\n",
    "\n",
    "            feed_dict = {ops['pointclouds_pl']: batch_data,\n",
    "                         ops['labels_pl']: batch_label,\n",
    "                         ops['alpha']: 1,  # No impact on evaluation,\n",
    "                         ops['is_training_pl']: is_training,\n",
    "                         }\n",
    "\n",
    "            loss, dist, max_pool = sess.run([ops['kmeans_loss'], ops['stack_dist'],\n",
    "                                             ops['max_pool']], feed_dict=feed_dict)\n",
    "            cluster_assign = np.zeros((cur_batch_size), dtype=int)\n",
    "            for i in range(cur_batch_size):\n",
    "                index_closest_cluster = np.argmin(dist[:, i])\n",
    "                cluster_assign[i] = index_closest_cluster\n",
    "\n",
    "            batch_cluster = np.array([np.where(r == 1)[0][0] for r in current_cluster[start_idx:end_idx]])\n",
    "\n",
    "            if len(y_val) == 0:\n",
    "                y_val = batch_cluster\n",
    "                y_assign = cluster_assign\n",
    "                y_pool = np.squeeze(max_pool)\n",
    "                y_mass = adds['masses'][start_idx:end_idx]\n",
    "            else:\n",
    "                y_val = np.concatenate((y_val, batch_cluster), axis=0)\n",
    "                y_assign = np.concatenate((y_assign, cluster_assign), axis=0)\n",
    "                y_pool = np.concatenate((y_pool, np.squeeze(max_pool)), axis=0)\n",
    "                y_mass = np.concatenate((y_mass, adds['masses'][start_idx:end_idx]), axis=0)\n",
    "\n",
    "    with h5py.File(os.path.join(H5_OUT, '{0}.h5'.format(FLAGS.name)), \"w\") as fh5:\n",
    "        dset = fh5.create_dataset(\"pid\", data=y_val)  # Real jet categories\n",
    "        dset = fh5.create_dataset(\"label\", data=y_assign)  # Cluster labeling\n",
    "        dset = fh5.create_dataset(\"max_pool\", data=y_pool)\n",
    "        dset = fh5.create_dataset(\"masses\", data=y_mass)\n",
    "    \n",
    "    print(\"Clustering accuracy is \",cluster_acc(y_val,y_assign))\n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     #### Batch Size : 512\n",
    ">     #### Point Number: 100\n",
    ">     #### Using GPUs: 0\n",
    ">     ### Starting evaluation\n",
    ">     INFO:tensorflow:Restoring parameters from /dbfs/databricks/driver/06_LHC/logs/train/1609765453.8899894/cluster.ckpt-29000\n",
    ">     model restored\n",
    ">     loaded 143984 events\n",
    ">     loaded 302664 events\n",
    ">     loaded 75666 events\n",
    ">     Clustering accuracy is  0.5542198233562315\n",
    "\n",
    "  \n",
    "\n",
    "The clustering accuracy for the fully trained model should be output\n",
    "above. In addition, the .h5 file ../h5/evaluate.h5 containing\n",
    "information about true and predicted labels (as well as masses and\n",
    "pooling) has been written. This can be used to make visualizations such\n",
    "as the one in the introduction notebook (taken from the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(H5_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[26]: ['evaluation.h5', '.h5']"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
