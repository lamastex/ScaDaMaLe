{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "#Load dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Creating a DataFrame of given iris dataset.\n",
    "import pandas as pd\n",
    "data=pd.DataFrame({\n",
    "    'sepal length':iris.data[:,0],\n",
    "    'sepal width':iris.data[:,1],\n",
    "    'petal length':iris.data[:,2],\n",
    "    'petal width':iris.data[:,3],\n",
    "    'species':iris.target\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Attempt at ugly MixUp version of the above training\n",
    "### Currently not working: need to do some one-hot version of the labels etc I guess\n",
    "###\n",
    "\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert dataframe to arrays and perform one-hot encoding\n",
    "\n",
    "X=data[['sepal length', 'sepal width', 'petal length', 'petal width']]  # Features\n",
    "y=data['species']  # Labels\n",
    "\n",
    "X = X.to_numpy()\n",
    "\n",
    "enc = LabelBinarizer()\n",
    "enc.fit(y)\n",
    "y = enc.transform(y)\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70\n",
    "X_train_base = X_train.copy()\n",
    "y_train_base = y_train.copy()\n",
    "\n",
    "###\n",
    "### Create the MixUp data set\n",
    "# number of MixUp extensions\n",
    "number_of_mixups = 9\n",
    "n = np.shape(X_train_base)[0]\n",
    "for i in range(number_of_mixups):\n",
    "  shuffled_indices = np.arange(n).tolist()\n",
    "  np.random.shuffle(shuffled_indices)\n",
    "  X_s = X_train_base[shuffled_indices]\n",
    "  y_s = y_train_base[shuffled_indices]\n",
    "  mixup_l = np.random.beta(0.2,0.2)\n",
    "  X_mixed = X_train_base*(1-mixup_l) + mixup_l*X_s\n",
    "  y_mixed = y_train_base*(1-mixup_l) + (mixup_l)*(y_s)\n",
    "  X_train = np.concatenate([X_train, X_mixed])\n",
    "  y_train = np.concatenate([y_train, y_mixed])\n",
    "\n",
    "\n",
    "\n",
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf=RandomForestRegressor(n_estimators=10)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_mixed,y_mixed)\n",
    "\n",
    "y_pred_probs=clf.predict(X_test)\n",
    "y_pred = np.zeros_like(y_pred_probs)\n",
    "y_pred[np.arange(len(y_pred_probs)), y_pred_probs.argmax(1)] = 1\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Accuracy: 0.9555555555555556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attempt to port the previous example to MNIST\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelBinarizer()\n",
    "enc.fit(y)\n",
    "y = enc.transform(y)\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70\n",
    "X_train_base = X_train.copy()\n",
    "y_train_base = y_train.copy()\n",
    "\n",
    "###\n",
    "### Create the MixUp data set\n",
    "# number of MixUp extensions\n",
    "number_of_mixups = 4\n",
    "n = np.shape(X_train_base)[0]\n",
    "for i in range(number_of_mixups):\n",
    "  shuffled_indices = np.arange(n).tolist()\n",
    "  np.random.shuffle(shuffled_indices)\n",
    "  X_s = X_train_base[shuffled_indices]\n",
    "  y_s = y_train_base[shuffled_indices]\n",
    "  mixup_l = np.random.beta(0.2,0.2)\n",
    "  X_mixed = X_train_base*(1-mixup_l) + mixup_l*X_s\n",
    "  y_mixed = y_train_base*(1-mixup_l) + (mixup_l)*(y_s)\n",
    "  X_train = np.concatenate([X_train, X_mixed])\n",
    "  y_train = np.concatenate([y_train, y_mixed])\n",
    "\n",
    "\n",
    "\n",
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf=RandomForestRegressor(n_estimators=10)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_mixed,y_mixed)\n",
    "\n",
    "y_pred_probs=clf.predict(X_test)\n",
    "y_pred = np.zeros_like(y_pred_probs)\n",
    "y_pred[np.arange(len(y_pred_probs)), y_pred_probs.argmax(1)] = 1\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Accuracy: 0.9771428571428571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attempt to port the previous example to MNIST\n",
    "#from sklearn.datasets import fetch_openml\n",
    "#X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "import tensorflow as tf\n",
    "(X, y),(testX,testY) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "X = X.reshape(60000, 28*28)\n",
    "\n",
    "enc = LabelBinarizer()\n",
    "enc.fit(y)\n",
    "y = enc.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    ">     Out[285]: (59940,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixes the issue \"AttributeError: 'ConsoleBuffer has no attribute 'fileno'\"\n",
    "import sys\n",
    "sys.stdout.fileno = lambda: False\n",
    "\n",
    "from tune_sklearn import TuneGridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "num_classes = 10\n",
    "np.random.seed(1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1, test_size=0.99) # 70\n",
    "X_train_base = X_train.copy()\n",
    "y_train_base = y_train.copy()\n",
    "\n",
    "# Create mixup data points\n",
    "number_of_mixups = 10\n",
    "n = np.shape(X_train_base)[0]\n",
    "for i in range(number_of_mixups):\n",
    "  shuffled_indices = np.arange(n).tolist()\n",
    "  np.random.shuffle(shuffled_indices)\n",
    "  X_s = X_train_base[shuffled_indices]\n",
    "  y_s = y_train_base[shuffled_indices]\n",
    "  mixup_l = np.random.beta(0.2,0.2)\n",
    "  X_mixed = X_train_base*(1-mixup_l) + mixup_l*X_s\n",
    "  y_mixed = y_train_base*(1-mixup_l) + (mixup_l)*(y_s)\n",
    "  X_train = np.concatenate([X_train, X_mixed])\n",
    "  y_train = np.concatenate([y_train, y_mixed])\n",
    "\n",
    "parameters = {\n",
    "    'n_estimators': range(10,11),\n",
    "    'max_depth': range(4,5)\n",
    "}\n",
    "\n",
    "# n_jobs=-1 enables use of all cores like Tune does\n",
    "tune_search = TuneGridSearchCV(\n",
    "    RandomForestRegressor(),\n",
    "    parameters#, early_stopping=\"MedianStoppingRule\", max_iters=10\n",
    ")\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "hist = tune_search.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(\"Sklearn Fit Time:\", end - start)\n",
    "y_pred_probs = tune_search.predict(X_test)\n",
    "y_pred = np.zeros_like(y_pred_probs)\n",
    "y_pred[np.arange(len(y_pred_probs)), y_pred_probs.argmax(1)] = 1\n",
    "#accuracy = np.count_nonzero(np.array(y_pred) == np.array(y_test)) / (num_classes*len(y_pred))\n",
    "#print(\"Sklearn Accuracy:\", accuracy)\n",
    "#acc2 = np.mean(np.argmax(y_pred,1) == np.argmax(y_test,1))\n",
    "#print(acc2)\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy3:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     (pid=12141) /databricks/python/lib/python3.7/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
    ">     (pid=12141)   \"multioutput='uniform_average').\", FutureWarning)\n",
    ">     (pid=12141) /databricks/python/lib/python3.7/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
    ">     (pid=12141)   \"multioutput='uniform_average').\", FutureWarning)\n",
    ">     (pid=12141) /databricks/python/lib/python3.7/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
    ">     (pid=12141)   \"multioutput='uniform_average').\", FutureWarning)\n",
    ">     (pid=12141) /databricks/python/lib/python3.7/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
    ">     (pid=12141)   \"multioutput='uniform_average').\", FutureWarning)\n",
    ">     (pid=12141) /databricks/python/lib/python3.7/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
    ">     (pid=12141)   \"multioutput='uniform_average').\", FutureWarning)\n",
    ">     Sklearn Fit Time: 59.81026768684387\n",
    ">     Accuracy3: 0.6262794612794613"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy3:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Accuracy3: 0.6641666666666667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Attempt at a nicer MixUp implementation. Unfortunately I cannot change the labels through this pipeline... Maybe it can be solved\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "# Note: the x-mixup works, but I have not managed to make it work for y\n",
    "\n",
    "class MixUpTransformer(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self):\n",
    "    print('\\nhehe1\\n')\n",
    "  def fit(self, x, y):\n",
    "    print('\\nhehe2\\n')\n",
    "    return self\n",
    "  def transform(self, x, y = None):\n",
    "    print('\\nhehe3\\n')\n",
    "    #y_s = y.copy()\n",
    "    n = np.shape(x)[0]\n",
    "    shuffled_indices = np.arange(n).tolist()\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    x_s = x.iloc[shuffled_indices]\n",
    "    x_mixed = x.mul(0.3).add(x_s).mul(0.7)\n",
    "    return x_mixed\n",
    "  \n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe1 = make_pipeline(MixUpTransformer(), RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     hehe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1.fit(X_train, y_train)\n",
    "y_pred=pipe1.predict(X_test)\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     hehe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixes the issue \"AttributeError: 'ConsoleBuffer has no attribute 'fileno'\"\n",
    "import sys\n",
    "sys.stdout.fileno = lambda: False\n",
    "\n",
    "from tune_sklearn import TuneGridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': range(100)\n",
    "}\n",
    "\n",
    "# n_jobs=-1 enables use of all cores like Tune does\n",
    "tune_search = TuneGridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    parameters,\n",
    "    early_stopping=\"MedianStoppingRule\",\n",
    "    max_iters=10\n",
    ")\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "sklearn_search.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(\"Sklearn Fit Time:\", end - start)\n",
    "pred = sklearn_search.predict(X_test)\n",
    "accuracy = np.count_nonzero(np.array(pred) == np.array(y_test)) / len(pred)\n",
    "print(\"Sklearn Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Sklearn Fit Time: 0.9184603691101074\n",
    ">     Sklearn Accuracy: 0.9111111111111111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import GridSearchCV\n",
    "from tune_sklearn import TuneGridSearchCV\n",
    "\n",
    "# Other imports\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Set training and validation sets\n",
    "X, y = make_classification(n_samples=11000, n_features=1000, n_informative=50, n_redundant=0, n_classes=10, class_sep=2.5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1000)\n",
    "\n",
    "# Example parameters to tune from SGDClassifier\n",
    "parameters = {\n",
    "    'alpha': [1e-4, 1e-1, 1],\n",
    "    'epsilon':[0.01, 0.1]\n",
    "}\n",
    "\n",
    "tune_search = TuneGridSearchCV(\n",
    "    SGDClassifier(),\n",
    "    parameters,\n",
    "    early_stopping=\"MedianStoppingRule\",\n",
    "    max_iters=10\n",
    ")\n",
    "\n",
    "import time # Just to compare fit times\n",
    "start = time.time()\n",
    "tune_search.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(\"Tune Fit Time:\", end - start)\n",
    "pred = tune_search.predict(X_test)\n",
    "accuracy = np.count_nonzero(np.array(pred) == np.array(y_test)) / len(pred)\n",
    "print(\"Tune Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Tune Fit Time: 41.66382598876953\n",
    ">     Tune Accuracy: 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# n_jobs=-1 enables use of all cores like Tune does\n",
    "sklearn_search = GridSearchCV(\n",
    "    SGDClassifier(),\n",
    "    parameters,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "sklearn_search.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(\"Sklearn Fit Time:\", end - start)\n",
    "pred = sklearn_search.predict(X_test)\n",
    "accuracy = np.count_nonzero(np.array(pred) == np.array(y_test)) / len(pred)\n",
    "print(\"Sklearn Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Sklearn Fit Time: 82.67133474349976\n",
    ">     Sklearn Accuracy: 0.876"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
