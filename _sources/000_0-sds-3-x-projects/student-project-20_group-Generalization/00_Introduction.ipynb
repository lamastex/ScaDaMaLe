{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we plan to study generalization in (different?)\n",
    "learning algorithms. In particular, we will examine the connection\n",
    "between generalization performance and (various?) complexity metrics,\n",
    "such as MixUp.\n",
    "\n",
    "Reference:\n",
    "https://parthnatekar.github.io/generalization.html?fbclid=IwAR1FFx31BjHcquOa0hng9aKT6mJREAF1w1jUggo7D1yKcJ5kPRm3B9wznSI\n",
    "\n",
    "Link to mixup: https://arxiv.org/abs/1710.09412\n",
    "\n",
    "Suggestion: \\* Scalable in the size of the hyperparameter search \\* Can\n",
    "use Tensorflow? \\* Can we even use kaggle tuner for the hyperparameter\n",
    "search?\n",
    "\n",
    "Possible data sets: \\* Covid19\n",
    "https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset\n",
    "\\* Image data of some kind, e.g. faces. Linear combination of images at\n",
    "least has a clear interpretation. \\* Breast Cancer Wisconsin\n",
    "(Diagnostic) Data Set - predicting benign/malignant based on digitized\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from tensorflow.keras import regularizers\n",
    "from pdb import set_trace\n",
    "from sparkdl import HorovodRunner\n",
    "#from kerastuner.tuners import RandomSearch\n",
    "#from kerastuner.engine.hyperparameters import HyperParameters\n",
    "#from kerastuner import Hyperband\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "print(local_device_protos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     [name: \"/device:CPU:0\"\n",
    ">     device_type: \"CPU\"\n",
    ">     memory_limit: 268435456\n",
    ">     locality {\n",
    ">     }\n",
    ">     incarnation: 12704549050924573955\n",
    ">     , name: \"/device:XLA_CPU:0\"\n",
    ">     device_type: \"XLA_CPU\"\n",
    ">     memory_limit: 17179869184\n",
    ">     locality {\n",
    ">     }\n",
    ">     incarnation: 11015930834247940706\n",
    ">     physical_device_desc: \"device: XLA_CPU device\"\n",
    ">     , name: \"/device:XLA_GPU:0\"\n",
    ">     device_type: \"XLA_GPU\"\n",
    ">     memory_limit: 17179869184\n",
    ">     locality {\n",
    ">     }\n",
    ">     incarnation: 9436066314374116018\n",
    ">     physical_device_desc: \"device: XLA_GPU device\"\n",
    ">     , name: \"/device:GPU:0\"\n",
    ">     device_type: \"GPU\"\n",
    ">     memory_limit: 4898684928\n",
    ">     locality {\n",
    ">       bus_id: 1\n",
    ">       links {\n",
    ">       }\n",
    ">     }\n",
    ">     incarnation: 15433283977610770069\n",
    ">     physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\"\n",
    ">     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mix_Sequential(tf.keras.Sequential):\n",
    "\n",
    "    def __init__(self,*params):\n",
    "        super(mix_Sequential,self).__init__(*params)\n",
    "        #self.__linear_func__ = lambda a,b,c :  c\n",
    "\n",
    "    def train_step(self,data):\n",
    "        X,Y = data\n",
    "        ind = np.random.randint(len(self.layers))\n",
    "        lam = tf.random.uniform((tf.shape(X)[0],), minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
    "        Xs,Ys = tf.random.shuffle(X, seed=1),tf.random.shuffle(Y, seed=1)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            for i in range(ind):\n",
    "                X = self.layers[i](X)\n",
    "                Xs = self.layers[i](Xs)\n",
    "        \n",
    "            X = self.linear_combine(lam,X,Xs)\n",
    "            Y = self.linear_combine(lam,Y,Ys)\n",
    "            for i in range(ind,len(self.layers)):\n",
    "                X = self.layers[i](X)\n",
    "\n",
    "\n",
    "            loss = self.compiled_loss(Y, X, regularization_losses=self.losses)\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        self.compiled_metrics.update_state(Y, X)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def linear_combine(self,lam,X,Xs):\n",
    "        l = tf.reshape(lam, (-1, *([1]*(len(X.shape)-1)) ))\n",
    "        Xl = l*X + (1-l)*Xs\n",
    "        return Xl\n",
    "        \n",
    "\n",
    "class mixup_Sequential(tf.keras.Sequential):\n",
    "\n",
    "    def __init__(self,*params):\n",
    "        super(mixup_Sequential,self).__init__(*params)\n",
    "        \n",
    "    def train_step(self,data):\n",
    "        X,Y = data\n",
    "        lam = tf.random.uniform((tf.shape(X)[0],), minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
    "        Xs,Ys = tf.random.shuffle(X, seed=1),tf.random.shuffle(Y, seed=1)\n",
    "\n",
    "        X = self.linear_combine(lam,X,Xs)\n",
    "        Y = self.linear_combine(lam,Y,Ys)\n",
    "        with tf.GradientTape() as tape:\n",
    "            for i in range(len(self.layers)):\n",
    "                X = self.layers[i](X)\n",
    "                \n",
    "\n",
    "            loss = self.compiled_loss(Y, X, regularization_losses=self.losses)\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        self.compiled_metrics.update_state(Y, X)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def linear_combine(self,lam,X,Xs):\n",
    "        l = tf.reshape(lam, (-1, *([1]*(len(X.shape)-1)) ))\n",
    "        Xl = l*X + (1-l)*Xs\n",
    "        return Xl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData,testData = tf.keras.datasets.cifar10.load_data()\n",
    "trainX,trainY = trainData\n",
    "testX,testY = testData\n",
    "trainX = tf.cast(trainX,tf.float32)\n",
    "testX = tf.cast(testX,tf.float32)\n",
    "trainY_oh = tf.one_hot(trainY,10)[:,0,:]\n",
    "trainY = tf.cast(trainY,tf.float32)\n",
    "testY_oh = tf.one_hot(testY,10)[:,0,:]\n",
    "trainY_oh = tf.cast(trainY_oh,tf.float32)\n",
    "testY_oh = tf.cast(testY_oh,tf.float32)\n",
    "trainY = trainY[:,0]\n",
    "#trainX = tf.expand_dims(trainX,3)\n",
    "#testX = tf.expand_dims(testX,3)\n",
    "trainX = trainX/255 * 2 - 2\n",
    "testX = testX/255 * 2 - 2\n",
    "print(testY_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    number_dense_layers = hp.Int(\"number_dense\",1,10)\n",
    "    number_units_dense = hp.Int(f\"number_neuron_dense\",min_value = 10,max_value = 50,step = 10)\n",
    "    number_conv = hp.Int(\"number_conv\",1,20)\n",
    "    type_of_model = hp.Choice(\"type_of_model\", [\"reg\",\"mix\",\"mixup\"])\n",
    "    lambd = hp.Float(\"lambd\",0,0.001)\n",
    "    \n",
    "    if type_of_model == \"reg\":\n",
    "        model = tf.keras.Sequential()\n",
    "    elif type_of_model == \"mix\":\n",
    "        model = mix_Sequential()\n",
    "    elif type_of_model == \"mixup\":\n",
    "        model = mixup_Sequential()\n",
    "    else:\n",
    "        raise Exception(f\"No model type called {type_of_model}\")\n",
    "\n",
    "    for i in range(number_conv):\n",
    "        model.add(tf.keras.layers.Conv2D(16+16*i,kernel_size = 3,activation= \"relu\",padding=\"same\"))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    for i in range(number_dense_layers):\n",
    "        model.add(tf.keras.layers.Dense(units = number_units_dense,activation= \"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(units = 10,activation= \"softmax\"))\n",
    "    model.compile(loss= \"CategoricalCrossentropy\",metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard\n",
    "%tensorboard --logdir logstb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbcallback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=\"logstb\", histogram_freq=2, write_graph=True, write_images=False,\n",
    "        update_freq='epoch', profile_batch=2, embeddings_freq=0)\n",
    "    \n",
    "early_stop_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    min_delta=0.01,\n",
    "    patience=5,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "tuner = RandomSearch(build_model,\n",
    "                     objective = \"val_accuracy\",\n",
    "                     max_trials = 50,\n",
    "                     executions_per_trial = 1,\n",
    "                     directory = \"logs\")\n",
    "tuner.search(x = trainX,\n",
    "             y = trainY_oh,\n",
    "             epochs = 50,\n",
    "             batch_size = 64,\n",
    "             validation_data = (testX,testY_oh),\n",
    "             callbacks = [tbcallback,early_stop_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K \n",
    "\n",
    "(x_train_p, y_train_p), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train_p  #[0:1000,:,:]\n",
    "y_train = y_train_p  #[0:1000]\n",
    "num_classes = 10\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255; x_test /= 255;\n",
    "image_size = 784 #28*28\n",
    "x_train = x_train.reshape(x_train.shape[0], image_size)\n",
    "x_test = x_test.reshape(x_test.shape[0], image_size)\n",
    "\n",
    "def create_dense(layer_sizes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer_sizes[0], activation='relu', input_shape=(image_size,)))\n",
    "    for s in layer_sizes[1:]:\n",
    "        model.add(Dense(units = s, activation = 'relu'))\n",
    "    model.add(Dense(units=num_classes, activation='softmax'))\n",
    "    model.compile(optimizer=keras.optimizers.SGD(lr = 0.003, momentum = 0.95), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "batch_size = 256\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "import random\n",
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.stdout.fileno = lambda: False\n",
    "\n",
    "# Limit the number of rows.\n",
    "reporter = CLIReporter(max_progress_rows=10)\n",
    "# Add a custom metric column, in addition to the default metrics.\n",
    "# Note that this must be a metric that is returned in your training results.\n",
    "reporter.add_metric_column(\"mixup_accuracy\")\n",
    "reporter.add_metric_column(\"test_accuracy\")\n",
    "\n",
    "# Randomizes the degree of mixup in test data, by choosing a\n",
    "# random mean for the truncated normal used in mixup_data().\n",
    "# To make the hyperparameter search fair, the same degree of\n",
    "# test mixup should be used for all choices of hyperparameters.\n",
    "mean_test = tf.random.uniform(shape=[], maxval=0.5, dtype=tf.dtypes.float32)\n",
    "\n",
    "def linear_combine(lam,X,Xs):\n",
    "    l = tf.reshape(lam, (-1, *([1]*(len(X.shape)-1)) ))\n",
    "    Xl = l*Xs + (1-l)*X\n",
    "    return Xl\n",
    "\n",
    "def mixup_data( X, Y, mean ):\n",
    "    if mean == 0: # No mixup; training and/or testing on unmixed data.\n",
    "      return X, Y\n",
    "    else:\n",
    "      lam = tf.random.truncated_normal((tf.shape(X)[0],), mean=mean, stddev=0.5*mean, dtype=tf.dtypes.float32)\n",
    "      #Xs,Ys = tf.random.shuffle(X, seed=1),tf.random.shuffle(Y, seed=1)\n",
    "\n",
    "      indices = tf.range(start=0, limit=tf.shape(X)[0], dtype=tf.int32)\n",
    "      idx = tf.random.shuffle(indices, seed = 1)\n",
    "      Xs, Ys = tf.gather(X, idx), tf.gather(Y, idx)\n",
    "\n",
    "      # Mixup: Form convex combinations of batch data.\n",
    "      # Important to form the exact same mix of labels.\n",
    "      X_mix = linear_combine(lam,X,Xs)\n",
    "      Y_mix = linear_combine(lam,Y,Ys)\n",
    "      return X_mix, Y_mix\n",
    "\n",
    "def training_function(config, checkpoint_dir=None):\n",
    "    # Hyperparameters\n",
    "    width, depth = config[\"width\"], config[\"depth\"]\n",
    "    model = create_dense( width*np.ones(depth) )\n",
    "\n",
    "    mean = config[\"mean\"]\n",
    "    x_train_mix, y_train_mix = mixup_data( x_train, y_train, mean )\n",
    "    history = model.fit(x_train_mix, y_train_mix, batch_size=batch_size, epochs=epochs, verbose=False)\n",
    "\n",
    "    x_mix, y_mix = mixup_data( x_test, y_test, mean_test )\n",
    "    # Compute loss (accuracy) for...\n",
    "    mix_loss, mix_acc = model.evaluate( x_mix, y_mix ) # ...mixed test data\n",
    "    test_loss, test_acc = model.evaluate( x_test, y_test ) # ...unmixed test data\n",
    "    train_acc = history.history['accuracy'][-1] # ...(mixed) training data\n",
    "    tune.report(mean_loss=train_acc, mixup_accuracy=mix_acc, test_accuracy = test_acc)\n",
    "\n",
    "\n",
    "analysis = tune.run(\n",
    "    training_function,\n",
    "    resources_per_trial={'cpu': 1, 'gpu': 1},\n",
    "    config={\n",
    "        \"width\": tune.grid_search([400]), # Change back to 300, 400\n",
    "        \"depth\": tune.grid_search([1]), # Change back to 1, 2\n",
    "        \"mean\": tune.grid_search([0, 0.1, 0.2, 0.35, 0.5])\n",
    "    },\n",
    "    local_dir='ray_results',\n",
    "    progress_reporter=reporter)\n",
    "\n",
    "print(\"Best config: \", analysis.get_best_config(\n",
    "    metric=\"mixup_accuracy\", mode=\"max\"))\n",
    "\n",
    "# Get a dataframe for analyzing trial results.\n",
    "df = analysis.results_df\n",
    "print(\"Degree of mixup for test data: mean_test = \", mean_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     2021-01-09 13:13:19,423\tINFO services.py:1173 -- View the Ray dashboard at http://127.0.0.1:8265\n",
    ">     2021-01-09 13:13:22,109\tINFO logger.py:627 -- pip install 'ray[tune]' to see TensorBoard files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary code - testing how to work with data in FileStore.\n",
    "#df = spark.read.format(\"image\").load(\"/FileStore/tables/Group20/seg_test/seg_test/buildings/20061.jpg\")\n",
    "#display(df)\n",
    "\n",
    "dir_test = \"/FileStore/tables/Group20/seg_test/seg_test/\"\n",
    "files = dbutils.fs.ls(dir_test + \"mountain/\")\n",
    "n = 0\n",
    "for image in files:\n",
    "  n += 1\n",
    "  \n",
    "print(\"Number of pictures: \", n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Number of pictures:  525"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"image\").load(dir_test)\n",
    "df.printSchema()\n",
    "display(df.select(\"20061.jpg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     root\n",
    ">      |-- image: struct (nullable = true)\n",
    ">      |    |-- origin: string (nullable = true)\n",
    ">      |    |-- height: integer (nullable = true)\n",
    ">      |    |-- width: integer (nullable = true)\n",
    ">      |    |-- nChannels: integer (nullable = true)\n",
    ">      |    |-- mode: integer (nullable = true)\n",
    ">      |    |-- data: binary (nullable = true)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
