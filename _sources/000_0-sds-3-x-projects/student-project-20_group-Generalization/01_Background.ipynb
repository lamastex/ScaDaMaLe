{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "============\n",
    "\n",
    "The goal of supervised machine learning is to predict labels given\n",
    "examples. Specifically, we want to choose some mapping *f*, referred to\n",
    "as a hypothesis, from a space of examples *X* to a space of labels *Y*.\n",
    "As a concrete example, *X* can be the set of pictures of cats and dogs\n",
    "of a given size, *Y* can be the set *{cat, dog}*, and *f* can be a\n",
    "neural network. To choose *f*, we rely on a set of labelled data.\n",
    "However, our true goal is to perform well on unseen data, i.e., test\n",
    "data. If an algorithm performs similarly well on unseen data as on the\n",
    "training data we used, we say that it *generalizes*.\n",
    "\n",
    "A pertinent question, then, is to explain why a model generalizes and\n",
    "using the answer to improve learning algorithms. For overparameterized\n",
    "deep learning methods, this question has yet to be answered\n",
    "conclusively. Recently, a training procedure called MixUp was proposed\n",
    "to improve the generalization capabilities of neural networks \\[\\[1\\]\\].\n",
    "The basic idea is that instead of feeding the raw training data to our\n",
    "supervised learning algorithm, we instead use convex combinations\n",
    "$$\\\\lambda x*1 + (1-\\\\lambda)x*2$$ of two randomly selected data points.\n",
    "The benefit of this is two-fold. First, it plays the role of data\n",
    "augmentation: the network will never see two completely identical\n",
    "training samples, since we constantly produce new random combinations.\n",
    "Second, the network is encouraged to behave nicely in-between training\n",
    "samples, which has the potential to reduce overfitting. A connection\n",
    "between performance on MixUp data and generalization abilities of\n",
    "networks trained without the MixUp procedure was also studied in\n",
    "\\[\\[2\\]\\].\n",
    "\n",
    "Project description\n",
    "===================\n",
    "\n",
    "In this project, we will investigate the connection between MixUp and\n",
    "generalization at a large scale by performing a distributed\n",
    "hyperparameter search. First, we will train neural networks without\n",
    "MixUp, and study the connection between MixUp performance and test\n",
    "error. Then, we will train the networks on MixUp data, and see whether\n",
    "directly optimizing MixUp performance will yield more beneficial test\n",
    "errors.\n",
    "\n",
    "To make the hyperparameter search distributed and scalable, we will use\n",
    "the Ray Tune package \\[\\[3\\]\\]. Furthermore, we will use Horovod to\n",
    "enable the individual networks to handle data in a distributed fashion\n",
    "as well \\[\\[4\\]\\]. Scalability thus enters our project in both the scope\n",
    "of the hyperparameter search and the size of the data set."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
