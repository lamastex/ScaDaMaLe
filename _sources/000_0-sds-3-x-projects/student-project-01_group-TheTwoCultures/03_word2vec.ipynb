{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification using Word2Vec\n",
    "=============================\n",
    "\n",
    "Word embeddings\n",
    "---------------\n",
    "\n",
    "Word embeddings map words to vectors of real numbers. Frequency\n",
    "analysis, which we did in a another notebook, is an example of this.\n",
    "There, the 1000 most common words in a collection of text words were\n",
    "mapped to a 1000-dimensional space using one-hot encoding, while the\n",
    "other words were sent to the zero vector. An array of words is mapped to\n",
    "the sum of the one-hot encoded vectors.\n",
    "\n",
    "A more sophisticated word embedding is Word2Vec, which uses the\n",
    "skip-gram model and hierarchical softmax. The idea is to map words to\n",
    "the vector so that it predicts the other words around it well. We refer\n",
    "to &lt;a href=\"https://arxiv.org/abs/1301.3781\"&gt;Efficient Estimation\n",
    "of Word Representations in Vector Space&lt;/a&gt; and &lt;a\n",
    "href=\"https://arxiv.org/abs/1310.4546\"&gt;Distributed Representations of\n",
    "Words and Phrases and their Compositionality&lt;/a&gt; for details.\n",
    "\n",
    "The practical difference is that Word2Vec maps every word to a non-zero\n",
    "vector, and that the output dimension can be chosen freely. Also, the\n",
    "embedding itself has to be trained before use, using some large\n",
    "collection of words. An array of words is mapped to the average of these\n",
    "words.\n",
    "\n",
    "This case study uses the sex forums on Flashback and Familjeliv. The aim\n",
    "is to determine which forum a thread comes from by using the resulting\n",
    "word embeddings, using logistic regression.\n",
    "\n",
    "Preamble\n",
    "--------\n",
    "\n",
    "This section loads libraries and imports functions from another\n",
    "notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// import required libraries\n",
    "import org.apache.spark.ml.feature.{Word2Vec,Word2VecModel}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.ml.feature.{Word2Vec, Word2VecModel}\n",
    ">     import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    ">     import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    ">     import org.apache.spark.sql.Row\n",
    ">     import org.apache.spark.ml.feature.RegexTokenizer\n",
    ">     import org.apache.spark.ml.classification.LogisticRegression\n",
    ">     import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/scalable-data-science/000_0-sds-3-x-projects/student-project-01_group-TheTwoCultures/01_load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Loading the data\n",
    "----------------\n",
    "\n",
    "To extract the data from the .xml-file we use get\\_dataset().\n",
    "\n",
    "Scraping the data takes quite some time, so we also supply a second cell\n",
    "that loads saved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// process .xml-files\n",
    "val file_name = \"dbfs:/datasets/student-project-01/familjeliv/familjeliv-sexsamlevnad.xml\"\n",
    "val df = get_dataset(file_name)\n",
    "val file_name2 = \"dbfs:/datasets/student-project-01/flashback/flashback-sex.xml\"\n",
    "val df2 = get_dataset(file_name2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// paths to saved dataframes\n",
    "val file_path_familjeliv = \"dbfs:/datasets/student-project-01/familjeliv/familjeliv-sexsamlevnad_df\" \n",
    "val file_path_flashback = \"dbfs:/datasets/student-project-01/flashback/flashback-sex_df\"\n",
    "\n",
    "// load saved data frame\n",
    "val df_familjeliv = load_df(file_path_familjeliv)\n",
    "val df_flashback = load_df(file_path_flashback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     file_path_familjeliv: String = dbfs:/datasets/student-project-01/familjeliv/familjeliv-sexsamlevnad_df\n",
    ">     file_path_flashback: String = dbfs:/datasets/student-project-01/flashback/flashback-sex_df\n",
    ">     df_familjeliv: org.apache.spark.sql.DataFrame = [thread_id: string, thread_title: string ... 5 more fields]\n",
    ">     df_flashback: org.apache.spark.sql.DataFrame = [thread_id: string, thread_title: string ... 5 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "The dataframes consist of 7 fields: \\* thread*id - a unique numerical\n",
    "signifier for each thread \\* thread*title - the title of the thread, set\n",
    "by the person who created it \\* w - a comma separated string of all\n",
    "posts in a thread \\* forum*id - a numerical forum signifier \\*\n",
    "forum*title - name of the forum to which the thread belongs \\* platform\n",
    "- the platform from which the thread comes (flashback or familjeliv) \\*\n",
    "corpus\\_id - the corpus from which the data was gathered\n",
    "\n",
    "Let's have a look at the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_familjeliv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "We add labels and merge the two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = df_flashback.withColumn(\"c\", lit(0.0)).union(df_familjeliv.withColumn(\"c\", lit(1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [thread_id: string, thread_title: string ... 6 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "Preprocessing the data\n",
    "----------------------\n",
    "\n",
    "Next, we must split and clean the text. For this we use Regex\n",
    "Tokenizers. We do not eliminate stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// define the tokenizer\n",
    "val tokenizer = new RegexTokenizer()\n",
    "  .setPattern(\"(?U),\") // break by commas\n",
    "  .setMinTokenLength(5) // Filter away tokens with length < 5\n",
    "  .setInputCol(\"w\") // name of the input column\n",
    "  .setOutputCol(\"text\") // name of the output column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_f0701eaf4f60\n",
    "\n",
    "  \n",
    "\n",
    "Let's tokenize and check out the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// define the thread title tokenizer\n",
    "\n",
    "val df_tokenized = tokenizer.transform(df)\n",
    "display(df_tokenized.select(\"w\",\"text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Define and training a Word2Vec model\n",
    "------------------------------------\n",
    "\n",
    "We use the text from the threads to train the Word2Vec model. First we\n",
    "define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// define the model\n",
    "val word2Vec = new Word2Vec()\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"result\")\n",
    "  .setVectorSize(200)\n",
    "  .setMinCount(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     word2Vec: org.apache.spark.ml.feature.Word2Vec = w2v_945abe6fab57\n",
    "\n",
    "  \n",
    "\n",
    "We train the model by fitting it to any dataframe we wish. Here, we use\n",
    "the tokenized one. Training the model takes roughly 2h30m, so we save\n",
    "the result to avoid the hassle of redoing calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// train it\n",
    "val word2Vec_model = word2Vec.fit(df_tokenized)\n",
    "\n",
    "// save it\n",
    "word2Vec_model.save(\"dbfs:/datasets/student-project-01/word2vec_model_sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "We can also load a saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// load a saved model\n",
    "\n",
    "val model = Word2VecModel.load(\"dbfs:/datasets/student-project-01/word2vec_model_sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     model: org.apache.spark.ml.feature.Word2VecModel = w2v_854b46dceacc\n",
    "\n",
    "  \n",
    "\n",
    ">     import org.apache.spark.sql.functions.{col, concat_ws, udf, flatten, explode, collect_list, collect_set, lit}\n",
    ">     import org.apache.spark.sql.types.{ArrayType, StructType, StructField, StringType, IntegerType}\n",
    ">     import com.databricks.spark.xml._\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     read_xml: (file_name: String)org.apache.spark.sql.DataFrame\n",
    ">     get_dataset: (file_name: String)org.apache.spark.sql.DataFrame\n",
    "\n",
    "  \n",
    "\n",
    ">     save_df: (df: org.apache.spark.sql.DataFrame, filePath: String)Unit\n",
    ">     load_df: (filePath: String)org.apache.spark.sql.DataFrame\n",
    ">     no_forums: (df: org.apache.spark.sql.DataFrame)Long\n",
    "\n",
    "  \n",
    "\n",
    ">     dbfs:/datasets/student-project-01/flashback/familjeliv-allmanna-ekonomi_df\n",
    ">     familjeliv-allmanna-ekonomi_df already exists!\n",
    ">     dbfs:/datasets/student-project-01/flashback/familjeliv-sexsamlevnad_df\n",
    ">     familjeliv-sexsamlevnad_df already exists!\n",
    ">     dbfs:/datasets/student-project-01/flashback/flashback-ekonomi_df\n",
    ">     flashback-ekonomi_df already exists!\n",
    ">     dbfs:/datasets/student-project-01/flashback/flashback-sex_df\n",
    ">     flashback-sex_df already exists!\n",
    ">     fl_root: String = dbfs:/datasets/student-project-01/familjeliv/\n",
    ">     fb_root: String = dbfs:/datasets/student-project-01/flashback/\n",
    ">     fl_data: Array[String] = Array(familjeliv-allmanna-ekonomi, familjeliv-sexsamlevnad)\n",
    ">     fb_data: Array[String] = Array(flashback-ekonomi, flashback-sex)\n",
    "\n",
    "  \n",
    "\n",
    "Embedding using Word2Vec\n",
    "------------------------\n",
    "\n",
    "Let's embedd the text and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// transform the text using the model\n",
    "\n",
    "val embedded_text = model.transform(df_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     embedded_text: org.apache.spark.sql.DataFrame = [thread_id: string, thread_title: string ... 8 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "Let's have a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(embedded_text.select(\"c\",\"result\",\"text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Classification using Word2Vec\n",
    "-----------------------------\n",
    "\n",
    "For classification we use logistic regression to compare with results\n",
    "from earlier. First we define the logistic regression model, using the\n",
    "same settings as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Logistic regression\n",
    "val logreg = new LogisticRegression()\n",
    "  .setLabelCol(\"c\")\n",
    "  .setFeaturesCol(\"result\")\n",
    "  .setMaxIter(100)\n",
    "  .setRegParam(0.0001)\n",
    "  .setElasticNetParam(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     logreg: org.apache.spark.ml.classification.LogisticRegression = logreg_55ef614f783e\n",
    "\n",
    "  \n",
    "\n",
    "The easiest way to do the classification is to gather the tokenizer,\n",
    "Word2Vec and logistic regression into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pipeline = new Pipeline().setStages(Array(tokenizer, word2Vec, logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     pipeline: org.apache.spark.ml.Pipeline = pipeline_2254409a91a2\n",
    "\n",
    "  \n",
    "\n",
    "Split the data into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val random_order = df.orderBy(rand())\n",
    "val splits = random_order.randomSplit(Array(0.8, 0.2))\n",
    "val training = splits(0)\n",
    "val test = splits(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     random_order: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [thread_id: string, thread_title: string ... 6 more fields]\n",
    ">     splits: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([thread_id: string, thread_title: string ... 6 more fields], [thread_id: string, thread_title: string ... 6 more fields])\n",
    ">     training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [thread_id: string, thread_title: string ... 6 more fields]\n",
    ">     test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [thread_id: string, thread_title: string ... 6 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "Fit the model to the training data. This will take a while, so we make\n",
    "sure to save the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// fit the model to the training data\n",
    "val logreg_model = pipeline.fit(training)\n",
    "\n",
    "// save the model to filesystem\n",
    "logreg_model.save(\"dbfs:/datasets/student-project-01/word2vec_logreg_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// load saved model\n",
    "\n",
    "val loaded_model = PipelineModel.load(\"dbfs:/datasets/student-project-01/word2vec_logreg_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     loaded_model: org.apache.spark.ml.PipelineModel = pipeline_25839437fccb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val predictions = loaded_model.transform(test).orderBy(rand())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     predictions: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [thread_id: string, thread_title: string ... 11 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"c\",\"prediction\",\"probability\").show(30,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +---+----------+------------------------------------------+\n",
    ">     |c  |prediction|probability                               |\n",
    ">     +---+----------+------------------------------------------+\n",
    ">     |1.0|1.0       |[0.005855361335036372,0.9941446386649635] |\n",
    ">     |1.0|1.0       |[0.2712120894396273,0.7287879105603726]   |\n",
    ">     |1.0|1.0       |[0.0017886928649958375,0.9982113071350042]|\n",
    ">     |1.0|1.0       |[2.263165652125581E-4,0.9997736834347875] |\n",
    ">     |0.0|1.0       |[0.42059601285820825,0.5794039871417918]  |\n",
    ">     |1.0|1.0       |[6.566687042616189E-4,0.9993433312957383] |\n",
    ">     |0.0|0.0       |[0.7054463412596114,0.29455365874038864]  |\n",
    ">     |1.0|1.0       |[0.03103196407369915,0.9689680359263009]  |\n",
    ">     |0.0|0.0       |[0.9294663779954874,0.07053362200451263]  |\n",
    ">     |1.0|1.0       |[0.13974006800394764,0.8602599319960523]  |\n",
    ">     |1.0|1.0       |[0.08228914085494436,0.9177108591450557]  |\n",
    ">     |0.0|0.0       |[0.9788989176701534,0.021101082329846567] |\n",
    ">     |0.0|0.0       |[0.9975070728891363,0.0024929271108637065]|\n",
    ">     |1.0|1.0       |[0.0010781075297480556,0.998921892470252] |\n",
    ">     |0.0|0.0       |[0.9253825302681451,0.07461746973185476]  |\n",
    ">     |1.0|1.0       |[0.01751495449884683,0.9824850455011531]  |\n",
    ">     |1.0|0.0       |[0.9864736560167631,0.013526343983237045] |\n",
    ">     |1.0|1.0       |[0.002472519507918196,0.9975274804920817] |\n",
    ">     |0.0|0.0       |[0.6174112612306129,0.3825887387693872]   |\n",
    ">     |0.0|0.0       |[0.7130899106721519,0.2869100893278482]   |\n",
    ">     |0.0|0.0       |[0.9263664682801233,0.07363353171987672]  |\n",
    ">     |0.0|0.0       |[0.9561455191484204,0.04385448085157954]  |\n",
    ">     |0.0|0.0       |[0.5835745861693306,0.41642541383066944]  |\n",
    ">     |1.0|1.0       |[0.4296249407516458,0.5703750592483542]   |\n",
    ">     |1.0|1.0       |[0.0032969395487662213,0.9967030604512337]|\n",
    ">     |1.0|1.0       |[0.008645133666934816,0.9913548663330651] |\n",
    ">     |1.0|1.0       |[4.1492836709996625E-5,0.9999585071632902]|\n",
    ">     |0.0|1.0       |[0.43037903982909986,0.5696209601709002]  |\n",
    ">     |0.0|1.0       |[0.43707897641990706,0.562921023580093]   |\n",
    ">     |1.0|1.0       |[0.29846393214228517,0.7015360678577148]  |\n",
    ">     +---+----------+------------------------------------------+\n",
    ">     only showing top 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val evaluator = new BinaryClassificationEvaluator().setLabelCol(\"c\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_0d518432d17a\n",
    ">     res13: Double = 0.9499399325125091\n",
    "\n",
    "  \n",
    "\n",
    "An AUCROC of 0.95 is good, but not notably better than the other,\n",
    "conceptually simpler model. More is not always better!\n",
    "\n",
    "Previously, we classified entire threads. Let's see if it works as well\n",
    "on thread titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_threads = df.select(\"c\",\"thread_title\").withColumnRenamed(\"thread_title\",\"w\")\n",
    "val evaluation = loaded_model.transform(df_threads).orderBy(rand())\n",
    "evaluator.evaluate(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     df_threads: org.apache.spark.sql.DataFrame = [c: double, w: string]\n",
    ">     evaluation: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [c: double, w: string ... 5 more fields]\n",
    ">     res14: Double = 0.5261045467524708\n",
    "\n",
    "  \n",
    "\n",
    "This did not work at all. It is essentially equivalent to guessing\n",
    "randomly. Thread titles contain only a few words, so this is not\n",
    "surprising.\n",
    "\n",
    "Note: the same model was used as for both classifying tasks. Since\n",
    "thread titles were not part of the threads, the entire dataset could\n",
    "conceivably be used for training. Whether or not this would improve\n",
    "results is unclear."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
