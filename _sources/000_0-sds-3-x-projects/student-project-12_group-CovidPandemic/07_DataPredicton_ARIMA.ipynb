{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction with Time Series model - ARIMA\n",
    "-----------------------------------------\n",
    "\n",
    "In this notebook, we do prediction with the time series method -\n",
    "Autoregressive integrated moving average (ARIMA). We preprocess the data\n",
    "and prepare for prediction. Then we predicted the new cases (smoothed)\n",
    "and new deaths (smoothed) for world and Sweden. We predict the future\n",
    "value from the history value. After the prediction part we evaluated our\n",
    "results.\n",
    "\n",
    "1. Import data and preprocess\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// %run \"./02_DataPreprocess\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "  \n",
    "\n",
    "2. Prepare for data\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cleaned_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_time_series.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     root\n",
    ">      |-- iso_code: string (nullable = true)\n",
    ">      |-- continent: string (nullable = false)\n",
    ">      |-- location: string (nullable = true)\n",
    ">      |-- date: string (nullable = true)\n",
    ">      |-- total_cases: double (nullable = false)\n",
    ">      |-- new_cases: double (nullable = true)\n",
    ">      |-- new_cases_smoothed: double (nullable = false)\n",
    ">      |-- total_deaths: double (nullable = false)\n",
    ">      |-- new_deaths: double (nullable = true)\n",
    ">      |-- new_deaths_smoothed: double (nullable = false)\n",
    ">      |-- reproduction_rate: double (nullable = false)\n",
    ">      |-- icu_patients: double (nullable = true)\n",
    ">      |-- icu_patients_per_million: double (nullable = true)\n",
    ">      |-- hosp_patients: double (nullable = true)\n",
    ">      |-- hosp_patients_per_million: double (nullable = true)\n",
    ">      |-- weekly_icu_admissions: double (nullable = true)\n",
    ">      |-- weekly_icu_admissions_per_million: double (nullable = true)\n",
    ">      |-- weekly_hosp_admissions: double (nullable = true)\n",
    ">      |-- weekly_hosp_admissions_per_million: double (nullable = true)\n",
    ">      |-- total_tests: double (nullable = false)\n",
    ">      |-- new_tests: double (nullable = true)\n",
    ">      |-- total_tests_per_thousand: double (nullable = true)\n",
    ">      |-- new_tests_per_thousand: double (nullable = true)\n",
    ">      |-- new_tests_smoothed: double (nullable = true)\n",
    ">      |-- new_tests_smoothed_per_thousand: double (nullable = true)\n",
    ">      |-- tests_per_case: double (nullable = true)\n",
    ">      |-- positive_rate: double (nullable = true)\n",
    ">      |-- tests_units: double (nullable = true)\n",
    ">      |-- stringency_index: double (nullable = false)\n",
    ">      |-- population: double (nullable = true)\n",
    ">      |-- population_density: double (nullable = true)\n",
    ">      |-- median_age: double (nullable = true)\n",
    ">      |-- aged_65_older: double (nullable = true)\n",
    ">      |-- aged_70_older: double (nullable = true)\n",
    ">      |-- gdp_per_capita: double (nullable = true)\n",
    ">      |-- extreme_poverty: double (nullable = true)\n",
    ">      |-- cardiovasc_death_rate: double (nullable = true)\n",
    ">      |-- diabetes_prevalence: double (nullable = true)\n",
    ">      |-- female_smokers: double (nullable = true)\n",
    ">      |-- male_smokers: double (nullable = true)\n",
    ">      |-- handwashing_facilities: double (nullable = true)\n",
    ">      |-- hospital_beds_per_thousand: double (nullable = true)\n",
    ">      |-- life_expectancy: double (nullable = true)\n",
    ">      |-- human_development_index: double (nullable = true)\n",
    ">      |-- total_cases_per_million: double (nullable = true)\n",
    ">      |-- new_cases_per_million: double (nullable = true)\n",
    ">      |-- new_cases_smoothed_per_million: double (nullable = true)\n",
    ">      |-- total_deaths_per_million: double (nullable = true)\n",
    ">      |-- new_deaths_per_million: double (nullable = true)\n",
    ">      |-- new_deaths_smoothed_per_million: double (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// There is no \"World\" in the 126 countries. we need to calculate it.\n",
    "val countries = df_cleaned_time_series.groupBy(\"location\").count().sort($\"location\")\n",
    "display(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "### 2.1 Data for all over the world\n",
    "\n",
    "#### 2.1.1 The smoothed new cases of the world.\n",
    "\n",
    "We use the smoothed new cases because the raw data fluctuates greatly by\n",
    "day - on workdays, there are more new cases than on weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// prediction for all over the world\n",
    "import org.apache.spark.sql.functions._\n",
    "// val df_world = df_cleaned_time_series.withColumn(\"date\", (col(\"date\").cast(\"Timestamp\"))).where(\"location == 'World'\").select($\"date\",$\"new_cases_smoothed\")\n",
    "\n",
    "val df_world = df_cleaned_time_series.groupBy(\"date\").sum(\"new_cases_smoothed\").sort(col(\"date\")).withColumnRenamed(\"sum(new_cases_smoothed)\",\"new_cases_smoothed\")\n",
    "\n",
    "display(df_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    ">     res14: Long = 62500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_world.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     root\n",
    ">      |-- date: string (nullable = true)\n",
    ">      |-- new_cases_smoothed: double (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_world.createOrReplaceTempView(\"df_world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "#### 2.1.2 The smoothed new deaths of the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val df_world_deaths = df_cleaned_time_series.withColumn(\"date\", (col(\"date\").cast(\"Timestamp\"))).where(\"location == 'World'\").select($\"date\",$\"new_deaths_smoothed\")\n",
    "val df_world_deaths = df_cleaned_time_series.groupBy(\"date\").sum(\"new_deaths_smoothed\").sort(col(\"date\")).withColumnRenamed(\"sum(new_deaths_smoothed)\",\"new_deaths_smoothed\")\n",
    "\n",
    "df_world_deaths.createOrReplaceTempView(\"df_world_deaths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     df_world_deaths: org.apache.spark.sql.DataFrame = [date: string, new_deaths_smoothed: double]\n",
    "\n",
    "  \n",
    "\n",
    "### 2.2 Data for Sweden\n",
    "\n",
    "#### 2.2.1 The smoothed new cases of Sweden\n",
    "\n",
    "In addition to the new cases all over the world, we also care about the\n",
    "cases in Sweden. Here we deal with smoothed new cases of Sweden.\n",
    "\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Select one contry for prediction\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val df_sw = df_cleaned_time_series.withColumn(\"date\", (col(\"date\").cast(\"Timestamp\"))).where(\"location == 'Sweden'\").select($\"date\",$\"new_cases_smoothed\")\n",
    "display(df_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    ">     df_filteredLocation: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     df_fillContinentNull: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     res15: df_filteredLocation.type = [iso_code: string, continent: string ... 48 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sw.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     root\n",
    ">      |-- date: timestamp (nullable = true)\n",
    ">      |-- new_cases_smoothed: double (nullable = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sw.createOrReplaceTempView(\"df_sw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "#### 2.2.2 The smoothed new deaths of Sweden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_sw_deaths = df_cleaned_time_series.withColumn(\"date\", (col(\"date\").cast(\"Timestamp\"))).where(\"location == 'Sweden'\").select($\"date\",$\"new_deaths_smoothed\")\n",
    "df_sw_deaths.createOrReplaceTempView(\"df_sw_deaths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     df_sw_deaths: org.apache.spark.sql.DataFrame = [date: timestamp, new_deaths_smoothed: double]\n",
    "\n",
    "  \n",
    "\n",
    ">     df_filtered_date: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     res17: df_fillContinentNull.type = [iso_code: string, continent: string ... 48 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "3. Time series regression with ARIMA\n",
    "------------------------------------\n",
    "\n",
    "ARIMA - Autoregressive Integrated Moving Average model. It's widely used\n",
    "in time series analysis. see defination here:\n",
    "https://en.wikipedia.org/wiki/Autoregressive*integrated*moving\\_average\n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    ">     df_cleaned_feature_permillion: org.apache.spark.sql.DataFrame = [iso_code: string, continent: string ... 48 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some libraries\n",
    "# dbutils.library.installPyPI('numpy','1.16.3')\n",
    "# dbutils.library.installPyPI('pandas','1.1.5')\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "### 3.1 Prediction for all over the world\n",
    "\n",
    "#### 3.1.1 Prediction of smoothed new cases (one\\_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from matplotlib import pyplot\n",
    "print(pandas.__version__)\n",
    "data = spark.table(\"df_world\")\n",
    "\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     1.0.1\n",
    ">     <class 'pyspark.sql.dataframe.DataFrame'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "data_pd = data.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import sklearn\n",
    "import statsmodels\n",
    "from datetime import date\n",
    "print(data_pd.columns)\n",
    "data_pd['date'] = pd.to_datetime(data_pd['date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Index(['date', 'new_cases_smoothed'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd.plot(x='date', y = 'new_cases_smoothed', figsize=(8,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def Predict_by_ARIMA(data_pd, one_step = True, training_length = 0.9):\n",
    "  data_pd1 = data_pd.set_index('date')\n",
    "  X = data_pd1.values\n",
    "  train_size = int(len(X) * training_length) #the length you need for training.\n",
    "  train, test = X[0:train_size], X[train_size:len(X)]\n",
    "  test_date = data_pd1.index[train_size:len(X)]\n",
    "  history = [x for x in train]\n",
    "  predictions = list()\n",
    "  print(\"training_series_size: \", train_size)\n",
    "  print(\"test_series_size: \", len(test))\n",
    "  for t in range(len(test)):\n",
    "    model = ARIMA(history, order=(2,1,0))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    if one_step:\n",
    "      obs = test[t] # use real value, only predict next step\n",
    "    else:\n",
    "      obs = yhat # use predicted value, predict all test data\n",
    "    history.append(obs)\n",
    "    current_date = test_date[t]\n",
    "    print(str(current_date.date()), 'pred=%f, gt=%f' % (yhat, obs))\n",
    "  return test, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_world, predictions_world = Predict_by_ARIMA(data_pd, True, 0.9)\n",
    "print(\"test size: \", len(test_world))\n",
    "print(\"predicted size: \", len(predictions_world))\n",
    "# plot\n",
    "fig_world = pyplot.figure()  \n",
    "pyplot.plot(test_world)\n",
    "pyplot.plot(predictions_world, color='red')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "#### 3.1.2 Prediction of smoothed new cases (multi\\_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_pd)\n",
    "_, predictions_world_multi = Predict_by_ARIMA(data_pd, False)\n",
    "print(\"test size: \", len(test_world))\n",
    "print(\"predicted size: \", len(predictions_world))\n",
    "# plot\n",
    "fig_world_multi = pyplot.figure()  \n",
    "pyplot.plot(test_world)\n",
    "pyplot.plot(predictions_world_multi, color='red')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "#### 3.1.3 Prediction of smoothed new deaths (one\\_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_world_death = spark.table(\"df_world_deaths\")\n",
    "data_world_death_pd = data_world_death.toPandas()\n",
    "print(data_world_death_pd.columns)\n",
    "data_world_death_pd['date'] = pd.to_datetime(data_world_death_pd['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Index(['date', 'new_deaths_smoothed'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_world_death, predictions_world_death = Predict_by_ARIMA(data_world_death_pd)\n",
    "print(\"test size: \", len(test_world_death))\n",
    "print(\"predicted size: \", len(predictions_world_death))\n",
    "# plot\n",
    "fig_world_death = pyplot.figure()  \n",
    "pyplot.plot(test_world_death)\n",
    "pyplot.plot(predictions_world_death, color='red')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "#### 3.1.4 Prediction of smoothed new deaths (multi\\_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions_world_death_multi = Predict_by_ARIMA(data_world_death_pd, False)\n",
    "print(\"test size: \", len(test_world_death))\n",
    "print(\"predicted size: \", len(predictions_world_death_multi))\n",
    "# plot\n",
    "fig_world_death = pyplot.figure()  \n",
    "pyplot.plot(test_world_death)\n",
    "pyplot.plot(predictions_world_death_multi, color='red')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### 3.2 Prediction for Sweden\n",
    "\n",
    "#### 3.2.1 Prediction of smoothed new cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "from pyspark.sql.functions import *\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import sklearn\n",
    "import statsmodels\n",
    "\n",
    "data_sw = spark.table(\"df_sw\")\n",
    "data_sw_pd = data_sw.toPandas()\n",
    "print(data_sw_pd.columns)\n",
    "data_sw_pd['date'] = pd.to_datetime(data_sw_pd['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Index(['date', 'new_cases_smoothed'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sw_pd.plot(x='date', y = 'new_cases_smoothed', figsize=(8,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sw, predictions_sw = Predict_by_ARIMA(data_sw_pd)\n",
    "print(\"test size: \", len(test_sw))\n",
    "print(\"predicted size: \", len(predictions_sw))\n",
    "# plot\n",
    "fig_sw = pyplot.figure()  \n",
    "pyplot.plot(test_sw)\n",
    "pyplot.plot(predictions_sw, color='red')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "#### 3.2.2 Prediction of smoothed new cases (multi\\_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions_sw_multi = Predict_by_ARIMA(data_sw_pd, False)\n",
    "print(\"test size: \", len(test_sw))\n",
    "print(\"predicted size: \", len(predictions_sw))\n",
    "# plot\n",
    "fig_sw = pyplot.figure()  \n",
    "pyplot.plot(test_sw)\n",
    "pyplot.plot(predictions_sw_multi, color='red')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "#### 3.2.3 Prediction of smoothed new deaths (one\\_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sw_death = spark.table(\"df_sw_deaths\")\n",
    "data_sw_death_pd = data_sw_death.toPandas()\n",
    "print(data_sw_death_pd.columns)\n",
    "data_sw_death_pd['date'] = pd.to_datetime(data_sw_death_pd['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Index(['date', 'new_deaths_smoothed'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sw_death, predictions_sw_death = Predict_by_ARIMA(data_sw_death_pd)\n",
    "print(\"test size: \", len(test_sw_death))\n",
    "print(\"predicted size: \", len(predictions_sw_death))\n",
    "# plot\n",
    "fig_sw_death = pyplot.figure()  \n",
    "pyplot.plot(test_sw_death)\n",
    "pyplot.plot(predictions_sw_death, color='red')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "#### 3.2.4 Prediction of smoothed new deaths (multi\\_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions_sw_death_multi = Predict_by_ARIMA(data_sw_death_pd, False)\n",
    "print(\"test size: \", len(test_sw_death))\n",
    "print(\"predicted size: \", len(predictions_sw_death_multi))\n",
    "# plot\n",
    "fig_sw_death = pyplot.figure()  \n",
    "pyplot.plot(test_sw_death)\n",
    "pyplot.plot(predictions_sw_death_multi, color='red')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "4. Evaluation\n",
    "-------------\n",
    "\n",
    "### 4.1 Evaluation of world result\n",
    "\n",
    "#### 4.1.1 Evaluation of new cases smoothed (one\\_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def Evaluation(test, predictions):\n",
    "  error_mse = mean_squared_error(test, predictions)\n",
    "  error_rmse = math.sqrt(error_mse)\n",
    "  error_abs = mean_absolute_error(test, predictions)\n",
    "  avg_gt = test[:,0].sum() / len(test)\n",
    "\n",
    "  mse_percentage = error_rmse / avg_gt * 100\n",
    "  abs_percentage = error_abs / avg_gt * 100\n",
    "  print('Average of groundtruth: %.3f' % avg_gt)\n",
    "  print('Test MSE: %.3f' % error_mse)\n",
    "  print('Test RMSE: %.3f' % error_rmse)\n",
    "  print('RMSE percentage error: %.3f' % mse_percentage, '%')\n",
    "  print('Test ABS: %.3f' % error_abs)\n",
    "  print('ABS percentage error: %.3f' % abs_percentage, '%')\n",
    "\n",
    "Evaluation(test_world, predictions_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 763744.916\n",
    ">     Test MSE: 1382104096.548\n",
    ">     Test RMSE: 37176.661\n",
    ">     RMSE percentage error: 4.868 %\n",
    ">     Test ABS: 19644.392\n",
    ">     ABS percentage error: 2.572 %\n",
    "\n",
    "  \n",
    "\n",
    "#### 4.1.2 Evaluation of new cases smoothed (multi\\_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation(test_world, predictions_world_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 763744.916\n",
    ">     Test MSE: 16372759781.212\n",
    ">     Test RMSE: 127956.085\n",
    ">     RMSE percentage error: 16.754 %\n",
    ">     Test ABS: 114514.607\n",
    ">     ABS percentage error: 14.994 %\n",
    "\n",
    "  \n",
    "\n",
    "#### 4.1.3 Evaluation of new death smoothed (one\\_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation(test_world_death, predictions_world_death)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 11412.091\n",
    ">     Test MSE: 358938.532\n",
    ">     Test RMSE: 599.115\n",
    ">     RMSE percentage error: 5.250 %\n",
    ">     Test ABS: 290.352\n",
    ">     ABS percentage error: 2.544 %\n",
    "\n",
    "  \n",
    "\n",
    "#### 4.1.4 Evaluation of new death smoothed (multi\\_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation(test_world_death, predictions_world_death_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 11412.091\n",
    ">     Test MSE: 5567620.973\n",
    ">     Test RMSE: 2359.581\n",
    ">     RMSE percentage error: 20.676 %\n",
    ">     Test ABS: 2110.218\n",
    ">     ABS percentage error: 18.491 %\n",
    "\n",
    "  \n",
    "\n",
    "### 4.2 Evaluation of Sweden results\n",
    "\n",
    "#### 4.2.1 Evaluation of new cases smoothed (one\\_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation(test_sw, predictions_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 4564.277\n",
    ">     Test MSE: 26457.989\n",
    ">     Test RMSE: 162.659\n",
    ">     RMSE percentage error: 3.564 %\n",
    ">     Test ABS: 82.412\n",
    ">     ABS percentage error: 1.806 %\n",
    "\n",
    "  \n",
    "\n",
    "#### 4.2.2 Evaluation of new cases smoothed (multi\\_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation(test_sw, predictions_sw_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 4564.277\n",
    ">     Test MSE: 1514028.102\n",
    ">     Test RMSE: 1230.458\n",
    ">     RMSE percentage error: 26.958 %\n",
    ">     Test ABS: 1172.056\n",
    ">     ABS percentage error: 25.679 %\n",
    "\n",
    "  \n",
    "\n",
    "#### 4.2.3 Evaluation of new death smoothed (one\\_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation(test_sw_death, predictions_sw_death)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 31.862\n",
    ">     Test MSE: 24.933\n",
    ">     Test RMSE: 4.993\n",
    ">     RMSE percentage error: 15.672 %\n",
    ">     Test ABS: 3.072\n",
    ">     ABS percentage error: 9.643 %\n",
    "\n",
    "  \n",
    "\n",
    "#### 4.2.4 Evaluation of new death smoothed (multi\\_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation(test_sw_death, predictions_sw_death_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 31.862\n",
    ">     Test MSE: 775.703\n",
    ">     Test RMSE: 27.851\n",
    ">     RMSE percentage error: 87.414 %\n",
    ">     Test ABS: 25.094\n",
    ">     ABS percentage error: 78.759 %\n",
    "\n",
    "  \n",
    "\n",
    "5. Conclusion and Reflections\n",
    "-----------------------------\n",
    "\n",
    "With this time series method - ARIMA, we can get quite resonable\n",
    "results. We predicted the new cases (smoothed) and new deaths (smoothed)\n",
    "for world and Sweden. The evaluation of one step shows that we can get\n",
    "good results with a small error. But the multi step results is not good\n",
    "when we want to predict long term results. The prediction model and\n",
    "evaluation function can also been used for other countries."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
