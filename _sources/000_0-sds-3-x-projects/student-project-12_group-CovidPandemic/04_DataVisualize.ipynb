{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Python interpreter will be restarted.\n",
    ">     Collecting plotly\n",
    ">       Downloading plotly-4.14.1-py2.py3-none-any.whl (13.2 MB)\n",
    ">     Requirement already satisfied: six in /databricks/python3/lib/python3.7/site-packages (from plotly) (1.14.0)\n",
    ">     Collecting retrying>=1.3.3\n",
    ">       Downloading retrying-1.3.3.tar.gz (10 kB)\n",
    ">     Building wheels for collected packages: retrying\n",
    ">       Building wheel for retrying (setup.py): started\n",
    ">       Building wheel for retrying (setup.py): finished with status 'done'\n",
    ">       Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11430 sha256=101a7a19dc37e3a3cc83c2afd803521c5a78523e38452aaf86894fda4d149735\n",
    ">       Stored in directory: /root/.cache/pip/wheels/f9/8d/8d/f6af3f7f9eea3553bc2fe6d53e4b287dad18b06a861ac56ddf\n",
    ">     Successfully built retrying\n",
    ">     Installing collected packages: retrying, plotly\n",
    ">     Successfully installed plotly-4.14.1 retrying-1.3.3\n",
    ">     Python interpreter will be restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"./DataPreprocess\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "### Show reproduction rate of selected countries i.e. Sweden, Germany, Danmark, Finland, Norway\n",
    "\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "  \n",
    "\n",
    ">     df_filteredLocation: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     df_fillContinentNull: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     res3: df_filteredLocation.type = [iso_code: string, continent: string ... 48 more fields]\n",
    "\n",
    "  \n",
    "\n",
    ">     df_filtered_date: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     res5: df_fillContinentNull.type = [iso_code: string, continent: string ... 48 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cleaned_time_series.select($\"reproduction_rate\", $\"location\", $\"date\").filter($\"reproduction_rate\".isNotNull).filter(\n",
    "                                                                                       $\"location\"===\"Sweden\" ||\n",
    "                                                                                       $\"location\"===\"Germany\" ||\n",
    "                                                                                       $\"location\"===\"Danmark\" ||\n",
    "                                                                                       $\"location\"===\"Finland\" ||\n",
    "                                                                                       $\"location\"===\"Norway\").sort(\"date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    ">     df_cleaned_feature_permillion: org.apache.spark.sql.DataFrame = [iso_code: string, continent: string ... 48 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "### Visualize total cases, total deaths, new cases and new deaths during pandemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_time_series.createOrReplaceTempView(\"visual_rdd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table = spark.table(\"visual_rdd\")\n",
    "\n",
    "country = np.array(test_table.select(\"iso_code\").rdd.map(lambda l: l[0]).collect())\n",
    "dates = np.array(test_table.select(\"date\").rdd.map(lambda l: l[0]).collect())\n",
    "total_cases = np.array(test_table.select(\"total_cases\").rdd.map(lambda l: l[0]).collect())\n",
    "total_deaths = np.array(test_table.select(\"total_deaths\").rdd.map(lambda l: l[0]).collect())\n",
    "new_cases = np.array(test_table.select(\"new_cases\").rdd.map(lambda l: l[0]).collect())\n",
    "new_deaths = np.array(test_table.select(\"new_deaths\").rdd.map(lambda l: l[0]).collect())\n",
    "\n",
    "visual_data = {'country':country.tolist(), 'total_cases':total_cases, 'date':dates, \n",
    "             'total_deaths': total_deaths, 'new_cases': new_cases, 'new_deaths': new_deaths}\n",
    "visual_df = pd.DataFrame(data = visual_data).sort_values(by='date')\n",
    "visual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Total Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(visual_df[~visual_df.country.str.contains(\"WLD\", na=False)], locations=\"country\",\n",
    "                    color=\"total_cases\", # total_cases is a column of gapminder\n",
    "                    hover_name=\"country\", # column to add to hover information\n",
    "                    color_continuous_scale=px.colors.sequential.Plasma,\n",
    "                    animation_frame = 'date')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Total Deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(visual_df[~visual_df.country.str.contains(\"WLD\", na=False)], locations=\"country\",\n",
    "                    color=\"total_deaths\", # total_deaths is a column of gapminder\n",
    "                    hover_name=\"country\", # column to add to hover information\n",
    "                    color_continuous_scale=px.colors.sequential.Plasma,\n",
    "                    animation_frame = 'date')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### New Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(visual_df[~visual_df.country.str.contains(\"WLD\", na=False)], locations=\"country\",\n",
    "                    color=\"new_cases\", # new_cases is a column of gapminder\n",
    "                    hover_name=\"country\", # column to add to hover information\n",
    "                    color_continuous_scale=px.colors.sequential.Plasma,\n",
    "                    animation_frame = 'date')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### New Deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(visual_df[~visual_df.country.str.contains(\"WLD\", na=False)], locations=\"country\",\n",
    "                    color=\"new_deaths\", # new_deaths is a column of gapminder\n",
    "                    hover_name=\"country\", # column to add to hover information\n",
    "                    color_continuous_scale=px.colors.sequential.Plasma,\n",
    "                    animation_frame = 'date')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
