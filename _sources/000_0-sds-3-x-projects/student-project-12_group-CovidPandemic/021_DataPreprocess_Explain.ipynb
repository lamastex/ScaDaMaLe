{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/datasets/group12/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "  \n",
    "\n",
    "Load parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.parquet(\"dbfs:/datasets/group12/analysis/*.parquet\")\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "Load csv file\n",
    "\n",
    "-&gt; %scala //if want to load csv\n",
    "\n",
    "val file*location = \"/datasets/group12/20*12*04*10*47*08.csv\" val\n",
    "file\\_type = \"csv\"\n",
    "\n",
    "// CSV options val infer*schema = \"true\" val first*row*is*header =\n",
    "\"true\" val delimiter = \",\"\n",
    "\n",
    "// The applied options are for CSV files. For other file types, these\n",
    "will be ignored. val df = spark.read.format(file*type)\n",
    ".option(\"inferSchema\", infer*schema) .option(\"header\",\n",
    "first*row*is*header) .option(\"sep\", delimiter) .load(file*location)\n",
    "\n",
    "display(df)\n",
    "\n",
    "Number of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res19: Long = 60544\n",
    "\n",
    "  \n",
    "\n",
    "Missing Features in data due to multiple web resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "for (c <- df.columns) {\n",
    "  println(c + \": \" + df.filter(col(c).isNull).count())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     iso_code: 316\n",
    ">     continent: 633\n",
    ">     location: 0\n",
    ">     date: 0\n",
    ">     total_cases: 8739\n",
    ">     new_cases: 123\n",
    ">     new_cases_smoothed: 1079\n",
    ">     total_deaths: 16597\n",
    ">     new_deaths: 123\n",
    ">     new_deaths_smoothed: 1079\n",
    ">     total_cases_per_million: 9040\n",
    ">     new_cases_per_million: 439\n",
    ">     new_cases_smoothed_per_million: 1390\n",
    ">     total_deaths_per_million: 16885\n",
    ">     new_deaths_per_million: 439\n",
    ">     new_deaths_smoothed_per_million: 1390\n",
    ">     reproduction_rate: 20494\n",
    ">     icu_patients: 55821\n",
    ">     icu_patients_per_million: 55821\n",
    ">     hosp_patients: 54834\n",
    ">     hosp_patients_per_million: 54834\n",
    ">     weekly_icu_admissions: 60083\n",
    ">     weekly_icu_admissions_per_million: 60083\n",
    ">     weekly_hosp_admissions: 59796\n",
    ">     weekly_hosp_admissions_per_million: 59796\n",
    ">     total_tests: 36667\n",
    ">     new_tests: 36819\n",
    ">     total_tests_per_thousand: 36667\n",
    ">     new_tests_per_thousand: 36819\n",
    ">     new_tests_smoothed: 34084\n",
    ">     new_tests_smoothed_per_thousand: 34084\n",
    ">     tests_per_case: 35333\n",
    ">     positive_rate: 35924\n",
    ">     tests_units: 60544\n",
    ">     stringency_index: 7787\n",
    ">     population: 316\n",
    ">     population_density: 1903\n",
    ">     median_age: 3160\n",
    ">     aged_65_older: 4115\n",
    ">     aged_70_older: 3476\n",
    ">     gdp_per_capita: 3167\n",
    ">     extreme_poverty: 21265\n",
    ">     cardiovasc_death_rate: 2598\n",
    ">     diabetes_prevalence: 1903\n",
    ">     female_smokers: 15569\n",
    ">     male_smokers: 16201\n",
    ">     handwashing_facilities: 30455\n",
    ">     hospital_beds_per_thousand: 7662\n",
    ">     life_expectancy: 632\n",
    ">     human_development_index: 2852\n",
    ">     import org.apache.spark.sql.functions._\n",
    "\n",
    "  \n",
    "\n",
    "Preprocessing\n",
    "-------------\n",
    "\n",
    "### 0. filter out data of HongKong and unknown location\n",
    "\n",
    "Here shows HK does not have meaningful value and there is one unknown\n",
    "international location in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.filter($\"location\"===\"Hong Kong\" || $\"iso_code\".isNull)) //HK data iteself is not complete for all dates, and all available data is null! HAVE TO FILTER IT OUT COMPLETELY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "190 valid countries data to continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_filteredLocation = df.filter($\"iso_code\"=!=\"HKG\").filter($\"iso_code\".isNotNull)\n",
    "display(df_filteredLocation.select($\"location\").distinct()) // 190 valid countries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "Fill missing continent value for World aggregate data NOTE: it will be\n",
    "filled as \"World\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_filteredLocation.where($\"continent\".isNull))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_fillContinentNull = df_filteredLocation.na.fill(\"World\",Array(\"continent\"))\n",
    "display(df_fillContinentNull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fillContinentNull.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res27: Long = 60158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "for (c <- df_fillContinentNull.columns) {\n",
    "  println(c + \": \" + df_fillContinentNull.filter(col(c).isNull).count())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     iso_code: 0\n",
    ">     continent: 0\n",
    ">     location: 0\n",
    ">     date: 0\n",
    ">     total_cases: 8654\n",
    ">     new_cases: 53\n",
    ">     new_cases_smoothed: 1004\n",
    ">     total_deaths: 16499\n",
    ">     new_deaths: 53\n",
    ">     new_deaths_smoothed: 1004\n",
    ">     total_cases_per_million: 8654\n",
    ">     new_cases_per_million: 53\n",
    ">     new_cases_smoothed_per_million: 1004\n",
    ">     total_deaths_per_million: 16499\n",
    ">     new_deaths_per_million: 53\n",
    ">     new_deaths_smoothed_per_million: 1004\n",
    ">     reproduction_rate: 20108\n",
    ">     icu_patients: 55435\n",
    ">     icu_patients_per_million: 55435\n",
    ">     hosp_patients: 54448\n",
    ">     hosp_patients_per_million: 54448\n",
    ">     weekly_icu_admissions: 59697\n",
    ">     weekly_icu_admissions_per_million: 59697\n",
    ">     weekly_hosp_admissions: 59410\n",
    ">     weekly_hosp_admissions_per_million: 59410\n",
    ">     total_tests: 36294\n",
    ">     new_tests: 36433\n",
    ">     total_tests_per_thousand: 36294\n",
    ">     new_tests_per_thousand: 36433\n",
    ">     new_tests_smoothed: 33753\n",
    ">     new_tests_smoothed_per_thousand: 33753\n",
    ">     tests_per_case: 34947\n",
    ">     positive_rate: 35538\n",
    ">     tests_units: 60158\n",
    ">     stringency_index: 7471\n",
    ">     population: 0\n",
    ">     population_density: 1587\n",
    ">     median_age: 2844\n",
    ">     aged_65_older: 3799\n",
    ">     aged_70_older: 3160\n",
    ">     gdp_per_capita: 2851\n",
    ">     extreme_poverty: 20879\n",
    ">     cardiovasc_death_rate: 2212\n",
    ">     diabetes_prevalence: 1587\n",
    ">     female_smokers: 15183\n",
    ">     male_smokers: 15815\n",
    ">     handwashing_facilities: 30069\n",
    ">     hospital_beds_per_thousand: 7276\n",
    ">     life_expectancy: 316\n",
    ">     human_development_index: 2536\n",
    ">     import org.apache.spark.sql.functions._\n",
    "\n",
    "  \n",
    "\n",
    "### 1. filter dates only from 2020-01-23 (to ensure all countries having 316 days logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_fillContinentNull.select($\"date\",$\"iso_code\").groupBy($\"iso_code\").count())  // some country starts logging data earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_filtered_date = df_fillContinentNull.filter($\"date\">\"2020-01-22\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     df_filtered_date: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_filtered_date.select($\"date\",$\"iso_code\").groupBy($\"iso_code\").count())  // all countries have 316 days logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "### 2. Fill missing value for total*cases, total*deaths, new*cases*smoothed, new*deaths*smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_filtered_date.select($\"date\",$\"iso_code\", $\"total_cases\", $\"total_deaths\", $\"new_cases\", $\"new_deaths\", $\"new_cases_smoothed\", $\"new_deaths_smoothed\").filter($\"new_cases_smoothed\".isNull || $\"new_deaths_smoothed\".isNull))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "All missing data of new*cases*smoothed and new*deaths*smoothed from\n",
    "early, so just fill with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_filtered_date.select($\"date\",$\"iso_code\", $\"total_cases\", $\"total_deaths\", $\"new_cases\", $\"new_deaths\", $\"new_cases_smoothed\", $\"new_deaths_smoothed\")\n",
    "        .filter($\"new_cases_smoothed\".isNull || $\"new_deaths_smoothed\".isNull).select($\"date\").distinct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_fillNullForSmooth = df_filtered_date.na.fill(0,Array(\"new_cases_smoothed\"))\n",
    "                           .na.fill(0,Array(\"new_deaths_smoothed\"))\n",
    "display(df_fillNullForSmooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "Fill total*deaths and total*cases null value\n",
    "\n",
    "Strictly, when new*cases is always 0, total*cases could be imputed as 0.\n",
    "The same apply to total\\_deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_NULL_total_cases = df_fillNullForSmooth.select($\"date\",$\"iso_code\", $\"total_cases\", $\"total_deaths\", $\"new_cases\", $\"new_deaths\", $\"new_cases_smoothed\", $\"new_deaths_smoothed\")\n",
    "                          .filter($\"total_cases\".isNull)\n",
    "\n",
    "\n",
    "display(df_NULL_total_cases.filter($\"new_cases\"===0).groupBy(\"iso_code\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "When total*case is Null, all previous new*cases is always 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NULL_total_cases.filter($\"total_cases\".isNull).groupBy(\"iso_code\").count().except(df_NULL_total_cases.filter($\"new_cases\"===0).groupBy(\"iso_code\").count()).show() // When total_case is Null, all new_cases is always 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +--------+-----+\n",
    ">     |iso_code|count|\n",
    ">     +--------+-----+\n",
    ">     +--------+-----+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_fillNullForTotalCases = df_fillNullForSmooth.na.fill(0, Array(\"total_cases\"))\n",
    "                               \n",
    "display(df_fillNullForTotalCases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_NULL_total_death = df_fillNullForTotalCases.select($\"date\",$\"iso_code\", $\"total_cases\", $\"total_deaths\", $\"new_cases\", $\"new_deaths\", $\"new_cases_smoothed\", $\"new_deaths_smoothed\")\n",
    "                          .filter($\"total_deaths\".isNull)\n",
    "\n",
    "\n",
    "display(df_NULL_total_death.filter($\"new_deaths\"===0).groupBy(\"iso_code\").count().sort())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "If total*deaths is Null when all new*deaths is always 0, then we could\n",
    "simply assign 0 for NULL, otherwise need to investigate more.\n",
    "\n",
    "Three countries (ISL, PNG, SVK) have abnormal correction on new\\_cases\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val abnormal_countries = df_NULL_total_death.filter($\"total_deaths\".isNull).groupBy(\"iso_code\").count().except(df_NULL_total_death.filter($\"new_deaths\"===0).groupBy(\"iso_code\").count())\n",
    "abnormal_countries.show()\n",
    "df_NULL_total_death.filter($\"new_deaths\"===0).groupBy(\"iso_code\").count().except(df_NULL_total_death.filter($\"total_deaths\".isNull).groupBy(\"iso_code\").count()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +--------+-----+\n",
    ">     |iso_code|count|\n",
    ">     +--------+-----+\n",
    ">     |     PNG|  186|\n",
    ">     |     SVK|   65|\n",
    ">     |     ISL|   54|\n",
    ">     +--------+-----+\n",
    ">\n",
    ">     +--------+-----+\n",
    ">     |iso_code|count|\n",
    ">     +--------+-----+\n",
    ">     |     PNG|  185|\n",
    ">     |     SVK|   64|\n",
    ">     |     ISL|   52|\n",
    ">     +--------+-----+\n",
    ">\n",
    ">     abnormal_countries: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, count: bigint]\n",
    "\n",
    "  \n",
    "\n",
    "show abnormal death correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_fillNullForSmooth.filter($\"iso_code\"===\"ISL\").sort(\"date\").filter($\"date\">\"2020-03-13\" && $\"date\"<\"2020-03-22\")) // death data correction between 2020-03-14 and 2020-03-21, total_deaths -> all 0, new_deaths -> all 0, new_deaths_smoothed -> all 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_fillNullForSmooth.filter($\"iso_code\"===\"PNG\").sort(\"date\").filter($\"date\">\"2020-07-19\" && $\"date\"<\"2020-07-24\" )) // death data correction between 2020-07-20 and 2020-07-22, total_deaths -> all 0, new_deaths -> all 0, new_deaths_smoothed -> all 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_fillNullForSmooth.filter($\"iso_code\"===\"SVK\").sort(\"date\").filter($\"date\">\"2020-03-16\" && $\"date\"<\"2020-03-23\")) // death data correction between 2020-03-18 and 2020-03-22, total_deaths -> all 0, new_deaths -> all 0, new_deaths_smoothed -> all 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "Correct new\\_deaths correction back to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_fillNullForTotalDeathsSpecial = df_fillNullForTotalCases.withColumn(\"total_deaths_correct\", \n",
    "                                        when(col(\"iso_code\").equalTo(\"ISL\")&&(col(\"date\")>\"2020-03-13\" && col(\"date\")<\"2020-03-22\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"PNG\")&&(col(\"date\")>\"2020-07-19\" && col(\"date\")<\"2020-07-23\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"SVK\")&&(col(\"date\")>\"2020-03-17\" && col(\"date\")<\"2020-03-23\"),0).otherwise(col(\"total_deaths\")))\n",
    "                            .withColumn(\"new_deaths_correct\", \n",
    "                                        when(col(\"iso_code\").equalTo(\"ISL\")&&(col(\"date\")>\"2020-03-13\" && col(\"date\")<\"2020-03-22\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"PNG\")&&(col(\"date\")>\"2020-07-19\" && col(\"date\")<\"2020-07-23\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"SVK\")&&(col(\"date\")>\"2020-03-17\" && col(\"date\")<\"2020-03-23\"),0).otherwise(col(\"new_deaths\")))\n",
    "                            .withColumn(\"new_deaths_smoothed_correct\", \n",
    "                                        when(col(\"iso_code\").equalTo(\"ISL\")&&(col(\"date\")>\"2020-03-13\" && col(\"date\")<\"2020-03-22\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"PNG\")&&(col(\"date\")>\"2020-07-19\" && col(\"date\")<\"2020-07-23\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"SVK\")&&(col(\"date\")>\"2020-03-17\" && col(\"date\")<\"2020-03-23\"),0).otherwise(col(\"new_deaths_smoothed\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     df_fillNullForTotalDeathsSpecial: org.apache.spark.sql.DataFrame = [iso_code: string, continent: string ... 51 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "Expect to see an empty table, so correction is right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_NULL_total_death_ = df_fillNullForTotalDeathsSpecial.select($\"date\",$\"iso_code\", $\"total_cases\", $\"total_deaths_correct\", $\"new_cases\", $\"new_deaths_correct\", $\"new_cases_smoothed\", $\"new_deaths_smoothed_correct\")\n",
    "                          .filter($\"total_deaths_correct\".isNull)\n",
    "\n",
    "\n",
    "df_NULL_total_death_.filter($\"total_deaths_correct\".isNull).groupBy(\"iso_code\").count().except(df_NULL_total_death_.filter($\"new_deaths_correct\"===0).groupBy(\"iso_code\").count()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +--------+-----+\n",
    ">     |iso_code|count|\n",
    ">     +--------+-----+\n",
    ">     +--------+-----+\n",
    ">\n",
    ">     df_NULL_total_death_: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [date: string, iso_code: string ... 6 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "fill rest NULL value for total\\_death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_fillNullForTotalDeaths = df_fillNullForTotalDeathsSpecial\n",
    "                                  .drop(\"new_deaths\", \"total_deaths\", \"new_deaths_smoothed\") // drop old column to rename\n",
    "                                  .withColumnRenamed(\"new_deaths_correct\",\"new_deaths\")\n",
    "                                  .withColumnRenamed(\"total_deaths_correct\",\"total_deaths\")\n",
    "                                  .withColumnRenamed(\"new_deaths_smoothed_correct\",\"new_deaths_smoothed\")\n",
    "                                  .na.fill(0, Array(\"total_deaths\"))\n",
    "                                  .select(df.columns.head, df.columns.tail: _*)\n",
    "display(df_fillNullForTotalDeaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "### All first 10 column is clean now!\n",
    "\n",
    "(All code above is for illustration, for processing just run cell below\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "// filter unknow and HK data\n",
    "val df_filteredLocation = df.filter($\"iso_code\"=!=\"HKG\").filter($\"iso_code\".isNotNull)\n",
    "\n",
    "// fill missing continent value for World data\n",
    "val df_fillContinentNull = df_filteredLocation.na.fill(\"World\",Array(\"continent\")).cache\n",
    "df_filteredLocation.unpersist()\n",
    "\n",
    "// filter date before 2020-01-23\n",
    "val df_filtered_date = df_fillContinentNull.filter($\"date\">\"2020-01-22\").cache\n",
    "df_fillContinentNull.unpersist()\n",
    "\n",
    "// fill missing for new_cases_smoothed and new_deaths_smoothed\n",
    "val df_fillNullForSmooth = df_filtered_date.na.fill(0,Array(\"new_cases_smoothed\"))\n",
    "                           .na.fill(0,Array(\"new_deaths_smoothed\"))\n",
    "                           .cache\n",
    "df_filtered_date.unpersist()\n",
    "\n",
    "// fill missing for total_cases\n",
    "val df_fillNullForTotalCases = df_fillNullForSmooth.na.fill(0, Array(\"total_cases\")).cache\n",
    "df_fillNullForSmooth.unpersist()\n",
    "\n",
    "// correct total_deaths, new_deaths, new_deaths_smoothed\n",
    "val df_fillNullForTotalDeathsSpecial = df_fillNullForTotalCases.withColumn(\"total_deaths_correct\", \n",
    "                                        when(col(\"iso_code\").equalTo(\"ISL\")&&(col(\"date\")>\"2020-03-13\" && col(\"date\")<\"2020-03-22\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"PNG\")&&(col(\"date\")>\"2020-07-19\" && col(\"date\")<\"2020-07-23\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"SVK\")&&(col(\"date\")>\"2020-03-17\" && col(\"date\")<\"2020-03-23\"),0).otherwise(col(\"total_deaths\")))\n",
    "                            .withColumn(\"new_deaths_correct\", \n",
    "                                        when(col(\"iso_code\").equalTo(\"ISL\")&&(col(\"date\")>\"2020-03-13\" && col(\"date\")<\"2020-03-22\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"PNG\")&&(col(\"date\")>\"2020-07-19\" && col(\"date\")<\"2020-07-23\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"SVK\")&&(col(\"date\")>\"2020-03-17\" && col(\"date\")<\"2020-03-23\"),0).otherwise(col(\"new_deaths\")))\n",
    "                            .withColumn(\"new_deaths_smoothed_correct\", \n",
    "                                        when(col(\"iso_code\").equalTo(\"ISL\")&&(col(\"date\")>\"2020-03-13\" && col(\"date\")<\"2020-03-22\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"PNG\")&&(col(\"date\")>\"2020-07-19\" && col(\"date\")<\"2020-07-23\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"SVK\")&&(col(\"date\")>\"2020-03-17\" && col(\"date\")<\"2020-03-23\"),0).otherwise(col(\"new_deaths_smoothed\")))\n",
    "                            .cache\n",
    "df_fillNullForTotalCases.unpersist()\n",
    "\n",
    "val df_cleaned = df_fillNullForTotalDeathsSpecial\n",
    "                                  .drop(\"new_deaths\", \"total_deaths\", \"new_deaths_smoothed\") // drop old column to rename\n",
    "                                  .withColumnRenamed(\"new_deaths_correct\",\"new_deaths\")\n",
    "                                  .withColumnRenamed(\"total_deaths_correct\",\"total_deaths\")\n",
    "                                  .withColumnRenamed(\"new_deaths_smoothed_correct\",\"new_deaths_smoothed\")\n",
    "                                  .na.fill(0, Array(\"total_deaths\"))\n",
    "                                  .select(df.columns.head, df.columns.tail: _*)\n",
    "                                  .cache\n",
    "df_fillNullForTotalDeathsSpecial.unpersist()\n",
    "\n",
    "display(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "for (c <- df_cleaned.columns) {\n",
    "  println(c + \": \" + df_cleaned.filter(col(c).isNull).count())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     iso_code: 0\n",
    ">     continent: 0\n",
    ">     location: 0\n",
    ">     date: 0\n",
    ">     total_cases: 0\n",
    ">     new_cases: 0\n",
    ">     new_cases_smoothed: 0\n",
    ">     total_deaths: 0\n",
    ">     new_deaths: 0\n",
    ">     new_deaths_smoothed: 0\n",
    ">     total_cases_per_million: 8607\n",
    ">     new_cases_per_million: 0\n",
    ">     new_cases_smoothed_per_million: 950\n",
    ">     total_deaths_per_million: 16447\n",
    ">     new_deaths_per_million: 0\n",
    ">     new_deaths_smoothed_per_million: 950\n",
    ">     reproduction_rate: 20054\n",
    ">     icu_patients: 55381\n",
    ">     icu_patients_per_million: 55381\n",
    ">     hosp_patients: 54394\n",
    ">     hosp_patients_per_million: 54394\n",
    ">     weekly_icu_admissions: 59643\n",
    ">     weekly_icu_admissions_per_million: 59643\n",
    ">     weekly_hosp_admissions: 59356\n",
    ">     weekly_hosp_admissions_per_million: 59356\n",
    ">     total_tests: 36288\n",
    ">     new_tests: 36426\n",
    ">     total_tests_per_thousand: 36288\n",
    ">     new_tests_per_thousand: 36426\n",
    ">     new_tests_smoothed: 33726\n",
    ">     new_tests_smoothed_per_thousand: 33726\n",
    ">     tests_per_case: 34893\n",
    ">     positive_rate: 35484\n",
    ">     tests_units: 60104\n",
    ">     stringency_index: 7470\n",
    ">     population: 0\n",
    ">     population_density: 1580\n",
    ">     median_age: 2844\n",
    ">     aged_65_older: 3792\n",
    ">     aged_70_older: 3160\n",
    ">     gdp_per_capita: 2844\n",
    ">     extreme_poverty: 20871\n",
    ">     cardiovasc_death_rate: 2212\n",
    ">     diabetes_prevalence: 1580\n",
    ">     female_smokers: 15176\n",
    ">     male_smokers: 15808\n",
    ">     handwashing_facilities: 30057\n",
    ">     hospital_beds_per_thousand: 7269\n",
    ">     life_expectancy: 316\n",
    ">     human_development_index: 2528\n",
    ">     import org.apache.spark.sql.functions._\n",
    "\n",
    "  \n",
    "\n",
    "### 3. select invariant (during pandemic) features for clustering\n",
    "\n",
    "double check whether they are constant for each country, and if not,\n",
    "change all the value to mean and filter out countries that have missing\n",
    "constant features\n",
    "\n",
    "Candidate list: - population - population*density - median*age -\n",
    "aged*65*older - aged*70*older - gdp*per*capita\n",
    "\n",
    "-   cardiovasc*death*rate\n",
    "-   diabetes\\_prevalence\n",
    "-   female\\_smokers\n",
    "-   male\\_smokers\n",
    "-   hospital*beds*per\\_thousand\n",
    "-   life\\_expectancy\n",
    "-   human*development*index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_invariantFeatures = df_cleaned.select($\"location\", $\"population\",$\"population_density\",\n",
    "                                             $\"median_age\", $\"aged_65_older\",\n",
    "                                             $\"aged_70_older\",$\"gdp_per_capita\",\n",
    "                                             $\"cardiovasc_death_rate\",$\"diabetes_prevalence\",\n",
    "                                             $\"female_smokers\",$\"male_smokers\",$\"hospital_beds_per_thousand\",\n",
    "                                             $\"life_expectancy\",$\"human_development_index\")\n",
    "display(df_invariantFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_invariantFeatures.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (c <- df_invariantFeatures.columns) {\n",
    "  println(c + \": \" + df_invariantFeatures.filter(col(c).isNull).count())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     location: 0\n",
    ">     population: 0\n",
    ">     population_density: 1580\n",
    ">     median_age: 2844\n",
    ">     aged_65_older: 3792\n",
    ">     aged_70_older: 3160\n",
    ">     gdp_per_capita: 2844\n",
    ">     cardiovasc_death_rate: 2212\n",
    ">     diabetes_prevalence: 1580\n",
    ">     female_smokers: 15176\n",
    ">     male_smokers: 15808\n",
    ">     hospital_beds_per_thousand: 7269\n",
    ">     life_expectancy: 316\n",
    ">     human_development_index: 2528\n",
    "\n",
    "  \n",
    "\n",
    "Although some countries seems like an outlier, it does have constant\n",
    "female*smokers and male*smokers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val constant_feature_checker = df_cleaned.groupBy(\"location\")\n",
    "          .agg(\n",
    "              //stddev(\"stringency_index\").as(\"std_si\"),         \n",
    "              stddev(\"population\").as(\"std_pop\"),           \n",
    "              stddev(\"population_density\").as(\"std_pd\"),\n",
    "              stddev(\"median_age\").as(\"std_ma\"),         \n",
    "              stddev(\"aged_65_older\").as(\"std_a65\"),           \n",
    "              stddev(\"aged_70_older\").as(\"std_a70\"),  \n",
    "              stddev(\"gdp_per_capita\").as(\"std_gdp\"),\n",
    "              stddev(\"cardiovasc_death_rate\").as(\"std_cdr\"),         \n",
    "              stddev(\"diabetes_prevalence\").as(\"std_dp\"),           \n",
    "              stddev(\"female_smokers\").as(\"std_fs\"),      \n",
    "              stddev(\"male_smokers\").as(\"std_ms\"),        \n",
    "              stddev(\"hospital_beds_per_thousand\").as(\"std_hbpt\"),           \n",
    "              stddev(\"life_expectancy\").as(\"std_le\"),\n",
    "              stddev(\"human_development_index\").as(\"std_hdi\")\n",
    "            )\n",
    "           .where(\n",
    "                  (col(\"std_pop\") > 0) || (col(\"std_pd\") > 1e-20) || (col(\"std_ma\") > 0) || (col(\"std_a65\") > 0) || (col(\"std_a70\") > 0) || (col(\"std_gdp\") > 0 ||\n",
    "                   col(\"std_cdr\") > 0) || (col(\"std_dp\") > 0) || (col(\"std_fs\") > 0) || (col(\"std_ms\") > 0) || (col(\"std_hbpt\") > 0) || (col(\"std_le\") > 0) || (col(\"std_hdi\") > 0))\n",
    "\n",
    "display(constant_feature_checker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "Each country have some constant features always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val distinct_features = df_invariantFeatures.distinct()\n",
    "\n",
    "display(distinct_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "In total, 126 countries have complete features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val valid_distinct_features = distinct_features.filter($\"population\".isNotNull && $\"population_density\".isNotNull && $\"median_age\".isNotNull && \n",
    "                         $\"aged_65_older\".isNotNull && $\"aged_70_older\".isNotNull && $\"gdp_per_capita\".isNotNull &&\n",
    "                         $\"cardiovasc_death_rate\".isNotNull && $\"diabetes_prevalence\".isNotNull && $\"female_smokers\".isNotNull && \n",
    "                         $\"male_smokers\".isNotNull && $\"hospital_beds_per_thousand\".isNotNull && $\"life_expectancy\".isNotNull &&\n",
    "                         $\"human_development_index\".isNotNull)\n",
    "display(valid_distinct_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "country list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(valid_distinct_features.select($\"location\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_distinct_features.select($\"location\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res69: Long = 126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_cleaned_feature = df_cleaned.filter($\"location\".isin(valid_distinct_features.select($\"location\").rdd.map(r => r(0)).collect().toSeq: _*))\n",
    "\n",
    "display(df_cleaned_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "### All data contains complete list of invariant time feature\n",
    "\n",
    "(All code above is for illustration, for processing just run cell below\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// select invariant features\n",
    "val df_invariantFeatures = df_cleaned.select($\"location\", $\"population\",$\"population_density\",\n",
    "                                             $\"median_age\", $\"aged_65_older\",\n",
    "                                             $\"aged_70_older\",$\"gdp_per_capita\",\n",
    "                                             $\"cardiovasc_death_rate\",$\"diabetes_prevalence\",\n",
    "                                             $\"female_smokers\",$\"male_smokers\",$\"hospital_beds_per_thousand\",\n",
    "                                             $\"life_expectancy\",$\"human_development_index\").cache\n",
    "\n",
    "// Extract valid distrinct features RDD\n",
    "val valid_distinct_features = df_invariantFeatures.distinct()\n",
    "                                 .filter($\"population\".isNotNull && $\"population_density\".isNotNull && $\"median_age\".isNotNull && \n",
    "                                 $\"aged_65_older\".isNotNull && $\"aged_70_older\".isNotNull && $\"gdp_per_capita\".isNotNull &&\n",
    "                                 $\"cardiovasc_death_rate\".isNotNull && $\"diabetes_prevalence\".isNotNull && $\"female_smokers\".isNotNull && \n",
    "                                 $\"male_smokers\".isNotNull && $\"hospital_beds_per_thousand\".isNotNull && $\"life_expectancy\".isNotNull &&\n",
    "                                 $\"human_development_index\".isNotNull).cache\n",
    "\n",
    "df_invariantFeatures.unpersist()\n",
    "\n",
    "// filter out NULL feature countries\n",
    "val df_cleaned_feature = df_cleaned.filter($\"location\".isin(valid_distinct_features.select($\"location\").rdd.map(r => r(0)).collect().toSeq: _*)).cache\n",
    "\n",
    "df_cleaned.unpersist()\n",
    "\n",
    "display(df_cleaned_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "for (c <- df_cleaned_feature.columns) {\n",
    "  println(c + \": \" + df_cleaned_feature.filter(col(c).isNull).count())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     iso_code: 0\n",
    ">     continent: 0\n",
    ">     location: 0\n",
    ">     date: 0\n",
    ">     total_cases: 0\n",
    ">     new_cases: 0\n",
    ">     new_cases_smoothed: 0\n",
    ">     total_deaths: 0\n",
    ">     new_deaths: 0\n",
    ">     new_deaths_smoothed: 0\n",
    ">     total_cases_per_million: 4677\n",
    ">     new_cases_per_million: 0\n",
    ">     new_cases_smoothed_per_million: 630\n",
    ">     total_deaths_per_million: 9173\n",
    ">     new_deaths_per_million: 0\n",
    ">     new_deaths_smoothed_per_million: 630\n",
    ">     reproduction_rate: 10911\n",
    ">     icu_patients: 35149\n",
    ">     icu_patients_per_million: 35149\n",
    ">     hosp_patients: 34162\n",
    ">     hosp_patients_per_million: 34162\n",
    ">     weekly_icu_admissions: 39411\n",
    ">     weekly_icu_admissions_per_million: 39411\n",
    ">     weekly_hosp_admissions: 39124\n",
    ">     weekly_hosp_admissions_per_million: 39124\n",
    ">     total_tests: 20010\n",
    ">     new_tests: 20437\n",
    ">     total_tests_per_thousand: 20010\n",
    ">     new_tests_per_thousand: 20437\n",
    ">     new_tests_smoothed: 18170\n",
    ">     new_tests_smoothed_per_thousand: 18170\n",
    ">     tests_per_case: 19063\n",
    ">     positive_rate: 19511\n",
    ">     tests_units: 39872\n",
    ">     stringency_index: 2223\n",
    ">     population: 0\n",
    ">     population_density: 0\n",
    ">     median_age: 0\n",
    ">     aged_65_older: 0\n",
    ">     aged_70_older: 0\n",
    ">     gdp_per_capita: 0\n",
    ">     extreme_poverty: 10758\n",
    ">     cardiovasc_death_rate: 0\n",
    ">     diabetes_prevalence: 0\n",
    ">     female_smokers: 0\n",
    ">     male_smokers: 0\n",
    ">     handwashing_facilities: 22788\n",
    ">     hospital_beds_per_thousand: 0\n",
    ">     life_expectancy: 0\n",
    ">     human_development_index: 0\n",
    ">     import org.apache.spark.sql.functions._\n",
    "\n",
    "  \n",
    "\n",
    "### 4. Imputing missing time series data of\n",
    "\n",
    "-   total*cases*per\\_million\n",
    "-   new*cases*per\\_million\n",
    "-   new*cases*smoothed*per*million\n",
    "-   total*deaths*per\\_million\n",
    "-   new*deaths*per\\_million\n",
    "-   new*deaths*smoothed*per*million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val per_million_data = df_cleaned_feature.select($\"location\", $\"date\", $\"iso_code\", $\"total_cases\", \n",
    "                                                 $\"total_deaths\", $\"new_cases\", $\"new_deaths\", $\"new_cases_smoothed\", \n",
    "                                                 $\"new_deaths_smoothed\", $\"population\", $\"population_density\", \n",
    "                                                 $\"total_cases_per_million\", $\"new_cases_per_million\", $\"new_cases_smoothed_per_million\", \n",
    "                                                 $\"total_deaths_per_million\", $\"new_deaths_per_million\", $\"new_deaths_smoothed_per_million\")\n",
    "\n",
    "display(per_million_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val per_million_data_corrected = per_million_data.withColumn(\"total_cases_per_million_correct\", per_million_data(\"total_cases\")/per_million_data(\"population\")*1000000)\n",
    "                                                 .withColumn(\"new_cases_per_million_correct\", per_million_data(\"new_cases\")/per_million_data(\"population\")*1000000)\n",
    "                                                 .withColumn(\"new_cases_smoothed_per_million_correct\", per_million_data(\"new_cases_smoothed\")/per_million_data(\"population\")*1000000)\n",
    "                                                 .withColumn(\"total_deaths_per_million_correct\", per_million_data(\"total_deaths\")/per_million_data(\"population\")*1000000)\n",
    "                                                 .withColumn(\"new_deaths_per_million_correct\", per_million_data(\"new_deaths\")/per_million_data(\"population\")*1000000)\n",
    "                                                 .withColumn(\"new_deaths_smoothed_per_million_correct\", per_million_data(\"new_deaths_smoothed\")/per_million_data(\"population\")*1000000)\n",
    "                                                 .drop(\"total_cases_per_million\", \"new_cases_per_million\", \"new_cases_smoothed_per_million\", \n",
    "                                                       \"total_deaths_per_million\", \"new_deaths_per_million\", \"new_deaths_smoothed_per_million\") // drop old column to rename\n",
    "                                                 .withColumnRenamed(\"total_cases_per_million_correct\",\"total_cases_per_million\")\n",
    "                                                 .withColumnRenamed(\"new_cases_per_million_correct\",\"new_cases_per_million\")\n",
    "                                                 .withColumnRenamed(\"new_cases_smoothed_per_million_correct\",\"new_cases_smoothed_per_million\")\n",
    "                                                 .withColumnRenamed(\"total_deaths_per_million_correct\",\"total_deaths_per_million\")\n",
    "                                                 .withColumnRenamed(\"new_deaths_per_million_correct\",\"new_deaths_per_million\")\n",
    "                                                 .withColumnRenamed(\"new_deaths_smoothed_per_million_correct\",\"new_deaths_smoothed_per_million\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     per_million_data_corrected: org.apache.spark.sql.DataFrame = [location: string, date: string ... 15 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_cleaned_feature_permillion = df_cleaned_feature.withColumn(\"total_cases_per_million_correct\", df_cleaned_feature(\"total_cases\")/df_cleaned_feature(\"population\")*1000000)\n",
    "                                                   .withColumn(\"new_cases_per_million_correct\", df_cleaned_feature(\"new_cases\")/df_cleaned_feature(\"population\")*1000000)\n",
    "                                                   .withColumn(\"new_cases_smoothed_per_million_correct\", df_cleaned_feature(\"new_cases_smoothed\")/df_cleaned_feature(\"population\")*1000000)\n",
    "                                                   .withColumn(\"total_deaths_per_million_correct\", df_cleaned_feature(\"total_deaths\")/df_cleaned_feature(\"population\")*1000000)\n",
    "                                                   .withColumn(\"new_deaths_per_million_correct\", df_cleaned_feature(\"new_deaths\")/df_cleaned_feature(\"population\")*1000000)\n",
    "                                                   .withColumn(\"new_deaths_smoothed_per_million_correct\", df_cleaned_feature(\"new_deaths_smoothed\")/df_cleaned_feature(\"population\")*1000000)\n",
    "                                                   .drop(\"total_cases_per_million\", \"new_cases_per_million\", \"new_cases_smoothed_per_million\", \n",
    "                                                         \"total_deaths_per_million\", \"new_deaths_per_million\", \"new_deaths_smoothed_per_million\") // drop old column to rename\n",
    "                                                   .withColumnRenamed(\"total_cases_per_million_correct\",\"total_cases_per_million\")\n",
    "                                                   .withColumnRenamed(\"new_cases_per_million_correct\",\"new_cases_per_million\")\n",
    "                                                   .withColumnRenamed(\"new_cases_smoothed_per_million_correct\",\"new_cases_smoothed_per_million\")\n",
    "                                                   .withColumnRenamed(\"total_deaths_per_million_correct\",\"total_deaths_per_million\")\n",
    "                                                   .withColumnRenamed(\"new_deaths_per_million_correct\",\"new_deaths_per_million\")\n",
    "                                                   .withColumnRenamed(\"new_deaths_smoothed_per_million_correct\",\"new_deaths_smoothed_per_million\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     df_cleaned_feature_permillion: org.apache.spark.sql.DataFrame = [iso_code: string, continent: string ... 48 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "### 5. Impute time series of\n",
    "\n",
    "-   reproduction\\_rate\n",
    "-   total\\_tests\n",
    "-   stringency\\_index\n",
    "-   total*tests*per\\_thousand\n",
    "\n",
    "fill null in reproduction\\_rate by last available and next available\n",
    "value\n",
    "\n",
    "All countries has missing data at beginning or in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cleaned_feature_permillion.select($\"reproduction_rate\", $\"location\", $\"date\").filter($\"reproduction_rate\".isNull).groupBy(\"location\").count().sort(\"location\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cleaned_feature_permillion.select($\"reproduction_rate\", $\"location\", $\"date\").filter($\"reproduction_rate\".isNull).groupBy(\"location\").agg(max(\"date\").as(\"max_date\"), min(\"date\").as(\"min_date\")).sort(\"location\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cleaned_feature_permillion.select($\"reproduction_rate\", $\"location\", $\"date\").filter($\"location\"===\"Albania\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val df_cleaned_reproduction_rate= df_cleaned_feature_permillion.select($\"reproduction_rate\", $\"location\", $\"date\")\n",
    "                              .withColumn(\"reproduction_rate\", last(\"reproduction_rate\", true)\n",
    "                                         .over(Window.partitionBy(\"location\").orderBy(\"date\").rowsBetween(-df_cleaned_feature_permillion.count(), 0)))\n",
    "                              .withColumn(\"reproduction_rate\", first(\"reproduction_rate\", true)\n",
    "                                         .over(Window.partitionBy(\"location\").orderBy(\"date\").rowsBetween(0, df_cleaned_feature_permillion.count())))\n",
    "                              .na.fill(0, Array(\"reproduction_rate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     import org.apache.spark.sql.expressions.Window\n",
    ">     df_cleaned_reproduction_rate: org.apache.spark.sql.DataFrame = [reproduction_rate: double, location: string ... 1 more field]\n",
    "\n",
    "  \n",
    "\n",
    "countries miss stringency\\_index value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cleaned_feature_permillion.select($\"stringency_index\", $\"location\", $\"date\").filter($\"stringency_index\".isNull).groupBy(\"location\").count().sort(\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "start and end date for null value of stringency\\_index for each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cleaned_feature_permillion.select($\"stringency_index\", $\"location\", $\"date\").filter($\"stringency_index\".isNull).groupBy(\"location\").agg(max(\"date\").as(\"max_date\"), min(\"date\").as(\"min_date\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val df_cleaned_stringency = df_cleaned_feature_permillion.select($\"stringency_index\", $\"location\", $\"date\")\n",
    "                              .withColumn(\"stringency_index_corect\", last(\"stringency_index\", true)\n",
    "                                         .over(Window.partitionBy(\"location\").orderBy(\"date\").rowsBetween(-df_cleaned_feature_permillion.count(), 0)))\n",
    "display(df_cleaned_stringency.filter($\"stringency_index\".isNull).filter($\"stringency_index_corect\".isNull).groupBy(\"location\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "  \n",
    "\n",
    "total\\_tests, impute by last available or next available value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val df_cleaned_total_cases = df_cleaned_feature_permillion.select($\"total_tests\", $\"location\", $\"date\")\n",
    "                              .withColumn(\"total_tests\", last(\"total_tests\", true)\n",
    "                                         .over(Window.partitionBy(\"location\").orderBy(\"date\").rowsBetween(-df_cleaned_feature_permillion.count(), 0)))\n",
    "                              .withColumn(\"total_tests\", first(\"total_tests\", true)\n",
    "                                         .over(Window.partitionBy(\"location\").orderBy(\"date\").rowsBetween(0, df_cleaned_feature_permillion.count())))\n",
    "                              .na.fill(0, Array(\"total_tests\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     import org.apache.spark.sql.expressions.Window\n",
    ">     df_cleaned_total_cases: org.apache.spark.sql.DataFrame = [total_tests: double, location: string ... 1 more field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cleaned_feature_permillion.select($\"total_tests\", $\"location\", $\"date\").filter($\"total_tests\".isNull).groupBy(\"location\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val total_tests_date_maxmin = df_cleaned_feature_permillion.select($\"total_tests\", $\"location\", $\"date\").filter($\"total_tests\".isNull).groupBy(\"location\").agg(max(\"date\").as(\"max_date\"), min(\"date\").as(\"min_date\"))\n",
    "display(total_tests_date_maxmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "process stringency*index, reproduction*rate, total*tests,\n",
    "total*tests*per*thousand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val df_cleaned_time_series = df_cleaned_feature_permillion\n",
    "                              .withColumn(\"reproduction_rate\", last(\"reproduction_rate\", true)\n",
    "                                         .over(Window.partitionBy(\"location\").orderBy(\"date\").rowsBetween(-df_cleaned_feature_permillion.count(), 0)))\n",
    "                              .withColumn(\"reproduction_rate\", first(\"reproduction_rate\", true)\n",
    "                                         .over(Window.partitionBy(\"location\").orderBy(\"date\").rowsBetween(0, df_cleaned_feature_permillion.count())))\n",
    "                              .na.fill(0, Array(\"reproduction_rate\"))\n",
    "                              .withColumn(\"stringency_index\", last(\"stringency_index\", true)\n",
    "                                         .over(Window.partitionBy(\"location\").orderBy(\"date\").rowsBetween(-df_cleaned_feature_permillion.count(), 0)))\n",
    "                              .na.fill(0, Array(\"stringency_index\"))\n",
    "                              .withColumn(\"total_tests\", last(\"total_tests\", true)\n",
    "                                         .over(Window.partitionBy(\"location\").orderBy(\"date\").rowsBetween(-df_cleaned_feature_permillion.count(), 0)))\n",
    "                              .withColumn(\"total_tests\", first(\"total_tests\", true)\n",
    "                                         .over(Window.partitionBy(\"location\").orderBy(\"date\").rowsBetween(0, df_cleaned_feature_permillion.count())))\n",
    "                              .na.fill(0, Array(\"total_tests\"))\n",
    "                              .withColumn(\"total_tests_per_thousand\", col(\"total_tests\")/col(\"population\")*1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     import org.apache.spark.sql.expressions.Window\n",
    ">     df_cleaned_time_series: org.apache.spark.sql.DataFrame = [iso_code: string, continent: string ... 48 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cleaned_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "for (c <- df_cleaned_time_series.columns) {\n",
    "  println(c + \": \" + df_cleaned_time_series.filter(col(c).isNull).count())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     iso_code: 0\n",
    ">     continent: 0\n",
    ">     location: 0\n",
    ">     date: 0\n",
    ">     total_cases: 0\n",
    ">     new_cases: 0\n",
    ">     new_cases_smoothed: 0\n",
    ">     total_deaths: 0\n",
    ">     new_deaths: 0\n",
    ">     new_deaths_smoothed: 0\n",
    ">     reproduction_rate: 0\n",
    ">     icu_patients: 35149\n",
    ">     icu_patients_per_million: 35149\n",
    ">     hosp_patients: 34162\n",
    ">     hosp_patients_per_million: 34162\n",
    ">     weekly_icu_admissions: 39411\n",
    ">     weekly_icu_admissions_per_million: 39411\n",
    ">     weekly_hosp_admissions: 39124\n",
    ">     weekly_hosp_admissions_per_million: 39124\n",
    ">     total_tests: 0\n",
    ">     new_tests: 20437\n",
    ">     total_tests_per_thousand: 0\n",
    ">     new_tests_per_thousand: 20437\n",
    ">     new_tests_smoothed: 18170\n",
    ">     new_tests_smoothed_per_thousand: 18170\n",
    ">     tests_per_case: 19063\n",
    ">     positive_rate: 19511\n",
    ">     tests_units: 39872\n",
    ">     stringency_index: 0\n",
    ">     population: 0\n",
    ">     population_density: 0\n",
    ">     median_age: 0\n",
    ">     aged_65_older: 0\n",
    ">     aged_70_older: 0\n",
    ">     gdp_per_capita: 0\n",
    ">     extreme_poverty: 10758\n",
    ">     cardiovasc_death_rate: 0\n",
    ">     diabetes_prevalence: 0\n",
    ">     female_smokers: 0\n",
    ">     male_smokers: 0\n",
    ">     handwashing_facilities: 22788\n",
    ">     hospital_beds_per_thousand: 0\n",
    ">     life_expectancy: 0\n",
    ">     human_development_index: 0\n",
    ">     total_cases_per_million: 0\n",
    ">     new_cases_per_million: 0\n",
    ">     new_cases_smoothed_per_million: 0\n",
    ">     total_deaths_per_million: 0\n",
    ">     new_deaths_per_million: 0\n",
    ">     new_deaths_smoothed_per_million: 0\n",
    ">     import org.apache.spark.sql.functions._"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
