{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Description and Introduction\n",
    "====================================\n",
    "\n",
    "Project Members: &lt;br&gt; Chi Zhang, zchi@chalmers.se; &lt;br&gt;\n",
    "Shuangshuang Chen, shuche@kth.se; &lt;br&gt; Magnus Tarle, tarle@kth.se\n",
    "\n",
    "The presentation record link is here:\n",
    "https://drive.google.com/drive/folders/13XwlItZ\\_qtOeBZ5TJfnCP1hqtQ9imRFq\n",
    "&lt;br&gt; Because the cluster seems quite slow when we recording the\n",
    "video and there are too many things to run within 20 minutes, we\n",
    "recorded the video after we finish running all the codes. And some parts\n",
    "of the video are speeded up to meet the 20 minutes requirement.\n",
    "\n",
    "Project Introduction - Analysis and Prediction of COVID-19 Data\n",
    "---------------------------------------------------------------\n",
    "\n",
    "In this project, we use both scala (data processing part) and python\n",
    "(algorithm part). We deal with scalable data, and use what we learned\n",
    "from the course.\n",
    "\n",
    "### 1. Project Plan\n",
    "\n",
    "In this project, we dealt with COVID-19 data, and do the following\n",
    "tasks:\n",
    "\n",
    "1.  Introduction\n",
    "2.  Struct stream data - to update our database once a day.\n",
    "3.  Preprocessing - clean data.\n",
    "4.  Visualization - visualize new cases on a world map, with different\n",
    "    time scope.\n",
    "5.  Statistics analysis - get the distributions, mean and std of\n",
    "    different varables.\n",
    "6.  Model 1: K-means - clustering of different countries\n",
    "7.  Model 2: Linear Regression - predict new cases of some contries from\n",
    "    other countries.\n",
    "8.  Model 3: Autoregressive integrated moving average (ARIMA) -\n",
    "    prediction for new cases and new deaths from past values.\n",
    "9.  Model 4: Gaussian Procecces (GP) - apply Gaussian Procecces (GP) to\n",
    "    predict mean and covariance of new cases and new deaths from past\n",
    "    values.\n",
    "10. Ending\n",
    "\n",
    "### 2. Tools and methods\n",
    "\n",
    "For the method part, this particular task is not suitable to use deep\n",
    "learning methods. We choose 4 methods related to Machine leanring for\n",
    "our clustering and prediction task: 1. Clustering model - K-means 2.\n",
    "Time series model - Autoregressive integrated moving average (ARIMA) 3.\n",
    "Gaussian Procecces (GP) 4. Linear Regression (LR)\n",
    "\n",
    "### 3. Data resources\n",
    "\n",
    "We found the following data resources. And finally we chose the 3rd\n",
    "dataset because it has the most features for us to use.\n",
    "\n",
    "1.  WHO: https://covid19.who.int/WHO-COVID-19-global-data.csv\n",
    "\n",
    "2.  COVID Tracking Project API at Cambridge:\n",
    "    https://api.covidtracking.com\n",
    "\n",
    "3.  Data on COVID-19 (coronavirus) by Our World in Data: git repo\n",
    "    https://github.com/owid/covid-19-data/tree/master/public/data\n",
    "\n",
    "4.  Sweden (proceesed by apify) API:\n",
    "    https://api.apify.com/v2/datasets/Nq3XwHX262iDwsFJS/items?format=json&clean=1\n",
    "\n",
    "Note that the preprocessing, visualization and analysis within these\n",
    "notebooks were made on this 3rd dataset, downloaded December 2020 to\n",
    "Databricks. One dataset from December 2020, \"owid-covid-data.csv\", can\n",
    "also be found in the same google drive folder as the video presentation.\n",
    "\n",
    "&lt;!-- \\#\\#\\# Some additional ideas (added by Magnus) 1. Join any of\n",
    "the above mentioned COVID-19 API:s with another API source to get new\n",
    "insights by looking at correlations, clustering. Might be difficult to\n",
    "find a good interesting source but there seems to be some datasources\n",
    "listed e.g. here:\n",
    "https://www.octoparse.com/blog/big-data-70-amazing-free-data-sources-you-should-know-for-2017.\n",
    "One could imagine e.g. crime vs covid or economic sector vs covid etc.\n",
    "2. Perhaps a bit algorithm specific - apply Gaussian Procecces (GP) to\n",
    "predict mean and covariance. Reason being I have for some time wanted to\n",
    "learn GP as they seem pretty useful. If you want to have an intro to GP,\n",
    "you could look at:\n",
    "http://krasserm.github.io/2018/03/19/gaussian-processes/ and\n",
    "https://medium.com/@dan\\_90739/being-bayesian-and-thinking-deep-time-series-prediction-with-uncertainty-25ff581b056c\n",
    "--&gt;\n",
    "\n",
    "### 4. Useful links\n",
    "\n",
    "1.  Description of each column in the \"Our World in Data\" dataset\n",
    "    https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-codebook.csv\n",
    "2.  If you want to have an intro to GP, you could look at:\n",
    "    http://krasserm.github.io/2018/03/19/gaussian-processes/ and\n",
    "    https://medium.com/@dan\\_90739/being-bayesian-and-thinking-deep-time-series-prediction-with-uncertainty-25ff581b056c\n",
    "3.  ARIMA - Autoregressive Integrated Moving Average model. It's widely\n",
    "    used in time series analysis. see defination here:\n",
    "    https://en.wikipedia.org/wiki/Autoregressive*integrated*moving\\_average\n",
    "\n",
    "### 5. Meeting Records:\n",
    "\n",
    "Weekly meetings for group discussion:\n",
    "\n",
    "1.  2020-12-01 Tuesday:\n",
    "\n",
    "-   Discussion about project topic\n",
    "-   Find data resources\n",
    "\n",
    "1.  2020-12-04 Friday:\n",
    "    -   Understand dataset 3, and manage download data\n",
    "    -   Set up processing pipeline into dataframe\n",
    "    -   Prepared for statistics analysis\n",
    "    -   Selected the column for analysis and machine learning\n",
    "    -   Finished struct stream for updating data\n",
    "2.  2020-12-08 Tuesday:\n",
    "    -   Exploited each chosen column, plot statistics\n",
    "    -   Dealt with missing data\n",
    "    -   Post-process for useful features from existed column (for\n",
    "        statistics analysis and prediction/regression)\n",
    "3.  2020-12-16 Wednesday:\n",
    "\n",
    "-   Finished statistics analysis, which correlation would interesting to\n",
    "    show\n",
    "-   Progress on visulization\n",
    "-   Progress on GP model\n",
    "-   Progress on LR model\n",
    "-   Progress on ARIMA model\n",
    "\n",
    "1.  2020-12-18 Friday:\n",
    "\n",
    "-   Progress on K-means model\n",
    "-   Finish visulization\n",
    "-   Finish other models\n",
    "-   Finish Evaluation\n",
    "\n",
    "1.  2021-01-07 Thursday:\n",
    "\n",
    "-   Finish all models\n",
    "-   Discussing about the final presentation\n",
    "\n",
    "Project Progress\n",
    "----------------\n",
    "\n",
    "### 0. Introduction of the data\n",
    "\n",
    "The columns we selected to analyze are: - continent - location - date -\n",
    "total*cases - new*cases - total*deaths - new*deaths - reproduction*rate\n",
    "- icu*patients - hosp*patients - weekly*icu*admissions -\n",
    "weekly*hosp*admissions - total*tests - new*tests - stingency*index -\n",
    "population - population*density - median*age - aged*65*older -\n",
    "aged*70*older - gdp*per*capita - extreme*poverty - cardiovasc*death*rate\n",
    "- diabetes*prevalence - female*smokers - male*smokers -\n",
    "hospital*beds*per*thousand - life*expectancy - human*development*index\n",
    "\n",
    "### 1. Streaming\n",
    "\n",
    "This part has been moved to separate files:\n",
    "\"DownloadFilesPeriodicallyScript\" and \"StreamToFile\".\n",
    "\n",
    "### 2. Preprocessing\n",
    "\n",
    "To **Rerun Steps 1-5** done in the notebook at: \\*\n",
    "`Workspace -> PATH_TO -> DataPreprocess]`\n",
    "\n",
    "just `run` the following command as shown in the cell below:\n",
    "\n",
    "`%scala   %run \"PATH_TO/DataPreprocess\"`\n",
    "\n",
    "-   *Note:* If you already evaluated the `%run ...` command above then:\n",
    "    -   first delete the cell by pressing on `x` on the top-right corner\n",
    "        of the cell and\n",
    "    -   revaluate the `run` command above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"./02_DataPreprocess\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### 3. Explosive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"./03_ExplosiveAnalysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"./04_DataVisualize\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### 5. Model 1: Clustering K-means\n",
    "\n",
    "Clustering of countries based on the dataset features of each country.\n",
    "\n",
    "This part is in notebook Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"./05_Clustering\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### 6. Model 2: Linear Regression (LR) Model\n",
    "\n",
    "Prediction with constant values, predict new cases of some contries from\n",
    "other countries.\n",
    "\n",
    "This part is in notebook DataPrediction\\_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"./06_DataPredicton_LR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### 7. Model 3: Autoregressive integrated moving average (ARIMA)\n",
    "\n",
    "prediction for new cases and new deaths from past values.\n",
    "\n",
    "This part is in notebook DataPrediction\\_ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"./07_DataPredicton_ARIMA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### 8. Model 4: Gaussian Procecces (GP)\n",
    "\n",
    "apply Gaussian Procecces (GP) to predict mean and covariance of new\n",
    "cases and new deaths from past values.\n",
    "\n",
    "This part is in notebook DataPrediction\\_GP\n",
    "\n",
    "[TABLE]\n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    ">     res14: Long = 62500\n",
    "\n",
    "  \n",
    "\n",
    ">     root\n",
    ">      |-- iso_code: string (nullable = true)\n",
    ">      |-- continent: string (nullable = false)\n",
    ">      |-- location: string (nullable = true)\n",
    ">      |-- date: string (nullable = true)\n",
    ">      |-- total_cases: double (nullable = false)\n",
    ">      |-- new_cases: double (nullable = true)\n",
    ">      |-- new_cases_smoothed: double (nullable = false)\n",
    ">      |-- total_deaths: double (nullable = false)\n",
    ">      |-- new_deaths: double (nullable = true)\n",
    ">      |-- new_deaths_smoothed: double (nullable = false)\n",
    ">      |-- reproduction_rate: double (nullable = false)\n",
    ">      |-- icu_patients: double (nullable = true)\n",
    ">      |-- icu_patients_per_million: double (nullable = true)\n",
    ">      |-- hosp_patients: double (nullable = true)\n",
    ">      |-- hosp_patients_per_million: double (nullable = true)\n",
    ">      |-- weekly_icu_admissions: double (nullable = true)\n",
    ">      |-- weekly_icu_admissions_per_million: double (nullable = true)\n",
    ">      |-- weekly_hosp_admissions: double (nullable = true)\n",
    ">      |-- weekly_hosp_admissions_per_million: double (nullable = true)\n",
    ">      |-- total_tests: double (nullable = false)\n",
    ">      |-- new_tests: double (nullable = true)\n",
    ">      |-- total_tests_per_thousand: double (nullable = true)\n",
    ">      |-- new_tests_per_thousand: double (nullable = true)\n",
    ">      |-- new_tests_smoothed: double (nullable = true)\n",
    ">      |-- new_tests_smoothed_per_thousand: double (nullable = true)\n",
    ">      |-- tests_per_case: double (nullable = true)\n",
    ">      |-- positive_rate: double (nullable = true)\n",
    ">      |-- tests_units: double (nullable = true)\n",
    ">      |-- stringency_index: double (nullable = false)\n",
    ">      |-- population: double (nullable = true)\n",
    ">      |-- population_density: double (nullable = true)\n",
    ">      |-- median_age: double (nullable = true)\n",
    ">      |-- aged_65_older: double (nullable = true)\n",
    ">      |-- aged_70_older: double (nullable = true)\n",
    ">      |-- gdp_per_capita: double (nullable = true)\n",
    ">      |-- extreme_poverty: double (nullable = true)\n",
    ">      |-- cardiovasc_death_rate: double (nullable = true)\n",
    ">      |-- diabetes_prevalence: double (nullable = true)\n",
    ">      |-- female_smokers: double (nullable = true)\n",
    ">      |-- male_smokers: double (nullable = true)\n",
    ">      |-- handwashing_facilities: double (nullable = true)\n",
    ">      |-- hospital_beds_per_thousand: double (nullable = true)\n",
    ">      |-- life_expectancy: double (nullable = true)\n",
    ">      |-- human_development_index: double (nullable = true)\n",
    ">      |-- total_cases_per_million: double (nullable = true)\n",
    ">      |-- new_cases_per_million: double (nullable = true)\n",
    ">      |-- new_cases_smoothed_per_million: double (nullable = true)\n",
    ">      |-- total_deaths_per_million: double (nullable = true)\n",
    ">      |-- new_deaths_per_million: double (nullable = true)\n",
    ">      |-- new_deaths_smoothed_per_million: double (nullable = true)\n",
    "\n",
    "  \n",
    "\n",
    ">     iso_code: 0\n",
    ">     continent: 0\n",
    ">     location: 0\n",
    ">     date: 0\n",
    ">     total_cases: 0\n",
    ">     new_cases: 0\n",
    ">     new_cases_smoothed: 0\n",
    ">     total_deaths: 0\n",
    ">     new_deaths: 0\n",
    ">     new_deaths_smoothed: 0\n",
    ">     reproduction_rate: 0\n",
    ">     icu_patients: 36018\n",
    ">     icu_patients_per_million: 36018\n",
    ">     hosp_patients: 34870\n",
    ">     hosp_patients_per_million: 34870\n",
    ">     weekly_icu_admissions: 41062\n",
    ">     weekly_icu_admissions_per_million: 41062\n",
    ">     weekly_hosp_admissions: 40715\n",
    ">     weekly_hosp_admissions_per_million: 40715\n",
    ">     total_tests: 0\n",
    ">     new_tests: 20510\n",
    ">     total_tests_per_thousand: 0\n",
    ">     new_tests_per_thousand: 20510\n",
    ">     new_tests_smoothed: 18176\n",
    ">     new_tests_smoothed_per_thousand: 18176\n",
    ">     tests_per_case: 19301\n",
    ">     positive_rate: 19749\n",
    ">     tests_units: 41600\n",
    ">     stringency_index: 0\n",
    ">     population: 0\n",
    ">     population_density: 0\n",
    ">     median_age: 0\n",
    ">     aged_65_older: 0\n",
    ">     aged_70_older: 0\n",
    ">     gdp_per_capita: 0\n",
    ">     extreme_poverty: 11168\n",
    ">     cardiovasc_death_rate: 0\n",
    ">     diabetes_prevalence: 0\n",
    ">     female_smokers: 0\n",
    ">     male_smokers: 0\n",
    ">     handwashing_facilities: 24124\n",
    ">     hospital_beds_per_thousand: 0\n",
    ">     life_expectancy: 0\n",
    ">     human_development_index: 0\n",
    ">     total_cases_per_million: 0\n",
    ">     new_cases_per_million: 0\n",
    ">     new_cases_smoothed_per_million: 0\n",
    ">     total_deaths_per_million: 0\n",
    ">     new_deaths_per_million: 0\n",
    ">     new_deaths_smoothed_per_million: 0\n",
    ">     import org.apache.spark.sql.functions._\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "  \n",
    "\n",
    ">     res32: Long = 159\n",
    "\n",
    "  \n",
    "\n",
    ">     df_filteredLocation: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     df_fillContinentNull: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     res15: df_filteredLocation.type = [iso_code: string, continent: string ... 48 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    ">     root\n",
    ">      |-- iso_code: string (nullable = true)\n",
    ">      |-- stringency_index: double (nullable = false)\n",
    ">      |-- population: double (nullable = true)\n",
    ">      |-- population_density: double (nullable = true)\n",
    ">      |-- gdp_per_capita: double (nullable = true)\n",
    ">      |-- diabetes_prevalence: double (nullable = true)\n",
    ">      |-- total_cases_per_million: double (nullable = true)\n",
    ">      |-- total_cases: double (nullable = false)\n",
    ">      |-- normal_stringency_index: double (nullable = true)\n",
    ">      |-- normal_population: double (nullable = true)\n",
    ">      |-- normal_population_density: double (nullable = true)\n",
    ">      |-- normal_gdp_per_capita: double (nullable = true)\n",
    ">      |-- normal_diabetes_prevalence: double (nullable = true)\n",
    ">      |-- log_total_cases_per_million: double (nullable = true)\n",
    "\n",
    "  \n",
    "\n",
    ">     df_by_location_normalized_selected: org.apache.spark.sql.DataFrame = [normal_stringency_index: double, normal_population: double ... 4 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    ">     df_filtered_date: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     res17: df_fillContinentNull.type = [iso_code: string, continent: string ... 48 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    ">     import org.apache.spark.ml.feature.VectorAssembler\n",
    ">     vectorizer: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_d4f952d2fd6f, handleInvalid=error, numInputCols=5\n",
    ">     dataset: org.apache.spark.sql.DataFrame = [features: vector, log_total_cases_per_million: double]\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    ">     split20: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, log_total_cases_per_million: double]\n",
    ">     split80: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, log_total_cases_per_million: double]\n",
    "\n",
    "  \n",
    "\n",
    ">     testSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, log_total_cases_per_million: double]\n",
    ">     trainingSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, log_total_cases_per_million: double]\n",
    "\n",
    "  \n",
    "\n",
    ">     res43: Long = 26\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    ">     res44: Long = 133\n",
    "\n",
    "  \n",
    "\n",
    ">     import org.apache.spark.ml.regression.LinearRegression\n",
    ">     import org.apache.spark.ml.regression.LinearRegressionModel\n",
    ">     import org.apache.spark.ml.Pipeline\n",
    ">     lr: org.apache.spark.ml.regression.LinearRegression = linReg_1077c2744dad\n",
    ">     lrModel: org.apache.spark.ml.regression.LinearRegressionModel = LinearRegressionModel: uid=linReg_1077c2744dad, numFeatures=5\n",
    "\n",
    "  \n",
    "\n",
    ">     Coefficients: [2.6214237261444726,-1.364306221013195,-1.3234981005291744,4.903123743799156,1.0283056897021905], Intercept: 6.69144939405342\n",
    ">     RMSE: 1.6058962464052953\n",
    ">     trainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@7545e754\n",
    "\n",
    "  \n",
    "\n",
    ">     +------------------+---------------------------+--------------------+\n",
    ">     |        prediction|log_total_cases_per_million|            features|\n",
    ">     +------------------+---------------------------+--------------------+\n",
    ">     |  6.32849475311866|          2.142543078223737|[0.15958180147058...|\n",
    ">     | 8.115061043502047|          7.528810569839765|[0.36167279411764...|\n",
    ">     |7.4629978084926085|          6.223671398897003|[0.64889705882352...|\n",
    ">     | 7.831137150153837|          6.524287884057365|[0.79779411764705...|\n",
    ">     |10.269420769779098|          5.843997267897207|[0.40429687499999...|\n",
    ">     | 7.289562258542387|         2.6304048908829563|[0.49471507352941...|\n",
    ">     | 9.963465130096221|          9.237218853465539|[0.57444852941176...|\n",
    ">     |13.213369998258525|         10.784078124343976|[0.74460018382352...|\n",
    ">     | 9.213228147281594|         10.572977149641726|[0.75528492647058...|\n",
    ">     | 9.085379666714468|          9.143147511395949|[0.82444852941176...|\n",
    ">     | 6.750739656770264|         10.952187342908323|[0.0,3.6806047717...|\n",
    ">     | 8.135800825140635|         10.373288860272808|[0.51056985294117...|\n",
    ">     | 7.507644816227433|         10.203096222487993|[0.55319393382352...|\n",
    ">     |10.048805671611255|          8.817232647019427|[0.56376378676470...|\n",
    ">     | 9.404481665413662|         10.158884909113675|[0.61695772058823...|\n",
    ">     | 9.488257390217871|         10.351014339169227|[0.64889705882352...|\n",
    ">     |  9.08870238448793|         10.291011744026449|[0.71806066176470...|\n",
    ">     |  8.89927809231843|         10.041688001727678|[0.72334558823529...|\n",
    ">     |  8.89927809231843|         10.041688001727678|[0.72334558823529...|\n",
    ">     |  9.35306553340342|          9.427461709192647|[0.75528492647058...|\n",
    ">     +------------------+---------------------------+--------------------+\n",
    ">     only showing top 20 rows\n",
    ">\n",
    ">     Root Mean Squared Error (RMSE) on test data = 2.225906214656472\n",
    ">     import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    ">     predictions: org.apache.spark.sql.DataFrame = [features: vector, log_total_cases_per_million: double ... 1 more field]\n",
    ">     evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_4b8a404d0038, metricName=rmse, throughOrigin=false\n",
    ">     rmse: Double = 2.225906214656472\n",
    "\n",
    "  \n",
    "\n",
    ">     df_cleaned_feature_permillion: org.apache.spark.sql.DataFrame = [iso_code: string, continent: string ... 48 more fields]\n",
    "\n",
    "  \n",
    "\n",
    ">     Root Mean Squared Error (RMSE) on test data = $rmse\n",
    ">     evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_4bcd4d071608, metricName=rmse, throughOrigin=false\n",
    ">     rmse: Double = 99893.56063834664\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"./08_DataPrediction_GP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
