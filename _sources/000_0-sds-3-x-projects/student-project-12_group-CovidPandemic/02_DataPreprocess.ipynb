{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/datasets/group12/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.parquet(\"dbfs:/datasets/group12/analysis/*.parquet\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res10: Long = 60544\n",
    "\n",
    "  \n",
    "\n",
    "Preprocessing\n",
    "-------------\n",
    "\n",
    "### 0. filter out data of HongKong and unknown location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// filter unknow and HK data\n",
    "val df_filteredLocation = df.filter($\"iso_code\"=!=\"HKG\").filter($\"iso_code\".isNotNull)\n",
    "\n",
    "// fill missing continent value for World data\n",
    "val df_fillContinentNull = df_filteredLocation.na.fill(\"World\",Array(\"continent\")).cache\n",
    "df_filteredLocation.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     df_filteredLocation: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     df_fillContinentNull: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     res4: df_filteredLocation.type = [iso_code: string, continent: string ... 48 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "### 1. fill dates from 2020-01-23 (to ensure all countries having 316 days loggings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// filter date before 2020-01-23\n",
    "val df_filtered_date = df_fillContinentNull.filter($\"date\">\"2020-01-22\").cache\n",
    "df_fillContinentNull.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     df_filtered_date: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     res6: df_fillContinentNull.type = [iso_code: string, continent: string ... 48 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "### 2. Fill missing value for total*cases, total*deaths, new*cases*smoothed, new*deaths*smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "// fill missing for new_cases_smoothed and new_deaths_smoothed\n",
    "val df_fillNullForSmooth = df_filtered_date.na.fill(0,Array(\"new_cases_smoothed\"))\n",
    "                           .na.fill(0,Array(\"new_deaths_smoothed\"))\n",
    "                           .cache\n",
    "df_filtered_date.unpersist()\n",
    "\n",
    "// fill missing for total_cases\n",
    "val df_fillNullForTotalCases = df_fillNullForSmooth.na.fill(0, Array(\"total_cases\")).cache\n",
    "df_fillNullForSmooth.unpersist()\n",
    "\n",
    "// correct total_deaths, new_deaths, new_deaths_smoothed\n",
    "val df_fillNullForTotalDeathsSpecial = df_fillNullForTotalCases.withColumn(\"total_deaths_correct\", \n",
    "                                        when(col(\"iso_code\").equalTo(\"ISL\")&&(col(\"date\")>\"2020-03-13\" && col(\"date\")<\"2020-03-22\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"PNG\")&&(col(\"date\")>\"2020-07-19\" && col(\"date\")<\"2020-07-23\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"SVK\")&&(col(\"date\")>\"2020-03-17\" && col(\"date\")<\"2020-03-23\"),0).otherwise(col(\"total_deaths\")))\n",
    "                            .withColumn(\"new_deaths_correct\", \n",
    "                                        when(col(\"iso_code\").equalTo(\"ISL\")&&(col(\"date\")>\"2020-03-13\" && col(\"date\")<\"2020-03-22\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"PNG\")&&(col(\"date\")>\"2020-07-19\" && col(\"date\")<\"2020-07-23\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"SVK\")&&(col(\"date\")>\"2020-03-17\" && col(\"date\")<\"2020-03-23\"),0).otherwise(col(\"new_deaths\")))\n",
    "                            .withColumn(\"new_deaths_smoothed_correct\", \n",
    "                                        when(col(\"iso_code\").equalTo(\"ISL\")&&(col(\"date\")>\"2020-03-13\" && col(\"date\")<\"2020-03-22\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"PNG\")&&(col(\"date\")>\"2020-07-19\" && col(\"date\")<\"2020-07-23\"),0)\n",
    "                                       .when(col(\"iso_code\").equalTo(\"SVK\")&&(col(\"date\")>\"2020-03-17\" && col(\"date\")<\"2020-03-23\"),0).otherwise(col(\"new_deaths_smoothed\")))\n",
    "                            .cache\n",
    "df_fillNullForTotalCases.unpersist()\n",
    "\n",
    "val df_cleaned = df_fillNullForTotalDeathsSpecial\n",
    "                                  .drop(\"new_deaths\", \"total_deaths\", \"new_deaths_smoothed\") // drop old column to rename\n",
    "                                  .withColumnRenamed(\"new_deaths_correct\",\"new_deaths\")\n",
    "                                  .withColumnRenamed(\"total_deaths_correct\",\"total_deaths\")\n",
    "                                  .withColumnRenamed(\"new_deaths_smoothed_correct\",\"new_deaths_smoothed\")\n",
    "                                  .na.fill(0, Array(\"total_deaths\"))\n",
    "                                  .select(df.columns.head, df.columns.tail: _*)\n",
    "                                  .cache\n",
    "df_fillNullForTotalDeathsSpecial.unpersist()\n",
    "\n",
    "display(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "### 3. Select invariant (during pandemic) features for clustering and filter out countries that have missing invariant features\n",
    "\n",
    "Invariant feature list: - population - population*density - median*age -\n",
    "aged*65*older - aged*70*older - gdp*per*capita - cardiovasc*death*rate -\n",
    "diabetes*prevalence - female*smokers - male*smokers -\n",
    "hospital*beds*per*thousand - life*expectancy - human*development\\_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// select invariant features\n",
    "val df_invariantFeatures = df_cleaned.select($\"iso_code\",$\"location\", $\"population\",$\"population_density\",\n",
    "                                             $\"median_age\", $\"aged_65_older\",\n",
    "                                             $\"aged_70_older\",$\"gdp_per_capita\",\n",
    "                                             $\"cardiovasc_death_rate\",$\"diabetes_prevalence\",\n",
    "                                             $\"female_smokers\",$\"male_smokers\",$\"hospital_beds_per_thousand\",\n",
    "                                             $\"life_expectancy\",$\"human_development_index\").cache\n",
    "\n",
    "// Extract valid distrinct features RDD\n",
    "val valid_distinct_features = df_invariantFeatures.distinct()\n",
    "                                 .filter($\"population\".isNotNull && $\"population_density\".isNotNull && $\"median_age\".isNotNull && \n",
    "                                 $\"aged_65_older\".isNotNull && $\"aged_70_older\".isNotNull && $\"gdp_per_capita\".isNotNull &&\n",
    "                                 $\"cardiovasc_death_rate\".isNotNull && $\"diabetes_prevalence\".isNotNull && $\"female_smokers\".isNotNull && \n",
    "                                 $\"male_smokers\".isNotNull && $\"hospital_beds_per_thousand\".isNotNull && $\"life_expectancy\".isNotNull &&\n",
    "                                 $\"human_development_index\".isNotNull)\n",
    "\n",
    "\n",
    "// filter out NULL feature countries\n",
    "val df_cleaned_feature = df_cleaned.filter($\"location\".isin(valid_distinct_features.select($\"location\").rdd.map(r => r(0)).collect().toSeq: _*)).cache\n",
    "\n",
    "df_cleaned.unpersist()\n",
    "\n",
    "display(df_cleaned_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "### 4. Imputing missing data for\n",
    "\n",
    "-   total*cases*per\\_million\n",
    "-   new*cases*per\\_million\n",
    "-   new*cases*smoothed*per*million\n",
    "-   total*deaths*per\\_million\n",
    "-   new*deaths*per\\_million\n",
    "-   new*deaths*smoothed*per*million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_cleaned_feature_permillion = df_cleaned_feature.withColumn(\"total_cases_per_million_correct\", df_cleaned_feature(\"total_cases\")/df_cleaned_feature(\"population\")*1000000)\n",
    "                                                   .withColumn(\"new_cases_per_million_correct\", df_cleaned_feature(\"new_cases\")/df_cleaned_feature(\"population\")*1000000)\n",
    "                                                   .withColumn(\"new_cases_smoothed_per_million_correct\", df_cleaned_feature(\"new_cases_smoothed\")/df_cleaned_feature(\"population\")*1000000)\n",
    "                                                   .withColumn(\"total_deaths_per_million_correct\", df_cleaned_feature(\"total_deaths\")/df_cleaned_feature(\"population\")*1000000)\n",
    "                                                   .withColumn(\"new_deaths_per_million_correct\", df_cleaned_feature(\"new_deaths\")/df_cleaned_feature(\"population\")*1000000)\n",
    "                                                   .withColumn(\"new_deaths_smoothed_per_million_correct\", df_cleaned_feature(\"new_deaths_smoothed\")/df_cleaned_feature(\"population\")*1000000)\n",
    "                                                   .drop(\"total_cases_per_million\", \"new_cases_per_million\", \"new_cases_smoothed_per_million\", \n",
    "                                                         \"total_deaths_per_million\", \"new_deaths_per_million\", \"new_deaths_smoothed_per_million\") // drop old column to rename\n",
    "                                                   .withColumnRenamed(\"total_cases_per_million_correct\",\"total_cases_per_million\")\n",
    "                                                   .withColumnRenamed(\"new_cases_per_million_correct\",\"new_cases_per_million\")\n",
    "                                                   .withColumnRenamed(\"new_cases_smoothed_per_million_correct\",\"new_cases_smoothed_per_million\")\n",
    "                                                   .withColumnRenamed(\"total_deaths_per_million_correct\",\"total_deaths_per_million\")\n",
    "                                                   .withColumnRenamed(\"new_deaths_per_million_correct\",\"new_deaths_per_million\")\n",
    "                                                   .withColumnRenamed(\"new_deaths_smoothed_per_million_correct\",\"new_deaths_smoothed_per_million\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     df_cleaned_feature_permillion: org.apache.spark.sql.DataFrame = [iso_code: string, continent: string ... 48 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "### 5. Impute time series data of\n",
    "\n",
    "-   reproduction\\_rate\n",
    "-   total\\_tests\n",
    "-   stringency\\_index\n",
    "-   total*tests*per\\_thousand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val df_cleaned_time_series = df_cleaned_feature_permillion\n",
    "                              .withColumn(\"reproduction_rate\", last(\"reproduction_rate\", true)\n",
    "                                         .over(Window.partitionBy(\"location\").orderBy(\"date\").rowsBetween(-df_cleaned_feature_permillion.count(), 0)))\n",
    "                              .withColumn(\"reproduction_rate\", first(\"reproduction_rate\", true)\n",
    "                                         .over(Window.partitionBy(\"location\").orderBy(\"date\").rowsBetween(0, df_cleaned_feature_permillion.count())))\n",
    "                              .na.fill(0, Array(\"reproduction_rate\"))\n",
    "                              .withColumn(\"stringency_index\", last(\"stringency_index\", true)\n",
    "                                         .over(Window.partitionBy(\"location\").orderBy(\"date\").rowsBetween(-df_cleaned_feature_permillion.count(), 0)))\n",
    "                              .na.fill(0, Array(\"stringency_index\"))\n",
    "                              .withColumn(\"total_tests\", last(\"total_tests\", true)\n",
    "                                         .over(Window.partitionBy(\"location\").orderBy(\"date\").rowsBetween(-df_cleaned_feature_permillion.count(), 0)))\n",
    "                              .withColumn(\"total_tests\", first(\"total_tests\", true)\n",
    "                                         .over(Window.partitionBy(\"location\").orderBy(\"date\").rowsBetween(0, df_cleaned_feature_permillion.count())))\n",
    "                              .na.fill(0, Array(\"total_tests\"))\n",
    "                              .withColumn(\"total_tests_per_thousand\", col(\"total_tests\")/col(\"population\")*1000)\n",
    "                              .cache\n",
    "\n",
    "df_cleaned_feature_permillion.unpersist()\n",
    "\n",
    "display(df_cleaned_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
