{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction with Linear Regression (LR) Model\n",
    "--------------------------------------------\n",
    "\n",
    "In this model, we use scala to process the data and predict the total\n",
    "cases. In this data set, there are many features which are constant for\n",
    "each country and donâ€™t change with time. So, we tried to predict the\n",
    "total cases on a selected date, from some countries to other countries,\n",
    "without considering the time series.\n",
    "\n",
    "1. Import data and preprocess\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// %run \"./02_DataPreprocess\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    ">     res61: Long = 62500\n",
    "\n",
    "  \n",
    "\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "  \n",
    "\n",
    ">     df_filteredLocation: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     df_fillContinentNull: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     res62: df_filteredLocation.type = [iso_code: string, continent: string ... 48 more fields]\n",
    "\n",
    "  \n",
    "\n",
    ">     df_filtered_date: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     res64: df_fillContinentNull.type = [iso_code: string, continent: string ... 48 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "2. Data process\n",
    "---------------\n",
    "\n",
    ">     df_cleaned_feature_permillion: org.apache.spark.sql.DataFrame = [iso_code: string, continent: string ... 48 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cleaned_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_time_series.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     root\n",
    ">      |-- iso_code: string (nullable = true)\n",
    ">      |-- continent: string (nullable = false)\n",
    ">      |-- location: string (nullable = true)\n",
    ">      |-- date: string (nullable = true)\n",
    ">      |-- total_cases: double (nullable = false)\n",
    ">      |-- new_cases: double (nullable = true)\n",
    ">      |-- new_cases_smoothed: double (nullable = false)\n",
    ">      |-- total_deaths: double (nullable = false)\n",
    ">      |-- new_deaths: double (nullable = true)\n",
    ">      |-- new_deaths_smoothed: double (nullable = false)\n",
    ">      |-- reproduction_rate: double (nullable = false)\n",
    ">      |-- icu_patients: double (nullable = true)\n",
    ">      |-- icu_patients_per_million: double (nullable = true)\n",
    ">      |-- hosp_patients: double (nullable = true)\n",
    ">      |-- hosp_patients_per_million: double (nullable = true)\n",
    ">      |-- weekly_icu_admissions: double (nullable = true)\n",
    ">      |-- weekly_icu_admissions_per_million: double (nullable = true)\n",
    ">      |-- weekly_hosp_admissions: double (nullable = true)\n",
    ">      |-- weekly_hosp_admissions_per_million: double (nullable = true)\n",
    ">      |-- total_tests: double (nullable = false)\n",
    ">      |-- new_tests: double (nullable = true)\n",
    ">      |-- total_tests_per_thousand: double (nullable = true)\n",
    ">      |-- new_tests_per_thousand: double (nullable = true)\n",
    ">      |-- new_tests_smoothed: double (nullable = true)\n",
    ">      |-- new_tests_smoothed_per_thousand: double (nullable = true)\n",
    ">      |-- tests_per_case: double (nullable = true)\n",
    ">      |-- positive_rate: double (nullable = true)\n",
    ">      |-- tests_units: double (nullable = true)\n",
    ">      |-- stringency_index: double (nullable = false)\n",
    ">      |-- population: double (nullable = true)\n",
    ">      |-- population_density: double (nullable = true)\n",
    ">      |-- median_age: double (nullable = true)\n",
    ">      |-- aged_65_older: double (nullable = true)\n",
    ">      |-- aged_70_older: double (nullable = true)\n",
    ">      |-- gdp_per_capita: double (nullable = true)\n",
    ">      |-- extreme_poverty: double (nullable = true)\n",
    ">      |-- cardiovasc_death_rate: double (nullable = true)\n",
    ">      |-- diabetes_prevalence: double (nullable = true)\n",
    ">      |-- female_smokers: double (nullable = true)\n",
    ">      |-- male_smokers: double (nullable = true)\n",
    ">      |-- handwashing_facilities: double (nullable = true)\n",
    ">      |-- hospital_beds_per_thousand: double (nullable = true)\n",
    ">      |-- life_expectancy: double (nullable = true)\n",
    ">      |-- human_development_index: double (nullable = true)\n",
    ">      |-- total_cases_per_million: double (nullable = true)\n",
    ">      |-- new_cases_per_million: double (nullable = true)\n",
    ">      |-- new_cases_smoothed_per_million: double (nullable = true)\n",
    ">      |-- total_deaths_per_million: double (nullable = true)\n",
    ">      |-- new_deaths_per_million: double (nullable = true)\n",
    ">      |-- new_deaths_smoothed_per_million: double (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "for (c <- df_cleaned_time_series.columns) {\n",
    "  println(c + \": \" + df_cleaned_time_series.filter(col(c).isNull).count())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     iso_code: 0\n",
    ">     continent: 0\n",
    ">     location: 0\n",
    ">     date: 0\n",
    ">     total_cases: 0\n",
    ">     new_cases: 0\n",
    ">     new_cases_smoothed: 0\n",
    ">     total_deaths: 0\n",
    ">     new_deaths: 0\n",
    ">     new_deaths_smoothed: 0\n",
    ">     reproduction_rate: 0\n",
    ">     icu_patients: 36018\n",
    ">     icu_patients_per_million: 36018\n",
    ">     hosp_patients: 34870\n",
    ">     hosp_patients_per_million: 34870\n",
    ">     weekly_icu_admissions: 41062\n",
    ">     weekly_icu_admissions_per_million: 41062\n",
    ">     weekly_hosp_admissions: 40715\n",
    ">     weekly_hosp_admissions_per_million: 40715\n",
    ">     total_tests: 0\n",
    ">     new_tests: 20510\n",
    ">     total_tests_per_thousand: 0\n",
    ">     new_tests_per_thousand: 20510\n",
    ">     new_tests_smoothed: 18176\n",
    ">     new_tests_smoothed_per_thousand: 18176\n",
    ">     tests_per_case: 19301\n",
    ">     positive_rate: 19749\n",
    ">     tests_units: 41600\n",
    ">     stringency_index: 0\n",
    ">     population: 0\n",
    ">     population_density: 0\n",
    ">     median_age: 0\n",
    ">     aged_65_older: 0\n",
    ">     aged_70_older: 0\n",
    ">     gdp_per_capita: 0\n",
    ">     extreme_poverty: 11168\n",
    ">     cardiovasc_death_rate: 0\n",
    ">     diabetes_prevalence: 0\n",
    ">     female_smokers: 0\n",
    ">     male_smokers: 0\n",
    ">     handwashing_facilities: 24124\n",
    ">     hospital_beds_per_thousand: 0\n",
    ">     life_expectancy: 0\n",
    ">     human_development_index: 0\n",
    ">     total_cases_per_million: 0\n",
    ">     new_cases_per_million: 0\n",
    ">     new_cases_smoothed_per_million: 0\n",
    ">     total_deaths_per_million: 0\n",
    ">     new_deaths_per_million: 0\n",
    ">     new_deaths_smoothed_per_million: 0\n",
    ">     import org.apache.spark.sql.functions._\n",
    "\n",
    "  \n",
    "\n",
    "Prepare the data for training. We choose a day we want to predict, and\n",
    "select the constant features, and select the target column for\n",
    "prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_by_location = df_cleaned_time_series.filter($\"date\" === \"2020-12-01\").sort($\"continent\").select($\"iso_code\",$\"stringency_index\", $\"population\",$\"population_density\",$\"gdp_per_capita\",$\"diabetes_prevalence\",$\"total_cases_per_million\",$\"total_cases\")\n",
    "display(df_by_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_location.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res145: Long = 159\n",
    "\n",
    "  \n",
    "\n",
    "Rescale the feature values and the target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Column\n",
    "val min_str_index = df_by_location.select(min($\"stringency_index\")).first()(0)\n",
    "val max_str_index = df_by_location.select(max($\"stringency_index\")).first()(0)\n",
    "val min_population = df_by_location.select(min($\"population\")).first()(0)\n",
    "val max_population = df_by_location.select(max($\"population\")).first()(0)\n",
    "val min_population_density = \n",
    "df_by_location.select(min($\"population_density\")).first()(0)\n",
    "val max_population_density = \n",
    "df_by_location.select(max($\"population_density\")).first()(0)\n",
    "val min_gdp_per_capita = df_by_location.select(min($\"gdp_per_capita\")).first()(0)\n",
    "val max_gdp_per_capita = df_by_location.select(max($\"gdp_per_capita\")).first()(0)\n",
    "val min_diabetes_prevalence = \n",
    "df_by_location.select(min($\"diabetes_prevalence\")).first()(0)\n",
    "val max_diabetes_prevalence = df_by_location.select(max($\"diabetes_prevalence\")).first()(0)\n",
    "\n",
    "val df_by_location_normalized = df_by_location\n",
    "  .withColumn(\"normal_stringency_index\",($\"stringency_index\" -lit(min_str_index))/(lit(max_str_index)-lit(min_str_index)))\n",
    "  .withColumn(\"normal_population\", ($\"population\" - lit(min_population))/(lit(max_population)-lit(min_population)))\n",
    "  .withColumn(\"normal_population_density\",($\"population_density\" - lit(min_population_density))/(lit(max_population_density) - lit(min_population_density)))\n",
    "  .withColumn(\"normal_gdp_per_capita\", ($\"gdp_per_capita\" - lit(min_gdp_per_capita))/(lit(max_gdp_per_capita)- lit(min_gdp_per_capita)))\n",
    "  .withColumn(\"normal_diabetes_prevalence\", ($\"diabetes_prevalence\" - lit(min_diabetes_prevalence))/lit(max_diabetes_prevalence) - lit(min_diabetes_prevalence)).withColumn(\"log_total_cases_per_million\", log($\"total_cases_per_million\")).toDF\n",
    "display(df_by_location_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_location_normalized.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     root\n",
    ">      |-- iso_code: string (nullable = true)\n",
    ">      |-- stringency_index: double (nullable = false)\n",
    ">      |-- population: double (nullable = true)\n",
    ">      |-- population_density: double (nullable = true)\n",
    ">      |-- gdp_per_capita: double (nullable = true)\n",
    ">      |-- diabetes_prevalence: double (nullable = true)\n",
    ">      |-- total_cases_per_million: double (nullable = true)\n",
    ">      |-- total_cases: double (nullable = false)\n",
    ">      |-- normal_stringency_index: double (nullable = true)\n",
    ">      |-- normal_population: double (nullable = true)\n",
    ">      |-- normal_population_density: double (nullable = true)\n",
    ">      |-- normal_gdp_per_capita: double (nullable = true)\n",
    ">      |-- normal_diabetes_prevalence: double (nullable = true)\n",
    ">      |-- log_total_cases_per_million: double (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_by_location_normalized_selected = df_by_location_normalized.select($\"normal_stringency_index\",$\"normal_population\",$\"normal_population_density\",$\"normal_gdp_per_capita\", $\"normal_diabetes_prevalence\",$\"log_total_cases_per_million\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     df_by_location_normalized_selected: org.apache.spark.sql.DataFrame = [normal_stringency_index: double, normal_population: double ... 4 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_by_location_normalized_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "3. Linear Regression from selected value to new cases\n",
    "-----------------------------------------------------\n",
    "\n",
    "These values are irrelevant to time, but relevant to country. So we try\n",
    "to predict the total case in some contries from the data in other\n",
    "contries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_location_normalized_selected.createOrReplaceTempView(\"covid_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val vectorizer =  new VectorAssembler()\n",
    ".setInputCols(Array(\"normal_stringency_index\", \"normal_population\", \"normal_population_density\", \"normal_gdp_per_capita\", \"normal_diabetes_prevalence\"))\n",
    ".setOutputCol(\"features\")\n",
    "\n",
    "// make a DataFrame called dataset from the table\n",
    "val dataset = vectorizer.transform(df_by_location_normalized_selected).select(\"features\",\"log_total_cases_per_million\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.ml.feature.VectorAssembler\n",
    ">     vectorizer: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_a8c5337c1334, handleInvalid=error, numInputCols=5\n",
    ">     dataset: org.apache.spark.sql.DataFrame = [features: vector, log_total_cases_per_million: double]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var Array(split20, split80) = dataset.randomSplit(Array(0.20, 0.80), 1800009193L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     split20: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, log_total_cases_per_million: double]\n",
    ">     split80: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, log_total_cases_per_million: double]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val testSet = split20.cache()\n",
    "\n",
    "val trainingSet = split80.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     testSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, log_total_cases_per_million: double]\n",
    ">     trainingSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, log_total_cases_per_million: double]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSet.count() // action to actually cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res156: Long = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet.count() // action to actually cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res157: Long = 133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.regression.LinearRegressionModel\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "// Let's initialize our linear regression learner\n",
    "val lr = new LinearRegression()\n",
    "// We use explain params to dump the parameters we can use\n",
    "lr.explainParams()\n",
    "// Now we set the parameters for the method\n",
    "lr.setPredictionCol(\"prediction\")\n",
    "  .setLabelCol(\"log_total_cases_per_million\")\n",
    "  .setMaxIter(100)\n",
    "  .setRegParam(0.1)\n",
    "val lrModel = lr.fit(trainingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.ml.regression.LinearRegression\n",
    ">     import org.apache.spark.ml.regression.LinearRegressionModel\n",
    ">     import org.apache.spark.ml.Pipeline\n",
    ">     lr: org.apache.spark.ml.regression.LinearRegression = linReg_04758f25dc55\n",
    ">     lrModel: org.apache.spark.ml.regression.LinearRegressionModel = LinearRegressionModel: uid=linReg_04758f25dc55, numFeatures=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trainingSummary = lrModel.summary\n",
    "\n",
    "println(s\"Coefficients: ${lrModel.coefficients}, Intercept: ${lrModel.intercept}\")\n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Coefficients: [2.6214237261445112,-1.3643062210132013,-1.3234981005291635,4.903123743799173,1.0283056897021852], Intercept: 6.691449394053385\n",
    ">     RMSE: 1.605896246405295\n",
    ">     trainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@3c0bba63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = lrModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"log_total_cases_per_million\", \"features\").show()\n",
    "\n",
    "// select (prediction, true label) and compute test error.\n",
    "val evaluator = new RegressionEvaluator()\n",
    "  .setLabelCol(\"log_total_cases_per_million\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"rmse\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +------------------+---------------------------+--------------------+\n",
    ">     |        prediction|log_total_cases_per_million|            features|\n",
    ">     +------------------+---------------------------+--------------------+\n",
    ">     | 6.328494753118636|          2.142543078223737|[0.15958180147058...|\n",
    ">     | 8.115061043502033|          7.528810569839765|[0.36167279411764...|\n",
    ">     | 7.462997808492604|          6.223671398897003|[0.64889705882352...|\n",
    ">     | 7.831137150153838|          6.524287884057365|[0.79779411764705...|\n",
    ">     |10.269420769779092|          5.843997267897207|[0.40429687499999...|\n",
    ">     | 7.289562258542376|         2.6304048908829563|[0.49471507352941...|\n",
    ">     | 9.963465130096218|          9.237218853465539|[0.57444852941176...|\n",
    ">     |13.213369998258539|         10.784078124343976|[0.74460018382352...|\n",
    ">     | 9.213228147281598|         10.572977149641726|[0.75528492647058...|\n",
    ">     | 9.085379666714474|          9.143147511395949|[0.82444852941176...|\n",
    ">     | 6.750739656770235|         10.952187342908323|[0.0,3.6806047717...|\n",
    ">     | 8.135800825140628|         10.373288860272808|[0.51056985294117...|\n",
    ">     | 7.507644816227425|         10.203096222487993|[0.55319393382352...|\n",
    ">     |10.048805671611257|          8.817232647019427|[0.56376378676470...|\n",
    ">     | 9.404481665413662|         10.158884909113675|[0.61695772058823...|\n",
    ">     | 9.488257390217875|         10.351014339169227|[0.64889705882352...|\n",
    ">     |  9.08870238448793|         10.291011744026449|[0.71806066176470...|\n",
    ">     | 8.899278092318433|         10.041688001727678|[0.72334558823529...|\n",
    ">     | 8.899278092318433|         10.041688001727678|[0.72334558823529...|\n",
    ">     | 9.353065533403424|          9.427461709192647|[0.75528492647058...|\n",
    ">     +------------------+---------------------------+--------------------+\n",
    ">     only showing top 20 rows\n",
    ">\n",
    ">     Root Mean Squared Error (RMSE) on test data = 2.2259062146564705\n",
    ">     import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    ">     predictions: org.apache.spark.sql.DataFrame = [features: vector, log_total_cases_per_million: double ... 1 more field]\n",
    ">     evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_575198e0fd5f, metricName=rmse, throughOrigin=false\n",
    ">     rmse: Double = 2.2259062146564705"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val predictions = lrModel.transform(testSet)\n",
    "display(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val new_predictions = predictions.withColumn(\"new_prediction\", exp($\"prediction\")).withColumn(\"total_cases_per_million\",exp($\"log_total_cases_per_million\")).select(\"new_prediction\", \"total_cases_per_million\", \"features\")\n",
    "display(new_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// select (prediction, true label) and compute test error.\n",
    "val evaluator = new RegressionEvaluator()\n",
    "  .setLabelCol(\"total_cases_per_million\")\n",
    "  .setPredictionCol(\"new_prediction\")\n",
    "  .setMetricName(\"rmse\")\n",
    "val rmse = evaluator.evaluate(new_predictions)\n",
    "println(\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Root Mean Squared Error (RMSE) on test data = $rmse\n",
    ">     evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_dcf763c5af79, metricName=rmse, throughOrigin=false\n",
    ">     rmse: Double = 99893.56063834814\n",
    "\n",
    "  \n",
    "\n",
    "4. Conclusion and Reflections\n",
    "-----------------------------\n",
    "\n",
    "We've tried several ways to preprocess the consant feature, but still\n",
    "didn't get a good result. We came to the conclusion that only predict\n",
    "the total cases of a country from other countries without considering\n",
    "the history time series values are not resonable. This is because the\n",
    "constant feature columns cannot reflect the total cases well. Therefore,\n",
    "we decided to use some time series methods to predict the value from the\n",
    "history value."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
