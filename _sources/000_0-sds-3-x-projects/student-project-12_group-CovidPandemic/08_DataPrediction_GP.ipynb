{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction with time series model - Gaussian Processes\n",
    "======================================================\n",
    "\n",
    "This notebook contains time series prediction with gaussian processes.\n",
    "The data used for prediction is new cases (smoothed) and new deaths\n",
    "(smoothed) for both an aggregated number of countries in the world and\n",
    "for Sweden. To implement the gaussian process model, the python package\n",
    "Gpytorch is used.\n",
    "\n",
    "1. Install, import, load and preprocess data\n",
    "--------------------------------------------\n",
    "\n",
    "Install, import and execute the other relevant notebooks here to load\n",
    "and preprocess data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gpytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Python interpreter will be restarted.\n",
    ">     Collecting gpytorch\n",
    ">       Downloading gpytorch-1.3.0.tar.gz (283 kB)\n",
    ">     Building wheels for collected packages: gpytorch\n",
    ">       Building wheel for gpytorch (setup.py): started\n",
    ">       Building wheel for gpytorch (setup.py): finished with status 'done'\n",
    ">       Created wheel for gpytorch: filename=gpytorch-1.3.0-py2.py3-none-any.whl size=473796 sha256=5882e250a68a9042a1e51e11617837c2e922878bd22e515cf9459b217c96ba2b\n",
    ">       Stored in directory: /root/.cache/pip/wheels/1d/f0/2c/2146864c1f7bd8a844c4143115c05c392da763fd8b249adb9d\n",
    ">     Successfully built gpytorch\n",
    ">     Installing collected packages: gpytorch\n",
    ">     Successfully installed gpytorch-1.3.0\n",
    ">     Python interpreter will be restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python imports\n",
    "import gpytorch as gpth\n",
    "import torch as th\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"./02_DataPreprocess\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "2. Additional data preprocessing in Scala\n",
    "-----------------------------------------\n",
    "\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "  \n",
    "\n",
    "### 2.1 World data preprocessing\n",
    "\n",
    ">     df_filteredLocation: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     df_fillContinentNull: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     res3: df_filteredLocation.type = [iso_code: string, continent: string ... 48 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// define dataframe summing up the new cases smoothed for each date\n",
    "val df_ncworld = df_cleaned_time_series.groupBy(\"date\").sum(\"new_cases_smoothed\").sort(col(\"date\")).withColumnRenamed(\"sum(new_cases_smoothed)\",\"new_cases_smoothed\")\n",
    "display(df_ncworld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    ">     df_filtered_date: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iso_code: string, continent: string ... 48 more fields]\n",
    ">     res5: df_fillContinentNull.type = [iso_code: string, continent: string ... 48 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    ">     df_cleaned_feature_permillion: org.apache.spark.sql.DataFrame = [iso_code: string, continent: string ... 48 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// define dataframe summing up the new deaths smoothed for each date\n",
    "val df_ndworld = df_cleaned_time_series.groupBy(\"date\").sum(\"new_deaths_smoothed\").sort(col(\"date\")).withColumnRenamed(\"sum(new_deaths_smoothed)\",\"new_deaths_smoothed\")\n",
    "display(df_ndworld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Add a time index for the date\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "val window_spec  = Window.orderBy($\"date\")\n",
    "\n",
    "val df_ncworld_indexed = df_ncworld.withColumn(\"time_idx\",row_number.over(window_spec))\n",
    "val df_ndworld_indexed = df_ndworld.withColumn(\"time_idx\",row_number.over(window_spec))\n",
    "display(df_ncworld_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Get max and min of time index\n",
    "import org.apache.spark.sql.functions.{min, max}\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val id_maxmin = df_ncworld_indexed.agg(max(\"time_idx\"), min(\"time_idx\")).head()\n",
    "val id_max: Int = id_maxmin.getInt(0)\n",
    "val id_min: Int = id_maxmin.getInt(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql.functions.{min, max}\n",
    ">     import org.apache.spark.sql.Row\n",
    ">     id_maxmin: org.apache.spark.sql.Row = [316,1]\n",
    ">     id_max: Int = 316\n",
    ">     id_min: Int = 1\n",
    "\n",
    "  \n",
    "\n",
    "Extract a window for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// define training and test data intervalls. test data is set to 10% of the total dataset time length.\n",
    "val test_wnd: Int = (0.1*id_max).toInt\n",
    "val train_wnd: Int = (0.9*id_max).toInt\n",
    "\n",
    "val df_ncworld_train = df_ncworld_indexed.where($\"time_idx\" > id_max-train_wnd-test_wnd && $\"time_idx\" <= id_max-test_wnd)\n",
    "val df_ncworld_test = df_ncworld_indexed.where($\"time_idx\" > id_max-test_wnd && $\"time_idx\" <= id_max)\n",
    "val df_ndworld_train = df_ndworld_indexed.where($\"time_idx\" > id_max-train_wnd-test_wnd && $\"time_idx\" <= id_max-test_wnd)\n",
    "val df_ndworld_test = df_ndworld_indexed.where($\"time_idx\" > id_max-test_wnd && $\"time_idx\" <= id_max)\n",
    "display(df_ncworld_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "Convert to python for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncworld_train.createOrReplaceTempView(\"df_ncworld_train\")\n",
    "df_ncworld_test.createOrReplaceTempView(\"df_ncworld_test\")\n",
    "df_ndworld_train.createOrReplaceTempView(\"df_ndworld_train\")\n",
    "df_ndworld_test.createOrReplaceTempView(\"df_ndworld_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncworld_train = spark.table(\"df_ncworld_train\")\n",
    "df_ncworld_test = spark.table(\"df_ncworld_test\")\n",
    "df_ndworld_train = spark.table(\"df_ndworld_train\")\n",
    "df_ndworld_test = spark.table(\"df_ndworld_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "### 2.2 Sweden preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_ncdenswe = df_cleaned_time_series.select($\"location\", $\"date\", $\"new_cases_smoothed\").where(expr(\"location = 'Sweden' or location = 'Denmark'\"))\n",
    "val df_nddenswe = df_cleaned_time_series.select($\"location\", $\"date\", $\"new_deaths_smoothed\").where(expr(\"location = 'Sweden' or location = 'Denmark'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     df_ncdenswe: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [location: string, date: string ... 1 more field]\n",
    ">     df_nddenswe: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [location: string, date: string ... 1 more field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Add a time index for the date\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "val window_spec  = Window.partitionBy(\"location\").orderBy($\"date\")\n",
    "\n",
    "val df_ncdenswe_indexed = df_ncdenswe.withColumn(\"time_idx\",row_number.over(window_spec))\n",
    "display(df_ncdenswe_indexed)\n",
    "\n",
    "val df_nddenswe_indexed = df_nddenswe.withColumn(\"time_idx\",row_number.over(window_spec))\n",
    "display(df_nddenswe_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val test_wnd: Int = (0.1*id_max).toInt\n",
    "val train_wnd: Int = (0.9*id_max).toInt\n",
    "val df_ncdenswe_train = df_ncdenswe_indexed.where($\"time_idx\" > id_max-train_wnd-test_wnd && $\"time_idx\" <= id_max-test_wnd)\n",
    "val df_ncdenswe_test = df_ncdenswe_indexed.where($\"time_idx\" > id_max-test_wnd && $\"time_idx\" <= id_max)\n",
    "val df_nddenswe_train = df_nddenswe_indexed.where($\"time_idx\" > id_max-train_wnd-test_wnd && $\"time_idx\" <= id_max-test_wnd)\n",
    "val df_nddenswe_test = df_nddenswe_indexed.where($\"time_idx\" > id_max-test_wnd && $\"time_idx\" <= id_max)\n",
    "display(df_ncdenswe_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncdenswe_train.createOrReplaceTempView(\"df_ncdenswe_train\")\n",
    "df_ncdenswe_test.createOrReplaceTempView(\"df_ncdenswe_test\")\n",
    "df_nddenswe_train.createOrReplaceTempView(\"df_nddenswe_train\")\n",
    "df_nddenswe_test.createOrReplaceTempView(\"df_nddenswe_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncdenswe_train = spark.table(\"df_ncdenswe_train\")\n",
    "df_ncdenswe_test = spark.table(\"df_ncdenswe_test\")\n",
    "df_nddenswe_train = spark.table(\"df_nddenswe_train\")\n",
    "df_nddenswe_test = spark.table(\"df_nddenswe_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "3. Time series prediction with Gaussian Processes\n",
    "-------------------------------------------------\n",
    "\n",
    "In this section we perform predictions based on the input data. Some\n",
    "additional preprocessing in Python is done as well. The transition from\n",
    "Scala to Python is motivated by the use of the python package Gpytorch\n",
    "for implementing the gaussian process model.\n",
    "\n",
    "### 3.1 World multistep prediction\n",
    "\n",
    "As similar operations are performed for processing data, a class is\n",
    "first defined to enable code reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GPDataSet():\n",
    "  def __init__(self, df_train, df_test, datacol, filterloc = None, add_input = None):\n",
    "    \"\"\"\n",
    "      class for processing input data to GP. As similar code is reused, this class enables some code reuse.\n",
    "      \n",
    "      param: 'df_train', training data dataframe\n",
    "      param: 'df_test', test data dataframe\n",
    "      param: 'datacol', data column in dataframe to perform predictions on, e.g. 'new_cases_smoothed'\n",
    "      param: 'filterloc', location column in dataframe to perform predictions on, e.g. 'Sweden'\n",
    "      param: 'add_input', additional location column in dataframe to use as input for predictions, e.g. 'Denmark'  \n",
    "    \"\"\"\n",
    "    self.df_train = df_train\n",
    "    self.df_test = df_test\n",
    "    self.datacol = datacol\n",
    "    self.filterloc = filterloc\n",
    "    self.add_input = add_input\n",
    "    self.num_xdim = None    \n",
    "\n",
    "        \n",
    "  def convert_to_numpy(self):\n",
    "    \"\"\"\n",
    "      convert dataframe to numpy arrays. This process may takes a while.\n",
    "    \"\"\"\n",
    "    # if no filter for location is specified\n",
    "    if self.filterloc is None:\n",
    "      x_train_np = np.array(self.df_train.orderBy(\"time_idx\").select(\"time_idx\").rdd.map(lambda x: x[0]).collect())\n",
    "      x_test_np = np.array(self.df_test.orderBy(\"time_idx\").select(\"time_idx\").rdd.map(lambda x: x[0]).collect())\n",
    "      y_train_np = np.array(self.df_train.orderBy(\"time_idx\").select(self.datacol).rdd.map(lambda x: x[0]).collect())    \n",
    "      y_test_np = np.array(self.df_test.orderBy(\"time_idx\").select(self.datacol).rdd.map(lambda x: x[0]).collect())    \n",
    "      num_xdim = 1      \n",
    "      \n",
    "    # if a filter for location is specified\n",
    "    else:\n",
    "      if self.add_input is None:\n",
    "        x_train_np = np.array(self.df_train.filter(col(\"location\") == self.filterloc).orderBy(\"time_idx\").select(\"time_idx\").rdd.map(lambda x: x[0]).collect())\n",
    "        x_test_np = np.array(self.df_test.filter(col(\"location\") == self.filterloc).orderBy(\"time_idx\").select(\"time_idx\").rdd.map(lambda x: x[0]).collect())\n",
    "        num_xdim = 1        \n",
    "     \n",
    "      # if prediction should add additional input from e.g. a neighbouring country\n",
    "      else: \n",
    "        x_train_time = np.array(self.df_train.filter(col(\"location\") == self.filterloc).orderBy(\"time_idx\").select(\"time_idx\").rdd.map(lambda x: x[0]).collect())\n",
    "        x_test_time = np.array(self.df_test.filter(col(\"location\") == self.filterloc).orderBy(\"time_idx\").select(\"time_idx\").rdd.map(lambda x: x[0]).collect())       \n",
    "        x_train_add = np.array(self.df_train.filter(col(\"location\") == self.add_input).orderBy(\"time_idx\").select(self.datacol).rdd.map(lambda x: x[0]).collect())\n",
    "        x_test_add = np.array(self.df_test.filter(col(\"location\") == self.add_input).orderBy(\"time_idx\").select(self.datacol).rdd.map(lambda x: x[0]).collect())    \n",
    "        x_train = np.stack((x_train_time, x_train_add), axis=0)\n",
    "        x_test = np.stack((x_test_time, x_test_add), axis=0)\n",
    "        x_train_np = np.moveaxis(x_train, 1, 0)\n",
    "        x_test_np = np.moveaxis(x_test, 1, 0)\n",
    "        num_xdim = 2\n",
    "                 \n",
    "      # output data \n",
    "      y_train_np = np.array(self.df_train.filter(col(\"location\") == self.filterloc).orderBy(\"time_idx\").select(self.datacol).rdd.map(lambda x: x[0]).collect())\n",
    "      y_test_np = np.array(self.df_test.filter(col(\"location\") == self.filterloc).orderBy(\"time_idx\").select(self.datacol).rdd.map(lambda x: x[0]).collect())\n",
    "      \n",
    "    self.x_train_np = x_train_np\n",
    "    self.x_test_np = x_test_np\n",
    "    self.y_train_np = y_train_np\n",
    "    self.y_test_np = y_test_np\n",
    "    self.num_xdim = num_xdim\n",
    "      \n",
    "  def plot_numpy_data(self):\n",
    "    \"\"\" \n",
    "      plot numpy arrays \n",
    "    \"\"\"   \n",
    "    if self.num_xdim == 2:\n",
    "      fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,6))\n",
    "      ax1.plot(self.x_train_np[:,0], self.y_train_np, 'k*')\n",
    "      ax1.legend(['train data'])\n",
    "      ax1.set_xlabel('time [days]')\n",
    "      ax1.set_ylabel('output')\n",
    "      ax1.set_title('training data')\n",
    "      ax1.grid()   \n",
    "      ax2.plot(self.x_train_np[:,0], self.x_train_np[:,1], 'k*')\n",
    "      ax2.legend(['train data'])\n",
    "      ax2.set_xlabel('time [days]')\n",
    "      ax2.set_ylabel('additional input')\n",
    "      ax2.set_title('training data')\n",
    "      ax2.grid()         \n",
    "    else:\n",
    "      fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "      ax.plot(self.x_train_np, self.y_train_np, 'k*')\n",
    "      ax.legend(['train data'])\n",
    "      ax.set_xlabel('time [days]')\n",
    "      ax.set_ylabel('output')\n",
    "      ax.set_title('training data')\n",
    "      ax.grid()      \n",
    "        \n",
    "  def get_train_length(self):\n",
    "      if self.num_xdim == 2:\n",
    "        return len(self.x_train_np[:,0])\n",
    "      else:\n",
    "        return len(self.x_train_np)\n",
    "\n",
    "  def process_numpy_data(self, nth_subsample = 4, window_red = 0.8):\n",
    "    \"\"\"\n",
    "      reduction of data by subsampling data and reducing length of data window. \n",
    "    \"\"\"\n",
    "    assert window_red > 0 and window_red <= 1, \"please adjust 'window_red' parameter to be between 0 and 1\"\n",
    "    start_idx = int((self.get_train_length())*window_red)\n",
    "    self.x_train = th.tensor(self.x_train_np[start_idx::nth_subsample], dtype=th.float)\n",
    "    self.x_test = th.tensor(self.x_test_np, dtype=th.float)\n",
    "    self.y_train = th.tensor(self.y_train_np[start_idx::nth_subsample], dtype=th.float)\n",
    "    self.y_test = th.tensor(self.y_test_np, dtype=th.float)    \n",
    "    self.normalize()\n",
    "    \n",
    "  def set_time_to_zero(self):\n",
    "    \"\"\"\n",
    "      sets the time vector to start at time zero\n",
    "    \"\"\"\n",
    "    if self.num_xdim == 2:\n",
    "      self.x_train_min = self.x_train[:,0].min()\n",
    "      self.x_train[:,0] = self.x_train[:,0] - self.x_train_min\n",
    "      self.x_test[:,0] = self.x_test[:,0] - self.x_train_min      \n",
    "    else:\n",
    "      self.x_train_min = self.x_train.min()\n",
    "      self.x_train = self.x_train - self.x_train_min\n",
    "      self.x_test = self.x_test - self.x_train_min\n",
    "      \n",
    "  def normalize(self):\n",
    "    \"\"\"\n",
    "      normalize the data to improve predictions\n",
    "    \"\"\"\n",
    "    self.set_time_to_zero()\n",
    "    \n",
    "    self.x_train_mean = self.x_train.mean()\n",
    "    self.x_train_std = self.x_train.std()\n",
    "    self.x_train = (self.x_train - self.x_train_mean) / self.x_train_std\n",
    "    self.x_test = (self.x_test - self.x_train_mean) / self.x_train_std     \n",
    "\n",
    "    self.y_train_mean = self.y_train.mean()\n",
    "    self.y_train_std = self.y_train.std()\n",
    "    self.y_train = (self.y_train - self.y_train_mean) / self.y_train_std\n",
    "    self.y_test = (self.y_test - self.y_train_mean) / self.y_train_std \n",
    "    self.data_normalized = True\n",
    "\n",
    "      \n",
    "  def plot_reduced_data(self):\n",
    "    \"\"\"\n",
    "      plots the reduced training data\n",
    "    \"\"\"\n",
    "    with th.no_grad():      \n",
    "      if self.num_xdim == 2:\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,6))\n",
    "        ax1.plot(self.x_train[:,0], self.y_train, 'k*')\n",
    "        ax1.legend(['train data'])\n",
    "        ax1.set_xlabel('time [days]')\n",
    "        ax1.set_ylabel('output')\n",
    "        ax1.set_title('training data')\n",
    "        ax1.grid()   \n",
    "        ax2.plot(self.x_train[:,0], self.x_train[:,1], 'k*')\n",
    "        ax2.legend(['train data'])\n",
    "        ax2.set_xlabel('time [days]')\n",
    "        ax2.set_ylabel('additional input')\n",
    "        ax2.set_title('training data')\n",
    "        ax2.grid()         \n",
    "      else:\n",
    "        fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "        ax.plot(self.x_train, self.y_train, 'k*')\n",
    "        ax.legend(['train data'])\n",
    "        ax.set_xlabel('time [days]')\n",
    "        ax.set_ylabel('output')\n",
    "        ax.set_title('training data')\n",
    "        ax.grid()     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Use class to convert dataframes to numpy arrays for further processing.\n",
    "Note, the conversion may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ncworld = GPDataSet(df_ncworld_train, df_ncworld_test, datacol = 'new_cases_smoothed', filterloc = None, add_input=None)\n",
    "ds_ndworld = GPDataSet(df_ndworld_train, df_ndworld_test, datacol = 'new_deaths_smoothed', filterloc = None, add_input=None)\n",
    "ds_ncworld.convert_to_numpy()\n",
    "ds_ndworld.convert_to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ncworld.plot_numpy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ndworld.plot_numpy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Process data by subsampling, reducing data window and normalize data.\n",
    "The gaussian process model is a so called non parametric model and will\n",
    "be mainly based on the data points. As such, to reduce the computation\n",
    "and the complexity of the model, we subsample and reduce the number of\n",
    "datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ncworld.process_numpy_data(nth_subsample = 4, window_red = 0.8)\n",
    "ds_ndworld.process_numpy_data(nth_subsample = 4, window_red = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Plot processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ncworld.plot_reduced_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ndworld.plot_reduced_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Define gaussian process classes using Gpytorch and different kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch as gpth\n",
    "class GPLinearRBF(gpth.models.ExactGP):\n",
    "  def __init__(self, train_x, train_y, likelihood):\n",
    "    super(GPLinearRBF, self).__init__(train_x, train_y, likelihood)\n",
    "    self.mean_module = gpth.means.ConstantMean()\n",
    "    self.covar_module = gpth.kernels.ScaleKernel(gpth.kernels.LinearKernel() + gpth.kernels.RBFKernel())\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x_mean = self.mean_module(x)\n",
    "    x_covar = self.covar_module(x)\n",
    "    return gpth.distributions.MultivariateNormal(x_mean, x_covar)    \n",
    "  \n",
    "class GPLinearMatern(gpth.models.ExactGP):\n",
    "  def __init__(self, train_x, train_y, likelihood):\n",
    "    super(GPLinearMatern, self).__init__(train_x, train_y, likelihood)\n",
    "    self.mean_module = gpth.means.ConstantMean()\n",
    "    self.covar_module = gpth.kernels.ScaleKernel(gpth.kernels.LinearKernel() + gpth.kernels.MaternKernel())\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x_mean = self.mean_module(x)\n",
    "    x_covar = self.covar_module(x)\n",
    "    return gpth.distributions.MultivariateNormal(x_mean, x_covar) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Define a training class for the Gaussian Process models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "class GPTrainer():\n",
    "  def __init__(self, gp_model, x_train, x_train_min, x_train_mean, x_train_std, x_test, y_train, y_test, y_train_mean, y_train_std, device='cpu', train_iter = 300, lr=0.1, verbose = True):\n",
    "    \"\"\" \n",
    "      class to manage training and prediction of data\n",
    "     \n",
    "      param: 'gp_model', name of gaussian process model including kernel to use\n",
    "      param: 'x_train', pytorch tensor (sequence, dim), normalized input training data, starting at time zero\n",
    "      param: 'x_train_min', pytorch tensor, start time of input training data\n",
    "      param: 'x_train_mean', pytorch tensor, mean used when normalizing input training data\n",
    "      param: 'x_train_std', pytorch tensor, std deviation used when normalizing input training data\n",
    "      param: 'x_test', pytorch tensor, normalized input test data, starting at time zero\n",
    "      param: 'y_train', pytorch tensor, normalized output training data      \n",
    "      param: 'y_train_mean', pytorch tensor, mean used when normalizing output training data\n",
    "      param: 'y_train_std', pytorch tensor, std deviation used when normalizing output training data \n",
    "      param: 'y_test', pytorch tensor, normalized output test data     \n",
    "      param: 'device', cpu or cuda. currently only tested for cpu.\n",
    "      param: 'train_iter', number of training iterations to fit kernel parameters to data\n",
    "      param: 'lr', learning rate\n",
    "      param: 'verbose', print information such as loss during training\n",
    "    \"\"\"\n",
    "    \n",
    "    # data\n",
    "    self.x_train = x_train.to(device)\n",
    "    self.x_train_min = x_train_min\n",
    "    self.x_train_mean = x_train_mean\n",
    "    self.x_train_std = x_train_std    \n",
    "    self.x_test = x_test.to(device)\n",
    "    self.x_cat = th.cat((x_train,x_test),dim=0).to(device) \n",
    "    self.y_train = y_train.to(device)\n",
    "    self.y_train_mean = y_train_mean\n",
    "    self.y_train_std = y_train_std\n",
    "    self.y_test = y_test.to(device)\n",
    "    self.preds = None\n",
    "    \n",
    "    # define GP likelihood\n",
    "    self.likelihood = gpth.likelihoods.GaussianLikelihood()    \n",
    "    \n",
    "    # GP model selection and init\n",
    "    assert gp_model == 'GPLinearRBF' or 'GPLinearMatern', \"Error: GP model selected is not defined\"\n",
    "    if gp_model == 'GPLinearRBF':\n",
    "      self.model = GPLinearRBF(self.x_train, self.y_train, self.likelihood).to(device)\n",
    "    if gp_model == 'GPLinearMatern':\n",
    "      self.model = GPLinearMatern(self.x_train, self.y_train, self.likelihood).to(device)\n",
    "      \n",
    "    # training param\n",
    "    self.train_iter = train_iter\n",
    "    self.lr = lr\n",
    "    self.device = device\n",
    "    self.optimizer = th.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "    self.loss_fn = gpth.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)   \n",
    "    self.verbose = verbose\n",
    "    \n",
    "    # plots\n",
    "    self.fig = None\n",
    "    self.ax = None\n",
    "      \n",
    "  def train(self):\n",
    "    \"\"\"\n",
    "      training of gaussian process model to fit kernel parameters to data\n",
    "    \"\"\"\n",
    "    self.model.train()\n",
    "    self.likelihood.train()\n",
    "    \n",
    "    for iter_idx in range(1,self.train_iter+1):\n",
    "      self.optimizer.zero_grad()\n",
    "      out = self.model(self.x_train)\n",
    "      loss = -self.loss_fn(out, self.y_train).mean()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "      if iter_idx % 10 == 0 and self.verbose is True:\n",
    "        print(f\"Iter: {iter_idx}, train_loss: {loss.item()}\")\n",
    "        \n",
    "  def prediction(self):\n",
    "    \"\"\"\n",
    "      predict data\n",
    "    \"\"\"\n",
    "    self.model.eval()\n",
    "    self.likelihood.eval()\n",
    "    with th.no_grad(): #, gpth.settings.fast_pred_var():  \n",
    "      self.preds = self.likelihood(self.model(self.x_cat))\n",
    "      \n",
    "  def denormalize_y(self, data):\n",
    "    \"\"\"\n",
    "      denormalize the output data\n",
    "    \"\"\"\n",
    "    return data*self.y_train_std + self.y_train_mean\n",
    "  \n",
    "  def denormalize_x(self, data):\n",
    "    \"\"\"\n",
    "      denormalize the input data\n",
    "    \"\"\"\n",
    "    return data*self.x_train_std + self.x_train_mean  \n",
    "      \n",
    "  def plot(self):\n",
    "    \"\"\"\n",
    "      plot the data\n",
    "    \"\"\"\n",
    "    with th.no_grad():\n",
    "      \n",
    "      # extract time index dimension\n",
    "      xdim = None\n",
    "      try:\n",
    "        _, xdim = self.x_train.shape\n",
    "      except:\n",
    "        pass\n",
    "      if xdim == None or xdim == 1:\n",
    "        x_train = self.denormalize_x(self.x_train)\n",
    "        x_test = self.denormalize_x(self.x_test)\n",
    "        x_cat = self.denormalize_x(self.x_cat)\n",
    "      elif xdim > 1:\n",
    "        x_train = self.denormalize_x(self.x_train)[:,0]\n",
    "        x_test = self.denormalize_x(self.x_test)[:,0]\n",
    "        x_cat = self.denormalize_x(self.x_cat)[:,0]\n",
    "        \n",
    "      # plot\n",
    "      self.fig, self.ax = plt.subplots(1,1, figsize=(12,6))\n",
    "      lower = self.denormalize_y(self.preds.mean - self.preds.variance.sqrt() * 1.96)\n",
    "      upper = self.denormalize_y(self.preds.mean + self.preds.variance.sqrt() * 1.96)\n",
    "      self.ax.plot(x_train.numpy()+self.x_train_min.numpy(), self.denormalize_y(self.y_train).numpy(), 'k*')\n",
    "      self.ax.plot(x_test.numpy()+self.x_train_min.numpy(), self.denormalize_y(self.y_test).numpy(), 'r*')\n",
    "      self.ax.plot(x_cat.numpy()+self.x_train_min.numpy(), self.denormalize_y(self.preds.mean).numpy(), 'b')\n",
    "      self.ax.fill_between(x_cat.numpy()+self.x_train_min.numpy(), lower.numpy(), upper.numpy(), alpha=0.3)\n",
    "      self.ax.legend(['train data', 'test data', 'predicted mean', 'predicted confidence 95%'])\n",
    "      self.ax.set_xlabel('time [days]')\n",
    "      self.ax.set_ylabel('prediction')\n",
    "      self.ax.set_title('prediction')\n",
    "      self.ax.grid()     \n",
    "      \n",
    "  def print_data_dim(self):\n",
    "    \"\"\"\n",
    "      print shapes for debug purpose\n",
    "    \"\"\"\n",
    "    print(\"data shapes:\")\n",
    "    print(f'x_train: {self.x_train.shape}')\n",
    "    print(f'x_test: {self.x_test.shape}')\n",
    "    print(f'x_cat: {self.x_cat.shape}')\n",
    "    print(f'y_train: {self.y_train.shape}')\n",
    "    print(f'y_test: {self.y_test.shape}')  \n",
    "    try:\n",
    "      print(f'preds mean: {self.preds.mean.shape}')\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  def evaluate(self):\n",
    "    \"\"\"\n",
    "      evaluation of predictions\n",
    "    \"\"\"\n",
    "    with th.no_grad():\n",
    "      # data to evaluate\n",
    "      test_data = self.denormalize_y(self.y_test) \n",
    "      predictions = self.denormalize_y(self.preds.mean[-len(self.y_test):])\n",
    "      \n",
    "      # evaluate\n",
    "      error_mse = mean_squared_error(test_data, predictions)\n",
    "      error_rmse = math.sqrt(error_mse)\n",
    "      error_abs = mean_absolute_error(test_data, predictions)\n",
    "      avg_gt = test_data.sum() / len(test_data)\n",
    "      mse_percentage = error_rmse / avg_gt * 100\n",
    "      abs_percentage = error_abs / avg_gt * 100\n",
    "      \n",
    "      # print\n",
    "      print('Average of groundtruth: %.3f' % avg_gt)\n",
    "      print('Test MSE: %.3f' % error_mse)\n",
    "      print('Test RMSE: %.3f' % error_rmse)\n",
    "      print('RMSE percentage error: %.3f' % mse_percentage, '%')\n",
    "      print('Test ABS: %.3f' % error_abs)\n",
    "      print('ABS percentage error: %.3f' % abs_percentage, '%')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Init the training class for the Gaussian Process models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ncworld = GPTrainer(gp_model='GPLinearRBF', x_train=ds_ncworld.x_train, x_train_min=ds_ncworld.x_train_min, x_train_mean=ds_ncworld.x_train_mean, x_train_std=ds_ncworld.x_train_std, x_test=ds_ncworld.x_test, y_train=ds_ncworld.y_train, y_test=ds_ncworld.y_test, y_train_mean=ds_ncworld.y_train_mean, y_train_std=ds_ncworld.y_train_std)\n",
    "\n",
    "pred_ndworld = GPTrainer(gp_model='GPLinearRBF', x_train=ds_ndworld.x_train, x_train_min=ds_ndworld.x_train_min, x_train_mean=ds_ndworld.x_train_mean, x_train_std=ds_ndworld.x_train_std, x_test=ds_ndworld.x_test, y_train=ds_ndworld.y_train, y_test=ds_ndworld.y_test, y_train_mean=ds_ndworld.y_train_mean, y_train_std=ds_ndworld.y_train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\ntraining new cases prediction model')\n",
    "pred_ncworld.train()\n",
    "print('\\ntraining new deaths prediction model')\n",
    "pred_ndworld.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     training new cases prediction model\n",
    ">     /databricks/python/lib/python3.7/site-packages/torch/autograd/__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
    ">       allow_unreachable=True)  # allow_unreachable flag\n",
    ">     Iter: 10, train_loss: 0.7546234130859375\n",
    ">     Iter: 20, train_loss: 0.4275757968425751\n",
    ">     Iter: 30, train_loss: 0.11226234585046768\n",
    ">     Iter: 40, train_loss: -0.15334300696849823\n",
    ">     Iter: 50, train_loss: -0.2943705916404724\n",
    ">     Iter: 60, train_loss: -0.30030307173728943\n",
    ">     Iter: 70, train_loss: -0.30076757073402405\n",
    ">     Iter: 80, train_loss: -0.30838844180107117\n",
    ">     Iter: 90, train_loss: -0.3079160749912262\n",
    ">     Iter: 100, train_loss: -0.30836227536201477\n",
    ">     Iter: 110, train_loss: -0.3085930645465851\n",
    ">     Iter: 120, train_loss: -0.3085639178752899\n",
    ">     Iter: 130, train_loss: -0.3086168169975281\n",
    ">     Iter: 140, train_loss: -0.3086094558238983\n",
    ">     Iter: 150, train_loss: -0.30860716104507446\n",
    ">     Iter: 160, train_loss: -0.3086104691028595\n",
    ">     Iter: 170, train_loss: -0.3086104691028595\n",
    ">     Iter: 180, train_loss: -0.30861249566078186\n",
    ">     Iter: 190, train_loss: -0.308610737323761\n",
    ">     Iter: 200, train_loss: -0.30861350893974304\n",
    ">     Iter: 210, train_loss: -0.3086123764514923\n",
    ">     Iter: 220, train_loss: -0.3086145520210266\n",
    ">     Iter: 230, train_loss: -0.30861225724220276\n",
    ">     Iter: 240, train_loss: -0.3086118698120117\n",
    ">     Iter: 250, train_loss: -0.30861696600914\n",
    ">     Iter: 260, train_loss: -0.30861350893974304\n",
    ">     Iter: 270, train_loss: -0.3086144030094147\n",
    ">     Iter: 280, train_loss: -0.3086142838001251\n",
    ">     Iter: 290, train_loss: -0.30861034989356995\n",
    ">     Iter: 300, train_loss: -0.3086152970790863\n",
    ">\n",
    ">     training new deaths prediction model\n",
    ">     Iter: 10, train_loss: 1.0905324220657349\n",
    ">     Iter: 20, train_loss: 0.9422212243080139\n",
    ">     Iter: 30, train_loss: 0.822321891784668\n",
    ">     Iter: 40, train_loss: 0.7358468174934387\n",
    ">     Iter: 50, train_loss: 0.6878499984741211\n",
    ">     Iter: 60, train_loss: 0.6686744689941406\n",
    ">     Iter: 70, train_loss: 0.6627503633499146\n",
    ">     Iter: 80, train_loss: 0.6610774993896484\n",
    ">     Iter: 90, train_loss: 0.6605528593063354\n",
    ">     Iter: 100, train_loss: 0.6603661775588989\n",
    ">     Iter: 110, train_loss: 0.6602908968925476\n",
    ">     Iter: 120, train_loss: 0.6602528095245361\n",
    ">     Iter: 130, train_loss: 0.660234272480011\n",
    ">     Iter: 140, train_loss: 0.6602233648300171\n",
    ">     Iter: 150, train_loss: 0.6602159142494202\n",
    ">     Iter: 160, train_loss: 0.6602100133895874\n",
    ">     Iter: 170, train_loss: 0.6602076292037964\n",
    ">     Iter: 180, train_loss: 0.6602047085762024\n",
    ">     Iter: 190, train_loss: 0.6602054238319397\n",
    ">     Iter: 200, train_loss: 0.6602044701576233\n",
    ">     Iter: 210, train_loss: 0.6602048277854919\n",
    ">     Iter: 220, train_loss: 0.6602048277854919\n",
    ">     Iter: 230, train_loss: 0.6602058410644531\n",
    ">     Iter: 240, train_loss: 0.6602041721343994\n",
    ">     Iter: 250, train_loss: 0.6602049469947815\n",
    ">     Iter: 260, train_loss: 0.660204291343689\n",
    ">     Iter: 270, train_loss: 0.660205602645874\n",
    ">     Iter: 280, train_loss: 0.6602062582969666\n",
    ">     Iter: 290, train_loss: 0.6602067351341248\n",
    ">     Iter: 300, train_loss: 0.6602053046226501\n",
    "\n",
    "  \n",
    "\n",
    "Prediction and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ncworld.prediction()\n",
    "pred_ncworld.plot()\n",
    "pred_ncworld.ax.set_ylabel('new cases smoothed')\n",
    "pred_ncworld.ax.set_title('new cases smoothed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ndworld.prediction()\n",
    "pred_ndworld.plot()\n",
    "pred_ndworld.ax.set_ylabel('new deaths smoothed')\n",
    "pred_ndworld.ax.set_title('new deaths smoothed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### 3.2 World onestep prediction\n",
    "\n",
    "To perform onestep ahead mean prediction, we define some additional\n",
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onestep_prediction(dataset):\n",
    "  onestep = th.cat((dataset.y_train, dataset.y_test), dim=0) # output vector\n",
    "  for idx in range(len(dataset.y_test)):\n",
    "    \n",
    "    # define training and test data. Training data is iteratively, step by step, expanded by the use of test data\n",
    "    x_train = th.cat((dataset.x_train, dataset.x_test[:idx]), dim=0)\n",
    "    x_test = dataset.x_test[idx:]\n",
    "    y_train = th.cat((dataset.y_train, dataset.y_test[:idx]), dim=0)\n",
    "    y_test = dataset.y_test[idx:]\n",
    "    \n",
    "    # create a gaussian process model, train and make predictions\n",
    "    pred_model = GPTrainer(gp_model='GPLinearRBF', x_train=x_train, x_train_min=dataset.x_train_min, x_train_mean=dataset.x_train_mean, x_train_std=dataset.x_train_std, x_test=x_test, y_train=y_train, y_test=y_test, y_train_mean=dataset.y_train_mean, y_train_std=dataset.y_train_std, verbose=False)\n",
    "    pred_model.train()\n",
    "    pred_model.prediction()\n",
    "    \n",
    "    # store one step predictions\n",
    "    onestep[len(dataset.y_train) + idx] = pred_model.preds.mean[len(dataset.x_train)+idx]\n",
    "\n",
    "  # plot results\n",
    "  fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "  ax.plot(pred_model.x_train_min + pred_model.denormalize_x(dataset.x_test), pred_model.denormalize_y(dataset.y_test),'*r', pred_model.x_train_min + pred_model.denormalize_x(dataset.x_test), pred_model.denormalize_y(onestep[len(dataset.y_train):]),'k*')\n",
    "  ax.legend(['test data', 'prediction mean'])\n",
    "  ax.set_xlabel('time [days]')\n",
    "  ax.set_ylabel('prediction mean')\n",
    "  ax.set_title('one step ahead prediction')\n",
    "  ax.grid()   \n",
    "  \n",
    "  # return onestep prediction\n",
    "  return onestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "We iteratively predict the next one step ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onestep_pred_ncworld = onestep_prediction(ds_ncworld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onestep_pred_ndworld = onestep_prediction(ds_ndworld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### 3.3 Sweden multistep prediction\n",
    "\n",
    "Use class to convert dataframes to numpy arrays for further processing.\n",
    "Note, the conversion may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ncswe = GPDataSet(df_ncdenswe_train, df_ncdenswe_test, datacol = 'new_cases_smoothed', filterloc = 'Sweden', add_input=None)\n",
    "ds_ndswe = GPDataSet(df_nddenswe_train, df_nddenswe_test, datacol = 'new_deaths_smoothed', filterloc = 'Sweden', add_input=None)\n",
    "ds_ncswe.convert_to_numpy()\n",
    "ds_ndswe.convert_to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Plot data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ncswe.plot_numpy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ndswe.plot_numpy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Process data by subsampling, reducing data window and normalize data.\n",
    "The gaussian process model is a so called non parametric model and will\n",
    "be mainly based on the data points. As such, to reduce the computation\n",
    "and the complexity of the model, we subsample and reduce the number of\n",
    "datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ncswe.process_numpy_data(nth_subsample = 4, window_red = 0.8)\n",
    "ds_ndswe.process_numpy_data(nth_subsample = 4, window_red = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Plot processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ncswe.plot_reduced_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ndswe.plot_reduced_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Init the training class for the Gaussian Process models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ncswe = GPTrainer(gp_model='GPLinearRBF', x_train=ds_ncswe.x_train, x_train_min=ds_ncswe.x_train_min, x_train_mean=ds_ncswe.x_train_mean, x_train_std=ds_ncswe.x_train_std, x_test=ds_ncswe.x_test, y_train=ds_ncswe.y_train, y_test=ds_ncswe.y_test, y_train_mean=ds_ncswe.y_train_mean, y_train_std=ds_ncswe.y_train_std)\n",
    "\n",
    "pred_ndswe = GPTrainer(gp_model='GPLinearRBF', x_train=ds_ndswe.x_train, x_train_min=ds_ndswe.x_train_min, x_train_mean=ds_ncswe.x_train_mean, x_train_std=ds_ncswe.x_train_std, x_test=ds_ndswe.x_test, y_train=ds_ndswe.y_train, y_test=ds_ndswe.y_test, y_train_mean=ds_ndswe.y_train_mean, y_train_std=ds_ndswe.y_train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\ntraining new cases prediction model')\n",
    "pred_ncswe.train()\n",
    "print('\\ntraining new deaths prediction model')\n",
    "pred_ndswe.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     training new cases prediction model\n",
    ">     Iter: 10, train_loss: 0.8349927663803101\n",
    ">     Iter: 20, train_loss: 0.5855213403701782\n",
    ">     Iter: 30, train_loss: 0.3725060224533081\n",
    ">     Iter: 40, train_loss: 0.2526648938655853\n",
    ">     Iter: 50, train_loss: 0.23845908045768738\n",
    ">     Iter: 60, train_loss: 0.2379620224237442\n",
    ">     Iter: 70, train_loss: 0.23536014556884766\n",
    ">     Iter: 80, train_loss: 0.234043687582016\n",
    ">     Iter: 90, train_loss: 0.23412634432315826\n",
    ">     Iter: 100, train_loss: 0.23409061133861542\n",
    ">     Iter: 110, train_loss: 0.23400363326072693\n",
    ">     Iter: 120, train_loss: 0.2339954972267151\n",
    ">     Iter: 130, train_loss: 0.2339944839477539\n",
    ">     Iter: 140, train_loss: 0.23399288952350616\n",
    ">     Iter: 150, train_loss: 0.2339918166399002\n",
    ">     Iter: 160, train_loss: 0.2339887022972107\n",
    ">     Iter: 170, train_loss: 0.2339930236339569\n",
    ">     Iter: 180, train_loss: 0.2339915633201599\n",
    ">     Iter: 190, train_loss: 0.2339925765991211\n",
    ">     Iter: 200, train_loss: 0.2339947372674942\n",
    ">     Iter: 210, train_loss: 0.23399461805820465\n",
    ">     Iter: 220, train_loss: 0.23399263620376587\n",
    ">     Iter: 230, train_loss: 0.2339930236339569\n",
    ">     Iter: 240, train_loss: 0.2339915633201599\n",
    ">     Iter: 250, train_loss: 0.23399244248867035\n",
    ">     Iter: 260, train_loss: 0.23398932814598083\n",
    ">     Iter: 270, train_loss: 0.23399288952350616\n",
    ">     Iter: 280, train_loss: 0.2339920699596405\n",
    ">     Iter: 290, train_loss: 0.23399271070957184\n",
    ">     Iter: 300, train_loss: 0.23399384319782257\n",
    ">\n",
    ">     training new deaths prediction model\n",
    ">     Iter: 10, train_loss: 1.2736952304840088\n",
    ">     Iter: 20, train_loss: 1.2659318447113037\n",
    ">     Iter: 30, train_loss: 1.2641785144805908\n",
    ">     Iter: 40, train_loss: 1.2638046741485596\n",
    ">     Iter: 50, train_loss: 1.2635420560836792\n",
    ">     Iter: 60, train_loss: 1.2633696794509888\n",
    ">     Iter: 70, train_loss: 1.2632778882980347\n",
    ">     Iter: 80, train_loss: 1.263228178024292\n",
    ">     Iter: 90, train_loss: 1.2631932497024536\n",
    ">     Iter: 100, train_loss: 1.2631678581237793\n",
    ">     Iter: 110, train_loss: 1.2631479501724243\n",
    ">     Iter: 120, train_loss: 1.2631317377090454\n",
    ">     Iter: 130, train_loss: 1.2631182670593262\n",
    ">     Iter: 140, train_loss: 1.2631065845489502\n",
    ">     Iter: 150, train_loss: 1.2630963325500488\n",
    ">     Iter: 160, train_loss: 1.263087511062622\n",
    ">     Iter: 170, train_loss: 1.2630798816680908\n",
    ">     Iter: 180, train_loss: 1.2630729675292969\n",
    ">     Iter: 190, train_loss: 1.2630666494369507\n",
    ">     Iter: 200, train_loss: 1.263061285018921\n",
    ">     Iter: 210, train_loss: 1.2630563974380493\n",
    ">     Iter: 220, train_loss: 1.2630518674850464\n",
    ">     Iter: 230, train_loss: 1.2630479335784912\n",
    ">     Iter: 240, train_loss: 1.263043999671936\n",
    ">     Iter: 250, train_loss: 1.2630406618118286\n",
    ">     Iter: 260, train_loss: 1.2630375623703003\n",
    ">     Iter: 270, train_loss: 1.263034701347351\n",
    ">     Iter: 280, train_loss: 1.263032078742981\n",
    ">     Iter: 290, train_loss: 1.2630293369293213\n",
    ">     Iter: 300, train_loss: 1.2630271911621094\n",
    "\n",
    "  \n",
    "\n",
    "Prediction and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ncswe.prediction()\n",
    "pred_ncswe.plot()\n",
    "pred_ncswe.ax.set_ylabel('new cases smoothed')\n",
    "pred_ncswe.ax.set_title('new cases smoothed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ndswe.prediction()\n",
    "pred_ndswe.plot()\n",
    "pred_ndswe.ax.set_ylabel('new deaths smoothed')\n",
    "pred_ndswe.ax.set_title('new deaths smoothed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### 3.4 Sweden onestep prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onestep_pred_ncswe = onestep_prediction(ds_ncswe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onestep_pred_ndswe = onestep_prediction(ds_ndswe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### 3.5 Sweden multistep prediction with additional data input from neighbouring country\n",
    "\n",
    "Assuming we knew the results from a neighbouring country and if data is\n",
    "correlated, we could presumably improve the prediction\n",
    "\n",
    "Plot resulting data used for prediction. Both plots appears to follow a\n",
    "form of trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ncswex = GPDataSet(df_ncdenswe_train, df_ncdenswe_test, datacol = 'new_cases_smoothed', filterloc = 'Sweden', add_input='Denmark')\n",
    "ds_ndswex = GPDataSet(df_nddenswe_train, df_nddenswe_test, datacol = 'new_deaths_smoothed', filterloc = 'Sweden', add_input='Denmark')\n",
    "ds_ncswex.convert_to_numpy()\n",
    "ds_ndswex.convert_to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Plot data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ncswex.plot_numpy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ndswex.plot_numpy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Process data by subsampling, reducing data window and normalize data.\n",
    "The gaussian process model is a so called non parametric model and will\n",
    "be mainly based on the data points. As such, to reduce the computation\n",
    "and the complexity of the model, we subsample and reduce the number of\n",
    "datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ncswex.process_numpy_data(nth_subsample = 4, window_red = 0.8)\n",
    "ds_ndswex.process_numpy_data(nth_subsample = 4, window_red = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Plot processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ncswex.plot_reduced_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ndswex.plot_reduced_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Init the training class for the Gaussian Process models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ncswex = GPTrainer(gp_model='GPLinearRBF', x_train=ds_ncswex.x_train, x_train_min=ds_ncswex.x_train_min, x_train_mean=ds_ncswex.x_train_mean, x_train_std=ds_ncswex.x_train_std, x_test=ds_ncswex.x_test, y_train=ds_ncswex.y_train, y_test=ds_ncswex.y_test, y_train_mean=ds_ncswex.y_train_mean, y_train_std=ds_ncswex.y_train_std)\n",
    "\n",
    "pred_ndswex = GPTrainer(gp_model='GPLinearRBF', x_train=ds_ndswex.x_train, x_train_min=ds_ndswex.x_train_min, x_train_mean=ds_ndswex.x_train_mean, x_train_std=ds_ndswex.x_train_std, x_test=ds_ndswex.x_test, y_train=ds_ndswex.y_train, y_test=ds_ndswex.y_test, y_train_mean=ds_ndswex.y_train_mean, y_train_std=ds_ndswex.y_train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\ntraining new cases prediction model')\n",
    "pred_ncswex.train()\n",
    "print('\\ntraining new deaths prediction model')\n",
    "pred_ndswex.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     training new cases prediction model\n",
    ">     Iter: 10, train_loss: 0.8944303393363953\n",
    ">     Iter: 20, train_loss: 0.7347151637077332\n",
    ">     Iter: 30, train_loss: 0.6328760981559753\n",
    ">     Iter: 40, train_loss: 0.4474466145038605\n",
    ">     Iter: 50, train_loss: 0.15617185831069946\n",
    ">     Iter: 60, train_loss: -0.015161959454417229\n",
    ">     Iter: 70, train_loss: -0.028500746935606003\n",
    ">     Iter: 80, train_loss: -0.049165789037942886\n",
    ">     Iter: 90, train_loss: -0.05050474777817726\n",
    ">     Iter: 100, train_loss: -0.05421492084860802\n",
    ">     Iter: 110, train_loss: -0.05606149137020111\n",
    ">     Iter: 120, train_loss: -0.0588647834956646\n",
    ">     Iter: 130, train_loss: -0.06143894046545029\n",
    ">     Iter: 140, train_loss: -0.06341520696878433\n",
    ">     Iter: 150, train_loss: -0.06587778776884079\n",
    ">     Iter: 160, train_loss: -0.06713174283504486\n",
    ">     Iter: 170, train_loss: -0.06902586668729782\n",
    ">     Iter: 180, train_loss: -0.07032934576272964\n",
    ">     Iter: 190, train_loss: -0.0719415694475174\n",
    ">     Iter: 200, train_loss: -0.07307198643684387\n",
    ">     Iter: 210, train_loss: -0.07436548918485641\n",
    ">     Iter: 220, train_loss: -0.07486489415168762\n",
    ">     Iter: 230, train_loss: -0.07584095001220703\n",
    ">     Iter: 240, train_loss: -0.07697887718677521\n",
    ">     Iter: 250, train_loss: -0.07766450196504593\n",
    ">     Iter: 260, train_loss: -0.07864208519458771\n",
    ">     Iter: 270, train_loss: -0.07991237938404083\n",
    ">     Iter: 280, train_loss: -0.08046308904886246\n",
    ">     Iter: 290, train_loss: -0.08109690248966217\n",
    ">     Iter: 300, train_loss: -0.081480473279953\n",
    ">\n",
    ">     training new deaths prediction model\n",
    ">     Iter: 10, train_loss: 1.2888128757476807\n",
    ">     Iter: 20, train_loss: 1.275627851486206\n",
    ">     Iter: 30, train_loss: 1.2696713209152222\n",
    ">     Iter: 40, train_loss: 1.2672677040100098\n",
    ">     Iter: 50, train_loss: 1.2659194469451904\n",
    ">     Iter: 60, train_loss: 1.2652630805969238\n",
    ">     Iter: 70, train_loss: 1.2648776769638062\n",
    ">     Iter: 80, train_loss: 1.2646286487579346\n",
    ">     Iter: 90, train_loss: 1.2644517421722412\n",
    ">     Iter: 100, train_loss: 1.2643193006515503\n",
    ">     Iter: 110, train_loss: 1.2642160654067993\n",
    ">     Iter: 120, train_loss: 1.2641332149505615\n",
    ">     Iter: 130, train_loss: 1.2640646696090698\n",
    ">     Iter: 140, train_loss: 1.2640074491500854\n",
    ">     Iter: 150, train_loss: 1.2639591693878174\n",
    ">     Iter: 160, train_loss: 1.2639178037643433\n",
    ">     Iter: 170, train_loss: 1.2638816833496094\n",
    ">     Iter: 180, train_loss: 1.263850450515747\n",
    ">     Iter: 190, train_loss: 1.2638229131698608\n",
    ">     Iter: 200, train_loss: 1.2637988328933716\n",
    ">     Iter: 210, train_loss: 1.2637768983840942\n",
    ">     Iter: 220, train_loss: 1.2637579441070557\n",
    ">     Iter: 230, train_loss: 1.2637404203414917\n",
    ">     Iter: 240, train_loss: 1.2637248039245605\n",
    ">     Iter: 250, train_loss: 1.263710379600525\n",
    ">     Iter: 260, train_loss: 1.2636975049972534\n",
    ">     Iter: 270, train_loss: 1.2636858224868774\n",
    ">     Iter: 280, train_loss: 1.263675332069397\n",
    ">     Iter: 290, train_loss: 1.2636650800704956\n",
    ">     Iter: 300, train_loss: 1.2636560201644897"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ncswex.prediction()\n",
    "pred_ncswex.plot()\n",
    "pred_ncswex.ax.set_ylabel('new cases smoothed')\n",
    "pred_ncswex.ax.set_title('new cases smoothed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ndswex.prediction()\n",
    "pred_ndswex.plot()\n",
    "pred_ndswex.ax.set_ylabel('new deaths smoothed')\n",
    "pred_ndswex.ax.set_title('new deaths smoothed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "4. Evaluation\n",
    "-------------\n",
    "\n",
    "### 4.1 World multistep\n",
    "\n",
    "Evaluation of new cases smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ncworld.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 554788.000\n",
    ">     Test MSE: 1850106112.000\n",
    ">     Test RMSE: 43012.860\n",
    ">     RMSE percentage error: 7.753 %\n",
    ">     Test ABS: 33457.289\n",
    ">     ABS percentage error: 6.031 %\n",
    "\n",
    "  \n",
    "\n",
    "Evaluation of new deaths smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ndworld.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 8842.582\n",
    ">     Test MSE: 8629836.000\n",
    ">     Test RMSE: 2937.658\n",
    ">     RMSE percentage error: 33.222 %\n",
    ">     Test ABS: 2727.578\n",
    ">     ABS percentage error: 30.846 %\n",
    "\n",
    "  \n",
    "\n",
    "### 4.2 World onestep\n",
    "\n",
    "To evaluate the onestep ahead prediction, we define an additional\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_data, prediction):\n",
    "  with th.no_grad():    \n",
    "    # evaluate\n",
    "    error_mse = mean_squared_error(test_data, prediction)\n",
    "    error_rmse = math.sqrt(error_mse)\n",
    "    error_abs = mean_absolute_error(test_data, prediction)\n",
    "    avg_gt = test_data.sum() / len(test_data)\n",
    "    mse_percentage = error_rmse / avg_gt * 100\n",
    "    abs_percentage = error_abs / avg_gt * 100\n",
    "\n",
    "    # print\n",
    "    print('Average of groundtruth: %.3f' % avg_gt)\n",
    "    print('Test MSE: %.3f' % error_mse)\n",
    "    print('Test RMSE: %.3f' % error_rmse)\n",
    "    print('RMSE percentage error: %.3f' % mse_percentage, '%')\n",
    "    print('Test ABS: %.3f' % error_abs)\n",
    "    print('ABS percentage error: %.3f' % abs_percentage, '%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Evaluation of new cases smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to evaluate\n",
    "test = pred_ncworld.denormalize_y(ds_ncworld.y_test) \n",
    "preds = pred_ncworld.denormalize_y(onestep_pred_ncworld[-len(ds_ncworld.y_test):])\n",
    "evaluate(test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 554788.000\n",
    ">     Test MSE: 38076252.000\n",
    ">     Test RMSE: 6170.596\n",
    ">     RMSE percentage error: 1.112 %\n",
    ">     Test ABS: 4394.894\n",
    ">     ABS percentage error: 0.792 %\n",
    "\n",
    "  \n",
    "\n",
    "Evaluation of new deaths smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to evaluate\n",
    "test = pred_ndworld.denormalize_y(ds_ndworld.y_test) \n",
    "preds = pred_ndworld.denormalize_y(onestep_pred_ndworld[-len(ds_ndworld.y_test):])\n",
    "evaluate(test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 8842.582\n",
    ">     Test MSE: 29195.436\n",
    ">     Test RMSE: 170.867\n",
    ">     RMSE percentage error: 1.932 %\n",
    ">     Test ABS: 137.978\n",
    ">     ABS percentage error: 1.560 %\n",
    "\n",
    "  \n",
    "\n",
    "### 4.2 Sweden multistep\n",
    "\n",
    "Evaluation of new cases smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ncswe.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 4238.664\n",
    ">     Test MSE: 4737463.500\n",
    ">     Test RMSE: 2176.572\n",
    ">     RMSE percentage error: 51.350 %\n",
    ">     Test ABS: 2068.833\n",
    ">     ABS percentage error: 48.809 %\n",
    "\n",
    "  \n",
    "\n",
    "Evaluation of new deaths smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ndswe.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 26.254\n",
    ">     Test MSE: 755.362\n",
    ">     Test RMSE: 27.484\n",
    ">     RMSE percentage error: 104.686 %\n",
    ">     Test ABS: 22.457\n",
    ">     ABS percentage error: 85.540 %\n",
    "\n",
    "  \n",
    "\n",
    "### 4.4 Sweden onestep\n",
    "\n",
    "Evaluation of new cases smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to evaluate\n",
    "test = pred_ncswe.denormalize_y(ds_ncswe.y_test) \n",
    "preds = pred_ncswe.denormalize_y(onestep_pred_ncswe[-len(ds_ncswe.y_test):])\n",
    "evaluate(test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 4238.664\n",
    ">     Test MSE: 77977.289\n",
    ">     Test RMSE: 279.244\n",
    ">     RMSE percentage error: 6.588 %\n",
    ">     Test ABS: 235.829\n",
    ">     ABS percentage error: 5.564 %\n",
    "\n",
    "  \n",
    "\n",
    "Evaluation of new deaths smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to evaluate\n",
    "test = pred_ndswe.denormalize_y(ds_ndswe.y_test) \n",
    "preds = pred_ndswe.denormalize_y(onestep_pred_ndswe[-len(ds_ndswe.y_test):])\n",
    "evaluate(test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 26.254\n",
    ">     Test MSE: 33.230\n",
    ">     Test RMSE: 5.765\n",
    ">     RMSE percentage error: 21.957 %\n",
    ">     Test ABS: 3.928\n",
    ">     ABS percentage error: 14.963 %\n",
    "\n",
    "  \n",
    "\n",
    "### 4.4 Sweden multistep with additional information\n",
    "\n",
    "Evaluation of new cases smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ncswex.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 4238.664\n",
    ">     Test MSE: 2658296.750\n",
    ">     Test RMSE: 1630.428\n",
    ">     RMSE percentage error: 38.466 %\n",
    ">     Test ABS: 1537.534\n",
    ">     ABS percentage error: 36.274 %\n",
    "\n",
    "  \n",
    "\n",
    "Evaluation of new deaths smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ndswex.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average of groundtruth: 26.254\n",
    ">     Test MSE: 756.587\n",
    ">     Test RMSE: 27.506\n",
    ">     RMSE percentage error: 104.771 %\n",
    ">     Test ABS: 22.476\n",
    ">     ABS percentage error: 85.612 %\n",
    "\n",
    "  \n",
    "\n",
    "5. Conclusions and reflections\n",
    "------------------------------\n",
    "\n",
    "Predictions using gaussian processes were made for both new cases\n",
    "smoothed and new deaths smoothed. This included an aggregation of many\n",
    "countries within the world as well as for Sweden. Making single step\n",
    "ahead predictions resulted naturally in smaller errors compared to the\n",
    "multistep predictions. The multistep prediction for Sweden could be\n",
    "improved for new cases smoothed using correlated data from a\n",
    "neighbouring country.\n",
    "\n",
    "We believe the Gaussian process model is a valuable tool for making\n",
    "predictions. With this work, we would like to highlight that the data\n",
    "points and kernel chosen for the Gaussian process heavily biases the\n",
    "model and strongly influences the predictions. In this project, we\n",
    "selected a combination of a linear kernel and a radial basis function.\n",
    "The reason being that there is a trend in the data and that nearby data\n",
    "points should be more similar than data points further away. By\n",
    "inspecting the data carefully, a more optimal kernel could likely be\n",
    "selected. Also, the confidence intervall provided with the gaussian\n",
    "process model is based on that the kernel is correctly representing the\n",
    "underlying distribution of data.\n",
    "\n",
    "In terms of scalability, the predictions are somewhat scalable as a user\n",
    "can define a window of data for making the predictions. Furthermore, GPU\n",
    "support could be included and approximations to the gaussian process\n",
    "model could be made.\n",
    "\n",
    "Compared to the ARIMA model, the gaussian process model performed in\n",
    "most cases slightly worse. However, this may be due to the selection of\n",
    "data points and kernel considering that the gaussian process model is\n",
    "heavily biased by these choices. One reflection is that if one\n",
    "approximately knows the distribution of the underlying data, a gaussian\n",
    "process model with a proper selected kernel may be a good choice."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
