{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used for running jobs, do not edit without stopping the scheduled jobs!\n",
    "========================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// parameter for number of minutes of streaming (can be used in Jobs feature)\n",
    "dbutils.widgets.text(\"nbr_minutes\", \"3\", label = \"Minutes of streaming (int)\")\n",
    "val nbr_minutes = dbutils.widgets.get(\"nbr_minutes\").toInt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     nbr_minutes: Int = 2\n",
    "\n",
    "  \n",
    "\n",
    "If the cluster was shut down, then start a new cluster and install the\n",
    "following libraries on it (via maven).\n",
    "\n",
    "-   gson with maven coordinates `com.google.code.gson:gson:2.8.6`\n",
    "-   twitter4j-examples with maven coordinates\n",
    "    `org.twitter4j:twitter4j-examples:4.0.7`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"./025_a_extendedTwitterUtils2run\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import twitter4j._\n",
    ">     import twitter4j.auth.Authorization\n",
    ">     import twitter4j.conf.ConfigurationBuilder\n",
    ">     import twitter4j.auth.OAuthAuthorization\n",
    ">     import org.apache.spark.streaming._\n",
    ">     import org.apache.spark.streaming.dstream._\n",
    ">     import org.apache.spark.storage.StorageLevel\n",
    ">     import org.apache.spark.streaming.receiver.Receiver\n",
    "\n",
    "  \n",
    "\n",
    ">     defined class ExtendedTwitterReceiver\n",
    "\n",
    "  \n",
    "\n",
    ">     defined class ExtendedTwitterInputDStream\n",
    "\n",
    "  \n",
    "\n",
    ">     import twitter4j.Status\n",
    ">     import twitter4j.auth.Authorization\n",
    ">     import org.apache.spark.storage.StorageLevel\n",
    ">     import org.apache.spark.streaming.StreamingContext\n",
    ">     import org.apache.spark.streaming.dstream.{ReceiverInputDStream, DStream}\n",
    ">     defined object ExtendedTwitterUtils\n",
    "\n",
    "  \n",
    "\n",
    ">     done running the extendedTwitterUtils2run notebook - ready to stream from twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"./025_b_TTTDFfunctions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     USAGE: val df = tweetsDF2TTTDF(tweetsJsonStringDF2TweetsDF(fromParquetFile2DF(\"parquetFileName\")))\n",
    ">                       val df = tweetsDF2TTTDF(tweetsIDLong_JsonStringPairDF2TweetsDF(fromParquetFile2DF(\"parquetFileName\")))\n",
    ">                       \n",
    ">     import org.apache.spark.sql.types.{StructType, StructField, StringType}\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     import org.apache.spark.sql.types._\n",
    ">     import org.apache.spark.sql.ColumnName\n",
    ">     import org.apache.spark.sql.DataFrame\n",
    ">     fromParquetFile2DF: (InputDFAsParquetFilePatternString: String)org.apache.spark.sql.DataFrame\n",
    ">     tweetsJsonStringDF2TweetsDF: (tweetsAsJsonStringInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
    ">     tweetsIDLong_JsonStringPairDF2TweetsDF: (tweetsAsIDLong_JsonStringInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
    ">     tweetsDF2TTTDF: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
    ">     tweetsDF2TTTDFWithURLsAndHashtags: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
    "\n",
    "  \n",
    "\n",
    ">     tweetsDF2TTTDFLightWeight: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
    "\n",
    "  \n",
    "\n",
    "### Loading twitter credentials\n",
    "\n",
    "At a later stage it would be nice to be able to load credentials based\n",
    "on the current user. We run the notebook containing keys using %run. As\n",
    "far as I understand this takes place in shell. Unfortunately the shell\n",
    "considers my username to be root.\n",
    "\n",
    "**%sh echo $USER**\n",
    "\n",
    "Meanwhile in scala we get the username in the form of an email address.\n",
    "\n",
    "**var usr = dbutils.notebook.getContext.tags(\"user\")**\n",
    "\n",
    "For now we can just specify the filename manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// needs upgraded databricks subscription, works on project shard\n",
    "// !!NOTE!! user directories seem to be private on the project shard currently but perhaps doublecheck this\n",
    "var usr = dbutils.notebook.getContext.tags(\"user\")\n",
    "var keys_notebook_location = \"/Users/\" + usr + \"/KeysAndTokens\"\n",
    "dbutils.notebook.run(keys_notebook_location, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Warning: No value returned from the notebook run. To return a value from a notebook, use dbutils.notebook.exit(value)\n",
    ">     usr: String = bokman@chalmers.se\n",
    ">     keys_notebook_location: String = /Users/bokman@chalmers.se/KeysAndTokens\n",
    ">     res18: String = null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.google.gson.Gson \n",
    "import org.apache.spark.sql.functions._\n",
    "//import org.apache.spark.sql.types._\n",
    "\n",
    "val outputDirectoryRoot = \"/datasets/ScaDaMaLe/twitter/student-project-10_group-Geosmus\" // output directory\n",
    "\n",
    "\n",
    "val batchInterval = 1 // in minutes\n",
    "val timeoutJobLength =  batchInterval * 5\n",
    "\n",
    "var newContextCreated = false\n",
    "var numTweetsCollected = 0L // track number of tweets collected\n",
    "\n",
    "// This is the function that creates the SteamingContext and sets up the Spark Streaming job.\n",
    "def streamFuncWithProcessing(): StreamingContext = {\n",
    "  // Create a Spark Streaming Context.\n",
    "  val ssc = new StreamingContext(sc, Minutes(batchInterval))\n",
    "  // Create the OAuth Twitter credentials \n",
    "  val auth = Some(new OAuthAuthorization(new ConfigurationBuilder().build()))\n",
    "  \n",
    "  // Create filter\n",
    "  val locationsQuery = new FilterQuery().locations(Array(-180.0, -90.0), Array(180.0, 90.0)) // all locations\n",
    "  \n",
    "  // Create a Twitter Stream for the input source.  \n",
    "  val twitterStream = ExtendedTwitterUtils.createStream(ssc, auth, Some(locationsQuery))\n",
    "  // Transform the discrete RDDs into JSON\n",
    "  val twitterStreamJson = twitterStream.map(x => { val gson = new Gson();\n",
    "                                                 val xJson = gson.toJson(x)\n",
    "                                                 xJson\n",
    "                                               }) \n",
    "  // take care\n",
    "  val partitionsEachInterval = 1 // This tells the number of partitions in each RDD of tweets in the DStream.\n",
    "  \n",
    "  // get some time fields from current `.Date()`, use the same for each batch in the job\n",
    "  val year = (new java.text.SimpleDateFormat(\"yyyy\")).format(new java.util.Date())\n",
    "  val month = (new java.text.SimpleDateFormat(\"MM\")).format(new java.util.Date())\n",
    "  val day = (new java.text.SimpleDateFormat(\"dd\")).format(new java.util.Date())\n",
    "  val hour = (new java.text.SimpleDateFormat(\"HH\")).format(new java.util.Date())\n",
    "  \n",
    "  // what we want done with each discrete RDD tuple: (rdd, time)\n",
    "  twitterStreamJson.foreachRDD((rdd, time) => { // for each filtered RDD in the DStream\n",
    "      val count = rdd.count() //We count because the following operations can only be applied to non-empty RDD's\n",
    "      if (count > 0) {\n",
    "        val outputRDD = rdd.repartition(partitionsEachInterval) // repartition as desired\n",
    "        // to write to parquet directly in append mode in one directory per 'time'------------       \n",
    "        val outputDF = outputRDD.toDF(\"tweetAsJsonString\")\n",
    "        val processedDF = tweetsDF2TTTDF(tweetsJsonStringDF2TweetsDF(outputDF)).filter($\"countryCode\" =!= lit(\"\"))\n",
    "\n",
    "        \n",
    "        // Writing the full processed df (We probably don't need it, but useful for exploring the data initially)\n",
    "        processedDF.write.mode(SaveMode.Append)\n",
    "                .parquet(outputDirectoryRoot + \"/\" + year + \"/\" + month + \"/\" + day + \"/\" + hour + \"/\" + time.milliseconds) \n",
    "        \n",
    "        // end of writing as parquet file-------------------------------------\n",
    "        numTweetsCollected += count // update with the latest count\n",
    "      }\n",
    "  })\n",
    "  newContextCreated = true\n",
    "  ssc\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import com.google.gson.Gson\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     outputDirectoryRoot: String = /datasets/ScaDaMaLe/twitter/student-project-10_group-Geosmus\n",
    ">     batchInterval: Int = 1\n",
    ">     timeoutJobLength: Int = 5\n",
    ">     newContextCreated: Boolean = false\n",
    ">     numTweetsCollected: Long = 0\n",
    ">     streamFuncWithProcessing: ()org.apache.spark.streaming.StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Now just use the function to create a Spark Streaming Context\n",
    "val ssc = StreamingContext.getActiveOrCreate(streamFuncWithProcessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@212f8bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// you only need one of these to start\n",
    "ssc.start()\n",
    "// ssc.awaitTerminationOrTimeout(30000) //time in milliseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Thread.sleep(nbr_minutes*60*1000) //time in milliseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTweetsCollected // number of tweets collected so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
