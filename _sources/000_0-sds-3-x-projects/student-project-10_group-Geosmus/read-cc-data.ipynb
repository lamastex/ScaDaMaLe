{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the cluster was shut down, then start a new cluster and install the\n",
    "following libraries on it (via maven).\n",
    "\n",
    "-   gson with maven coordinates `com.google.code.gson:gson:2.8.6`\n",
    "-   twitter4j-examples with maven coordinates\n",
    "    `org.twitter4j:twitter4j-examples:4.0.7`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"./025_a_extendedTwitterUtils2run\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import twitter4j._\n",
    ">     import twitter4j.auth.Authorization\n",
    ">     import twitter4j.conf.ConfigurationBuilder\n",
    ">     import twitter4j.auth.OAuthAuthorization\n",
    ">     import org.apache.spark.streaming._\n",
    ">     import org.apache.spark.streaming.dstream._\n",
    ">     import org.apache.spark.storage.StorageLevel\n",
    ">     import org.apache.spark.streaming.receiver.Receiver\n",
    "\n",
    "  \n",
    "\n",
    ">     defined class ExtendedTwitterReceiver\n",
    "\n",
    "  \n",
    "\n",
    ">     defined class ExtendedTwitterInputDStream\n",
    "\n",
    "  \n",
    "\n",
    ">     import twitter4j.Status\n",
    ">     import twitter4j.auth.Authorization\n",
    ">     import org.apache.spark.storage.StorageLevel\n",
    ">     import org.apache.spark.streaming.StreamingContext\n",
    ">     import org.apache.spark.streaming.dstream.{ReceiverInputDStream, DStream}\n",
    ">     defined object ExtendedTwitterUtils\n",
    "\n",
    "  \n",
    "\n",
    ">     done running the extendedTwitterUtils2run notebook - ready to stream from twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"./025_b_TTTDFfunctions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     USAGE: val df = tweetsDF2TTTDF(tweetsJsonStringDF2TweetsDF(fromParquetFile2DF(\"parquetFileName\")))\n",
    ">                       val df = tweetsDF2TTTDF(tweetsIDLong_JsonStringPairDF2TweetsDF(fromParquetFile2DF(\"parquetFileName\")))\n",
    ">                       \n",
    ">     import org.apache.spark.sql.types.{StructType, StructField, StringType}\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     import org.apache.spark.sql.types._\n",
    ">     import org.apache.spark.sql.ColumnName\n",
    ">     import org.apache.spark.sql.DataFrame\n",
    ">     fromParquetFile2DF: (InputDFAsParquetFilePatternString: String)org.apache.spark.sql.DataFrame\n",
    ">     tweetsJsonStringDF2TweetsDF: (tweetsAsJsonStringInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
    ">     tweetsIDLong_JsonStringPairDF2TweetsDF: (tweetsAsIDLong_JsonStringInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
    ">     tweetsDF2TTTDF: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
    ">     tweetsDF2TTTDFWithURLsAndHashtags: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
    "\n",
    "  \n",
    "\n",
    ">     tweetsDF2TTTDFLightWeight: (tweetsInputDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val outputDirectoryRoot = \"/datasets/ScaDaMaLe/twitter/student-project-10_group-Geosmus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     outputDirectoryRoot: String = /datasets/ScaDaMaLe/twitter/student-project-10_group-Geosmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /datasets/ScaDaMaLe/twitter/student-project-10_group-Geosmus/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(outputDirectoryRoot)) // outputDirectoryRoot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs/datasets\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val fullDF = fromParquetFile2DF(outputDirectoryRoot + \"/202*/*/*/*/*/*\")\n",
    "val ccDF = fullDF.groupBy($\"countryCode\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     fullDF: org.apache.spark.sql.DataFrame = [CurrentTweetDate: timestamp, CurrentTwID: bigint ... 7 more fields]\n",
    ">     ccDF: org.apache.spark.sql.DataFrame = [countryCode: string, count: bigint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res8: Long = 938714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ccDF.orderBy($\"count\".desc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDF.columns.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     CurrentTweetDate\n",
    ">     CurrentTwID\n",
    ">     lang\n",
    ">     countryCode\n",
    ">     favouritesCount\n",
    ">     followersCount\n",
    ">     friendsCount\n",
    ">     isGeoEnabled\n",
    ">     CurrentTweet"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
