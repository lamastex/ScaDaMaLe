{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the GQA Scene Graph Dataset Structure and Properties\n",
    "==============================================================\n",
    "\n",
    "-   This project aims to explore the scene graphs in the Genome Question\n",
    "    Answering (GQA) dataset \\[1\\].\n",
    "\n",
    "-   The structure, properties, and motifs of the ground truth data will\n",
    "    be analysed.\n",
    "\n",
    "**Adam Dahlgren, Pavlo Melnyk, Emanuel Sanchez Aimar**\n",
    "\n",
    "Graph structure\n",
    "---------------\n",
    "\n",
    "-   We want to extract the names of objects we see in the images and use\n",
    "    their id's as vertices.\n",
    "-   For one object category, we will have multiple id's and hence\n",
    "    multiple vertices. In contrast, one vertex will represent an object\n",
    "    category in the merged graph.\n",
    "-   Object attributes are used as part of the vertices (in some graph\n",
    "    representations we exploit).\n",
    "-   The edge properties are the names of the relations (provided in\n",
    "    JSON-files).\n",
    "\n",
    "Loading data\n",
    "------------\n",
    "\n",
    "-   We read the scene graph data as JSON files. Below is the example\n",
    "    JSON object given by the GQA website, for scene graph 2407890."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "# Had to change weather 'none' to '\"none\"' for the string to parse\n",
    "json_example_str = '{\"2407890\": {\"width\": 640,\"height\": 480,\"location\": \"living room\",\"weather\": \"none\",\"objects\": {\"271881\": {\"name\": \"chair\",\"x\": 220,\"y\": 310,\"w\": 50,\"h\": 80,\"attributes\": [\"brown\", \"wooden\", \"small\"],\"relations\": {\"32452\": {\"name\": \"on\",\"object\": \"275312\"},\"32452\": {\"name\": \"near\",\"object\": \"279472\"}}}}}}'\n",
    "json_rdd = sc.parallelize([json_example_str])\n",
    "example_json_df = spark.read.json(json_rdd, multiLine=True)\n",
    "example_json_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +--------------------+\n",
    ">     |             2407890|\n",
    ">     +--------------------+\n",
    ">     |[480, living room...|\n",
    ">     +--------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_json_df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[3]: Row(2407890=Row(height=480, location='living room', objects=Row(271881=Row(attributes=['brown', 'wooden', 'small'], h=80, name='chair', relations=Row(32452=None, 32452=Row(name='near', object='279472')), w=50, x=220, y=310)), weather='none', width=640))\n",
    "\n",
    "  \n",
    "\n",
    "### Reading JSON files\n",
    "\n",
    "-   Due to issues with the JSON files and how Spark reads them, we need\n",
    "    to parse the files using pure Python. Otherwise, we get stuck in a\n",
    "    loop and finally crash the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train and validation graph data:\n",
    "f_train = open(\"/dbfs/FileStore/shared_uploads/scenegraph_motifs/train_sceneGraphs.json\")\n",
    "train_scene_data = json.load(f_train)\n",
    "\n",
    "f_val = open(\"/dbfs/FileStore/shared_uploads/scenegraph_motifs/val_sceneGraphs.json\")\n",
    "val_scene_data = json.load(f_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "### Parsing graph structure\n",
    "\n",
    "-   We use a Pythonic way to parse the JSON-files and obtain the\n",
    "    vertices and edges of the graphs, provided vertex and edge schemas,\n",
    "    respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pythonic way of doing it, parsing a JSON graph representation.\n",
    "# Creates vertices with the graph id, object name and id, optionally includes the attibutes\n",
    "def json_to_vertices_edges(graph_json, scene_graph_id, include_object_attributes=False):\n",
    "  vertices = []\n",
    "  edges = []\n",
    "  obj_id_to_name = {}\n",
    "  \n",
    "  vertex_ids = graph_json['objects']\n",
    "  \n",
    "  for vertex_id in vertex_ids:   \n",
    "    vertex_obj = graph_json['objects'][vertex_id]\n",
    "    name = vertex_obj['name']\n",
    "    vertices_data = [scene_graph_id, vertex_id, name]\n",
    "    \n",
    "    if vertex_id not in obj_id_to_name:\n",
    "      obj_id_to_name[vertex_id] = name\n",
    "      \n",
    "    if include_object_attributes:\n",
    "      attributes = vertex_obj['attributes']  \n",
    "      vertices_data.append(attributes)\n",
    "      \n",
    "    vertices.append(tuple(vertices_data))\n",
    "    \n",
    "    for relation in vertex_obj['relations']:\n",
    "        src = vertex_id\n",
    "        dst = relation['object']\n",
    "        name = relation['name']\n",
    "        edges.append([src, dst, name])\n",
    "        \n",
    "  for i in range(len(edges)):\n",
    "    src_type = obj_id_to_name[edges[i][0]]\n",
    "    dst_type = obj_id_to_name[edges[i][1]]\n",
    "    edges[i].append(src_type)\n",
    "    edges[i].append(dst_type)\n",
    "    \n",
    "  return (vertices, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_scene_graphs(scene_graphs_json, vertex_schema, edge_schema):  \n",
    "  vertices = []\n",
    "  edges = []\n",
    "  \n",
    "  # if vertice_schema has a field for attributes:\n",
    "  include_object_attributes = len(vertex_schema) == 4\n",
    "     \n",
    "  for scene_graph_id in scene_graphs_json:\n",
    "    vs, es = json_to_vertices_edges(scene_graphs_json[scene_graph_id], scene_graph_id, include_object_attributes)\n",
    "    vertices += vs\n",
    "    edges += es\n",
    "    \n",
    "  vertices = spark.createDataFrame(vertices, vertex_schema)\n",
    "  edges = spark.createDataFrame(edges, edge_schema)\n",
    "  \n",
    "  return GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, ArrayType, IntegerType, StringType\n",
    "\n",
    "# create schemas for scene graphs:\n",
    "vertex_schema = StructType([\n",
    "  StructField(\"graph_id\", StringType(), False), StructField(\"id\", StringType(), False), StructField(\"object_name\", StringType(), False)\n",
    "])\n",
    "\n",
    "vertex_schema_with_attr  = StructType([\n",
    "  StructField(\"graph_id\", StringType(), False), \n",
    "  StructField(\"id\", StringType(), False), \n",
    "  StructField(\"object_name\", StringType(), False), \n",
    "  StructField(\"attributes\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "edge_schema = StructType([\n",
    "  StructField(\"src\", StringType(), False), StructField(\"dst\", StringType(), False), StructField(\"relation_name\", StringType(), False),\n",
    "  StructField(\"src_type\", StringType(), False), StructField(\"dst_type\", StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the length of the vertice schemas to parse the graph from the json files appropriately:\n",
    "len(vertex_schema), len(vertex_schema_with_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[8]: (3, 4)\n",
    "\n",
    "  \n",
    "\n",
    "We add attributes to vertices and types to edges in the graph structure\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "-   If vertices have attributes, we can get more descriptive answers to\n",
    "    our queries like \"Objects of type 'person' are 15 times 'next-to'\n",
    "    objects of type 'banana' ('yellow', 'small'); 10 times 'next-to'\n",
    "    objects of type 'banana' ('green' 'banana')\".\n",
    "\n",
    "-   We can do more interesting queries if the edges disclose what\n",
    "    type/name the source and destination has.\n",
    "\n",
    "-   For instance, it is then possible to group the edges not only by the\n",
    "    ID but also by which type of objects they are connected to,\n",
    "    answering questions like \"How often are objects of type 'person' in\n",
    "    the relation 'next-to' with objects of type 'banana'?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Perhaps merge train+val and produce results for all three (train, val, train+val)?\n",
    "scene_graphs_train = parse_scene_graphs(train_scene_data, vertex_schema_with_attr, edge_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_graphs_train_without_attributes = GraphFrame(scene_graphs_train.vertices.select('graph_id', 'id', 'object_name'), scene_graphs_train.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_graphs_val = parse_scene_graphs(val_scene_data, vertex_schema_with_attr, edge_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# person next-to banana (yellow, small) vs person next-to banana (green)\n",
    "display(scene_graphs_train.find('(a)-[ab]->(b)').filter(\"(a.object_name = 'person') and (b.object_name = 'banana')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scene_graphs_train.vertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scene_graphs_val.vertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scene_graphs_train.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scene_graphs_val.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "Analysis of original graph\n",
    "--------------------------\n",
    "\n",
    "-   The original graph consists of multiple graphs, each representing an\n",
    "    image.\n",
    "\n",
    "-   Number of objects per image (graph):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_graphs = scene_graphs_train.vertices.groupBy('graph_id')\n",
    "display(grouped_graphs.count().sort('count', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graphs/Scenes/Images): {}\".format(scene_graphs_train.vertices.select('graph_id').distinct().count()))\n",
    "print(\"Objects: {}\".format(scene_graphs_train.vertices.count()))\n",
    "print(\"Relations: {}\".format(scene_graphs_train.edges.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Graphs/Scenes/Images): 74289\n",
    ">     Objects: 1231134\n",
    ">     Relations: 3795907"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scene_graphs_train.degrees.sort([\"degree\"],ascending=[0]).limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "  \n",
    "\n",
    "### Finding most common attributes\n",
    "\n",
    "-   \"Which object characteristics are the most common?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "# the attributes are sequences: we need to split them;\n",
    "# explode the attributes in the vertices graph:\n",
    "explodedAttributes = scene_graphs_train.vertices.select(\"id\", \"object_name\", explode(scene_graphs_train.vertices.attributes).alias(\"attribute\"))\n",
    "explodedAttributes.printSchema()\n",
    "display(explodedAttributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "-   Above we see the object-attribute pairs seen in the dataset.\n",
    "\n",
    "#### Most used attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topAttributes = explodedAttributes.groupBy(\"attribute\")\n",
    "display(topAttributes.count().sort(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topAttributes = explodedAttributes.groupBy(\"attribute\")\n",
    "display(topAttributes.count().sort(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "-   7 out of the top 10 attributes are colors, where `white` is seen\n",
    "    92659 times, and `black` 59617 times.\n",
    "\n",
    "-   We see a *long tail-end* distribution with only 68 out of a 617\n",
    "    attributes being seen more than a 1000 times in the dataset, and\n",
    "    around 300 attributes are seen less than 100 times (e.g.,\n",
    "    `breakable` is seen 15 times, `wrist` 14 times, and `immature` 3\n",
    "    times).\n",
    "\n",
    "### Finding most common objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topObjects = scene_graphs_train.vertices.groupBy(\"object_name\")\n",
    "topObjects = topObjects.count()\n",
    "display(topObjects.sort(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(topObjects.sort(\"count\", ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "-   Again, we see that a few object types account for most of the\n",
    "    occurences. Interestingly, `man` (31370) and `person` (20218) is\n",
    "    seen three and two times more than `woman` (11355), respectively.\n",
    "    Apparently, `window`s are really important in this dataset, comming\n",
    "    out on top with 35907 occurences.\n",
    "\n",
    "-   The top 259 object types are seen more than 1000 times, and after\n",
    "    819 objects are seen less than 100 times.\n",
    "\n",
    "-   Looking at the tail-end of the distribution, we see that `pikachu`\n",
    "    is mentioned once, whereas, e.g., `warderobe` (5) and `robot`(8) are\n",
    "    rarely seen which was not expected.\n",
    "\n",
    "-   The nature of the GQA dataset suggests its general-purpose\n",
    "    applicability. However, the skewed object categories distribution\n",
    "    shown above implies otherwise.\n",
    "\n",
    "### Finding most common object pairs\n",
    "\n",
    "-   \"What are the most common two adjacent object categories in the\n",
    "    graphs?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topPairs = scene_graphs_train.edges.groupBy(\"src_type\", \"dst_type\")\n",
    "display(topPairs.count().sort(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topPairs = scene_graphs_train.edges.groupBy(\"src_type\", \"relation_name\", \"dst_type\")\n",
    "display(topPairs.count().sort(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "-   In the tables above, we see that the most common relations reflect\n",
    "    spatial properties such as `to the right of` with `windows`\n",
    "    symmetrically related to each other standing for 2 x 28944\n",
    "    occurances.\n",
    "\n",
    "-   The most common relations are primarily between objects of the same\n",
    "    category.\n",
    "\n",
    "-   The first 'action'-encoding relation is seen in the 15th most common\n",
    "    triple `man-wearing-shirt` (5254).\n",
    "\n",
    "### Finding most common relations\n",
    "\n",
    "-   Could we categorise the edges according to what semantic function\n",
    "    they play?\n",
    "\n",
    "-   For instance, filtering out all relations that are spatial\n",
    "    (`behind`, `to the left of`, etc.).\n",
    "\n",
    "-   Suggested categories: *spatial*, *actions*, and *semantic*\n",
    "    relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topPairs = scene_graphs_train.edges.groupBy(\"relation_name\")\n",
    "display(topPairs.count().sort(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "-   The most common relations are spatial, overwhelmingly, with\n",
    "    `to the left of` and `to the right of` accounting for 1.7 million\n",
    "    occurences each.\n",
    "\n",
    "-   In contrast, the third most common relation `on` is seen \"only\"\n",
    "    90804 times. Out of the top 30 relations, 23 are spatial. Common\n",
    "    actions can be seen as few times as 28, as in the case of `opening`.\n",
    "\n",
    "-   Some of these relations encode both spatial and actions, such as in\n",
    "    `sitting on`.\n",
    "\n",
    "-   This shows some ambiguity in how the relation names are chosen, and\n",
    "    how this relates to the attributes, such as `sitting`, `looking`,\n",
    "    `lying`, that can also be encoded as object attributes.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "-   Next, we filter out relations that begin with `to the`, `in`, `on`,\n",
    "    `behind of`, or `in front of`, in order to bring forth more of the\n",
    "    non-spatial relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also possible to do:\n",
    "# from pyspark.sql.functions import udf\n",
    "#from pyspark.sql.types import BooleanType\n",
    "\n",
    "#filtered_df = spark_df.filter(udf(lambda target: target.startswith('good'), \n",
    "#                                  BooleanType())(spark_df.target))\n",
    "\n",
    "topPairs = scene_graphs_train.edges.filter(\"(relation_name NOT LIKE 'to the%') and (relation_name NOT LIKE '%on') and (relation_name NOT LIKE '%in') and (relation_name NOT LIKE '% of')\").groupBy(\"src_type\", \"relation_name\", \"dst_type\")\n",
    "display(topPairs.count().sort(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "-   We see in the pie chart above that once we filter out the most\n",
    "    common spatial relations, the remainder is dominated by `wearing`\n",
    "    and the occasional associative `of` (as in, e.g., `head-of-man`).\n",
    "\n",
    "-   These relations make up almost half of the non-spatial relations.\n",
    "\n",
    "TODO - Report statistics on attributes, similarly to how src/dst types\n",
    "are used above. Correlation metrics between vertex types and attributes?\n",
    "\n",
    "### Finding motifs\n",
    "\n",
    "TODO - finding other interesting motifs that we can motivate from a\n",
    "semantic perspective, e.g. looking at triangles, or more complex\n",
    "child-parent tree relations (e.g. one parent with exactly two children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_graphs_train_without_attributes_graphid = GraphFrame(scene_graphs_train_without_attributes.vertices.drop('graph_id').drop('id').selectExpr('object_name as id'), scene_graphs_train.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "motifs = scene_graph_train_without_attributes_graphid.find(\"(a)-[ab]->(b); (b)-[bc]->(c)\").filter(\"(a.object_name NOT LIKE b.object_name) and (a.object_name NOT LIKE c.object_name)\")\n",
    "\n",
    "display(motifs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - display motifs in a nicer way:\n",
    "#display(motifs.select('ab').rdd.map(lambda t: (t[-2], t[-3], t[-1])).toDF())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs_sorted = motifs.distinct()\n",
    "display(motifs_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs_sorted.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "-   Find *circular* motifs, i.e., motifs of type\n",
    "    `A -> relation_ab -> B -> relation_bc -> C -> relation_ca -> A`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circular_motifs = scene_graphs_train.find(\"(a)-[ab]->(b); (b)-[bc]->(c); (c)-[ca]->(a)\")\n",
    "\n",
    "display(circular_motifs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circular_motifs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "This gives us 7 million cycles of length 3. However, this is most likely\n",
    "dominated by the most common spatial relations. In the cell below, we\n",
    "filter out these spatial relations and count cycles again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circular_motifs = scene_graphs_train.find(\"(a)-[ab]->(b); (b)-[bc]->(c); (c)-[ca]->(a)\").filter(\"(ab.relation_name NOT LIKE 'to the%') and (bc.relation_name NOT LIKE 'to the%') and (ca.relation_name NOT LIKE 'to the%') and (ab.relation_name NOT LIKE '% of') and (bc.relation_name NOT LIKE '% of') and (ca.relation_name NOT LIKE '% of')\")\n",
    "\n",
    "display(circular_motifs.select('ab', 'bc', 'ca'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circular_motifs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "-   Without the most common spatial relations, we now have a\n",
    "    significantly lower amount, 18805, of cycles of length 3.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "-   Find *symmetric* motifs, i.e., motifs of type\n",
    "    `A -> relation_ab -> B -> relation_ab -> A`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symmetric_motifs = scene_graphs_train.find(\"(a)-[ab]->(b); (b)-[ba]->(a)\").filter(\"ab.relation_name LIKE ba.relation_name\")\n",
    "\n",
    "display(symmetric_motifs)\n",
    "\n",
    "#val motifs = tripGraphPrime.\n",
    "#  find(\"(a)-[ab]->(b); (b)-[bc]->(c)\").\n",
    "#  filter(\"(b.id = 'SFO') and (ab.delay > 500 or bc.delay > 500) and bc.tripid > ab.tripid and bc.tripid < ab.tripid + 10000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symmetric_motifs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symmetric_motifs = scene_graphs_train.find(\"(a)-[ab]->(b); (b)-[ba]->(a)\").filter(\"ab.relation_name LIKE ba.relation_name\").filter(\"(ab.relation_name NOT LIKE 'near') and (ab.relation_name NOT LIKE '% of')\")\n",
    "\n",
    "display(symmetric_motifs.select('ab', 'ba'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symmetric_motifs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "-   The symmetric relations that are spatial behave as expected, and\n",
    "    removing the most common ones shows that we have 3693 such symmetric\n",
    "    relations.\n",
    "\n",
    "-   However, when looking at the filtered symmetric motifs we can see\n",
    "    examples such as 'boy-wearing-boy' and 'hot dog-wrapped in-hot dog'.\n",
    "\n",
    "-   These examples of symmetric action relations seem to poorly reflect\n",
    "    the expected structure of a scene graph.\n",
    "\n",
    "-   We assume that this is either an artifact of the human annotations\n",
    "    containing noise, or that the sought after denseness of the graphs\n",
    "    used describing the images create these kinds of errors.\n",
    "\n",
    "### Object ranking using PageRank\n",
    "\n",
    "TODO - fix pagerank in terms of performance: perhaps, remove spatial\n",
    "edges (e.g. to the left/right of).\n",
    "\n",
    "UPDATE 21-12-2020: sorting and groupping (groupBy) almost never work\n",
    "(either finishes within a couple of minutes or runs almost infinitely\n",
    "slow/never finishes) possibly due to cluster overload; pagerank itself\n",
    "returns all ones (approximately, and identical values) after the initial\n",
    "run and sometimes produces meaningful results when re-run.\n",
    "\n",
    "`scene_graph_without_attributes = GraphFrame(scene_graphs_train.vertices.drop('attributes'), scene_graphs_train.edges)`\n",
    "\n",
    "`ranks = scene_graph_without_attributes.pageRank(resetProbability=0.15, tol=0.01)`\n",
    "\n",
    "`display(ranks.vertices)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - This does not really give us anything at the moment\n",
    "# TODO - display the object names within one image and their correspoinding pageranks.\n",
    "\n",
    "# Apparently, using the previous graph frame created for another task \n",
    "temp = GraphFrame(scene_graphs_train.vertices.select('graph_id', 'id', 'object_name'), scene_graphs_train.edges)\n",
    "\n",
    "ranks = temp.pageRank(resetProbability=0.15, tol=0.01)\n",
    "display(ranks.vertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ranks = ranks.vertices.sort('pagerank', ascending=False)\n",
    "display(sorted_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_graphs_without_attributes = GraphFrame(scene_graphs_val.vertices.select('graph_id', 'id', 'object_name'), scene_graphs_val.edges)\n",
    "val_ranks = val_graphs_without_attributes.pageRank(resetProbability=0.15, tol=0.01)\n",
    "display(val_ranks.vertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sorted_ranks = val_ranks.vertices.sort('pagerank', ascending=False)\n",
    "display(val_sorted_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_pagerank_sums_objects = ranks.vertices.groupBy('object_name').sum('pagerank')\n",
    "graph_pagerank_sums_objects.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_pagerank_sums_objects_sorted = graph_pagerank_sums_objects.sort('sum(pagerank)',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(graph_pagerank_sums_objects_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "-   Here we see that the summed (accumulated) PageRank per object\n",
    "    category reflects the number of occurences for each object (see the\n",
    "    `topObjects` section). At least for the top 10 in this table.\n",
    "\n",
    "-   This verifies that the most common objects are highly connected with\n",
    "    others in their respective scene graphs.\n",
    "\n",
    "-   We therefore conclude that they do not necessarily have a high\n",
    "    information gain.\n",
    "\n",
    "-   A high accumulated PageRank suggests the general nature of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topObjects = topObjects.sort('object_name', ascending=False)\n",
    "graph_pagerank_sums_objects_sorted = graph_pagerank_sums_objects_sorted.sort('object_name', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "graph_pagerank_joined = graph_pagerank_sums_objects_sorted.join(topObjects, \"object_name\").withColumn('normalize(pagerank)', F.col('sum(pagerank)') / F.col('count'))\n",
    "display(graph_pagerank_joined.sort('normalize(pagerank)', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "-   We further normalise the PageRank values, i.e., divide by the number\n",
    "    of occurences per object category in the scenes.\n",
    "\n",
    "-   We observe that, in contrast to the accumulated PageRank, the\n",
    "    normalised values reflect the uniquness of object categorires: the\n",
    "    fewer the occurences, the higher the normalised PageRank.\n",
    "\n",
    "-   For example, `wolves` occurs only once in the entire dataset, and\n",
    "    its corresponding PageRank (accumulated equals normalised in this\n",
    "    case) is the highest of all, followed by the glorius `pikachu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(graph_pagerank_joined.sort('sum(pagerank)', ascending=False).limit(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "-   In the above table, we see that the normalised PageRank for the top\n",
    "    30 objects has a different ordering than the summed PageRanks.\n",
    "\n",
    "-   For example, `sky` has the highest normalised PageRank, and the most\n",
    "    common category `window` has the lowest.\n",
    "\n",
    "-   This could be a reflection of the fact that `sky` most likely acts\n",
    "    as an *anchor* object in the image, being a background to which\n",
    "    everything else is related.\n",
    "\n",
    "-   On the contrary, while `window` might be prevalent in many images,\n",
    "    it is assumed to more often act as a foreground object rather than\n",
    "    `sky`.\n",
    "\n",
    "-   Nonetheless, these conlusions correspond well to the above analysis\n",
    "    of objects generality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranks.vertices.groupBy('graph_id').count().show()\n",
    "\n",
    "graph_pagerank_sums = ranks.vertices.groupBy('graph_id').sum('pagerank')\n",
    "graph_pagerank_sums.show()\n",
    "# df_branksasket1.groupby('graph_id','Item_name').agg({'Price': 'count'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +--------+------------------+\n",
    ">     |graph_id|     sum(pagerank)|\n",
    ">     +--------+------------------+\n",
    ">     | 2394929|10.000000000001032|\n",
    ">     | 2383053|21.000000000002167|\n",
    ">     | 2381043|21.000000000002167|\n",
    ">     | 2332729|11.000000000001137|\n",
    ">     | 2366493|30.000000000003094|\n",
    ">     | 2377871|26.000000000002682|\n",
    ">     | 2350068|12.000000000001238|\n",
    ">     | 1591809| 22.00000000000227|\n",
    ">     | 1592764|21.000000000002167|\n",
    ">     | 2411119|27.000000000002785|\n",
    ">     | 2337597|23.000000000002373|\n",
    ">     | 2406784| 9.000000000000929|\n",
    ">     | 2319630| 28.00000000000289|\n",
    ">     | 2365546|17.000000000001755|\n",
    ">     | 2357438|20.000000000002064|\n",
    ">     | 2395199|24.000000000002476|\n",
    ">     | 2348208| 43.00000000000446|\n",
    ">     | 2341350|16.000000000001652|\n",
    ">     | 2397463|27.000000000002785|\n",
    ">     | 2370847| 19.00000000000196|\n",
    ">     +--------+------------------+\n",
    ">     only showing top 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_val_pagerank_sums = val_ranks.vertices.groupBy('graph_id').sum('pagerank')\n",
    "graph_val_pagerank_sums.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +--------+------------------+\n",
    ">     |graph_id|     sum(pagerank)|\n",
    ">     +--------+------------------+\n",
    ">     | 2316914|13.999999999999774|\n",
    ">     | 2336843|1.9999999999999671|\n",
    ">     | 2409544|16.999999999999726|\n",
    ">     | 2385432| 18.99999999999969|\n",
    ">     | 2338120| 6.999999999999886|\n",
    ">     | 2315739|  7.99999999999987|\n",
    ">     | 2346829| 17.99999999999971|\n",
    ">     | 2411941|11.999999999999806|\n",
    ">     | 2371809|16.999999999999726|\n",
    ">     | 2326319|30.999999999999478|\n",
    ">     |    1669|20.999999999999655|\n",
    ">     | 2318182|16.999999999999726|\n",
    ">     | 2371850| 5.999999999999902|\n",
    ">     | 2349988| 4.999999999999918|\n",
    ">     | 2357784|20.999999999999655|\n",
    ">     | 2331673|16.999999999999726|\n",
    ">     | 2359775|16.999999999999726|\n",
    ">     |  498144| 12.99999999999979|\n",
    ">     | 2331455| 26.99999999999955|\n",
    ">     | 2327585| 6.999999999999886|\n",
    ">     +--------+------------------+\n",
    ">     only showing top 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(graph_val_pagerank_sums.sort('sum(pagerank)',ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "### Merging vertices\n",
    "\n",
    "-   We use object names (object categories with or without attributes)\n",
    "    instead of IDs as vertex identifier to merge all scene graphs (each\n",
    "    with its `graph_id`) into one *meta-graph*.\n",
    "\n",
    "-   This enables us to analyse, e.g., how object types relate to each\n",
    "    other *in general*, and how connected components can be formed based\n",
    "    on specific image contexts.\n",
    "\n",
    "-   The key intuition is that it could allow us to detect connected\n",
    "    components representing *scene categories* such as `traffic` or\n",
    "    `bathroom`, i.e., meta-understanding of images as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_vertices = scene_graphs_val.vertices.selectExpr('object_name as id', 'attributes as attributes')\n",
    "display(merged_vertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_vertices.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[83]: 174331"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_vertices = merged_vertices.distinct()\n",
    "display(merged_vertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_vertices.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[85]: 22243\n",
    "\n",
    "  \n",
    "\n",
    "-   We see that there are X unique combinations of objects and\n",
    "    attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_vertices_without_attributes = merged_vertices.select('id').distinct()\n",
    "display(merged_vertices_without_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_vertices_without_attributes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[87]: 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_edges = scene_graphs_val.edges.selectExpr('src_type as src', 'dst_type as dst', 'relation_name as relation_name')\n",
    "display(merged_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_edges.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[89]: 534889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_graphs_merged = GraphFrame(merged_vertices, merged_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scene_graphs_merged.vertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scene_graphs_merged.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_graphs_merged_without_attributes = GraphFrame(merged_vertices_without_attributes, merged_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scene_graphs_merged_without_attributes.vertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scene_graphs_merged_without_attributes.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "### Computing the Connected Components\n",
    "\n",
    "-   Here we compute the connected components of the merged scene graphs\n",
    "    (one with the object attributes included and the other without).\n",
    "\n",
    "-   Before merging, the connected components should roughly correspond\n",
    "    to the number of scene graphs, as they are made up of at least 1\n",
    "    connected component each.\n",
    "\n",
    "-   In the merged graphs, we can expect a much smaller set of connected\n",
    "    components, and we hypothesize that these could correspond to *scene\n",
    "    categories* (image classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir(\"/tmp/scene-graph-motifs-connected-components\")\n",
    "connected_components = scene_graphs_merged.connectedComponents()\n",
    "display(connected_components)\n",
    "\n",
    "# displays the index of a component for a given object category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "-   The number of connected components are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_count = connected_components.groupBy('component')\n",
    "display(components_count.count().sort(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_components_without_attributes = scene_graphs_merged_without_attributes.connectedComponents()\n",
    "display(connected_components_without_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_count = connected_components_without_attributes.groupBy('component')\n",
    "display(components_count.count().sort(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "  \n",
    "\n",
    "-   These results indicate that the merged graph is too dense due to the\n",
    "    generic relations (e.g., the spatial relations 'next-to' et al.)\n",
    "    connecting all objects into one big chunk.\n",
    "\n",
    "-   Removing some of these most occuring relations could show an\n",
    "    underlying graph structure that is more interesting.\n",
    "\n",
    "General discussion\n",
    "==================\n",
    "\n",
    "First we recap the main points of the results of our analysis.\n",
    "\n",
    "Objects\n",
    "-------\n",
    "\n",
    "-   Interestingly, `man` (31370) and `person` (20218) is seen three and\n",
    "    two times more than `woman` (11355), respectively.\n",
    "-   The nature of the GQA dataset suggests its general-purpose\n",
    "    applicability. However, the skewed object categories distribution\n",
    "    shown above implies otherwise.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "\n",
    "-   Our analysis of the original dataset shows that a few of the most\n",
    "    commonly annotated attributes account for the majority of all\n",
    "    annotations.\n",
    "-   Most of the common attributes are colors, `black` and `white` being\n",
    "    the most common. `white` is seen 92659 times, and `black` 59617\n",
    "    times.\n",
    "-   We suspect that since the dataset is generated using human\n",
    "    annotators, many of the less common annotations, such as attributes\n",
    "    occuring less than 100 times, are more error prone and might have a\n",
    "    high noise to label ratio.\n",
    "\n",
    "Relations\n",
    "---------\n",
    "\n",
    "-   The most common relations are, overwhelmingly, *spatial* properties\n",
    "    with `to the left of` and `to the right of`, accounting for 1.7\n",
    "    million occurences each.\n",
    "-   The most common relations are primarily between objects of the same\n",
    "    category.\n",
    "-   For instance, `windows` are symmetrically related to each other\n",
    "    making up 2 x 28944 occurances.\n",
    "-   Some of these relations encode both *spatial* and *action* relation\n",
    "    categories, e.g., `sitting on`.\n",
    "\n",
    "PageRank\n",
    "--------\n",
    "\n",
    "-   Our page rank results mainly reflect the number of occurences of\n",
    "    each object category,\n",
    "\n",
    "To summarize, we see that GQA still has a lot of room for improvement in\n",
    "terms of the distribution of objects and relations."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
