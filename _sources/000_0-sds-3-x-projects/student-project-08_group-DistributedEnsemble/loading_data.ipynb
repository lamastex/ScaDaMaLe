{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TOY DATA**\n",
    "\n",
    "Data with Gaussian clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "from random import randrange\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gaussian_RDD(means, variances, num_observations, class_proportions, train_test_split=False):\n",
    "  \"\"\"Create toy Gaussian classification data\n",
    "  Let C := number of clusters/classes and P := number of data features\n",
    "  \n",
    "  Args: \n",
    "    means (np.array[float]): mean vector of shape (C, P)\n",
    "    variances (np.array[float]): vector of variances, shape (C, P)\n",
    "    num_observations (scalar[int]): the total number of observations in the final data set\n",
    "    class_proportions (np.array[float]): vector of class proportions, length C\n",
    "    train_test_split: whether to split the data into train/test sets or not\n",
    "    \n",
    "  Returns:\n",
    "    Gaussian data, RDD of tuples (list(features), int(label))\n",
    "  \"\"\"\n",
    "  \n",
    "  assert means.shape[0] == variances.shape[0]\n",
    "  assert means.shape[1] == variances.shape[1]\n",
    "  assert class_proportions.sum() == 1\n",
    "    \n",
    "  num_classes = means.shape[0]\n",
    "  num_features = means.shape[1]\n",
    "  \n",
    "  data_rdd = sc.emptyRDD() \n",
    "  for k in range(num_classes):\n",
    "    \n",
    "    # Generate standard normal data\n",
    "    class_size = int(num_observations * class_proportions[k])\n",
    "    class_rdd = RandomRDDs.normalVectorRDD(sc, numRows=class_size, numCols=num_features, numPartitions=1) #, seed=123)\n",
    "\n",
    "    # Map to true distribution\n",
    "    class_rdd_transformed = class_rdd.map(lambda v: means[k, :] + (variances[k, :]**0.5) * v)\n",
    "    \n",
    "    # Add labels\n",
    "    class_rdd_w_label = class_rdd_transformed.map(lambda v: (v, k)) \n",
    "    \n",
    "    data_rdd = data_rdd.union(class_rdd_w_label)\n",
    "    \n",
    "  # We will shuffle and repartition the data\n",
    "  num_partitions = 10\n",
    "  shuffled_rdd =  data_rdd.sortBy(lambda v: randrange(num_observations)).repartition(num_partitions)\n",
    "  final_rdd = shuffled_rdd.map(tuple).map(lambda v: (list(v[0]), int(v[1])))\n",
    "  \n",
    "  if train_test_split:\n",
    "    train_rdd, test_rdd = final_rdd.randomSplit(weights=[0.8, 0.2], seed=12)\n",
    "    final_rdd = (train_rdd, test_rdd)\n",
    "    \n",
    "  return final_rdd                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example use\n",
    "# Test: 3 classes, 2 features\n",
    "means = np.array([[-1, -10], [1, 10], [0, -1]])\n",
    "variances = np.ones((3, 2))\n",
    "\n",
    "# We should shuffle this\n",
    "num_observations=1000\n",
    "class_proportions = np.array([0.3, 0.3, 0.4]) # Sorry, this should have been an array\n",
    "my_rdd = create_gaussian_RDD(means, variances, num_observations, class_proportions)\n",
    "my_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "**FRAUD DATA**\n",
    "\n",
    "This data is from https://www.kaggle.com/ntnu-testimon/paysim1 \\[1\\]. It\n",
    "is actually synthetic but should have more complex features than the toy\n",
    "data above. Number of data points in total: 6,362,620. Number of\n",
    "features (kept): 7 (wherof one is categorical). Number of classes: 2\n",
    "(fraud/not fraud).\n",
    "\n",
    "\\[1\\] E. A. Lopez-Rojas , A. Elmir, and S. Axelsson. \"PaySim: A\n",
    "financial mobile money simulator for fraud detection\". In: The 28th\n",
    "European Modeling and Simulation Symposium-EMSS, Larnaca, Cyprus. 2016\n",
    "(PaySim first paper of the simulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fraud_data(train_test_split=False):\n",
    "  \"\"\"Load and preprocess synthetic fraud data set\n",
    "  \n",
    "  Args:\n",
    "     train_test_split: whether to split the data into train/test sets or not\n",
    "    \n",
    "  Returns:\n",
    "    Fraud data, RDD of tuples (list(features), int(label))\n",
    "  \"\"\"\n",
    "  \n",
    "  # File location and type\n",
    "  file_location = \"/FileStore/tables/financial_fraud_data.csv\"  # It seems perhaps like these files are not saved for long?\n",
    "  file_type = \"csv\"\n",
    "\n",
    "  # CSV options\n",
    "  infer_schema = \"true\"\n",
    "  first_row_is_header = \"true\"\n",
    "  delimiter = \",\"\n",
    "\n",
    "  # Load the data from file\n",
    "  df = spark.read.format(file_type) \\\n",
    "      .option(\"inferSchema\", infer_schema) \\\n",
    "      .option(\"header\", first_row_is_header) \\\n",
    "      .option(\"sep\", delimiter) \\\n",
    "      .load(file_location).select(\"step\", \"type\", \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\", \"isFraud\") \\\n",
    "  \n",
    "  col_num = [\"step\", \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\"]\n",
    "  col_cat = \"type\"\n",
    "\n",
    "  # Rename target column\n",
    "  df_renamed = df.withColumnRenamed(\"isFraud\", \"label\")\n",
    "\n",
    "  # Convert qualitative variable to one-hot\n",
    "  indexer = StringIndexer(inputCol = col_cat, outputCol = \"type_ind\")\n",
    "  oh_encoder = OneHotEncoder(inputCol = \"type_ind\", outputCol = \"type_oh\")\n",
    "\n",
    "  # Scale numerical features\n",
    "  va = VectorAssembler(inputCols = col_num_f, outputCol = \"numerical_features\")\n",
    "  scaler = StandardScaler(inputCol = \"numerical_features\", outputCol=\"scaled\")\n",
    "\n",
    "  # Merge all features in one column\n",
    "  va2 = VectorAssembler(inputCols = [\"scaled\", \"type_oh\"], outputCol = \"features\")\n",
    "\n",
    "  # Apply pipeline\n",
    "  pipeline = Pipeline(stages = [indexer, oh_encoder, va, scaler, va2])\n",
    "  final_df = pipeline.fit(df_renamed).transform(df_renamed).select(\"features\", \"label\")\n",
    "\n",
    "  # Convert to RDD \n",
    "  final_rdd = final_df.rdd.map(tuple).map(lambda v: (list(v[0]), int(v[1])))\n",
    "  \n",
    "  if train_test_split:\n",
    "    train_rdd, test_rdd = final_rdd.randomSplit(weights=[0.8, 0.2], seed=12)\n",
    "    final_rdd = (train_rdd, test_rdd)\n",
    "  \n",
    "  return final_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_rdd = load_fraud_data()\n",
    "fraud_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "**FIRE WALL DATA**\n",
    "\n",
    "Data from UCI repository:\n",
    "https://archive.ics.uci.edu/ml/datasets/Internet+Firewall+Data \\[2\\].\n",
    "Number of data points: 65,532. Number of features: 11 (all numerical).\n",
    "Number of classes: 4 (allow/deny/drop/reset both).\n",
    "\n",
    "\\[2\\] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository\n",
    "\\[http://archive.ics.uci.edu/ml\\]. Irvine, CA: University of California,\n",
    "School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_firewall_data(train_test_split=False):\n",
    "  \"\"\"Load and preprocess firewall data\n",
    "  Args:\n",
    "     train_test_split: whether to split the data into train/test sets or not\n",
    "  \n",
    "  Returns:\n",
    "     Firewall data, RDD of tuples (list(features), int(label))\n",
    "  \"\"\"\n",
    "  \n",
    "  # File location and type\n",
    "  file_location = \"/FileStore/shared_uploads/amanda.olmin@liu.se/fire_wall_data.csv\" \n",
    "  file_type = \"csv\"\n",
    "\n",
    "  # CSV options\n",
    "  infer_schema = \"true\"\n",
    "  first_row_is_header = \"true\"\n",
    "  delimiter = \",\"\n",
    "\n",
    "  # Load the data from file\n",
    "  df = spark.read.format(file_type) \\\n",
    "      .option(\"inferSchema\", infer_schema) \\\n",
    "      .option(\"header\", first_row_is_header) \\\n",
    "      .option(\"sep\", delimiter) \\\n",
    "      .load(file_location)\n",
    "  \n",
    "  # Preprocess data\n",
    "  col_num = [\"Source Port\", \"Destination Port\", \"NAT Source Port\", \"NAT Destination Port\", \"Bytes\", \"Bytes Sent\", \"Bytes Received\", \"Packets\", \"Elapsed Time (sec)\", \"pkts_sent\", \"pkts_received\"]\n",
    "\n",
    "  # Index qualitative variable\n",
    "  indexer = StringIndexer(inputCol = \"Action\", outputCol = \"label\")\n",
    "\n",
    "  # Scale numerical features\n",
    "  va = VectorAssembler(inputCols = col_num, outputCol = \"numerical_features\") \n",
    "  scaler = StandardScaler(inputCol = \"numerical_features\", outputCol = \"features\")\n",
    "  \n",
    "  # Apply pipeline \n",
    "  pipeline = Pipeline(stages=[indexer, va, scaler])\n",
    "  final_df = pipeline.fit(df).transform(df).select(\"features\", \"label\") \n",
    "  \n",
    "  # Convert to RDD\n",
    "  final_rdd = final_df.rdd.map(tuple).map(lambda v: (list(v[0]), int(v[1])))\n",
    "  \n",
    "  if train_test_split:\n",
    "    train_rdd, test_rdd = final_rdd.randomSplit(weights=[0.8, 0.2], seed=12)\n",
    "    final_rdd = (train_rdd, test_rdd)\n",
    "\n",
    "  return final_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firewall_rdd = load_firewall_data()\n",
    "firewall_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[3]: [(tensor([3.7509e+00, 2.8701e-03, 2.4845e+00, 5.4419e-03, 3.1503e-05, 2.4555e-05,\n",
    ">               3.3696e-05, 3.8964e-04, 9.9186e-02, 3.1067e-04, 4.4978e-04],\n",
    ">              dtype=torch.float64),\n",
    ">       tensor(0, dtype=torch.int32)),\n",
    ">      (tensor([3.6877e+00, 1.8353e-01, 2.5606e+00, 3.4798e-01, 8.4863e-04, 4.1796e-04,\n",
    ">               1.2861e-03, 3.7015e-03, 5.6205e-02, 3.1067e-03, 4.0480e-03],\n",
    ">              dtype=torch.float64),\n",
    ">       tensor(0, dtype=torch.int32)),\n",
    ">      (tensor([4.5104e-01, 2.7251e+00, 1.9692e+00, 5.1669e+00, 4.2361e-05, 3.0824e-05,\n",
    ">               4.8717e-05, 3.8964e-04, 3.9641e+00, 3.1067e-04, 4.4978e-04],\n",
    ">              dtype=torch.float64),\n",
    ">       tensor(0, dtype=torch.int32)),\n",
    ">      (tensor([3.3137e+00, 1.8353e-01, 2.3009e+00, 3.4798e-01, 5.9216e-04, 3.7564e-04,\n",
    ">               7.6689e-04, 2.9223e-03, 5.6205e-02, 2.4853e-03, 3.1484e-03],\n",
    ">              dtype=torch.float64),\n",
    ">       tensor(0, dtype=torch.int32)),\n",
    ">      (tensor([3.2776e+00, 2.3990e-02, 2.0868e+00, 4.5486e-02, 4.5134e-03, 1.7706e-03,\n",
    ">               7.5430e-03, 6.0394e-03, 5.2899e-02, 4.0387e-03, 8.0960e-03],\n",
    ">              dtype=torch.float64),\n",
    ">       tensor(0, dtype=torch.int32))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The functions below will return RDDs with tuples (tensor(features), tensor(label))\n",
    "\n",
    "# One way to create a \"batch\" iterator from the RDDs is with itertoolz\n",
    "from toolz.itertoolz import partition_all\n",
    "\n",
    "def get_batched_iterator(input_rdd):\n",
    "  return input_rdd.mapPartitions(lambda partition: partition_all(batch_size, partition)).toLocalIterator()\n",
    "\n",
    "#print(next(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
