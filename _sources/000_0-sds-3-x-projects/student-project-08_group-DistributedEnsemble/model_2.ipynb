{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "def load_firewall_data():\n",
    "  # File location and type\n",
    "  file_location_fw = \"/FileStore/shared_uploads/amanda.olmin@liu.se/fire_wall_data.csv\" \n",
    "  file_type = \"csv\"\n",
    "\n",
    "  # CSV options\n",
    "  infer_schema = \"true\"\n",
    "  first_row_is_header = \"true\"\n",
    "  delimiter = \",\"\n",
    "\n",
    "  # Read the data\n",
    "  df_fw = spark.read.format(file_type) \\\n",
    "    .option(\"inferSchema\", infer_schema) \\\n",
    "    .option(\"header\", first_row_is_header) \\\n",
    "    .option(\"sep\", delimiter) \\\n",
    "    .load(file_location_fw)\n",
    "  \n",
    "  # Preprocess data\n",
    "  col_num_fw = [\"Source Port\", \"Destination Port\", \"NAT Source Port\", \"NAT Destination Port\", \"Bytes\", \"Bytes Sent\", \"Bytes Received\", \"Packets\", \"Elapsed Time (sec)\", \"pkts_sent\", \"pkts_received\"]\n",
    "\n",
    "  # Index qualitative variable\n",
    "  indexer_fw = StringIndexer(inputCol = \"Action\", outputCol = \"label\")\n",
    "\n",
    "  # Some scaling\n",
    "  va_fw = VectorAssembler(inputCols = col_num_fw, outputCol = \"numerical_features\") \n",
    "  scaler_fw = StandardScaler(inputCol = \"numerical_features\", outputCol = \"features\")\n",
    "\n",
    "  # Pipeline \n",
    "  pipeline_fw = Pipeline(stages=[indexer_fw, va_fw, scaler_fw])\n",
    "\n",
    "  df_final_fw = pipeline_fw.fit(df_fw).transform(df_fw).select(\"features\", \"label\")\n",
    "\n",
    "  # Convert to RDD\n",
    "  #firewall_rdd = df_final_fw.rdd.map(tuple).map(lambda v: (torch.tensor(v[0], dtype=torch.float64), torch.tensor(v[1], dtype=torch.int32)))\n",
    "  firewall_rdd = df_final_fw.rdd.map(tuple).map(lambda v: (list(v[0]), int(v[1])))\n",
    "  \n",
    "  return firewall_rdd\n",
    "\n",
    "def load_fraud_data():\n",
    "  # File location and type\n",
    "  file_location_f = \"/FileStore/shared_uploads/amanda.olmin@liu.se/financial_fraud_data.csv\"  # It seems perhaps like these files are not saved for long?\n",
    "  file_type = \"csv\"\n",
    "\n",
    "  # CSV options\n",
    "  infer_schema = \"true\"\n",
    "  first_row_is_header = \"true\"\n",
    "  delimiter = \",\"\n",
    "\n",
    "  # Read the data\n",
    "  df_f = spark.read.format(file_type) \\\n",
    "    .option(\"inferSchema\", infer_schema) \\\n",
    "    .option(\"header\", first_row_is_header) \\\n",
    "    .option(\"sep\", delimiter) \\\n",
    "    .load(file_location_f).select(\"step\", \"type\", \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\", \"isFraud\") \\\n",
    "\n",
    "  # removed features flaggedFraud, nameOrig, nameDest, perhaps we could have kept at least flaggedFraud\n",
    "  # Preprocess data\n",
    "  \n",
    "  # There should be no missing values and \"type\" should have 5 (?) levels\n",
    "  col_num_f = [\"step\", \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\"]\n",
    "  col_cat_f = \"type\"\n",
    "\n",
    "  # Rename target column\n",
    "  df_renamed_f = df_f.withColumnRenamed(\"isFraud\", \"label\")\n",
    "\n",
    "  # Convert qualitative variable to onehot\n",
    "  indexer_f = StringIndexer(inputCol = col_cat_f, outputCol = \"type_ind\")\n",
    "  oh_encoder_f = OneHotEncoder(inputCol = \"type_ind\", outputCol = \"type_oh\")\n",
    "\n",
    "  # We will do some scaling\n",
    "  va_f = VectorAssembler(inputCols = col_num_f, outputCol = \"numerical_features\")\n",
    "  scaler_f = StandardScaler(inputCol = \"numerical_features\", outputCol=\"scaled\")\n",
    "\n",
    "  # Now, merge all features in one column\n",
    "  va2_f = VectorAssembler(inputCols = [\"scaled\", \"type_oh\"], outputCol = \"features\")\n",
    "\n",
    "  # Pipeline\n",
    "  pipeline_f = Pipeline(stages = [indexer_f, oh_encoder_f, va_f, scaler_f, va2_f])\n",
    "\n",
    "  df_final_f = pipeline_f.fit(df_renamed_f).transform(df_renamed_f).select(\"features\", \"label\")\n",
    "\n",
    "  # Convert to RDD and tensors\n",
    "  fraud_rdd = df_final_f.rdd.map(tuple).map(lambda v: (list(v[0]), int(v[1])))\n",
    "  \n",
    "  return fraud_rdd\n",
    "\n",
    "\n",
    "from toolz.itertoolz import partition_all\n",
    "\n",
    "batch_size=2000\n",
    "\n",
    "def get_batched_iterator(input_rdd):\n",
    "  #return input_rdd.mapPartitions(lambda partition: partition_all(batch_size, partition)).toLocalIterator()\n",
    "  return input_rdd.mapPartitions(lambda partition: partition_all(batch_size, partition))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "\n",
    "#Feedforward network for classification\n",
    "class MLP(nn.Module):\n",
    "  \n",
    "  def __init__(self,shape):\n",
    "    #shape: number of neurons in each layer (including the input and output layers)\n",
    "    super(MLP,self).__init__()\n",
    "    \n",
    "    self.units=nn.ModuleList()\n",
    "    for i in range(len(shape)-1):\n",
    "      self.units.append(nn.Linear(shape[i],shape[i+1]))\n",
    "    \n",
    "    self._shape=shape\n",
    "    self._nlayers=len(shape)\n",
    "  \n",
    "  def forward(self,x):\n",
    "    \n",
    "    y=x\n",
    "    \n",
    "    for i,layer in enumerate(self.units):\n",
    "      if i<self._nlayers-2:\n",
    "        y=nn.functional.tanh(layer(y))\n",
    "      else:\n",
    "        y=nn.functional.softmax(layer(y),dim=1)\n",
    "    \n",
    "    return y\n",
    "  \n",
    "  # Suggestion for helper function / Jakob\n",
    "  @staticmethod\n",
    "  def from_state_dict(state_dict, net_shape):\n",
    "      net=MLP(net_shape)\n",
    "      net.load_state_dict(state_dict)\n",
    "      return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The main training function (is run on driver nodes).\n",
    "def Train(net_params,net_shape,x,y):\n",
    "#net_params,net_shape: initial parameters and shape of the feedforward network \n",
    "#x,y: training data\n",
    "\n",
    "  numepochs=50\n",
    "  batchsize=10\n",
    "  \n",
    "  ndata=x.shape[0]\n",
    "  \n",
    "  net=MLP.from_state_dict(net_params,net_shape)\n",
    "  \n",
    "  opt=optim.Adam(net.parameters())\n",
    "  loss=nn.CrossEntropyLoss()\n",
    "  \n",
    "  for i in range(numepochs):\n",
    "    for j in range(ndata//batchsize):\n",
    "      \n",
    "      opt.zero_grad()\n",
    "      \n",
    "      k1=j*batchsize\n",
    "      k2=min((j+1)*batchsize,ndata)\n",
    "      xb=x[k1:k2,:]\n",
    "      yb=y[k1:k2]\n",
    "      \n",
    "      yhat=net(xb)\n",
    "      err=loss(yhat,yb)\n",
    "      err.backward()\n",
    "  \n",
    "      opt.step()\n",
    "  \n",
    "  err=loss(net(x),y)\n",
    "  lossval=float(err.detach().numpy())\n",
    "  \n",
    "  #returns parameters of the trained network, network shape, and loss\n",
    "  return (net.state_dict(),net_shape,lossval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Totensor(d):\n",
    "  x=[v[0] for v in d]\n",
    "  y=[v[1] for v in d]\n",
    "  x=torch.tensor(x,dtype=torch.float)\n",
    "  y=torch.tensor(y,dtype=torch.long)\n",
    "  \n",
    "  return (x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models=5 #ensemble size\n",
    "model_data=[] #pairs of model parameters and their training data\n",
    "shapes=[] #shape of networks\n",
    "inputdims=11 #features dimensions\n",
    "nclasses=4 #number of classes\n",
    "\n",
    "dataiterator=get_batched_iterator(load_firewall_data())\n",
    "dataiterator=dataiterator.map(Totensor).toLocalIterator()\n",
    "\n",
    "#initialization\n",
    "for i in range(n_models):\n",
    "  \n",
    "  #pick random number of hidden layers and neurons for each network\n",
    "  nhidden=random.randint(1,4)\n",
    "  shape=[inputdims]\n",
    "  for k in range(nhidden):\n",
    "    shape.append(random.randint(5,15))\n",
    "  shape.append(nclasses)\n",
    "  \n",
    "  net=MLP(shape)\n",
    "  shapes.append(shape)\n",
    "  \n",
    "  #-data loading\n",
    "  d=next(dataiterator)\n",
    "  #x=[v[0] for v in d]\n",
    "  #y=[v[1] for v in d]\n",
    "  x=d[0]\n",
    "  y=d[1]\n",
    "  #x=torch.tensor(x,dtype=torch.float)\n",
    "  #y=torch.tensor(y,dtype=torch.long)\n",
    "  #print shapes of input and target tensors\n",
    "  print(x.shape)\n",
    "  print(y.shape)\n",
    "  #-\n",
    "  \n",
    "  #model_data.append((net.state_dict(),shape,x,y))\n",
    "  model_data.append((net.state_dict(),x,y))\n",
    "\n",
    "#distribute the array\n",
    "model_data_par= sc.parallelize(model_data)\n",
    "#main training map\n",
    "models_trained= model_data_par.map(lambda t: Train(*t))\n",
    "\n",
    "#models_trained_c=models_trained.collect()\n",
    "#print(\"losses:\")\n",
    "#print([t[1] for t in models_trained_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     torch.Size([2000, 11])\n",
    ">     torch.Size([2000])\n",
    ">     torch.Size([2000, 11])\n",
    ">     torch.Size([2000])\n",
    ">     torch.Size([2000, 11])\n",
    ">     torch.Size([2000])\n",
    ">     torch.Size([2000, 11])\n",
    ">     torch.Size([2000])\n",
    ">     torch.Size([2000, 11])\n",
    ">     torch.Size([2000])\n",
    "\n",
    "  \n",
    "\n",
    "MLP with auto-inferred `shapes` param\n",
    "-------------------------------------\n",
    "\n",
    "Not a big change, but it is nice to be able to create models from a list\n",
    "of paths to state dicts -- saved to disk -- without having to know the\n",
    "shapes a priori.\n",
    "\n",
    "The important changes are: - added static methods in `MLP` class -\n",
    "`Train` function removes the `shapes` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from toolz.itertoolz import cons\n",
    "\n",
    "#Feedforward network for classification\n",
    "class MLP(nn.Module):\n",
    "  \n",
    "  def __init__(self,shape):\n",
    "    #shape: number of neurons in each layer (including the input and output layers)\n",
    "    super(MLP,self).__init__()\n",
    "    \n",
    "    self.units=nn.ModuleList()\n",
    "    for i in range(len(shape)-1):\n",
    "      self.units.append(nn.Linear(shape[i],shape[i+1]))\n",
    "    \n",
    "    self._shape=shape\n",
    "    self._nlayers=len(shape)\n",
    "  \n",
    "  def forward(self,x):\n",
    "    \n",
    "    y=x\n",
    "    \n",
    "    for i,layer in enumerate(self.units):\n",
    "      if i<self._nlayers-2:\n",
    "        y=nn.functional.tanh(layer(y))\n",
    "      else:\n",
    "        y=nn.functional.softmax(layer(y),dim=1)\n",
    "    \n",
    "    return y\n",
    "  \n",
    "  @staticmethod\n",
    "  def from_state_dict(state_dict):\n",
    "    net_shape = MLP.shape_from_state_dict(state_dict)\n",
    "    net=MLP(net_shape)\n",
    "    net.load_state_dict(state_dict)\n",
    "    return net\n",
    "\n",
    "  @staticmethod\n",
    "  def shape_from_state_dict(state_dict):\n",
    "    \"\"\"Infer MLP layer shapes from state_dict\"\"\"\n",
    "    iter_ = iter(state_dict.items())\n",
    "    _, input_size = next(iter_)\n",
    "    bias_tensors = filter(lambda key_val: key_val[0].find(\"bias\") != -1, iter_)\n",
    "    shapes = map(lambda key_val: key_val[1].size(0), bias_tensors)\n",
    "    return list(cons(input_size.size(1), shapes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(torch.utils.data.Dataset):\n",
    "  def __init__(self, x, y):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.x.shape[0]\n",
    "\n",
    "  def __getitem__(self, ind):\n",
    "    x = self.x[ind]\n",
    "    y = self.y[ind]\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The main training function (is run on driver nodes).\n",
    "def Train(net_params,x,y):\n",
    "#net_params: initial parameters of the feedforward network \n",
    "#x,y: training data\n",
    "\n",
    "  n_epochs=400\n",
    "  batchsize=10\n",
    "  \n",
    "  net=MLP.from_state_dict(net_params)\n",
    "  \n",
    "  train_data = DataSet(x, y)\n",
    "  dataloader = torch.utils.data.DataLoader(train_data, batch_size=batchsize)\n",
    "  \n",
    "  opt=optim.Adam(net.parameters())\n",
    "  loss=nn.CrossEntropyLoss()\n",
    "\n",
    "  for i in range(n_epochs):\n",
    "    for batch in dataloader:\n",
    "      \n",
    "      opt.zero_grad()\n",
    "      \n",
    "      xb,yb=batch\n",
    "      \n",
    "      yhat=net(xb)\n",
    "      err=loss(yhat,yb)\n",
    "      err.backward()\n",
    "  \n",
    "      opt.step()\n",
    "  \n",
    "  err=loss(net(x),y)\n",
    "  lossval=float(err.detach().numpy())\n",
    "  \n",
    "  #returns parameters of the trained network and loss\n",
    "  return (net.state_dict(),lossval)\n",
    "\n",
    "\n",
    "def Predict(net_params, x):\n",
    "  net = MLP.from_state_dict(net_params)\n",
    "  return net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
