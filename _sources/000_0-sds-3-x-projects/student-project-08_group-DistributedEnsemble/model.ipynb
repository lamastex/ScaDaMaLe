{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "\n",
    "#Feedforward network for classification\n",
    "class MLP(nn.Module):\n",
    "  \n",
    "  def __init__(self,shape):\n",
    "    #shape: number of neurons in each layer (including the input and output layers)\n",
    "    super(MLP,self).__init__()\n",
    "    \n",
    "    self.units=nn.ModuleList()\n",
    "    for i in range(len(shape)-1):\n",
    "      self.units.append(nn.Linear(shape[i],shape[i+1]))\n",
    "    \n",
    "    self._shape=shape\n",
    "    self._nlayers=len(shape)\n",
    "  \n",
    "  def forward(self,x):\n",
    "    \n",
    "    y=x\n",
    "    \n",
    "    for i,layer in enumerate(self.units):\n",
    "      if i<self._nlayers-2:\n",
    "        y=nn.functional.tanh(layer(y))\n",
    "      else:\n",
    "        y=nn.functional.softmax(layer(y),dim=1)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updates the model parameters with one step of stochastic gradient descent given a batch of labeled data\n",
    "def SGDStep(net_params,net_shape,x,y,lr=0.1):\n",
    "  \n",
    "  #x=torch.Tensor(x)\n",
    "  #y=torch.Tensor(y)\n",
    "  \n",
    "  net=MLP(net_shape)\n",
    "  net.load_state_dict(net_params)\n",
    "  \n",
    "  opt=optim.SGD(net.parameters(),lr)\n",
    "  opt.zero_grad()\n",
    "  loss=nn.CrossEntropyLoss()\n",
    "  \n",
    "  yhat=net(x)\n",
    "  err=loss(yhat,y)\n",
    "  err.backward()\n",
    "  \n",
    "  opt.step()\n",
    "  \n",
    "  lossval=float(err.detach().numpy())\n",
    "  \n",
    "  #returns updated parameters, network shape, and loss\n",
    "  return (net.state_dict(),net_shape,lossval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models=5 #ensemble size\n",
    "model_data=[] #pairs of model parameters and their training data\n",
    "shapes=[] #shape of networks\n",
    "inputdims=10 #features dimensions\n",
    "nclasses=2 #number of classes\n",
    "\n",
    "#initialization\n",
    "for i in range(n_models):\n",
    "  \n",
    "  #pick random number of hidden layers and neurons for each network\n",
    "  nhidden=random.randint(1,4)\n",
    "  shape=[inputdims]\n",
    "  for k in range(nhidden):\n",
    "    shape.append(random.randint(5,15))\n",
    "  shape.append(nclasses)\n",
    "  \n",
    "  net=MLP(shape)\n",
    "  shapes.append(shape)\n",
    "  \n",
    "  #-to be replaced with batch loader\n",
    "  x=torch.rand([10,inputdims])\n",
    "  y=torch.ones([10,]).long()\n",
    "  #-\n",
    "  \n",
    "  model_data.append((net.state_dict(),shape,x,y))\n",
    "  \n",
    "\n",
    "#main training loop\n",
    "numepochs=6\n",
    "for epoch in range(numepochs):\n",
    "  \n",
    "  model_data_par=sc.parallelize(model_data)\n",
    "\n",
    "  updated_models= model_data_par.map(lambda t: SGDStep(*t)) \n",
    "  \n",
    "  updated_models=updated_models.collect()\n",
    "  print(\"loss:\")\n",
    "  print([u[2] for u in updated_models])\n",
    "  \n",
    "  #loading batches of data, and reconstructing the model-data array\n",
    "  model_data=[]\n",
    "  for i in range(n_models):\n",
    "    #-to be replaced with batch loader\n",
    "    x=torch.rand([10,inputdims])\n",
    "    y=torch.ones([10,]).long()\n",
    "    #-\n",
    "    model_data.append((updated_models[i][0],shapes[i],x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     loss:\n",
    ">     [0.7414336800575256, 0.5925530195236206, 0.6060353517532349, 0.6709138751029968, 0.6918854117393494]\n",
    ">     loss:\n",
    ">     [0.708870530128479, 0.5514313578605652, 0.5812880396842957, 0.6448978781700134, 0.6799639463424683]\n",
    ">     loss:\n",
    ">     [0.6816596984863281, 0.5315981507301331, 0.5583221316337585, 0.6201316118240356, 0.6510061025619507]\n",
    ">     loss:\n",
    ">     [0.654758870601654, 0.5064709186553955, 0.5390928983688354, 0.597870945930481, 0.6252564191818237]\n",
    ">     loss:\n",
    ">     [0.631588876247406, 0.4890592098236084, 0.5212031602859497, 0.5759772658348083, 0.6056066751480103]\n",
    ">     loss:\n",
    ">     [0.6112779378890991, 0.4709406793117523, 0.5067512392997742, 0.5547378063201904, 0.5784258842468262]"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
