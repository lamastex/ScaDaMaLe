{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ScaDaMaLe project: Distributed ensembles\n",
    "----------------------------------------\n",
    "\n",
    "*Amanda Olmin, Amirhossein Ahmadian and Jakob Lindqvist*\n",
    "\n",
    "In this project, we create a distributed ensemble of neural networks\n",
    "that we can train and make predictions with in a distributed fashion,\n",
    "and we also apply this model to the out-of-distribution detection\n",
    "problem \\[2\\] (detecting inputs that are highly dissimilar from the\n",
    "training data).\n",
    "\n",
    "#### Why ensembles?\n",
    "\n",
    "Ensembles of neural networks - often have better predictive performance\n",
    "than single ensemble members \\[1\\] - have shown to provide reliable\n",
    "uncertainty estimates\n",
    "\n",
    "The latter quality is beneficial in itself but is especially useful when\n",
    "it comes to tasks such as out-of-distribution detection, where a modelâ€™s\n",
    "uncertainty estimates can be used to determine if a sample is\n",
    "in-distribution or not. We demonstrate this in the experiments below.\n",
    "\n",
    "#### Distributed ensembles\n",
    "\n",
    "In Spark, it is common to distribute *data* over several worker nodes.\n",
    "In this way, the same function is performed on several nodes on\n",
    "different parts of the data. The result from each node is then\n",
    "communicated and aggregated to a final function output. Similarily, we\n",
    "can train a neural network (a single ensemble member) in a distributed\n",
    "way by distributing the data that we use to train it. This can for\n",
    "example be done using the built-in MLP and MLPC classes in Pyspark\n",
    "\\[3\\]. However, this approach requires continuous communication between\n",
    "nodes to update model weights (possibly at every iteration) since every\n",
    "node keeps its own version of the model weights. The approach therefore\n",
    "scales badly as - the number of model parameters grow (more information\n",
    "to communicate between nodes) - when the complexity of the training\n",
    "algorithm increases, e.g. we wish to use a stochastic training algorithm\n",
    "\n",
    "In this regard, the communication becomes a bottleneck. Asynchronous\n",
    "updating can reduce the amount of communication, but might also hurt\n",
    "model performance \\[4\\].\n",
    "\n",
    "Considering that the ensemble members are independent models, they never\n",
    "need to communicate during the training phase. Hence, training ensemble\n",
    "members in a way that requires the otherwise independent training\n",
    "processes to integrate or synchronize, would cause unnecessary costs,\n",
    "for example since the training processes all need to communicate through\n",
    "the driver node. The same holds for prediction; no communication is\n",
    "needed between ensemble members except at the very end when the\n",
    "predictions are aggregated.\n",
    "\n",
    "To avoid unnecessary communication, we distribute the *ensemble members*\n",
    "and train them on separate worker nodes such that we - are able to train\n",
    "several ensemble members in parallell (limited by the number of nodes in\n",
    "our cluster) and independently - avoid communication between worker\n",
    "nodes\n",
    "\n",
    "To achieve this, we implement our own training processes below. In\n",
    "addition, we implement our own MLP class with the help of PyTorch. MLP\n",
    "objects and their training data are then distributed on worker nodes\n",
    "using Spark. This is not only to avoid distributing the training data\n",
    "over several nodes during training but also to package the ensemble\n",
    "members in a way that makes it possible for us to send them between the\n",
    "driver and the worker nodes prior to and at the end of training.\n",
    "\n",
    "&lt;img\n",
    "src=\"files/shared*uploads/amanda.olmin@liu.se/distributed*fig\\_small.png\"/&gt;\n",
    "\n",
    "Imports\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "import random\n",
    "from pathlib import Path\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.rdd import PipelinedRDD\n",
    "from toolz.itertoolz import partition_all\n",
    "from toolz.itertoolz import cons\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Data\n",
    "----\n",
    "\n",
    "We introduce the functions that we use to load the data for the\n",
    "experiments that we conduct. We split the available training data\n",
    "between ensemble members using sampling with or without replacement. The\n",
    "number of training data points that we can distribute to each ensemble\n",
    "member is only limited by the memory available to each worker node.\n",
    "\n",
    "**TOY DATA**\n",
    "\n",
    "We create a function for generating data consisting of Gaussian\n",
    "clusters. The function takes as input, user defined means and variances\n",
    "for each cluster in the data as well as the total number of observations\n",
    "and a vector of intended class proportions. It also comes with an option\n",
    "to split the final RDD into train and test sets.\n",
    "\n",
    "We will use this data later on to demonstrate our distributed ensembles\n",
    "framework as well as to generate out-of-distribution data for OOD\n",
    "detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gaussian_RDD(means, variances, num_observations, class_proportions, train_test_split=False):\n",
    "  \"\"\"Create toy Gaussian classification data\n",
    "  Let C := number of clusters/classes and P := number of data features\n",
    "  \n",
    "  Args: \n",
    "    means (np.array[float]): mean vector of shape (C, P)\n",
    "    variances (np.array[float]): vector of variances, shape (C, P)\n",
    "    num_observations (scalar[int]): the total number of observations in the final data set\n",
    "    class_proportions (np.array[float]): vector of class proportions, length C\n",
    "    train_test_split: whether to split the data into train/test sets or not\n",
    "    \n",
    "  Returns:\n",
    "    Gaussian data, RDD of tuples (list(features), int(label))\n",
    "  \"\"\"\n",
    "  \n",
    "  assert means.shape[0] == variances.shape[0]\n",
    "  assert means.shape[1] == variances.shape[1]\n",
    "  assert class_proportions.sum() == 1\n",
    "    \n",
    "  num_classes = means.shape[0]\n",
    "  num_features = means.shape[1]\n",
    "  \n",
    "  data_rdd = sc.emptyRDD() \n",
    "  for k in range(num_classes):\n",
    "    \n",
    "    # Generate standard normal data\n",
    "    class_size = int(num_observations * class_proportions[k])\n",
    "    class_rdd = RandomRDDs.normalVectorRDD(sc, numRows=class_size, numCols=num_features, numPartitions=1) #, seed=123)\n",
    "\n",
    "    # Map to true distribution\n",
    "    class_rdd_transformed = class_rdd.map(lambda v: means[k, :] + (variances[k, :]**0.5) * v)\n",
    "    \n",
    "    # Add labels\n",
    "    class_rdd_w_label = class_rdd_transformed.map(lambda v: (v, k)) \n",
    "    \n",
    "    data_rdd = data_rdd.union(class_rdd_w_label)\n",
    "    \n",
    "  # We will shuffle and repartition the data\n",
    "  num_partitions = 10\n",
    "  shuffled_rdd =  data_rdd.sortBy(lambda v: randrange(num_observations)).repartition(num_partitions)\n",
    "  final_rdd = shuffled_rdd.map(tuple).map(lambda v: (list(v[0]), int(v[1])))\n",
    "  \n",
    "  if train_test_split:\n",
    "    train_rdd, test_rdd = final_rdd.randomSplit(weights=[0.8, 0.2], seed=12)\n",
    "    final_rdd = (train_rdd, test_rdd)\n",
    "    \n",
    "  return final_rdd                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "**FIRE WALL DATA**\n",
    "\n",
    "We will also consider some real data. The dataset that we will use\n",
    "consits of traffic from a firewall tracking record. We have accessed it\n",
    "through the UCI Machine Learning repository \\[4\\]:\n",
    "https://archive.ics.uci.edu/ml/datasets/Internet+Firewall+Data.\n",
    "\n",
    "-   Number of data points: 65,532.\n",
    "\n",
    "-   Number of features: 11 (all numerical).\n",
    "\n",
    "-   Number of classes: 4 (allow/deny/drop/reset both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_firewall_data(train_test_split=False,file_location=\"/FileStore/shared_uploads/amanda.olmin@liu.se/fire_wall_data.csv\"):\n",
    "  \"\"\"Load and preprocess firewall data\n",
    "  Args:\n",
    "     file_location: file location from which to load the data\n",
    "     train_test_split: whether to split the data into train/test sets or not\n",
    "  \n",
    "  Returns:\n",
    "     Firewall data, RDD of tuples (list(features), int(label))\n",
    "  \"\"\"\n",
    "  \n",
    "  # File location and type\n",
    "  # file_location = \"/FileStore/shared_uploads/amanda.olmin@liu.se/fire_wall_data.csv\" \n",
    "  file_type = \"csv\"\n",
    "\n",
    "  # CSV options\n",
    "  infer_schema = \"true\"\n",
    "  first_row_is_header = \"true\"\n",
    "  delimiter = \",\"\n",
    "\n",
    "  # Load the data from file\n",
    "  df = spark.read.format(file_type) \\\n",
    "      .option(\"inferSchema\", infer_schema) \\\n",
    "      .option(\"header\", first_row_is_header) \\\n",
    "      .option(\"sep\", delimiter) \\\n",
    "      .load(file_location)\n",
    "  \n",
    "  # Preprocess data\n",
    "  col_num = [\"Source Port\", \"Destination Port\", \"NAT Source Port\", \"NAT Destination Port\", \"Bytes\", \"Bytes Sent\", \"Bytes Received\", \"Packets\", \"Elapsed Time (sec)\", \"pkts_sent\", \"pkts_received\"]\n",
    "\n",
    "  # Index qualitative variable\n",
    "  indexer = StringIndexer(inputCol = \"Action\", outputCol = \"label\")\n",
    "\n",
    "  # Scale numerical features\n",
    "  va = VectorAssembler(inputCols = col_num, outputCol = \"numerical_features\") \n",
    "  scaler = StandardScaler(inputCol = \"numerical_features\", outputCol = \"features\")\n",
    "  \n",
    "  # Apply pipeline \n",
    "  pipeline = Pipeline(stages=[indexer, va, scaler])\n",
    "  final_df = pipeline.fit(df).transform(df).select(\"features\", \"label\") \n",
    "  \n",
    "  # Convert to RDD\n",
    "  final_rdd = final_df.rdd.map(tuple).map(lambda v: (list(v[0]), int(v[1])))\n",
    "  \n",
    "  if train_test_split:\n",
    "    train_rdd, test_rdd = final_rdd.randomSplit(weights=[0.8, 0.2], seed=12)\n",
    "    final_rdd = (train_rdd, test_rdd)\n",
    "\n",
    "  return final_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "\\*\\* RDD partition \\*\\*\n",
    "\n",
    "Below, we provide a function that partitions an RDD. We will use it to\n",
    "distribute data between ensemble members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partitioned_rdd(input_rdd, partition_size=1000):\n",
    "  \"\"\"Partition RDD \n",
    "  \n",
    "  Args:\n",
    "    input_rdd: RDD to be partitioned\n",
    "  \n",
    "  Returns:\n",
    "    Partitioned RDD\n",
    "  \"\"\"\n",
    "  \n",
    "  return input_rdd.mapPartitions(lambda partition: partition_all(partition_size, partition))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Distributed ensemble of neural networks\n",
    "---------------------------------------\n",
    "\n",
    "**PyTorch Model**\n",
    "\n",
    "To implement the ensemble members, we first write an ordinary\n",
    "feedforward (MLP) neural network class using PyTorch, which has a\n",
    "Softmax output and Tanh activation functions. The number of layers and\n",
    "neurons in each layer is passed as an argument to the constructor of\n",
    "this class. Moreover, any instance of this network class (parameters and\n",
    "structure) can be easily stored in and loaded from a state dictionary\n",
    "(state\\_dict) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feedforward network for classification\n",
    "class MLP(nn.Module):\n",
    "  \n",
    "  def __init__(self,shape):\n",
    "    #shape: number of neurons in each layer (including the input and output layers)\n",
    "    super(MLP,self).__init__()\n",
    "    \n",
    "    self.units=nn.ModuleList()\n",
    "    for i in range(len(shape)-1):\n",
    "      self.units.append(nn.Linear(shape[i],shape[i+1]))\n",
    "    \n",
    "    self._shape=shape\n",
    "    self._nlayers=len(shape)\n",
    "  \n",
    "  def forward(self,x):\n",
    "    \n",
    "    y=x\n",
    "    \n",
    "    for i,layer in enumerate(self.units):\n",
    "      if i<self._nlayers-2:\n",
    "        y=nn.functional.tanh(layer(y))\n",
    "      else:\n",
    "        y=nn.functional.softmax(layer(y),dim=1)\n",
    "    \n",
    "    return y\n",
    "  \n",
    "  #constructing an instance of this class based on a state dictionary (network parameters)\n",
    "  @staticmethod\n",
    "  def from_state_dict(state_dict):\n",
    "    net_shape = MLP.shape_from_state_dict(state_dict)\n",
    "    net=MLP(net_shape)\n",
    "    net.load_state_dict(state_dict)\n",
    "    return net\n",
    "\n",
    "  @staticmethod\n",
    "  def shape_from_state_dict(state_dict):\n",
    "    \"\"\"Infer MLP layer shapes from state_dict\"\"\"\n",
    "    iter_ = iter(state_dict.items())\n",
    "    _, input_size = next(iter_)\n",
    "    bias_tensors = filter(lambda key_val: key_val[0].find(\"bias\") != -1, iter_)\n",
    "    shapes = map(lambda key_val: key_val[1].size(0), bias_tensors)\n",
    "    return list(cons(input_size.size(1), shapes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "**Functions for training and testing networks**\n",
    "\n",
    "Here we have some functions that are used to train/test each individual\n",
    "network in the ensemble. The *Train* function takes the initial weights\n",
    "of a network, trains it on a set of input-taraget data based on\n",
    "stochastic gradient optimization and cross-entropy loss, and returns the\n",
    "state dictionary of the trained network. PyTorch's backpropagation and\n",
    "optimization tools are used to implement this function as usual. The\n",
    "*Predict* function simply takes the state dictionary corresponding to a\n",
    "network as well as a data point (or batch of data), and returns the\n",
    "output (probabilities) of the network at that point.\n",
    "\n",
    "We note that Spark can automatically distribute these functions on the\n",
    "nodes, and thus writing them for a distributed ensemble is not basically\n",
    "different from a local setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility class for pytorch data loader\n",
    "class DataSet(torch.utils.data.Dataset):\n",
    "  def __init__(self, x, y):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.x.shape[0]\n",
    "\n",
    "  def __getitem__(self, ind):\n",
    "    x = self.x[ind]\n",
    "    y = self.y[ind]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "#The main training function (is run on worker nodes)\n",
    "def Train(net_params,x,y):\n",
    "  #net_params: initial parameters of the feedforward network (state dictionary) \n",
    "  #x,y: training data (pytorch tensors)\n",
    "\n",
    "  n_epochs=100\n",
    "  batchsize=10\n",
    "  \n",
    "  net=MLP.from_state_dict(net_params)\n",
    "  \n",
    "  train_data = DataSet(x, y)\n",
    "  dataloader = torch.utils.data.DataLoader(train_data, batch_size=batchsize)\n",
    "  \n",
    "  opt=optim.Adam(net.parameters())\n",
    "  loss=nn.CrossEntropyLoss()\n",
    "\n",
    "  for i in range(n_epochs):\n",
    "    for batch in dataloader:\n",
    "      \n",
    "      opt.zero_grad()\n",
    "      \n",
    "      xb,yb=batch\n",
    "      \n",
    "      yhat=net(xb)\n",
    "      err=loss(yhat,yb)\n",
    "      err.backward()\n",
    "  \n",
    "      opt.step()\n",
    "  \n",
    "  err=loss(net(x),y)\n",
    "  lossval=float(err.detach().numpy())\n",
    "  \n",
    "  #returns parameters of the trained network and loss\n",
    "  return (net.state_dict(),lossval)\n",
    "\n",
    "#Get the output of a feedforward network given an input tensor\n",
    "def Predict(net_params, x):\n",
    "  #net_params: parameters (state dictionary) of the network\n",
    "  #x: input (pytorch tensor)\n",
    "  net = MLP.from_state_dict(net_params)\n",
    "  net.eval()\n",
    "  return net(x)\n",
    "\n",
    "#Reshaping and converting the tuples stored in a dataset RDD into input and target tensors\n",
    "def Totensor(d):\n",
    "  #d: the dataset (list of tuples)\n",
    "  \n",
    "  x=[v[0] for v in d]\n",
    "  y=[v[1] for v in d]\n",
    "  x=torch.tensor(x,dtype=torch.float)\n",
    "  y=torch.tensor(y,dtype=torch.long)\n",
    "  \n",
    "  return (x,y)\n",
    "\n",
    "def make_prediction(state_dict, x):\n",
    "  print(state_dict)\n",
    "  return Predict(state_dict, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "**Creating an ensemble of networks, and training them in parallel**\n",
    "\n",
    "We now use the class and functions defined above to create an ensemble\n",
    "of feedforward neural networks, and train it in a distributed fashion,\n",
    "where each network is trained on a single worker independently from the\n",
    "other ones. Firstly, several networks are initialized using the MLP\n",
    "class with a random number of hidden layers and neurons, and random\n",
    "initial weights. Using randomness helps to increase the diversity in the\n",
    "ensemble (without which, the outputs of ensemble members could get\n",
    "correlated with each other).\n",
    "\n",
    "As mentioned before, the training data is partioned into equal size\n",
    "parts, and each of the networks in the ensemble is assigned one part.\n",
    "Since the dataset is assumed to be an RDD (to let it be huge), an\n",
    "iterator object is needed which collects one part of the data RDD\n",
    "(transfers it from the cloud to the driver node) in each call. Note that\n",
    "we here implicitly assume that each part of the data (but not the whole\n",
    "dataset) fits into the memory of a single machine.\n",
    "\n",
    "After constructing the network object and loading data for each member\n",
    "of the ensemble, the state dictionary of the network and its\n",
    "corresponding training data are packed into a tuple, and appended to a\n",
    "list. The list of state\\_dict/data tuples is then parallelized to obtain\n",
    "an Spark RDD. We found out that it is difficult to directly put the\n",
    "PyTorch neural network objects in an RDD, apparently becasue Spark does\n",
    "not know by default how to encode these objects and transfer them\n",
    "between nodes. Therefore, we use the state dictionary instead, which\n",
    "contains all the necessary information about a network.\n",
    "\n",
    "Finally, the network training function (*Train* defined above) is\n",
    "applied to each element of the model/data RDD, in the form of a *map*\n",
    "operation. This tells Spark to run the function on each element in\n",
    "parallel (on worker machines) independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(n_models, inputdims, nclasses, max_layers, min_neurons, max_neurons, data_iterator):\n",
    "  \"\"\"Constructing and training a distributed ensemble of feedforward networks\n",
    "  \n",
    "    Args:\n",
    "      n_models: number of the ensemble memebrs\n",
    "      inputdims: number of features dimesnions\n",
    "      nclasses: number of the classes\n",
    "      max_layers: maximum allowed number of hidden layers for the networks\n",
    "      min_neurons,max_neurons: the valid range for the number of neurons in each hidden layer\n",
    "      data_iterator: a Python iterator over the parts of the training data (one part per each member of the ensemble)\n",
    "\n",
    "    Returns: a list of state dictionaries of the trained networks\n",
    "  \"\"\"\n",
    "  \n",
    "  # initialization\n",
    "  model_data=[] # pairs of model parameters and their training data\n",
    "  for i in range(n_models):\n",
    "    # pick random number of hidden layers and neurons for each network\n",
    "    nhidden=random.randint(1, max_layers)\n",
    "    shape=[inputdims]\n",
    "    for k in range(nhidden):\n",
    "      shape.append(random.randint(min_neurons, max_neurons))\n",
    "    shape.append(nclasses)\n",
    "    \n",
    "    net=MLP(shape)\n",
    "    \n",
    "    #fetch the next part of data\n",
    "    d=next(data_iterator)\n",
    "    x=d[0]\n",
    "    y=d[1]\n",
    "  \n",
    "    model_data.append((net.state_dict(),x,y))\n",
    "\n",
    "  # distribute the array\n",
    "  model_data_par= sc.parallelize(model_data)\n",
    "  # execute the train function on the worker nodes\n",
    "  models_trained = model_data_par.map(lambda t: Train(*t))\n",
    "  \n",
    "  #transfer the trained models and loss values to the driver\n",
    "  models_trained.collect()\n",
    "  \n",
    "  #print the training loss values\n",
    "  print(\"training losses:\")\n",
    "  print([v[1] for v in models_trained])\n",
    "\n",
    "  # return the state dicts\n",
    "  return [v[0] for v in models_trained]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "\\*\\* Utility functions for saving and loading the ensemble model from\n",
    "the disk \\*\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models_distr(models, dir_, model_names=None):\n",
    "  dir_ = Path(dir_)\n",
    "  dir_.mkdir(exist_ok=True, parents=True)\n",
    "  \n",
    "  if model_names is None:\n",
    "    model_names = [f\"m{idx}.pt\" for idx in range(0, models.count())]\n",
    "  assert len(model_names) == models.count()\n",
    "  model_paths = [dir_ / model_name for model_name in model_names]\n",
    "  model_paths = sc.parallelize(model_paths)\n",
    "  models.zip(model_paths).foreach(lambda dict_and_path: torch.save(*dict_and_path))\n",
    "\n",
    "def save_models(models, dir_, model_names=None):\n",
    "  dir_ = Path(dir_)\n",
    "  dir_.mkdir(exist_ok=True, parents=True)\n",
    "  \n",
    "  if model_names is None:\n",
    "    model_names = [f\"m{idx}.pt\" for idx in range(0, len(models))]\n",
    "  assert len(model_names) == len(models)\n",
    "  model_paths = [dir_ / model_name for model_name in model_names]\n",
    "  for state_dict, path in zip(models, model_paths):\n",
    "    torch.save(state_dict, path)\n",
    "  \n",
    "def load_models(model_names, dir_):\n",
    "  dir_ = Path(dir_)\n",
    "  model_paths = [dir_ / model_name for model_name in model_names]\n",
    "  state_dicts = [torch.load(path) for path in model_paths]\n",
    "  return sc.parallelize(state_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Distributed ensembles prediction API\n",
    "====================================\n",
    "\n",
    "From the training process we get a distributed iterator `models` over\n",
    "the trained models. (NB. the `train_ensemble` function actually collects\n",
    "the trained models for convenience.) Internally this is an iterator over\n",
    "`torch.state_dicts` holding the param's of each model respectively.\n",
    "\n",
    "There are different ways in which we can do predictions:\n",
    "\n",
    "-   Distributed predictions with `ens_preds(models, test_x)`, which maps\n",
    "    the combined model and test data to predictions for each data point.\n",
    "    This iterator can be collected to a list of the predictions for each\n",
    "    ensemble member, or further processed in a distributed and\n",
    "    functional manner. This is the most flexible variant since it\n",
    "    preserves the prediction of every member on every datapoint. It is\n",
    "    also the most expensive (if we do collect all the data).\n",
    "\n",
    "-   Reduced/aggregated predictions with\n",
    "    `ens_preds_reduced(models, test_x, red_fn)`. Working with an\n",
    "    ensemble, we are often concerned with some aggregate of the members'\n",
    "    predictions, eg., the average prediction. For this we provide an\n",
    "    reducing version of `ens_preds` where the user need only supply the\n",
    "    reduce function `red_fn`, describing how to combine the predictions\n",
    "    of two ensemble members. For instance, if you would like to get the\n",
    "    average probability vector of a classifier ensemble for every data\n",
    "    point you would use:\n",
    "    `python   avg_prob_vecs = ens_preds_reduced(models, x, lambda x, y: (x+y)/2)`\n",
    "    Internally, this simply calls `.reduce(red_fn)` on the iterator\n",
    "    returned from `ens_preds`. This is merely a convenience function.\n",
    "\n",
    "-   Metrics per ensemble member. If the number of test samples is large,\n",
    "    we will collect a lot of predictions over the cluster. If we know\n",
    "    that we only want an aggregate metric for each member across the\n",
    "    whole test data, we use the `ens_metrics` method for aggregation on\n",
    "    the worker nodes.\n",
    "    `python   avg_acc_per_member = ens_metrics(models, test_input, test_true_labels, <list of metric functions>)`\n",
    "    Note that each metric function must be on the form: f: R^(N x D\\_x)\n",
    "    x R^(N) --&gt; T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ens_preds(models, test_x):\n",
    "  \"\"\"Distributed ensemble predictions\n",
    "  Takes a set of models and test data and makes distributed predictions\n",
    "  Let N := number of data points and D_x := the dimension of a single datapoint x\n",
    "  \n",
    "  Args:\n",
    "    models (list[state_dict]): set of models represented as a list (state_dict, shape)\n",
    "    test_x (torch.Tensor): Tensor of size (N, D_x)\n",
    "  \n",
    "  Returns:\n",
    "    Distributed iterator over the predictions. E.g. an iterator over probability vectors in the case of a classifier ens.\n",
    "  \"\"\"\n",
    "  pred_iter = _pred_models_iter(models, test_x)\n",
    "  return pred_iter.map(lambda t: Predict(*t))\n",
    "\n",
    "def ens_preds_reduced(models, test_x, red_fn):\n",
    "  \"\"\"Reduced/aggregated ensemble predictions\n",
    "  Takes a set of models and test data and makes distributed predictions and reduces them with a provided `red_fn`\n",
    "  Let N := number of data points and D_x := the dimension of a single datapoint x\n",
    "  \n",
    "  Args:\n",
    "    models (list[state_dict]): set of models represented as a list (state_dict, shape)\n",
    "    test_x (torch.Tensor): Tensor of size (N, D_x)\n",
    "    red_fn function: f: R^D_x x R^D_x --> R^D_x\n",
    "  \n",
    "  Returns:\n",
    "    Single reduced/aggregated prediction of the whole ensemble\n",
    "  \"\"\"\n",
    "  return ens_preds(models, test_x).reduce(red_fn)\n",
    "\n",
    "def ens_metrics(models, test_x, test_y, metrics):\n",
    "  \"\"\"Distributed ensemble metrics\n",
    "  Takes a set of models and test data, predicts probability vectors and calculates the provided metrics\n",
    "  given true labels `test_y`\n",
    "  Let N := number of data points and D_x := the dimension of a single datapoint x\n",
    "  \n",
    "  Args:\n",
    "    models (list[state_dict]): set of models represented as a list (state_dict, shape)\n",
    "    test_x (torch.Tensor): Tensor of size (N, D_x)\n",
    "    test_y (torch.Tensor): Tensor of size (N). NB: hard labels\n",
    "    metrics (list[functions]): List of functions where each funcion f: R^(N x D_x) x R^(N) --> T, where T is a generic output type.\n",
    "  \"\"\"\n",
    "  return ens_preds(models, test_x).map(lambda prob_vecs: [metric(prob_vecs, test_y) for metric in metrics])\n",
    "\n",
    "\n",
    "def _pred_models_iter(models, test_x):\n",
    "  \"\"\"Helper function to generate a distributed iterator over models and test data\n",
    "  NB: the same `test_x` is given to all elements in the iterator\n",
    "  \n",
    "  Args:\n",
    "    models (list[state_dict]): set of models represented as a list (state_dict, shape)\n",
    "    test_x (torch.Tensor): Tensor of size (N, D_x)\n",
    "  \"\"\"\n",
    "  if isinstance(models, PipelinedRDD):\n",
    "    return models.map(lambda model: (model, test_x))\n",
    "  elif isinstance(models, list):\n",
    "    models_and_data = [(params, test_x) for params in models]\n",
    "    return sc.parallelize(models_and_data)\n",
    "  else:\n",
    "    raise TypeError(\"'models' must be an RDD or a list\")\n",
    "\n",
    "def avg_accuracy(prob_vecs, labels):\n",
    "  \"\"\"Example metrics function: average accuracy\n",
    "  Let N := number of data points and C := the number of classes\n",
    "  \n",
    "  Args:\n",
    "    prob_vecs (torch.Tensor): Tensor of size (N, C)\n",
    "    labels (torch.Tensor): Tensor of size (N), hard labels, with classes corresponding to indices 0, ..., C-1\n",
    "  \n",
    "  Returns:\n",
    "    torch.Tensor: Tensor of size (N), average accuracy over all datapoints.\n",
    "  \"\"\"\n",
    "  hard_preds = torch.argmax(prob_vecs, 1)\n",
    "  return (hard_preds == labels).float().mean()\n",
    "\n",
    "def entropy(prob_vecs):\n",
    "  return - (prob_vecs * torch.log(prob_vecs)).sum(1)\n",
    "\n",
    "def avg_entropy(prob_vec_1, prob_vec_2):\n",
    "  e_1 = entropy(prob_vec_1)\n",
    "  e_2 = entropy(prob_vec_2)\n",
    "  return (e_1 + e_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Application example: Distributed predictions\n",
    "============================================\n",
    "\n",
    "Let's first demonstrate our distributed ensembles with a simple toy\n",
    "example. We'll create gaussian toy data with three slightly overlapping\n",
    "clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.array([(0, 0), (1,0), (1, 1)])\n",
    "variances = 0.1 * np.ones((3, 2))\n",
    "num_observations = 5000\n",
    "class_proportions = np.array([1/3, 1/3, 1/3])\n",
    "data_train, data_test = create_gaussian_RDD(means, variances, num_observations, class_proportions, train_test_split=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Now we'll create and distributedly train a classifier ensemble and save\n",
    "it to file. This is not necessary, we can -- in fact -- make predictions\n",
    "with the trained ensemble without ever collecting it from the worker\n",
    "nodes, but in most use cases it will be convenient to save the ensemble\n",
    "on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator=get_partitioned_rdd(data_train).map(Totensor).toLocalIterator()\n",
    "n_models=5 # ensemble size\n",
    "inputdims=2 # features dimensions\n",
    "nclasses=3 # number of classes\n",
    "max_layers=2\n",
    "min_neurons=2\n",
    "max_neurons=5\n",
    "\n",
    "models_trained = train_ensemble(n_models, inputdims, nclasses, max_layers, min_neurons, max_neurons, data_iterator)\n",
    "saved_models_dir = Path(\"saved_models/gaussian\")\n",
    "save_models(models_trained, saved_models_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     training losses:\n",
    ">     [0.6217136383056641, 0.6165631413459778, 0.6497721076011658, 0.6303632259368896, 0.6337271928787231]\n",
    "\n",
    "  \n",
    "\n",
    "Making distributed predictions\n",
    "------------------------------\n",
    "\n",
    "With the trained ensemble we can make predictions and calculate metrics,\n",
    "all in a distributed manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xx, test_yy = Totensor(data_test.collect())\n",
    "model_names = [f\"m{idx}.pt\" for idx in range(n_models)]\n",
    "models = load_models(model_names, saved_models_dir).collect()\n",
    "avg_prob_vecs = ens_preds_reduced(models, test_xx, lambda x, y: (x+y)/2) # (A single) Average prob. vec for all data points.\n",
    "avg_acc = ens_metrics(models, test_xx, test_yy, [avg_accuracy]).collect() # Average acc. for each ens. over all data points\n",
    "\n",
    "print(f\"Average accuracy for each ensemble member: {[acc[0].item() for acc in avg_acc]}\")\n",
    "print(f\"Average accuracy for the whole ensemble: {avg_accuracy(avg_prob_vecs, test_yy).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Average accuracy for each ensemble member: [0.9213930368423462, 0.9283581972122192, 0.937313437461853, 0.9363183975219727, 0.9313432574272156]\n",
    ">     Average accuracy for the whole ensemble: 0.9343283772468567\n",
    "\n",
    "  \n",
    "\n",
    "We can also make use of the uncertainty description provided by the\n",
    "ensemble. We'll plot the test data, each point coloured the predicted\n",
    "distribution, which will illustrate the certain predictions with\n",
    "distinct colur and uncertain with muddied colours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = avg_prob_vecs.detach().numpy()\n",
    "hard_preds = avg_prob_vecs.argmax(1).detach().numpy()\n",
    "every_nth = 5\n",
    "train_xx, train_yy = Totensor(data_train.collect())\n",
    "\n",
    "(fig, (ax_1, ax_2)) = plt.subplots(1, 2)\n",
    "# For the train data we use the true labels to simulate a completely certain prediction.\n",
    "color_map = {0: [1, 0 ,0], 1: [0, 1, 0], 2: [0, 0, 1]}\n",
    "ax_1.scatter(train_xx[:, 0], train_xx[:, 1], c=[color_map[class_.item()] for class_ in train_yy], label=\"Train\")\n",
    "ax_2.scatter(test_xx[::every_nth, 0], test_xx[::every_nth, 1], c=preds[::every_nth], label=\"Test\")\n",
    "ax_1.set_title(\"Train\")\n",
    "ax_2.set_title(\"Test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Application example: Out of distribution detection\n",
    "==================================================\n",
    "\n",
    "Our distributed ensemble can be used for out of distribution (OOD)\n",
    "detection. A simple way is to measure the entropy of the combined\n",
    "ensemble prediction; high entropy signals weird data, not seen in the\n",
    "training distribution.\n",
    "\n",
    "\"Real world\" out of distribution data can be hard to come by, but a\n",
    "typical example would be images in different contexts. E.g. scenic\n",
    "vistas or pathology scans may share the same feature space but have very\n",
    "different distribution. For the data we have collected, no such OOD set\n",
    "exists, so we will showcase it with an OOD set of gaussian noise. Of\n",
    "course, noise that is very far from the in distribution (ID) data will\n",
    "saturate the classifiers softmax for one element, actually yielding very\n",
    "confident, low entropy, nonsense predictions.\n",
    "\n",
    "Regardless, let's see how to do this with the distributed ensemble.\n",
    "First, we train it and again, save the trained parameters to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = load_firewall_data(True)\n",
    "data_iterator=get_partitioned_rdd(data_train).map(Totensor).toLocalIterator()\n",
    "\n",
    "n_models=10\n",
    "models_trained=train_ensemble(n_models,\n",
    "                              inputdims=11,\n",
    "                              nclasses=4,\n",
    "                              max_layers=4,\n",
    "                              min_neurons=5,\n",
    "                              max_neurons=15,\n",
    "                              data_iterator=data_iterator) \n",
    "saved_models_dir = Path(\"saved_models/firewall\")\n",
    "save_models(models_trained, saved_models_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     training losses:\n",
    ">     [0.7487162947654724, 0.7488684058189392, 0.7517361044883728, 0.7516648173332214, 0.752688467502594, 0.7555721998214722, 0.751698911190033, 0.7477037906646729, 0.7496815919876099, 0.7518306374549866]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ood_data(test_x, num_samples):\n",
    "  num_test_samples, dim_x = test_x.size()\n",
    "  random_mean = np.random.rand(dim_x).reshape(1, dim_x)\n",
    "  random_cov = np.random.rand(dim_x).reshape(1, dim_x) * 10\n",
    "  ood_x, _ = Totensor(create_gaussian_RDD(random_mean, random_cov, num_test_samples, np.array([1.0]), train_test_split=False).collect())\n",
    "  return ood_x\n",
    "\n",
    "  \n",
    "data = data_test.collect()\n",
    "batch_size = -1\n",
    "batch = data[0:batch_size]\n",
    "test_xx, test_yy = Totensor(batch)\n",
    "ood_x = gen_ood_data(test_xx, batch_size)\n",
    "\n",
    "models_p = load_models(model_names, saved_models_dir).collect()\n",
    "\n",
    "# We can either calculate the average entropy of the ensemble members\n",
    "avg_entropy_id = ens_preds(models_p, test_xx).map(entropy).reduce(lambda x, y: (x+y)/2).detach().numpy()\n",
    "avg_entropy_ood = ens_preds(models_p, ood_x).map(entropy).reduce(lambda x, y: (x+y)/2).detach().numpy()\n",
    "\n",
    "# ... or we the entropy of the average ensemble prediction.\n",
    "entropy_avg_id = entropy(ens_preds_reduced(models_p, test_xx, lambda x, y: (x+y)/2)).detach().numpy()\n",
    "entropy_avg_ood = entropy(ens_preds_reduced(models_p, ood_x, lambda x, y: (x+y)/2)).detach().numpy()\n",
    "\n",
    "# Set entropy measure\n",
    "entropy_id = avg_entropy_id\n",
    "entropy_ood = avg_entropy_ood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_hist(id_, ood, n_bins, upper_x_bound):\n",
    "  (fig, (ax_1, ax_2)) = plt.subplots(2, 1)\n",
    "  _plot_hist(ax_1, id_, n_bins, \"ID\", \"b\", upper_x_bound)\n",
    "  _plot_hist(ax_2, ood, n_bins, \"OOD\", \"r\", upper_x_bound)\n",
    "  fig.suptitle(\"Entropy histogram\")\n",
    "  ax_2.set_xlabel(\"entropy\")\n",
    "  plt.show()\n",
    "\n",
    "def _plot_hist(ax, counts, n_bins, label, color, upper_x_bound):\n",
    "    ax.hist(counts, bins=n_bins, label=label, color=color, density=True)\n",
    "    ax.set_xbound(lower = 0.0, upper = upper_x_bound)\n",
    "    ax.set_ylabel(\"rel freq\")\n",
    "    ax.legend()\n",
    "  \n",
    "n_bins = 100\n",
    "entropy_bound = 0.15\n",
    "entropy_hist(entropy_id, entropy_ood, n_bins, entropy_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ood(entropies, cut_off_entropy):\n",
    "  return entropies > cut_off_entropy\n",
    "\n",
    "def fpr_and_tpr(id_, ood, res):\n",
    "  max_entropy = max(id_.max(), ood.max())\n",
    "  # max_entropy = id_.max()\n",
    "  thresholds = np.arange(0.0, max_entropy, max_entropy / res)\n",
    "  roc = np.array([(fpr(id_, th), tpr(ood, th)) for th in thresholds])\n",
    "  roc = roc[roc[:,0].argsort()]\n",
    "  fprs, tprs = (roc[:, 0], roc[:, 1])\n",
    "  return fprs, tprs\n",
    "\n",
    "def fpr(id_, th):\n",
    "  id_pred = is_ood(id_, th)\n",
    "  fp = id_pred.sum()\n",
    "  tn = id_pred.shape[0] - fp\n",
    "  return fp / (tn + fp)\n",
    "\n",
    "def tpr(ood, th):\n",
    "  ood_pred = is_ood(ood, th)\n",
    "  tp = ood_pred.sum()\n",
    "  fn = ood_pred.shape[0] - tp\n",
    "  return tp / (tp + fn)\n",
    "\n",
    "fpr, tpr = fpr_and_tpr(avg_entropy_id, avg_entropy_ood, res = 100)\n",
    "(fig, ax) = plt.subplots()\n",
    "\n",
    "ax.plot(fpr, tpr)\n",
    "ax.set_xlabel(\"FPR\")\n",
    "ax.set_ylabel(\"TPR\")\n",
    "ax.set_title(\"ROC\")\n",
    "print(f\"AUROC: {np.trapz(tpr, fpr)}\") # Not printing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AUROC: {np.trapz(tpr, fpr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     AUROC: 0.5802254130886811\n",
    "\n",
    "  \n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "\\[1\\] Lakshminarayanan, B., Pritzel, A., & Blundell, C. (2017). Simple\n",
    "and scalable predictive uncertainty estimation using deep ensembles. In\n",
    "Advances in neural information processing systems (pp. 6402-6413).\n",
    "\n",
    "\\[2\\] Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin,\n",
    "S., ... & Snoek, J. (2019). Can you trust your model's uncertainty?\n",
    "Evaluating predictive uncertainty under dataset shift. In Advances in\n",
    "Neural Information Processing Systems (pp. 13991-14002).\n",
    "\n",
    "\\[3\\] Apache Spark. (2021, 01, 11). Classification and Regression\n",
    "\\[https://spark.apache.org/docs/latest/ml-classification-regression.html\\].\n",
    "\n",
    "\\[4\\] Chen, J., Pan, X., Monga, R., Bengio, S., & Jozefowicz, R. (2016).\n",
    "Revisiting distributed synchronous SGD. arXiv preprint arXiv:1604.00981.\n",
    "\n",
    "\\[5\\] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository\n",
    "\\[http://archive.ics.uci.edu/ml\\]. Irvine, CA: University of California,\n",
    "School of Information and Computer Science."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
