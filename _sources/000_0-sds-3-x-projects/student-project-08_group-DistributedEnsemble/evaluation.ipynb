{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Amir's Train function\n",
    "def predict(net_params, net_shape, test_x):\n",
    "  \"\"\"Make predictions with the MLP net\n",
    "  Let N := number of data points and D_x := the dimension of a single datapoint x\n",
    "  \n",
    "  Args: \n",
    "    net_params (OrderedDict): Trained parameters as a state dict\n",
    "    net_shape (list[int]): layer sizes\n",
    "    test_x (torch.Tensor): Test data, tensor of shape (N, D_x)\n",
    "  \n",
    "  Returns:\n",
    "    predictions (torch.Tensor)\n",
    "  \"\"\"\n",
    "  net=MLP(net_shape)\n",
    "  net.load_state_dict(net_params)\n",
    "  net.eval()\n",
    "  \n",
    "  # I suggest adding this convenience function into the MLP class\n",
    "  #net = MLP.from_state_dict(net_params, net_shape)\n",
    "  return net(test_x)\n",
    "\n",
    "def ens_preds(models, test_x):\n",
    "  \"\"\"Distributed ensemble predictions\n",
    "  Takes a set of models and test data and makes distributed predictions\n",
    "  Let N := number of data points and D_x := the dimension of a single datapoint x\n",
    "  \n",
    "  Args:\n",
    "    models (list[state_dict, shape]): set of models represented as a list (state_dict, shape)\n",
    "    test_x (torch.Tensor): Tensor of size (N, D_x)\n",
    "  \n",
    "  Returns:\n",
    "    Distributed iterator over the predictions. E.g. an iterator over probability vectors in the case of a classifier ens.\n",
    "  \"\"\"\n",
    "  pred_iter = pred_models_iter(models, test_x)\n",
    "  return pred_iter.map(lambda t: predict(*t))\n",
    "\n",
    "def ens_metrics(models, test_x, test_y, metrics):\n",
    "  \"\"\"Distributed ensemble metrics\n",
    "  Takes a set of models and test data, predicts probability vectors and calculates the provided metrics\n",
    "  given true labels `test_y`\n",
    "  Let N := number of data points and D_x := the dimension of a single datapoint x\n",
    "  \n",
    "  Args:\n",
    "    models (list[state_dict, shape]): set of models represented as a list (state_dict, shape)\n",
    "    test_x (torch.Tensor): Tensor of size (N, D_x)\n",
    "    test_y (torch.Tensor): Tensor of size (N). NB: hard labels\n",
    "    metrics (list[functions]): List of functions where each funcion f: R^(N x D_x) x R^(N) --> T, where is a generic output type.\n",
    "  \"\"\"\n",
    "  return ens_preds(models, test_x).map(lambda prob_vecs: [metric(prob_vecs, test_y) for metric in metrics])\n",
    "\n",
    "def ens_preds_reduced(models, test_x, red_fn):\n",
    "  \"\"\"Distributed ensemble predictions\n",
    "  Takes a set of models and test data and makes distributed predictions and reduces them with a provided `red_fn`\n",
    "  Let N := number of data points and D_x := the dimension of a single datapoint x\n",
    "  \n",
    "  Args:\n",
    "    models (list[state_dict, shape]): set of models represented as a list (state_dict, shape)\n",
    "    test_x (torch.Tensor): Tensor of size (N, D_x)\n",
    "    red_fn function: f: R^D_x x R^D_x --> R^D_x\n",
    "  \n",
    "  Returns:\n",
    "    Single reduced/aggregated prediction of the whole ensemble\n",
    "  \"\"\"\n",
    "  pred_iter = pred_models_iter(models, test_x)\n",
    "  return pred_iter.map(lambda t: Predict(*t)).reduce(red_fn)\n",
    "\n",
    "\n",
    "def avg_accuracy(prob_vecs, labels):\n",
    "  \"\"\"Example metrics function: average accuracy\n",
    "  Let N := number of data points and C := the number of classes\n",
    "  \n",
    "  Args:\n",
    "    prob_vecs (torch.Tensor): Tensor of size (N, C)\n",
    "    labels (torch.Tensor): Tensor of size (N), hard labels, with classes corresponding to indices 0, ..., C-1\n",
    "  \n",
    "  Returns:\n",
    "    torch.Tensor: Tensor of size (N), average accuracy over all datapoints.\n",
    "  \"\"\"\n",
    "  hard_preds = torch.argmax(prob_vecs, 1)\n",
    "  return (hard_preds == labels).float().mean()\n",
    "  \n",
    "def _pred_models_iter(models, test_x):\n",
    "  \"\"\"Helper function to generate a distributed iterator over models and test data\n",
    "  NB: the same `test_x` is given to all elements in the iterator\n",
    "  \n",
    "  Args:\n",
    "    models (list[state_dict, shape]): set of models represented as a list (state_dict, shape)\n",
    "    test_x (torch.Tensor): Tensor of size (N, D_x)\n",
    "  \"\"\"\n",
    "  models_and_data = [(params, shape, test_x) for params, shape in models]\n",
    "  return sc.parallelize(models_and_data)\n",
    "\n",
    "def novelty_detection():\n",
    "  pass\n",
    "def entropy():\n",
    "  pass\n",
    "def auroc():\n",
    "  pass\n",
    "# Rather dumb way of getting test data, but it works fine.\n",
    "data = get_batched_iterator(load_firewall_data())\n",
    "batch = next(data)\n",
    "x=[v[0] for v in d]\n",
    "x=torch.tensor(x,dtype=torch.float)\n",
    "y=[v[1] for v in d]\n",
    "y=torch.tensor(y,dtype=torch.long)\n",
    "\n",
    "# Non-distr. list of models\n",
    "models = [(params, shape) for params, shape, _ in models_trained_c]\n",
    "\n",
    "avg_acc = ens_metrics(models, x, y, [avg_accuracy]).collect() # Average acc. for each ens. over all data points\n",
    "avg_prob_vecs = ens_preds_reduced(models, x, lambda x, y: (x+y)/2) # (A single) Average prob. vec for all data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
