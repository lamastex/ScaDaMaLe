{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data\n",
    "===================\n",
    "\n",
    "Here the raw Ethereum transaction data read from google big query is\n",
    "preprocessed. - Remove any rows with nulls - Drop all self-loops -\n",
    "Enumerate all the distict addresses - Make a canonical ordering for the\n",
    "edges - Each edge will point from lower to higher index - The sign of\n",
    "the transaction is changed for flipped edges - Aggregate transactions\n",
    "based on src, dst pair - Enumerate the edges with a unique edge id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "### Load data into DataFrame\n",
    "\n",
    "And drop nans and self-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"FileStore/tables/ethereum_march_2018_2020\"\n",
    "\n",
    "df = spark.read.format('csv').option(\"header\", \"true\").load(data_path)\\\n",
    "  .select(F.col(\"from_address\"), F.col(\"to_address\"), F.col(\"value\"))\\\n",
    "  .na.drop()\\\n",
    "  .where(F.col(\"from_address\") != F.col(\"to_address\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "### Enumerate the addresses with a unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = df.select(F.col(\"from_address\").alias(\"address\")).union(df.select(F.col(\"to_address\").alias(\"address\"))).distinct()\n",
    "address_window = Window.orderBy(\"address\")\n",
    "addresses = addresses.withColumn(\"id\", F.row_number().over(address_window))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "### Make the edges canonical\n",
    "\n",
    "-   Each edge will point from lower to higher index\n",
    "-   The sign of the transaction is changed for flipped edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchange string addresses for node ids\n",
    "df_with_ids = df.join(addresses.withColumnRenamed(\"address\", \"to_address\").withColumnRenamed(\"id\", \"dst__\"), on=\"to_address\")\\\n",
    "  .join(addresses.withColumnRenamed(\"address\", \"from_address\").withColumnRenamed(\"id\", \"src__\"), on=\"from_address\")\n",
    "\n",
    "canonical_edges = df_with_ids.withColumn(\"src\",\n",
    "  F.when(F.col(\"dst__\") > F.col(\"src__\"), F.col(\"src__\")).otherwise(F.col(\"dst__\"))\n",
    ").withColumn(\"dst\",\n",
    "  F.when(F.col(\"dst__\") > F.col(\"src__\"), F.col(\"dst__\")).otherwise(F.col(\"src__\"))\n",
    ").withColumn(\"direction__\",\n",
    "  F.when(F.col(\"dst__\") > F.col(\"src__\"), 1).otherwise(-1)\n",
    ").withColumn(\"flow\",\n",
    "F.col(\"value\") * F.col(\"direction__\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "### Group the edges by source (src) and destination (dst) by taking the sum of the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_canonical_edges = canonical_edges.select(F.col(\"src\"), F.col(\"dst\"), F.col(\"flow\")).groupBy(F.col(\"src\"), F.col(\"dst\")).agg(F.sum(F.col(\"flow\")).alias(\"flow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "### Enumerate the edges with a unique edge id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_window = Window.orderBy(F.col(\"src\"), F.col(\"dst\"))\n",
    "grouped_canonical_edges = grouped_canonical_edges.withColumn(\"id\", F.row_number().over(edges_window))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "### Save the results in parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_edges_path = \"/projects/group21/test_ethereum_canonical_edges\"\n",
    "preprocessed_addresses_path = \"/projects/group21/test_ethereum_addresses\"\n",
    "\n",
    "grouped_canonical_edges.write.format('parquet').mode(\"overwrite\").save(preprocessed_edges_path)\n",
    "addresses.write.format('parquet').mode(\"overwrite\").save(preprocessed_addresses_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
