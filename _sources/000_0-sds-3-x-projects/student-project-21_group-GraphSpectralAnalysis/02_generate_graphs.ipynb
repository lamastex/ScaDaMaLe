{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random graphs\n",
    "======================\n",
    "\n",
    "Here random graphs are generated, first using Erdös-Renyi method and\n",
    "then using R-MAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.graphx.util.GraphGenerators\n",
    "import scala.util.Random\n",
    "import org.apache.spark.sql.{Row, DataFrame}\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.{functions => F}\n",
    "import org.apache.spark.sql.types.{IntegerType, LongType, DoubleType, StringType, StructField, StructType}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.graphx.util.GraphGenerators\n",
    ">     import scala.util.Random\n",
    ">     import org.apache.spark.sql.{Row, DataFrame}\n",
    ">     import org.apache.spark.sql.expressions.Window\n",
    ">     import org.apache.spark.sql.{functions=>F}\n",
    ">     import org.apache.spark.sql.types.{IntegerType, LongType, DoubleType, StringType, StructField, StructType}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Values taken from the Ethereum graph\n",
    "val numNodes = 1520925\n",
    "val numEdges = 2152835"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     numNodes: Int = 1520925\n",
    ">     numEdges: Int = 2152835\n",
    "\n",
    "  \n",
    "\n",
    "Function for making a canonical ordering for the edges of a graph\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "-   Input is a dataframe with rows of \"src\" and \"dst\" node numbers\n",
    "-   A new node id is computed such that the nodes have ids 0,1,2,...\n",
    "-   The canonical ordering is made such that each edge will point from\n",
    "    lower to higher index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeEdgesCanonical (edgeDF : org.apache.spark.sql.DataFrame): org.apache.spark.sql.DataFrame = {\n",
    "  // Remove self-loops\n",
    "  val edgeDFClean = edgeDF.distinct().where(F.col(\"src\") =!= F.col(\"dst\"))\n",
    "  \n",
    "  // Provide each node with an index id\n",
    "  val nodes = edgeDFClean.select(F.col(\"src\").alias(\"node\")).union(edgeDFClean.select(F.col(\"dst\").alias(\"node\"))).distinct()\n",
    "  val nodes_window = Window.orderBy(\"node\")\n",
    "  val nodesWithids = nodes.withColumn(\"id\", F.row_number().over(nodes_window))\n",
    "  \n",
    "  // Add the canonical node ids to the edgeDF and drop the old ids\n",
    "  val dstNodes = nodesWithids.withColumnRenamed(\"node\", \"dst\").withColumnRenamed(\"id\", \"dst__\")\n",
    "  val srcNodes = nodesWithids.withColumnRenamed(\"node\", \"src\").withColumnRenamed(\"id\", \"src__\")\n",
    "  val edgesWithBothIds = edgeDFClean.join(dstNodes, dstNodes(\"dst\") === edgeDFClean(\"dst\"))\n",
    "                           .join(srcNodes, srcNodes(\"src\") === edgeDFClean(\"src\"))\n",
    "                           .drop(\"src\").drop(\"dst\")\n",
    "  \n",
    "  val edgesWithCanonicalIds = edgesWithBothIds.withColumn(\"src\",\n",
    "                    F.when(F.col(\"dst__\") > F.col(\"src__\"), F.col(\"src__\")).otherwise(F.col(\"dst__\"))\n",
    "                  ).withColumn(\"dst\",\n",
    "                    F.when(F.col(\"dst__\") > F.col(\"src__\"), F.col(\"dst__\")).otherwise(F.col(\"src__\"))\n",
    "                  ).drop(\"src__\").drop(\"dst__\").distinct().where(F.col(\"src\") =!= F.col(\"dst\"))\n",
    "  \n",
    "  val edges_window = Window.orderBy(F.col(\"src\"), F.col(\"dst\"))\n",
    "  val GroupedCanonicalEdges = edgesWithCanonicalIds.withColumn(\"id\", F.row_number().over(edges_window))\n",
    "  return GroupedCanonicalEdges\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     makeEdgesCanonical: (edgeDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
    "\n",
    "  \n",
    "\n",
    "Generate Erdös-Renyi graph (uniform edge sampling)\n",
    "--------------------------------------------------\n",
    "\n",
    "#### Function for sampling an Erdös-Renyi graph\n",
    "\n",
    "The resulting graph will have at most the number of nodes given by\n",
    "numNodes and at most numEdges edges. The number of nodes is less than\n",
    "numNodes if some nodes did not have an edge to another node. The number\n",
    "of edges is less than numEdges if some edges are duplicates or if some\n",
    "edges are self-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleERGraph (numNodes : Int, numEdges : Int, iter : Int): org.apache.spark.sql.DataFrame = {\n",
    "  val randomEdges = sc.parallelize(0 until numEdges).map {\n",
    "    idx =>\n",
    "      val random = new Random(42 + iter * numEdges + idx)\n",
    "      val src = random.nextInt(numNodes)\n",
    "      val dst = random.nextInt(numNodes)\n",
    "      if (src > dst) Row(dst, src) else Row(src, dst)\n",
    "  }\n",
    "\n",
    "  val schema = new StructType()\n",
    "    .add(StructField(\"src\", IntegerType, true))\n",
    "    .add(StructField(\"dst\", IntegerType, true))\n",
    "\n",
    "  val groupedCanonicalEdges = makeEdgesCanonical(spark.createDataFrame(randomEdges, schema))\n",
    "  return groupedCanonicalEdges\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     sampleERGraph: (numNodes: Int, numEdges: Int, iter: Int)org.apache.spark.sql.DataFrame\n",
    "\n",
    "  \n",
    "\n",
    "#### Sample and save 10 different Erdös-Renyi graphs with different seeds and save each to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i <- 0 to 9) {\n",
    "  val groupedCanonicalEdges = sampleERGraph(numNodes, numEdges, iter=i)\n",
    "  groupedCanonicalEdges.write.format(\"parquet\").mode(\"overwrite\").save(\"/projects/group21/uniform_random_graph\" + i)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Generate R-MAT graph\n",
    "--------------------\n",
    "\n",
    "#### The default parameters for R-MAT generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"RMAT a: \" + GraphGenerators.RMATa)\n",
    "println(\"RMAT b: \" + GraphGenerators.RMATb)\n",
    "println(\"RMAT c: \" + GraphGenerators.RMATc)\n",
    "println(\"RMAT d: \" + GraphGenerators.RMATd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     RMATa: 0.45\n",
    ">     RMATb: 0.15\n",
    ">     RMATc: 0.15\n",
    ">     RMATd: 0.25\n",
    "\n",
    "  \n",
    "\n",
    "#### Function for generating a R-MAT graph, storing the edges as a Dataframe and applying makeEdgesCanonical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleRMATGraph (numNodes : Int, numEdges : Int): org.apache.spark.sql.DataFrame = {\n",
    "  val rmatGraphraw = GraphGenerators.rmatGraph(sc=spark.sparkContext, requestedNumVertices=numNodes, numEdges=numEdges)\n",
    "  val rmatedges = rmatGraphraw.edges.map{ \n",
    "    edge => Row(edge.srcId, edge.dstId)\n",
    "  }\n",
    "\n",
    "  val schema = new StructType()\n",
    "    .add(StructField(\"src\", LongType, true))\n",
    "    .add(StructField(\"dst\", LongType, true))\n",
    "\n",
    "  val rmatGroupedCanonicalEdges = makeEdgesCanonical(spark.createDataFrame(rmatedges, schema))\n",
    "  return rmatGroupedCanonicalEdges\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     sampleRMATGraph: (numNodes: Int, numEdges: Int)org.apache.spark.sql.DataFrame\n",
    "\n",
    "  \n",
    "\n",
    "#### Sample 10 R-MAT graphs and save each to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i <- 0 to 9) {\n",
    "  val groupedCanonicalEdges = sampleRMATGraph(numNodes, numEdges)\n",
    "  groupedCanonicalEdges.write.format(\"parquet\").mode(\"overwrite\").save(\"/projects/group21/rmat_random_graph\" + i)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
