{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.graphx.util.GraphGenerators\n",
    "import scala.util.Random\n",
    "import org.apache.spark.sql.{Row, DataFrame}\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.{functions => F}\n",
    "import org.apache.spark.sql.types.{IntegerType, LongType, DoubleType, StringType, StructField, StructType}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.graphx.util.GraphGenerators\n",
    ">     import scala.util.Random\n",
    ">     import org.apache.spark.sql.{Row, DataFrame}\n",
    ">     import org.apache.spark.sql.expressions.Window\n",
    ">     import org.apache.spark.sql.{functions=>F}\n",
    ">     import org.apache.spark.sql.types.{IntegerType, LongType, DoubleType, StringType, StructField, StructType}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val numNodes = 1520925\n",
    "val numEdges = 2152835"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     numNodes: Int = 1520925\n",
    ">     numEdges: Int = 2152835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeEdgesCanonical (edgeDF : org.apache.spark.sql.DataFrame): org.apache.spark.sql.DataFrame = {\n",
    "  // Provide each node with an index id\n",
    "  val nodes = edgeDF.select(F.col(\"src\").alias(\"node\")).union(edgeDF.select(F.col(\"dst\").alias(\"node\"))).distinct()\n",
    "  val nodes_window = Window.orderBy(\"node\")\n",
    "  val nodesWithids = nodes.withColumn(\"id\", F.row_number().over(nodes_window))\n",
    "  \n",
    "  // Add the canonical node ids to the edgeDF and drop the old ids\n",
    "  val dstNodes = nodesWithids.withColumnRenamed(\"node\", \"dst\").withColumnRenamed(\"id\", \"dst__\")\n",
    "  val srcNodes = nodesWithids.withColumnRenamed(\"node\", \"src\").withColumnRenamed(\"id\", \"src__\")\n",
    "  val edgesWithBothIds = edgeDF.join(dstNodes, dstNodes(\"dst\") === edgeDF(\"dst\"))\n",
    "                           .join(srcNodes, srcNodes(\"src\") === edgeDF(\"src\"))\n",
    "                           .drop(\"src\").drop(\"dst\")\n",
    "  \n",
    "  val edgesWithCanonicalIds = edgesWithBothIds.withColumn(\"src\",\n",
    "                    F.when(F.col(\"dst__\") > F.col(\"src__\"), F.col(\"src__\")).otherwise(F.col(\"dst__\"))\n",
    "                  ).withColumn(\"dst\",\n",
    "                    F.when(F.col(\"dst__\") > F.col(\"src__\"), F.col(\"dst__\")).otherwise(F.col(\"src__\"))\n",
    "                  ).drop(\"src__\").drop(\"dst__\").distinct().where(F.col(\"src\") =!= F.col(\"dst\"))\n",
    "  \n",
    "  val edges_window = Window.orderBy(F.col(\"src\"), F.col(\"dst\"))\n",
    "  val GroupedCanonicalEdges = edgesWithCanonicalIds.withColumn(\"id\", F.row_number().over(edges_window))\n",
    "  return GroupedCanonicalEdges\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     makeEdgesCanonical: (edgeDF: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
    "\n",
    "  \n",
    "\n",
    "### Generate ErdÃ¶s-Renyi graph (uniform edge sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val nodes = sc.parallelize(0 until numNodes)\n",
    "val randomEdges = sc.parallelize(0 until numEdges).map {\n",
    "  idx =>\n",
    "    val random = new Random(42 + idx)\n",
    "    val src = random.nextInt(numNodes)\n",
    "    val dst = random.nextInt(numNodes)\n",
    "    if (src > dst) Row(dst, src) else Row(src, dst)\n",
    "}\n",
    "\n",
    "val schema = new StructType()\n",
    "  .add(StructField(\"src\", IntegerType, true))\n",
    "  .add(StructField(\"dst\", IntegerType, true))\n",
    "\n",
    "val groupedCanonicalEdges = makeEdgesCanonical(spark.createDataFrame(randomEdges, schema))\n",
    "val test = groupedCanonicalEdges.describe()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +-------+-----------------+------------------+-----------------+\n",
    ">     |summary|              src|               dst|               id|\n",
    ">     +-------+-----------------+------------------+-----------------+\n",
    ">     |  count|          2152831|           2152831|          2152831|\n",
    ">     |   mean|503954.5198787085|1007302.6882690745|        1076416.0|\n",
    ">     | stddev|356344.9662673391|356346.95043392474|621468.9230224366|\n",
    ">     |    min|                1|              1230|                1|\n",
    ">     |    max|          1510148|           1511245|          2152831|\n",
    ">     +-------+-----------------+------------------+-----------------+\n",
    ">\n",
    ">     nodes: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[587] at parallelize at command-685894176419819:1\n",
    ">     randomEdges: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[589] at map at command-685894176419819:2\n",
    ">     schema: org.apache.spark.sql.types.StructType = StructType(StructField(src,IntegerType,true), StructField(dst,IntegerType,true))\n",
    ">     groupedCanonicalEdges: org.apache.spark.sql.DataFrame = [src: int, dst: int ... 1 more field]\n",
    ">     test: org.apache.spark.sql.DataFrame = [summary: string, src: string ... 2 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedCanonicalEdges.write.format(\"parquet\").mode(\"overwrite\").save(\"/projects/group21/uniform_random_graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "### Generate RMAT graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rmatGraphraw = GraphGenerators.rmatGraph(sc=spark.sparkContext, requestedNumVertices=numNodes, numEdges=numEdges).removeSelfEdges().convertToCanonicalEdges()\n",
    "val rmatedges = rmatGraphraw.edges.map{ \n",
    "  edge => Row(edge.srcId, edge.dstId)\n",
    "}\n",
    "\n",
    "val schema = new StructType()\n",
    "  .add(StructField(\"src\", LongType, true))\n",
    "  .add(StructField(\"dst\", LongType, true))\n",
    "\n",
    "val rmatGroupedCanonicalEdges = makeEdgesCanonical(spark.createDataFrame(rmatedges, schema))\n",
    "val test = rmatGroupedCanonicalEdges.describe()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +-------+-----------------+------------------+-----------------+\n",
    ">     |summary|              src|               dst|               id|\n",
    ">     +-------+-----------------+------------------+-----------------+\n",
    ">     |  count|          2150915|           2150915|          2150915|\n",
    ">     |   mean|270135.6247001857|481234.69323520456|        1075458.0|\n",
    ">     | stddev|225243.9236773024|244586.28516170362|620915.8214645528|\n",
    ">     |    min|                1|                 3|                1|\n",
    ">     |    max|           895135|            895435|          2150915|\n",
    ">     +-------+-----------------+------------------+-----------------+\n",
    ">\n",
    ">     rmatGraphraw: org.apache.spark.graphx.Graph[Int,Int] = org.apache.spark.graphx.impl.GraphImpl@a9b071f\n",
    ">     rmatedges: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[516] at map at command-685894176419823:2\n",
    ">     schema: org.apache.spark.sql.types.StructType = StructType(StructField(src,LongType,true), StructField(dst,LongType,true))\n",
    ">     rmatGroupedCanonicalEdges: org.apache.spark.sql.DataFrame = [src: int, dst: int ... 1 more field]\n",
    ">     test: org.apache.spark.sql.DataFrame = [summary: string, src: string ... 2 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmatGroupedCanonicalEdges.write.format(\"parquet\").mode(\"overwrite\").save(\"/projects/group21/rmat_random_graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
