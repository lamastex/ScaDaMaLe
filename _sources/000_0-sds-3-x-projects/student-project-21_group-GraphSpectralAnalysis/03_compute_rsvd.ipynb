{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute RSVD\n",
    "============\n",
    "\n",
    "Here we read the preprcessed data and compute the rSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.criteo.rsvd._\n",
    "import scala.util.Random\n",
    "import org.apache.spark.mllib.linalg.distributed.MatrixEntry\n",
    "import org.apache.spark.sql.functions.{min, max}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import com.criteo.rsvd._\n",
    ">     import scala.util.Random\n",
    ">     import org.apache.spark.mllib.linalg.distributed.MatrixEntry\n",
    ">     import org.apache.spark.sql.functions.{min, max}\n",
    "\n",
    "  \n",
    "\n",
    "### Set up RSVD config with JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// code snippet for saving config as json\n",
    "val config_map = Map(\"embeddingDim\" -> 100, \"oversample\" -> 30, \"powerIter\" -> 1, \"seed\" -> 0, \"blockSize\" -> 50000, \"partitionWidthInBlocks\" -> 35, \"partitionHeightInBlocks\" -> 10)\n",
    "val config_spark_save = config_map.toSeq.toDF(\"key\",\"value\")\n",
    "config_spark_save.write.mode(\"overwrite\").json(\"/projects/group21/rsvd_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     config_map: scala.collection.immutable.Map[String,Int] = Map(seed -> 0, oversample -> 30, blockSize -> 50000, partitionWidthInBlocks -> 35, partitionHeightInBlocks -> 10, powerIter -> 1, embeddingDim -> 100)\n",
    ">     config_spark_save: org.apache.spark.sql.DataFrame = [key: string, value: int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// load config from json (assuming only integer values)\n",
    "val config_spark = spark.read.json(\"/projects/group21/rsvd_config.json\").rdd.map(r => (r(0).toString -> r(1).toString.toInt)).collect.toMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     config_spark: scala.collection.immutable.Map[String,Int] = Map(seed -> 0, oversample -> 30, blockSize -> 50000, partitionWidthInBlocks -> 35, partitionHeightInBlocks -> 10, powerIter -> 1, embeddingDim -> 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create RSVD configuration\n",
    "val config = RSVDConfig(\n",
    "  embeddingDim = config_spark(\"embeddingDim\"),\n",
    "  oversample = config_spark(\"oversample\"),\n",
    "  powerIter = config_spark(\"powerIter\"),\n",
    "  seed = config_spark(\"seed\"),\n",
    "  blockSize = config_spark(\"blockSize\"),\n",
    "  partitionWidthInBlocks = config_spark(\"partitionWidthInBlocks\"),\n",
    "  partitionHeightInBlocks = config_spark(\"partitionHeightInBlocks\"),\n",
    "  computeLeftSingularVectors = false,\n",
    "  computeRightSingularVectors = false\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     config: com.criteo.rsvd.RSVDConfig = RSVDConfig(100,30,1,0,50000,35,10,false,false)\n",
    "\n",
    "  \n",
    "\n",
    "### Create pipeline for computing RSVD from dataframe of edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeRSVD (groupedCanonicalEdges : org.apache.spark.sql.DataFrame, config : RSVDConfig): RsvdResults = {\n",
    "  val matHeight = groupedCanonicalEdges.count()\n",
    "  val Row(maxValue: Int) = groupedCanonicalEdges.agg(max(\"dst\")).head\n",
    "  val matWidth = maxValue\n",
    "  val incidenceMatrixEntries = groupedCanonicalEdges.rdd.flatMap{\n",
    "    case Row(src: Int, dst: Int, id: Int) => List(MatrixEntry(id-1, src-1, -1), MatrixEntry(id-1, dst-1, 1))\n",
    "  }\n",
    "  // Create block matrix and compute RSVD\n",
    "  val matrixToDecompose = BlockMatrix.fromMatrixEntries(incidenceMatrixEntries, matHeight = matHeight, matWidth = matWidth, config.blockSize, config.partitionHeightInBlocks, config.partitionWidthInBlocks)\n",
    "  return RSVD.run(matrixToDecompose, config, sc)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     computeRSVD: (groupedCanonicalEdges: org.apache.spark.sql.DataFrame, config: com.criteo.rsvd.RSVDConfig)com.criteo.rsvd.RsvdResults\n",
    "\n",
    "  \n",
    "\n",
    "### Compute and save RSVD for Ethereum graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val groupedCanonicalEdges = spark.read.format(\"parquet\").load(\"/projects/group21/test_ethereum_canonical_edges\").drop(\"flow\")\n",
    "val rsvd_results_path: String = \"/projects/group21/test_ethereum_\"\n",
    "\n",
    "val RsvdResults(leftSingularVectors, singularValues, rightSingularVectors) = computeRSVD(groupedCanonicalEdges, config)\n",
    "val singularDF = sc.parallelize(singularValues.toArray).toDF()\n",
    "\n",
    "singularDF.write.format(\"parquet\").mode(\"overwrite\").save(rsvd_results_path + \"SingularValues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     groupedCanonicalEdges: org.apache.spark.sql.DataFrame = [src: int, dst: int ... 1 more field]\n",
    ">     rsvd_results_path: String = /projects/group21/test_ethereum_\n",
    ">     leftSingularVectors: Option[com.criteo.rsvd.SkinnyBlockMatrix] = None\n",
    ">     singularValues: breeze.linalg.DenseVector[Double] = DenseVector(458.40347228630355, 345.1960245414591, 297.38956434965013, 271.6473930317088, 223.76310129031808, 206.09769977563982, 189.1963246644718, 137.79672308619433, 137.56623843203414, 135.54290900239602, 132.08931171241727, 128.72107390801995, 120.93923408578102, 120.62389811722166, 119.5447107766826, 118.21384174672244, 112.19577063002343, 111.39418637351413, 106.97086678678656, 106.5641595917104, 102.47323443175746, 100.63351038159678, 99.9528847938503, 96.35068331317144, 94.63883930018115, 93.63420288135472, 91.14614276423288, 85.69575649594555, 85.33070560572034, 82.7806418087701, 79.46368530163006, 78.30172308417197, 77.52345859456187, 75.89415601567978, 75.17444923371288, 74.60719378218755, 72.76955624490425, 72.46205544360612, 72.11945747937338, 71.36743768743939, 69.68190130698407, 69.34880256578178, 69.23852630919075, 68.349699932709, 68.08810547606782, 67.58052315764029, 67.00671105014123, 66.7375533289733, 66.50556789291336, 65.61826598949622, 65.26026410908351, 64.08933544145536, 63.75670758508847, 63.284124094998404, 61.879367517237704, 61.72183276227414, 60.67402992766071, 60.11054425362685, 60.043685003620446, 59.48403109878358, 58.701932680150286, 58.32674555053222, 57.33272290092479, 56.718331334692344, 55.98877873308879, 55.47326395906372, 54.952717142978095, 54.58134293797019, 54.224229790295766, 53.448450653730504, 53.27335173954756, 53.015192945979386, 52.83027431053384, 52.538750139678044, 52.04259394726296, 51.959976901583175, 51.465183251045325, 51.275215752441895, 50.96925279272193, 50.85019051825306, 50.55657458775341, 50.27755187580496, 49.79019131986308, 49.466635437303324, 49.313416136040296, 49.14614394421691, 48.816142694566054, 48.382666803452764, 48.214673282871296, 48.1100921416426, 47.881278635894425, 47.61474033120143, 47.42272886233879, 47.203789646001, 46.978521992237454, 46.83620532681898, 46.53323067801463, 46.02425975867161, 45.681123495339364, 45.61205652605019)\n",
    ">     rightSingularVectors: Option[com.criteo.rsvd.SkinnyBlockMatrix] = None\n",
    ">     singularDF: org.apache.spark.sql.DataFrame = [value: double]\n",
    "\n",
    "  \n",
    "\n",
    "### Compute and save RSVD for Erd√∂s-Renyi graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i <- 0 to 9) {\n",
    "  val groupedCanonicalEdges = spark.read.format(\"parquet\").load(\"/projects/group21/uniform_random_graph\" + i)\n",
    "  val rsvd_results_path: String = \"/projects/group21/uniform_random_graph_\"\n",
    "\n",
    "  val RsvdResults(leftSingularVectors, singularValues, rightSingularVectors) = computeRSVD(groupedCanonicalEdges, config)\n",
    "  \n",
    "  val singularDF = sc.parallelize(singularValues.toArray).toDF()\n",
    "\n",
    "  singularDF.write.format(\"parquet\").mode(\"overwrite\").save(rsvd_results_path + \"SingularValues\" + i)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Compute and save RSVD for R-MAT graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i <- 0 to 9) {\n",
    "  val groupedCanonicalEdges = spark.read.format(\"parquet\").load(\"/projects/group21/rmat_random_graph\" + i)\n",
    "  val rsvd_results_path: String = \"/projects/group21/rmat_random_graph_\"\n",
    "\n",
    "  val RsvdResults(leftSingularVectors, singularValues, rightSingularVectors) = computeRSVD(groupedCanonicalEdges, config)\n",
    "  \n",
    "  val singularDF = sc.parallelize(singularValues.toArray).toDF()\n",
    "\n",
    "  singularDF.write.format(\"parquet\").mode(\"overwrite\").save(rsvd_results_path + \"SingularValues\" + i)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
