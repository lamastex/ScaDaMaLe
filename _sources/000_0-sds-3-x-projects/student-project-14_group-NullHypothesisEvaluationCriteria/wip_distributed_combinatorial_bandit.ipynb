{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// (copied from http://lamastex.org/lmse/mep/src/GraphXShortestWeightedPaths.html)\n",
    "\n",
    "import scala.util.Random\n",
    "import scala.math.max\n",
    "\n",
    "import scala.reflect.ClassTag\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.graphx.{Graph, VertexId}\n",
    "import org.apache.spark.graphx.util.GraphGenerators\n",
    "\n",
    "\n",
    "// A graph with edge attributes containing distances\n",
    "//val graphSeed = 123L\n",
    "val graphSeed = 3L\n",
    "val numVertices = 50\n",
    "val graph: Graph[Long, Double] = GraphGenerators.logNormalGraph(sc, numVertices = numVertices, seed=graphSeed).mapEdges { e => \n",
    "  // to make things nicer we assign 0 distance to itself\n",
    "  if (e.srcId == e.dstId) 0.0 else Random.nextDouble()\n",
    "}.groupEdges((attr1, attr2) => max(attr1, attr2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import scala.util.Random\n",
    ">     import scala.math.max\n",
    ">     import scala.reflect.ClassTag\n",
    ">     import org.apache.spark.graphx._\n",
    ">     import org.apache.spark.graphx.{Graph, VertexId}\n",
    ">     import org.apache.spark.graphx.util.GraphGenerators\n",
    ">     graphSeed: Long = 3\n",
    ">     numVertices: Int = 50\n",
    ">     graph: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@4d95da76\n",
    "\n",
    "  \n",
    "\n",
    "The Shortest Path algorithm\n",
    "---------------------------\n",
    "\n",
    "The implemented shorthest path alborithm uses the the distributed Pregel\n",
    "algorithm and is divided into two parts.\n",
    "\n",
    "The first part is based on the code in\n",
    "`998_EX_01_GraphXShortestWeightedPaths`. As the original code did not\n",
    "have have all functionality desiered functionality, the algorithm did\n",
    "find the shortest distance but didn't keep track of the path itself, the\n",
    "algorithm was extend with this functionality.\n",
    "\n",
    "The first part takes a graph, where the edges are double values\n",
    "representing the cost of trevelling between its connected nodes, and an\n",
    "array of the ids of each goal node. As output, it provides a graph where\n",
    "each node containse a Map-object of the different landmarks/goal nodes.\n",
    "When a lookup is made in the map from a specific node, a tuple contaning\n",
    "the shortest distance, the id of the next node in the path and the id of\n",
    "the current node. The last element serves no pupose in the final results\n",
    "but is used as a form of stopping critera in the algorithm.\n",
    "\n",
    "The second part transforms the output of the first part to a \"path\n",
    "graph\" where each edge is marked with either a 1 or a 0 depending on if\n",
    "it is used in a path between a starting node and a goal node. Altough\n",
    "this recursion can be performed on a single machine for small examples,\n",
    "this procedure is also implemented using the Pregel algorithm to handle\n",
    "situations of millions of edges.\n",
    "\n",
    "The input of the second part is the graph created in the first part as\n",
    "well as the id of a single goal node and a start node. The goal node has\n",
    "to be in the set of goal nodes used in the first part. This part outputs\n",
    "a \"path graph\" where each edge is given the value 1 or 0 depending on if\n",
    "it is on the shortest path or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.reflect.ClassTag\n",
    "import org.apache.spark.graphx._\n",
    "\n",
    "/**\n",
    " * Computes shortest weighted paths to the given set of goal nodes, returning a graph where each\n",
    " * vertex attribute is a map containing the shortest-path distance to each reachable landmark.\n",
    " * Currently supports only Graph of [VD, Double], where VD is an arbitrary vertex type.\n",
    " *\n",
    " * The object also include a function which transforms the resulting graph into a path_graph between a \n",
    " * specific starting node and goal node. Each edge in the path_grpah is either 1 or 0 depending if it is \n",
    " * the shortest path or not.\n",
    " *\n",
    " */\n",
    "object ShortestPath extends Serializable {\n",
    "\n",
    "  // When finding the shortest path each node stores a map from the itself to each goal node.\n",
    "  // The map returns an array includeing the total distance to the goal node as well as the\n",
    "  // next node pn the shortest path to the goal node. The last value in the array is only \n",
    "  // populated with the nodes own id and is only used for computational convenience. \n",
    "  type SPMap = Map[VertexId, Tuple3[Double, VertexId, VertexId]]\n",
    "  \n",
    "  // PN holds the information of the path nodes which are used for creating a path graph\n",
    "  // PN = ('Distance left to goal node', 'Next path node id', 'Goal node', 'Is on path')\n",
    "  type PN = Tuple4[Double, VertexId, VertexId, Boolean] \n",
    "  \n",
    "  private val INITIAL_DIST = 0.0\n",
    "  private val DEFAULT_ID = -1L\n",
    "  private val INFINITY = Int.MaxValue.toDouble\n",
    "\n",
    "  private def makeMap(x: (VertexId, Tuple3[Double, VertexId, VertexId])*) = Map(x: _*)\n",
    "  \n",
    "  private def incrementMap(spmap: SPMap, delta: Double, id: VertexId): SPMap = { \n",
    "    spmap.map { case (v, d) => v -> (Tuple3(d._1 + delta, d._3, id)) }\n",
    "  }\n",
    "\n",
    "  private def addMaps(spmap1: SPMap, spmap2: SPMap): SPMap = {\n",
    "    (spmap1.keySet ++ spmap2.keySet).map {\n",
    "    k =>{\n",
    "        if (spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1 < spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1) \n",
    "                k -> (Tuple3(spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1, \n",
    "                             spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._2, \n",
    "                             spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._3))\n",
    "        else \n",
    "                k -> (Tuple3(spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1, \n",
    "                             spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._2, \n",
    "                             spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._3))\n",
    "        }\n",
    "    }.toMap\n",
    "  }\n",
    "  \n",
    "  // at this point it does not really matter what vertex type is\n",
    "  def run[VD](graph: Graph[VD, Double], landmarks: Seq[VertexId]): Graph[SPMap, Double] = {\n",
    "    val spGraph = graph.mapVertices { (vid, attr) =>\n",
    "      // initial value for itself is 0.0 as Double\n",
    "      if (landmarks.contains(vid)) makeMap(vid -> Tuple3(INITIAL_DIST, DEFAULT_ID, DEFAULT_ID)) else makeMap()\n",
    "    }\n",
    "\n",
    "    val initMaps = makeMap()\n",
    "\n",
    "    def vProg(id: VertexId, attr: SPMap, msg: SPMap): SPMap = {\n",
    "      addMaps(attr, msg)\n",
    "    }\n",
    "\n",
    "    def sendMsg(edge: EdgeTriplet[SPMap, Double]): Iterator[(VertexId, SPMap)] = {\n",
    "      val newAttr = incrementMap(edge.dstAttr, edge.attr, edge.srcId) // newAttr is the value \n",
    "      if (edge.srcAttr._1 > addMaps(newAttr, edge.srcAttr)._1) Iterator((edge.srcId, newAttr))\n",
    "      else Iterator.empty\n",
    "    }\n",
    "\n",
    "    Pregel(spGraph, initMaps)(vProg, sendMsg, addMaps)\n",
    "  }\n",
    "  \n",
    "  def create_path_graph[VD](graph: Graph[SPMap, Double], goalId: VertexId, startId: VertexId): Graph[PN, Int] = {\n",
    "    // For a given goal node we remove the lookup map and extend the state to a Tuple5 with the goal id and a boolean\n",
    "    val path = graph.mapEdges(e => 0)\n",
    "              .mapVertices((vertixId, attr) => {\n",
    "                if (attr.contains(goalId)) {\n",
    "                  val path_step = attr(goalId)\n",
    "                  if (vertixId == path_step._3 && path_step._2 == -1L)\n",
    "                    (path_step._1, goalId, goalId, false) // while we are at it, we clean up the state a bit\n",
    "                  else  \n",
    "                    (path_step._1, path_step._2, goalId, false)\n",
    "                } else// If the vertice does not have a map to our goal we add a default value to it\n",
    "                    (INFINITY, -1L, -1L, false)\n",
    "              })\n",
    "\n",
    "      def mergeMsg(msg1: VertexId, msg2: VertexId): VertexId = { // we should only get one msg\n",
    "          msg2\n",
    "      }\n",
    "\n",
    "      def vprog(id: VertexId, attr: PN, msg: VertexId): PN = {\n",
    "        // Check that the current node is the one adressed in the message\n",
    "        if (id == msg)\n",
    "          (attr._1, attr._2, attr._3, true)\n",
    "        else // If the message is not addressed to the current node (happens for inital message), use the old value \n",
    "          attr\n",
    "      }\n",
    "      def sendMsg(triplet: EdgeTriplet[PN, Int]): Iterator[(VertexId, VertexId)] = {\n",
    "        // If dstId is the next node on the path and has not yet been activated\n",
    "        if (triplet.srcAttr._2 == triplet.dstId && triplet.srcAttr._4 && !triplet.dstAttr._4) \n",
    "          Iterator((triplet.dstId, triplet.dstId))// Send next msg\n",
    "        else\n",
    "          Iterator.empty// Do nothing\n",
    "      }\n",
    "\n",
    "      Pregel(path, startId)(vprog, sendMsg, mergeMsg).mapTriplets(triplet => {\n",
    "        if(triplet.srcAttr._2 == triplet.dstId && triplet.srcAttr._4)\n",
    "          1\n",
    "        else\n",
    "          0\n",
    "      })\n",
    "  }\n",
    "}\n",
    "\n",
    "println(\"Usage: val result = GraphXShortestWeightedPaths.run(graph, Seq(4L, 0L, 9L))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Usage: val result = GraphXShortestWeightedPaths.run(graph, Seq(4L, 0L, 9L))\n",
    ">     import scala.reflect.ClassTag\n",
    ">     import org.apache.spark.graphx._\n",
    ">     defined object ShortestPath\n",
    "\n",
    "  \n",
    "\n",
    "To make the code somewhat more accesible, we wrap the execution of the\n",
    "two parts above in a new function called `shortestPath`. This new\n",
    "function takes the id of the start node and a single goal node as well\n",
    "as the input graph as input. The function then ouputs the path graph\n",
    "mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Random\n",
    "\n",
    "def shortestPath(srcId : Long, dstId : Long, graph : Graph[Long, Double], placeholder: Boolean) : Graph[Long, Double] = {\n",
    "  if (placeholder) {\n",
    "    return graph.mapEdges(e => Random.nextInt(2))\n",
    "  } else {\n",
    "    val distanceGraph = ShortestPath.run(graph, Seq(dstId))\n",
    "    val pathGraph = ShortestPath.create_path_graph(distanceGraph, dstId, srcId)\n",
    "    return pathGraph.mapVertices((vid, attr) => 0L).mapEdges(e => e.attr)\n",
    "  }\n",
    "}\n",
    "\n",
    "def shortestPath(srcId : Long, dstId : Long, graph : Graph[Long, Double]) : Graph[Long, Double] = {\n",
    "  return shortestPath(srcId, dstId, graph, false)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import scala.util.Random\n",
    ">     shortestPath: (srcId: Long, dstId: Long, graph: org.apache.spark.graphx.Graph[Long,Double], placeholder: Boolean)org.apache.spark.graphx.Graph[Long,Double] <and> (srcId: Long, dstId: Long, graph: org.apache.spark.graphx.Graph[Long,Double])org.apache.spark.graphx.Graph[Long,Double]\n",
    ">     shortestPath: (srcId: Long, dstId: Long, graph: org.apache.spark.graphx.Graph[Long,Double], placeholder: Boolean)org.apache.spark.graphx.Graph[Long,Double] <and> (srcId: Long, dstId: Long, graph: org.apache.spark.graphx.Graph[Long,Double])org.apache.spark.graphx.Graph[Long,Double]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.reflect.ClassTag\n",
    "\n",
    "// # Merge edge attributes of two (identical in structure) graphs\n",
    "def mergeEdgeAttributes[ED1 : ClassTag, ED2 : ClassTag](firstGraph : Graph[Long, ED1], secondGraph : Graph[Long, ED2]) : Graph[Long, (ED1, ED2)] = {\n",
    "  return Graph(firstGraph.vertices, firstGraph.edges.innerJoin(secondGraph.edges) {(id1, id2, first, second) => (first, second)}) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import scala.reflect.ClassTag\n",
    ">     mergeEdgeAttributes: [ED1, ED2](firstGraph: org.apache.spark.graphx.Graph[Long,ED1], secondGraph: org.apache.spark.graphx.Graph[Long,ED2])(implicit evidence$1: scala.reflect.ClassTag[ED1], implicit evidence$2: scala.reflect.ClassTag[ED2])org.apache.spark.graphx.Graph[Long,(ED1, ED2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Test graph\n",
    "val graph: Graph[Long, Double] = Graph.fromEdges(spark.sparkContext.parallelize(1 until numVertices-1).flatMap(vid => List(Edge(0L, vid.toLong, 0.0), Edge(vid.toLong, numVertices-1, 0.0))), 0L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     graph: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@6c5928bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Random\n",
    "\n",
    "def graphRandomGaussian(graph : Graph[Long, (Double, Double)], seed : Int, eps : Double, sparkSqlRandom : Boolean) : Graph[Long, Double] = {\n",
    "  if (sparkSqlRandom) {\n",
    "    return Graph(graph.vertices, graph.edges.toDF.select($\"srcId\", $\"dstId\", $\"attr._1\" + org.apache.spark.sql.functions.sqrt($\"attr._2\") * org.apache.spark.sql.functions.randn(seed)).rdd.map(r => Edge(r.getLong(0), r.getLong(1), r.getDouble(2)))).mapEdges(e => scala.math.max(eps, e.attr))\n",
    "  } else {\n",
    "    return graph.mapEdges(e => scala.math.max(eps, e.attr._1 + Random.nextGaussian() * scala.math.sqrt(e.attr._2)))\n",
    "  }\n",
    "} \n",
    "\n",
    "def graphRandomGaussian(graph : Graph[Long, (Double, Double)], seed : Int, eps : Double) : Graph[Long, Double] = {\n",
    "  return graphRandomGaussian(graph, seed, eps, true)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import scala.util.Random\n",
    ">     graphRandomGaussian: (graph: org.apache.spark.graphx.Graph[Long,(Double, Double)], seed: Int, eps: Double, sparkSqlRandom: Boolean)org.apache.spark.graphx.Graph[Long,Double] <and> (graph: org.apache.spark.graphx.Graph[Long,(Double, Double)], seed: Int, eps: Double)org.apache.spark.graphx.Graph[Long,Double]\n",
    ">     graphRandomGaussian: (graph: org.apache.spark.graphx.Graph[Long,(Double, Double)], seed: Int, eps: Double, sparkSqlRandom: Boolean)org.apache.spark.graphx.Graph[Long,Double] <and> (graph: org.apache.spark.graphx.Graph[Long,(Double, Double)], seed: Int, eps: Double)org.apache.spark.graphx.Graph[Long,Double]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.math.sqrt\n",
    "import scala.math.max\n",
    "\n",
    "println(\"Starting experiment!\")\n",
    "val startTime = System.currentTimeMillis()\n",
    "\n",
    "val seed = 1000\n",
    "val eps = 0.00001\n",
    "\n",
    "// # Horizon N\n",
    "val N = 30\n",
    "\n",
    "// # Source and destination node IDs\n",
    "val srcId = 0L\n",
    "val dstId = numVertices - 1L\n",
    "\n",
    "// # Assumption: Gaussian rewards with known variance\n",
    "// # Prior graph (map weight to prior mean and variance)\n",
    "// TODO: Prior mean from real graph\n",
    "val priorMean = 100.0\n",
    "val priorVar = 10.0\n",
    "val prior = graph.mapEdges(e => (priorMean, priorVar))\n",
    "var posterior = prior\n",
    "\n",
    "// # Environment (sample true environment from prior)\n",
    "val trueVar = 10.0\n",
    "val env = graphRandomGaussian(prior, seed, eps).mapEdges(e => (e.attr, trueVar))\n",
    "\n",
    "// # For regret calculations\n",
    "val optimal = shortestPath(srcId, dstId, env.mapEdges(e => e.attr._1))\n",
    "val optimalExpectedCost = mergeEdgeAttributes(optimal, env).edges.map(e => e.attr._1 * e.attr._2._1).reduce(_ + _)\n",
    "val optimalPathEdges = optimal.edges.filter(e => e.attr == 1).map(e => (e.srcId, e.dstId)).collect()\n",
    "printf(\"Optimal path edges: [%s]\\n\", optimalPathEdges.mkString(\",\"))\n",
    "\n",
    "// # Array with instant regret values\n",
    "val instantRegretValues = new Array[Double](N)\n",
    "var lastAction = optimal\n",
    "\n",
    "// # Run experiment for N iterations\n",
    "for (t <- 0 until N) {\n",
    "  printf(\"Iteration %d, elapsed time: %d ms\", t, System.currentTimeMillis() - startTime)\n",
    "  \n",
    "  // # Find action (super arm) using posterior sampling\n",
    "  val sampledParameters = graphRandomGaussian(posterior, seed, eps)\n",
    "  val action = shortestPath(srcId, dstId, sampledParameters)\n",
    "  lastAction = action\n",
    "  \n",
    "  // # Apply action on environments (assuming path is indicated by 1-valued edge attributes) and observe realized costs\n",
    "  val realizedEnv = graphRandomGaussian(env, seed, eps)\n",
    "  val observation = mergeEdgeAttributes(action, realizedEnv).mapEdges(e => e.attr._1 * e.attr._1)\n",
    "  \n",
    "  // # Update posterior\n",
    "  posterior = mergeEdgeAttributes(observation, posterior).mapEdges(e => {\n",
    "    val obs = e.attr._1\n",
    "    val pMean = e.attr._2._1\n",
    "    val pVar = e.attr._2._2\n",
    "    ((1/(1/trueVar + 1/pVar))*(obs/trueVar + pMean/pVar), (1/(1/trueVar + 1/pVar)))\n",
    "  })\n",
    "  \n",
    "  // # Calculate regret\n",
    "  //val actionExpectedCost = mergeEdgeAttributes(action, env).edges.map(e => e.attr._1 * e.attr._2._1).reduce(_ + _)\n",
    "  //val instantRegret = actionExpectedCost - optimalExpectedCost\n",
    "  //instantRegretValues(t) = instantRegret\n",
    "  //val actionPathEdges = action.edges.filter(e => e.attr == 1).map(e => (e.srcId, e.dstId)).collect()\n",
    "  //printf(\", instant regret: %.2f\", instantRegret)\n",
    "  //printf(\", action path edges: [%s]\", actionPathEdges.mkString(\",\"))\n",
    "  printf(\"\\n\")\n",
    "}\n",
    "\n",
    "val endTime = System.currentTimeMillis()\n",
    "printf(\"Finished experiment! Elapsed time:%d\\n\", endTime - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Starting experiment!\n",
    ">     Optimal path edges: [(0,4),(4,9)]\n",
    ">     Iteration 0, elapsed time: 124927 ms\n",
    ">     Iteration 1, elapsed time: 218948 ms\n",
    ">     Iteration 2, elapsed time: 254865 ms\n",
    ">     Iteration 3, elapsed time: 300192 ms\n",
    ">     Iteration 4, elapsed time: 384399 ms\n",
    ">     Iteration 5, elapsed time: 415356 ms\n",
    ">     Iteration 6, elapsed time: 473626 ms\n",
    ">     Iteration 7, elapsed time: 603751 ms\n",
    ">     Iteration 8, elapsed time: 649864 ms\n",
    ">     Iteration 9, elapsed time: 657970 ms\n",
    ">     Iteration 10, elapsed time: 711303 ms\n",
    ">     Iteration 11, elapsed time: 733911 ms\n",
    ">     Iteration 12, elapsed time: 768825 ms\n",
    ">     Iteration 13, elapsed time: 864619 ms\n",
    ">     Iteration 14, elapsed time: 908692 ms\n",
    ">     Iteration 15, elapsed time: 969860 ms\n",
    ">     Iteration 16, elapsed time: 1034139 ms\n",
    ">     Iteration 17, elapsed time: 1089197 ms\n",
    ">     Iteration 18, elapsed time: 1184161 ms\n",
    ">     Iteration 19, elapsed time: 1264011 ms\n",
    ">     Iteration 20, elapsed time: 1334011 ms\n",
    ">     Iteration 21, elapsed time: 1395009 ms\n",
    ">     Iteration 22, elapsed time: 1493679 ms\n",
    ">     Iteration 23, elapsed time: 1573276 ms\n",
    ">     Iteration 24, elapsed time: 1634326 ms\n",
    ">     Iteration 25, elapsed time: 1733819 ms\n",
    ">     Iteration 26, elapsed time: 1820353 ms\n",
    ">     Iteration 27, elapsed time: 1875346 ms\n",
    ">     Iteration 28, elapsed time: 1948843 ms\n",
    ">     Iteration 29, elapsed time: 2048672 ms\n",
    ">     Finished experiment! Elapsed time:2119322\n",
    ">     import scala.math.sqrt\n",
    ">     import scala.math.max\n",
    ">     startTime: Long = 1609958630541\n",
    ">     seed: Int = 1000\n",
    ">     eps: Double = 1.0E-5\n",
    ">     N: Int = 30\n",
    ">     srcId: Long = 0\n",
    ">     dstId: Long = 9\n",
    ">     priorMean: Double = 100.0\n",
    ">     priorVar: Double = 10.0\n",
    ">     prior: org.apache.spark.graphx.Graph[Long,(Double, Double)] = org.apache.spark.graphx.impl.GraphImpl@5e483f07\n",
    ">     posterior: org.apache.spark.graphx.Graph[Long,(Double, Double)] = org.apache.spark.graphx.impl.GraphImpl@4a4132b1\n",
    ">     trueVar: Double = 10.0\n",
    ">     env: org.apache.spark.graphx.Graph[Long,(Double, Double)] = org.apache.spark.graphx.impl.GraphImpl@636784b2\n",
    ">     optimal: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@77c0d44c\n",
    ">     optimalExpectedCost: Double = 190.72770275035734\n",
    ">     optimalPathEdges: Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = Array((0,4), (4,9))\n",
    ">     instantRegretValues: Array[Double] = Array(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
    ">     lastAction: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@138e7325\n",
    ">     endTime: Long = 1609960749863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Random\n",
    "import scala.math.sqrt\n",
    "import scala.math.max\n",
    "\n",
    "println(\"Starting experiment!\")\n",
    "val startTime = System.currentTimeMillis()\n",
    "\n",
    "val seed = 1000\n",
    "val rng = new Random(seed)\n",
    "val eps = 0.00001\n",
    "\n",
    "// # Horizon N\n",
    "val N = 30\n",
    "\n",
    "// # Source and destination node IDs\n",
    "val srcId = 0L\n",
    "val dstId = numVertices - 1L\n",
    "\n",
    "// # Assumption: Gaussian rewards with known variance\n",
    "// # Prior graph (map weight to prior mean and variance)\n",
    "// TODO: Prior mean from real graph\n",
    "val priorMean = 100.0\n",
    "val priorVar = 10.0\n",
    "val prior = graph.mapEdges(e => (priorMean, priorVar))\n",
    "var posterior = prior\n",
    "\n",
    "// # Environment (sample true environment from prior)\n",
    "val trueVar = 10.0\n",
    "val env = prior.mapEdges(e => (max(eps, e.attr._1 + rng.nextGaussian() * sqrt(e.attr._2)), trueVar))\n",
    "\n",
    "// # For regret calculations\n",
    "val optimal = shortestPath(srcId, dstId, env.mapEdges(e => e.attr._1))\n",
    "val optimalExpectedCost = mergeEdgeAttributes(optimal, env).edges.map(e => e.attr._1 * e.attr._2._1).reduce(_ + _)\n",
    "val optimalPathEdges = optimal.edges.filter(e => e.attr == 1).map(e => (e.srcId, e.dstId)).collect()\n",
    "printf(\"Optimal path edges: [%s]\\n\", optimalPathEdges.mkString(\",\"))\n",
    "\n",
    "val sampledParameters = posterior.mapEdges(e => max(eps, e.attr._1 + rng.nextGaussian() * sqrt(e.attr._2)))\n",
    "val action = shortestPath(srcId, dstId, sampledParameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Starting experiment!\n",
    ">     Optimal path edges: [(0,5),(5,9)]\n",
    ">     import scala.util.Random\n",
    ">     import scala.math.sqrt\n",
    ">     import scala.math.max\n",
    ">     startTime: Long = 1609932752437\n",
    ">     seed: Int = 1000\n",
    ">     rng: scala.util.Random = scala.util.Random@55ffb303\n",
    ">     eps: Double = 1.0E-5\n",
    ">     N: Int = 30\n",
    ">     srcId: Long = 0\n",
    ">     dstId: Long = 9\n",
    ">     priorMean: Double = 100.0\n",
    ">     priorVar: Double = 10.0\n",
    ">     prior: org.apache.spark.graphx.Graph[Long,(Double, Double)] = org.apache.spark.graphx.impl.GraphImpl@660b2c60\n",
    ">     posterior: org.apache.spark.graphx.Graph[Long,(Double, Double)] = org.apache.spark.graphx.impl.GraphImpl@660b2c60\n",
    ">     trueVar: Double = 10.0\n",
    ">     env: org.apache.spark.graphx.Graph[Long,(Double, Double)] = org.apache.spark.graphx.impl.GraphImpl@15febbf8\n",
    ">     optimal: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@2f6f564e\n",
    ">     optimalExpectedCost: Double = 207.25786634313445\n",
    ">     optimalPathEdges: Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = Array((0,5), (5,9))\n",
    ">     sampledParameters: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@258e9bf1\n",
    ">     action: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@6c700943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(optimal.edges.toDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(env.edges.toDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mergeEdgeAttributes(optimal, env).edges.map(e => e.attr._1 * e.attr._2._1).toDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mergeEdgeAttributes(action, env).edges.map(e => e.attr._1 * e.attr._2._1).toDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "  \n",
    "\n",
    "Graph visualization\n",
    "-------------------\n",
    "\n",
    "what do we want to visualize??\n",
    "\n",
    "shortest path for small algorithm???\n",
    "\n",
    "But first, we need to initialize the d3 package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package d3\n",
    "\n",
    "\n",
    "// We use a package object so that we can define top level classes like Edge that need to be used in other cells\n",
    "// This was modified by Ivan Sadikov to make sure it is compatible the latest databricks notebook\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "import com.databricks.backend.daemon.driver.EnhancedRDDFunctions.displayHTML\n",
    "\n",
    "case class Edge(src: String, dest: String, count: Long)\n",
    "\n",
    "case class Node(name: String)\n",
    "case class Link(source: Int, target: Int, value: Long)\n",
    "case class Graph(nodes: Seq[Node], links: Seq[Link])\n",
    "\n",
    "object graphs {\n",
    "// val sqlContext = SQLContext.getOrCreate(org.apache.spark.SparkContext.getOrCreate())  /// fix\n",
    "val sqlContext = SparkSession.builder().getOrCreate().sqlContext\n",
    "import sqlContext.implicits._\n",
    "  \n",
    "def force(clicks: Dataset[Edge], height: Int = 100, width: Int = 960): Unit = {\n",
    "  val data = clicks.collect()\n",
    "  val nodes = (data.map(_.src) ++ data.map(_.dest)).map(_.replaceAll(\"_\", \" \")).toSet.toSeq.map(Node)\n",
    "  val links = data.map { t =>\n",
    "    Link(nodes.indexWhere(_.name == t.src.replaceAll(\"_\", \" \")), nodes.indexWhere(_.name == t.dest.replaceAll(\"_\", \" \")), t.count / 20 + 1)\n",
    "  }\n",
    "  showGraph(height, width, Seq(Graph(nodes, links)).toDF().toJSON.first())\n",
    "}\n",
    "\n",
    "/**\n",
    " * Displays a force directed graph using d3\n",
    " * input: {\"nodes\": [{\"name\": \"...\"}], \"links\": [{\"source\": 1, \"target\": 2, \"value\": 0}]}\n",
    " */\n",
    "def showGraph(height: Int, width: Int, graph: String): Unit = {\n",
    "\n",
    "displayHTML(s\"\"\"\n",
    "<style>\n",
    "\n",
    ".node_circle {\n",
    "  stroke: #777;\n",
    "  stroke-width: 1.3px;\n",
    "}\n",
    "\n",
    ".node_label {\n",
    "  pointer-events: none;\n",
    "}\n",
    "\n",
    ".link {\n",
    "  stroke: #777;\n",
    "  stroke-opacity: .2;\n",
    "}\n",
    "\n",
    ".node_count {\n",
    "  stroke: #777;\n",
    "  stroke-width: 1.0px;\n",
    "  fill: #999;\n",
    "}\n",
    "\n",
    "text.legend {\n",
    "  font-family: Verdana;\n",
    "  font-size: 13px;\n",
    "  fill: #000;\n",
    "}\n",
    "\n",
    ".node text {\n",
    "  font-family: \"Helvetica Neue\",\"Helvetica\",\"Arial\",sans-serif;\n",
    "  font-size: 17px;\n",
    "  font-weight: 200;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\n",
    "<div id=\"clicks-graph\">\n",
    "<script src=\"//d3js.org/d3.v3.min.js\"></script>\n",
    "<script>\n",
    "\n",
    "var graph = $graph;\n",
    "\n",
    "var width = $width,\n",
    "    height = $height;\n",
    "\n",
    "var color = d3.scale.category20();\n",
    "\n",
    "var force = d3.layout.force()\n",
    "    .charge(-700)\n",
    "    .linkDistance(180)\n",
    "    .size([width, height]);\n",
    "\n",
    "var svg = d3.select(\"#clicks-graph\").append(\"svg\")\n",
    "    .attr(\"width\", width)\n",
    "    .attr(\"height\", height);\n",
    "    \n",
    "force\n",
    "    .nodes(graph.nodes)\n",
    "    .links(graph.links)\n",
    "    .start();\n",
    "\n",
    "var link = svg.selectAll(\".link\")\n",
    "    .data(graph.links)\n",
    "    .enter().append(\"line\")\n",
    "    .attr(\"class\", \"link\")\n",
    "    .style(\"stroke-width\", function(d) { return Math.sqrt(d.value); });\n",
    "\n",
    "var node = svg.selectAll(\".node\")\n",
    "    .data(graph.nodes)\n",
    "    .enter().append(\"g\")\n",
    "    .attr(\"class\", \"node\")\n",
    "    .call(force.drag);\n",
    "\n",
    "node.append(\"circle\")\n",
    "    .attr(\"r\", 10)\n",
    "    .style(\"fill\", function (d) {\n",
    "    if (d.name.startsWith(\"other\")) { return color(1); } else { return color(2); };\n",
    "})\n",
    "\n",
    "node.append(\"text\")\n",
    "      .attr(\"dx\", 10)\n",
    "      .attr(\"dy\", \".35em\")\n",
    "      .text(function(d) { return d.name });\n",
    "      \n",
    "//Now we are giving the SVGs co-ordinates - the force layout is generating the co-ordinates which this code is using to update the attributes of the SVG elements\n",
    "force.on(\"tick\", function () {\n",
    "    link.attr(\"x1\", function (d) {\n",
    "        return d.source.x;\n",
    "    })\n",
    "        .attr(\"y1\", function (d) {\n",
    "        return d.source.y;\n",
    "    })\n",
    "        .attr(\"x2\", function (d) {\n",
    "        return d.target.x;\n",
    "    })\n",
    "        .attr(\"y2\", function (d) {\n",
    "        return d.target.y;\n",
    "    });\n",
    "    d3.selectAll(\"circle\").attr(\"cx\", function (d) {\n",
    "        return d.x;\n",
    "    })\n",
    "        .attr(\"cy\", function (d) {\n",
    "        return d.y;\n",
    "    });\n",
    "    d3.selectAll(\"text\").attr(\"x\", function (d) {\n",
    "        return d.x;\n",
    "    })\n",
    "        .attr(\"y\", function (d) {\n",
    "        return d.y;\n",
    "    });\n",
    "});\n",
    "</script>\n",
    "</div>\n",
    "\"\"\")\n",
    "}\n",
    "  \n",
    "  def help() = {\n",
    "displayHTML(\"\"\"\n",
    "<p>\n",
    "Produces a force-directed graph given a collection of edges of the following form:</br>\n",
    "<tt><font color=\"#a71d5d\">case class</font> <font color=\"#795da3\">Edge</font>(<font color=\"#ed6a43\">src</font>: <font color=\"#a71d5d\">String</font>, <font color=\"#ed6a43\">dest</font>: <font color=\"#a71d5d\">String</font>, <font color=\"#ed6a43\">count</font>: <font color=\"#a71d5d\">Long</font>)</tt>\n",
    "</p>\n",
    "<p>Usage:<br/>\n",
    "<tt><font color=\"#a71d5d\">import</font> <font color=\"#ed6a43\">d3._</font></tt><br/>\n",
    "<tt><font color=\"#795da3\">graphs.force</font>(</br>\n",
    "&nbsp;&nbsp;<font color=\"#ed6a43\">height</font> = <font color=\"#795da3\">500</font>,<br/>\n",
    "&nbsp;&nbsp;<font color=\"#ed6a43\">width</font> = <font color=\"#795da3\">500</font>,<br/>\n",
    "&nbsp;&nbsp;<font color=\"#ed6a43\">clicks</font>: <font color=\"#795da3\">Dataset</font>[<font color=\"#795da3\">Edge</font>])</tt>\n",
    "</p>\"\"\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Warning: classes defined within packages cannot be redefined without a cluster restart.\n",
    ">     Compilation successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.graphframes.GraphFrame\n",
    "import d3._\n",
    "import org.apache.spark.sql.functions.lit // import the lit function in sql\n",
    "val priorShortestPath = GraphFrame.fromGraphX(graph.mapEdges(e => e.attr.toInt*10000))\n",
    "\n",
    "d3.graphs.force(\n",
    "  height = 500,\n",
    "  width = 1000,\n",
    "  clicks = priorShortestPath.edges.select($\"src\", $\"dst\".as(\"dest\"), $\"attr\".as(\"count\")).as[d3.Edge])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val posteriorShortestPath = GraphFrame.fromGraphX(lastAction.mapEdges(e => e.attr.toInt*10000))\n",
    "\n",
    "d3.graphs.force(\n",
    "  height = 500,\n",
    "  width = 1000,\n",
    "  clicks = posteriorShortestPath.edges.select($\"src\", $\"dst\".as(\"dest\"), $\"attr\".as(\"count\")).as[d3.Edge])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(posteriorShortestPath.edges.select($\"src\", $\"dst\".as(\"dest\"), $\"attr\".as(\"count\")).as[d3.Edge])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "We can also show the cumulative regret. As the algorithm reaches a final\n",
    "solution, the instantaneous regret should decrease and the cumulative\n",
    "regret should reach a plateau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cumulativeRegret = instantRegretValues.scanLeft(0.0)(_ + _)\n",
    "val df = spark.sparkContext.parallelize((1 to N) zip cumulativeRegret).toDF(\"Iteration (t)\",\"Cumulative regret\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This is just to be able to debug the graph\n",
    "\n",
    "import scala.util.Random\n",
    "import scala.math.sqrt\n",
    "import scala.math.max\n",
    "\n",
    "println(\"Graph export!\")\n",
    "\n",
    "val eps = 0.00001\n",
    "\n",
    "// # Assumption: Gaussian rewards with known variance\n",
    "// # Prior graph (map weight to prior mean and variance)\n",
    "// TODO: Prior mean from real graph\n",
    "val priorMean = 100.0\n",
    "val priorVar = 10.0\n",
    "val prior = graph.mapEdges(e => (priorMean, priorVar))\n",
    "\n",
    "// # Environment (sample true environment from prior)\n",
    "val env = prior.mapEdges(e => (max(eps, e.attr._1 + Random.nextGaussian() * sqrt(e.attr._2)), 0.0))\n",
    "\n",
    "// # Display DF\n",
    "display(env.edges.toDF.select($\"srcId\".as(\"nodeFrom\"), $\"dstId\".as(\"nodeTo\"), $\"attr._1\".as(\"Length\"), \n",
    "                              $\"attr._2\".as(\"fromLat\"), $\"attr._2\".as(\"fromLon\"),\n",
    "                              $\"attr._2\".as(\"toLat\"), $\"attr._2\".as(\"toLon\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
