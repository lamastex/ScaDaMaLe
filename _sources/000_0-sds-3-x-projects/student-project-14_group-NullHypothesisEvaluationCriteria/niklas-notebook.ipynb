{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// (copied from http://lamastex.org/lmse/mep/src/GraphXShortestWeightedPaths.html)\n",
    "\n",
    "import scala.util.Random\n",
    "import scala.math.max\n",
    "\n",
    "import org.apache.spark.graphx.{Graph, VertexId}\n",
    "import org.apache.spark.graphx.util.GraphGenerators\n",
    "\n",
    "// A graph with edge attributes containing distances\n",
    "val numVertices = 100000\n",
    "val graph: Graph[Long, Double] = GraphGenerators.logNormalGraph(sc, numVertices = numVertices, seed=123L).mapEdges { e => \n",
    "  // to make things nicer we assign 0 distance to itself\n",
    "  if (e.srcId == e.dstId) 0.0 else Random.nextDouble()\n",
    "}.groupEdges((attr1, attr2) => max(attr1, attr2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import scala.util.Random\n",
    ">     import scala.math.max\n",
    ">     import org.apache.spark.graphx.{Graph, VertexId}\n",
    ">     import org.apache.spark.graphx.util.GraphGenerators\n",
    ">     numVertices: Int = 100000\n",
    ">     graph: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@2aab3532"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// # High-level overview of the structure (pseudo code)\n",
    "\n",
    "// # Horizon N\n",
    "\n",
    "// # Environment\n",
    "// val true_mean = 1\n",
    "// val true_var = 1\n",
    "// val env = graph.map(attr -> (true_mean, true_var))\n",
    "\n",
    "// # Assumption: Gaussian rewards with known variance\n",
    "// # Prior graph (map weight to prior mean and variance)\n",
    "// val prior_mean = 1\n",
    "// val prior_var = 1\n",
    "// var prior = graph.map(attr -> (prior_mean, prior_var))\n",
    "\n",
    "// # For regret calculations\n",
    "// val optimal = ShortestPath.apply(env.map(attr -> attr._1))\n",
    "// val optimal_expected_cost = optimal.join(env).map((o, e) -> o.attr * e.attr._1).reduce(_ + _)\n",
    "\n",
    "// # List with instant regret values\n",
    "// val instant_regret_values = List(N)\n",
    "\n",
    "// For t = 1,..,N:\n",
    "\n",
    "    // # Find action (super arm)\n",
    "    // If agent is greedy:\n",
    "        // val action = ShortestPath.apply(prior.map(attr -> attr._1))\n",
    "    // Else If agent is Thompson Sampling:\n",
    "        // val sampled = prior.map(attr -> Gaussian.random(attr._1, attr._2))\n",
    "        // val action = ShortestPath.apply(sampled)\n",
    "    \n",
    "    // # Apply action on environments (assuming path is indicated by 1-valued edge attributes)\n",
    "    // val observation = action.join(env.map(attr -> Gaussian.random(attr._1, attr._2))).map((a, e) -> a.attr * e.attr)\n",
    "    \n",
    "    // # Update posterior\n",
    "    // val posterior = observation.join(prior).map((o, p) -> ((1/(1/true_var + 1/p.attr._2))*(o.attr/true_var + p.attr._1/p.attr_2), (1/(1/true_var + 1/p.attr._2))))\n",
    "\n",
    "    // # Set next prior to current posterior\n",
    "    // prior = posterior\n",
    "\n",
    "    // # Calculate regret\n",
    "    // val action_expected_cost = action.join(env).map((a, e) -> a.attr * e.attr._1).reduce(_ + _)\n",
    "    // val instant_regret = action_expected_cost - optimal_expected_cost\n",
    "    // instant_regret_value(t) = instant_regret\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.reflect.ClassTag\n",
    "import org.apache.spark.graphx._\n",
    "\n",
    "/**\n",
    " * Computes shortest weighted paths to the given set of landmark vertices, returning a graph where each\n",
    " * vertex attribute is a map containing the shortest-path distance to each reachable landmark.\n",
    " * Currently supports only Graph of [VD, Double], where VD is an arbitrary vertex type.\n",
    " *\n",
    " * The object also include a function which transforms the resulting graph into a path_graph between a \n",
    " * specific starting node and goal node. Each edge in the path_grpah is either 1 or 0 depending if it is \n",
    " * the shortest path or not.\n",
    " *\n",
    " */\n",
    "object ShortestPath extends Serializable {\n",
    "\n",
    "  // When finding the shortest path each node stores a map from the itself to each goal node.\n",
    "  // The map returns an array includeing the total distance to the goal node as well as the\n",
    "  // next node pn the shortest path to the goal node. The last value in the array is only \n",
    "  // populated with the nodes own id and is only used for computational convenience. \n",
    "  type SPMap = Map[VertexId, Tuple3[Double, VertexId, VertexId]]\n",
    "  \n",
    "  // PN holds the information of the path nodes which are used for creating a path graph\n",
    "  // PN = ('Distance left to goal node', 'Next path node id', 'Goal node', 'Is on path')\n",
    "  type PN = Tuple4[Double, VertexId, VertexId, Boolean] \n",
    "  \n",
    "  private val INITIAL_DIST = 0.0\n",
    "  private val DEFAULT_ID = -1L\n",
    "  private val INFINITY = Int.MaxValue.toDouble\n",
    "\n",
    "  private def makeMap(x: (VertexId, Tuple3[Double, VertexId, VertexId])*) = Map(x: _*)\n",
    "  \n",
    "  private def incrementMap(spmap: SPMap, delta: Double, id: VertexId): SPMap = { \n",
    "    spmap.map { case (v, d) => v -> (Tuple3(d._1 + delta, d._3, id)) }\n",
    "  }\n",
    "\n",
    "  private def addMaps(spmap1: SPMap, spmap2: SPMap): SPMap = {\n",
    "    (spmap1.keySet ++ spmap2.keySet).map {\n",
    "    k =>{\n",
    "        if (spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1 < spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1) \n",
    "                k -> (Tuple3(spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1, \n",
    "                             spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._2, \n",
    "                             spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._3))\n",
    "        else \n",
    "                k -> (Tuple3(spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1, \n",
    "                             spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._2, \n",
    "                             spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._3))\n",
    "        }\n",
    "    }.toMap\n",
    "  }\n",
    "  \n",
    "  // at this point it does not really matter what vertex type is\n",
    "  def run[VD](graph: Graph[VD, Double], landmarks: Seq[VertexId]): Graph[SPMap, Double] = {\n",
    "    val spGraph = graph.mapVertices { (vid, attr) =>\n",
    "      // initial value for itself is 0.0 as Double\n",
    "      if (landmarks.contains(vid)) makeMap(vid -> Tuple3(INITIAL_DIST, DEFAULT_ID, DEFAULT_ID)) else makeMap()\n",
    "    }\n",
    "\n",
    "    val initMaps = makeMap()\n",
    "\n",
    "    def vProg(id: VertexId, attr: SPMap, msg: SPMap): SPMap = {\n",
    "      addMaps(attr, msg)\n",
    "    }\n",
    "\n",
    "    def sendMsg(edge: EdgeTriplet[SPMap, Double]): Iterator[(VertexId, SPMap)] = {\n",
    "      val newAttr = incrementMap(edge.dstAttr, edge.attr, edge.srcId)\n",
    "      if (edge.srcAttr != addMaps(newAttr, edge.srcAttr)) Iterator((edge.srcId, newAttr))\n",
    "      else Iterator.empty\n",
    "    }\n",
    "\n",
    "    Pregel(spGraph, initMaps)(vProg, sendMsg, addMaps)\n",
    "  }\n",
    "  \n",
    "  def create_path_graph[VD](graph: Graph[SPMap, Double], goalId: VertexId, startId: VertexId): Graph[PN, Int] = {\n",
    "    // For a given goal node we remove the lookup map and extend the state to a Tuple5 with the goal id and a boolean\n",
    "    val path = graph.mapEdges(e => 0)\n",
    "              .mapVertices((vertixId, attr) => {\n",
    "                if (attr.contains(goalId)) {\n",
    "                  val path_step = attr(goalId)\n",
    "                  if (vertixId == path_step._3 && path_step._2 == -1L)\n",
    "                    (path_step._1, goalId, goalId, false) // while we are at it, we clean up the state a bit\n",
    "                  else  \n",
    "                    (path_step._1, path_step._2, goalId, false)\n",
    "                } else// If the vertice does not have a map to our goal we add a default value to it\n",
    "                    (INFINITY, -1L, -1L, false)\n",
    "              })\n",
    "\n",
    "      def mergeMsg(msg1: VertexId, msg2: VertexId): VertexId = { // we should only get one msg\n",
    "          msg2\n",
    "      }\n",
    "\n",
    "      def vprog(id: VertexId, attr: PN, msg: VertexId): PN = {\n",
    "        // Check that the current node is the one adressed in the message\n",
    "        if (id == msg)\n",
    "          (attr._1, attr._2, attr._3, true)\n",
    "        else // If the message is not addressed to the current node (happens for inital message), use the old value \n",
    "          attr\n",
    "      }\n",
    "      def sendMsg(triplet: EdgeTriplet[PN, Int]): Iterator[(VertexId, VertexId)] = {\n",
    "        // If dstId is the next node on the path and has not yet been activated\n",
    "        if (triplet.srcAttr._2 == triplet.dstId && triplet.srcAttr._4 && !triplet.dstAttr._4) \n",
    "          Iterator((triplet.dstId, triplet.dstId))// Send next msg\n",
    "        else\n",
    "          Iterator.empty// Do nothing\n",
    "      }\n",
    "\n",
    "      Pregel(path, startId)(vprog, sendMsg, mergeMsg).mapTriplets(triplet => {\n",
    "        if(triplet.srcAttr._2 == triplet.dstId && triplet.srcAttr._4)\n",
    "          1\n",
    "        else\n",
    "          0\n",
    "      })\n",
    "  }\n",
    "}\n",
    "\n",
    "println(\"Usage: val result = GraphXShortestWeightedPaths.run(graph, Seq(4L, 0L, 9L))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Usage: val result = GraphXShortestWeightedPaths.run(graph, Seq(4L, 0L, 9L))\n",
    ">     import scala.reflect.ClassTag\n",
    ">     import org.apache.spark.graphx._\n",
    ">     defined object ShortestPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Random\n",
    "\n",
    "def shortestPath(srcId : Long, dstId : Long, graph : Graph[Long, Double], placeholder: Boolean) : Graph[Long, Double] = {\n",
    "  if (placeholder) {\n",
    "    return graph.mapEdges(e => Random.nextInt(2))\n",
    "  } else {\n",
    "    val distanceGraph = ShortestPath.run(graph, Seq(dstId))\n",
    "    val pathGraph = ShortestPath.create_path_graph(distanceGraph, dstId, srcId)\n",
    "    return pathGraph.mapVertices((vid, attr) => 0L).mapEdges(e => e.attr)\n",
    "  }\n",
    "}\n",
    "\n",
    "def shortestPath(srcId : Long, dstId : Long, graph : Graph[Long, Double]) : Graph[Long, Double] = {\n",
    "  return shortestPath(srcId, dstId, graph, false)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import scala.util.Random\n",
    ">     shortestPath: (srcId: Long, dstId: Long, graph: org.apache.spark.graphx.Graph[Long,Double], placeholder: Boolean)org.apache.spark.graphx.Graph[Long,Double] <and> (srcId: Long, dstId: Long, graph: org.apache.spark.graphx.Graph[Long,Double])org.apache.spark.graphx.Graph[Long,Double]\n",
    ">     shortestPath: (srcId: Long, dstId: Long, graph: org.apache.spark.graphx.Graph[Long,Double], placeholder: Boolean)org.apache.spark.graphx.Graph[Long,Double] <and> (srcId: Long, dstId: Long, graph: org.apache.spark.graphx.Graph[Long,Double])org.apache.spark.graphx.Graph[Long,Double]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val wrapperPath = shortestPath(0L, 99L, graph)\n",
    "//display(wrapperPath.edges.toDF.filter('attr === 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val srcId = 0L\n",
    "//val dstId = numVertices - 1L\n",
    "//val distanceGraph = ShortestPath.run(graph, Seq(dstId))\n",
    "//val pathGraph = ShortestPath.create_path_graph(distanceGraph, dstId, srcId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//import org.apache.spark.sql.functions._ \n",
    "//display(distanceGraph.vertices.toDF.orderBy(asc(\"_1\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//display(pathGraph.edges.toDF.filter('attr === 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//display(graph.edges.toDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.reflect.ClassTag\n",
    "\n",
    "// # Merge edge attributes of two (identical in structure) graphs\n",
    "def mergeEdgeAttributes[ED1 : ClassTag, ED2 : ClassTag](firstGraph : Graph[Long, ED1], secondGraph : Graph[Long, ED2]) : Graph[Long, (ED1, ED2)] = {\n",
    "  return Graph(firstGraph.vertices, firstGraph.edges.innerJoin(secondGraph.edges) {(id1, id2, first, second) => (first, second)}) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import scala.reflect.ClassTag\n",
    ">     mergeEdgeAttributes: [ED1, ED2](firstGraph: org.apache.spark.graphx.Graph[Long,ED1], secondGraph: org.apache.spark.graphx.Graph[Long,ED2])(implicit evidence$1: scala.reflect.ClassTag[ED1], implicit evidence$2: scala.reflect.ClassTag[ED2])org.apache.spark.graphx.Graph[Long,(ED1, ED2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Random\n",
    "import scala.math.sqrt\n",
    "import scala.math.max\n",
    "\n",
    "println(\"Starting experiment!\")\n",
    "val startTime = System.currentTimeMillis()\n",
    "\n",
    "val eps = 0.00001\n",
    "\n",
    "// # Horizon N\n",
    "val N = 10\n",
    "\n",
    "// # Source and destination node IDs\n",
    "val srcId = 0L\n",
    "val dstId = numVertices - 1L\n",
    "\n",
    "// # Environment\n",
    "val trueMean = 1.0\n",
    "val trueVar = 1.0\n",
    "// TODO: Real environment (either random or from road map)\n",
    "val env = graph.mapEdges(e => (trueMean, trueVar))\n",
    "\n",
    "// # Assumption: Gaussian rewards with known variance\n",
    "// # Prior graph (map weight to prior mean and variance)\n",
    "val priorMean = 1.0\n",
    "val priorVar = 1.0\n",
    "val prior = graph.mapEdges(e => (priorMean, priorVar))\n",
    "var posterior = prior\n",
    "\n",
    "// # For regret calculations\n",
    "val optimal = shortestPath(srcId, dstId, env.mapEdges(e => e.attr._1))\n",
    "val optimalExpectedCost = mergeEdgeAttributes(optimal, env).edges.map(e => e.attr._1 * e.attr._2._1).reduce(_ + _)\n",
    "\n",
    "// # Array with instant regret values\n",
    "val instantRegretValues = new Array[Double](N)\n",
    "\n",
    "// # Run experiment for N iterations\n",
    "for (t <- 0 until N) {\n",
    "  printf(\"Iteration %d, elapsed time: %d ms\\n\", t, System.currentTimeMillis() - startTime)\n",
    "  \n",
    "  // # Find action (super arm) using posterior sampling\n",
    "  val sampledParameters = posterior.mapEdges(e => max(eps, e.attr._1 + Random.nextGaussian() * sqrt(e.attr._2)))\n",
    "  val action = shortestPath(srcId, dstId, sampledParameters)\n",
    "  \n",
    "  // # Apply action on environments (assuming path is indicated by 1-valued edge attributes) and observe realized costs\n",
    "  val realizedEnv = env.mapEdges(e => max(eps, e.attr._1 + Random.nextGaussian() * sqrt(e.attr._2)))\n",
    "  val observation = mergeEdgeAttributes(action, realizedEnv).mapEdges(e => e.attr._1 * e.attr._1)\n",
    "  \n",
    "  // # Update posterior\n",
    "  posterior = mergeEdgeAttributes(observation, posterior).mapEdges(e => {\n",
    "    val obs = e.attr._1\n",
    "    val pMean = e.attr._2._1\n",
    "    val pVar = e.attr._2._2\n",
    "    ((1/(1/trueVar + 1/pVar))*(obs/trueVar + pMean/pVar), (1/(1/trueVar + 1/pVar)))\n",
    "  })\n",
    "  \n",
    "  // # Calculate regret\n",
    "  val actionExpectedCost = mergeEdgeAttributes(action, env).edges.map(e => e.attr._1 * e.attr._2._1).reduce(_ + _)\n",
    "  val instantRegret = actionExpectedCost - optimalExpectedCost\n",
    "  instantRegretValues(t) = instantRegret\n",
    "}\n",
    "\n",
    "val endTime = System.currentTimeMillis()\n",
    "printf(\"Finished experiment! Elapsed time:%d\\n\", endTime - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Starting experiment!\n",
    ">     Iteration 0, elapsed time: 30437 ms\n",
    ">     Iteration 1, elapsed time: 83241 ms\n",
    ">     Iteration 2, elapsed time: 203896 ms\n",
    ">     Iteration 3, elapsed time: 268759 ms\n",
    ">     Iteration 4, elapsed time: 324692 ms\n",
    ">     Iteration 5, elapsed time: 452711 ms\n",
    ">     Iteration 6, elapsed time: 512567 ms\n",
    ">     Iteration 7, elapsed time: 583455 ms\n",
    ">     Iteration 8, elapsed time: 675289 ms\n",
    ">     Iteration 9, elapsed time: 721292 ms\n",
    ">     Finished experiment! Elapsed time:800570\n",
    ">     import scala.util.Random\n",
    ">     import scala.math.sqrt\n",
    ">     import scala.math.max\n",
    ">     startTime: Long = 1608632014009\n",
    ">     eps: Double = 1.0E-5\n",
    ">     N: Int = 10\n",
    ">     srcId: Long = 0\n",
    ">     dstId: Long = 99999\n",
    ">     trueMean: Double = 1.0\n",
    ">     trueVar: Double = 1.0\n",
    ">     env: org.apache.spark.graphx.Graph[Long,(Double, Double)] = org.apache.spark.graphx.impl.GraphImpl@45b8191e\n",
    ">     priorMean: Double = 1.0\n",
    ">     priorVar: Double = 1.0\n",
    ">     prior: org.apache.spark.graphx.Graph[Long,(Double, Double)] = org.apache.spark.graphx.impl.GraphImpl@2f453bc2\n",
    ">     posterior: org.apache.spark.graphx.Graph[Long,(Double, Double)] = org.apache.spark.graphx.impl.GraphImpl@7f828d32\n",
    ">     optimal: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@3bf78954\n",
    ">     optimalExpectedCost: Double = 3.0\n",
    ">     instantRegretValues: Array[Double] = Array(2.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0)\n",
    ">     endTime: Long = 1608632814579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Visualization\n",
    "val df = spark.sparkContext.parallelize((1 to N) zip instantRegretValues).toDF(\"iteration (t)\",\"instant regret\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +-------------+--------------+\n",
    ">     |iteration (t)|instant regret|\n",
    ">     +-------------+--------------+\n",
    ">     |            1|           1.0|\n",
    ">     |            2|           1.0|\n",
    ">     |            3|           1.0|\n",
    ">     |            4|           0.0|\n",
    ">     |            5|           1.0|\n",
    ">     |            6|           0.0|\n",
    ">     |            7|           1.0|\n",
    ">     |            8|           1.0|\n",
    ">     |            9|           1.0|\n",
    ">     |           10|           0.0|\n",
    ">     +-------------+--------------+\n",
    ">\n",
    ">     df: org.apache.spark.sql.DataFrame = [iteration (t): int, instant regret: double]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.edges.toDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res34: Long = 12654999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val testArray = Array(0,1,2,3,4,5,6,7,8,9)\n",
    "println(testArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     [I@4cb3c696\n",
    ">     testArray: Array[Int] = Array(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val testZipped = testArray.slice(0,testArray.size-1) zip testArray.slice(1,testArray.size)\n",
    "val testZipped = testArray.sliding(2,1).toArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     testZipped: Array[Array[Int]] = Array(Array(0, 1), Array(1, 2), Array(2, 3), Array(3, 4), Array(4, 5), Array(5, 6), Array(6, 7), Array(7, 8), Array(8, 9))"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
