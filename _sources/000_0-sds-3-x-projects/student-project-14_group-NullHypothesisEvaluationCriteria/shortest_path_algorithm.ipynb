{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending spark.graphx.lib.ShortestPaths to GraphXShortestWeightedPaths\n",
    "=======================================================================\n",
    "\n",
    "### 2016-2020, Ivan Sadikov and Raazesh Sainudiin\n",
    "\n",
    "We extend Shortest Paths algorithm in Spark's GraphX Library to allow\n",
    "for user-specified edge-weights as an edge attribute.\n",
    "\n",
    "This is part of *Project MEP: Meme Evolution Programme* and supported by\n",
    "databricks academic partners program.\n",
    "\n",
    "The analysis is available in the following databricks notebook: \\*\n",
    "<http://lamastex.org/lmse/mep/src/GraphXShortestWeightedPaths.html>\n",
    "\n",
    "\\`\\`\\` Copyright 2016 Ivan Sadikov and Raazesh Sainudiin\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n",
    "not use this file except in compliance with the License. You may obtain\n",
    "a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License. \\`\\`\\`\n",
    "\n",
    "### Let's modify shortest paths algorithm to allow for user-specified edge-weights\n",
    "\n",
    "Update shortest paths algorithm to work over edge attribute of\n",
    "edge-weights as Double, key concepts are: - we increment map with delta,\n",
    "which is `edge.attr` - edge attribute is anything numeric, tested on\n",
    "Double - infinity value is not infinity, but `Integer.MAX_VALUE`\n",
    "\n",
    "Modifying the following code: \\*\n",
    "https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/ShortestPaths.scala\n",
    "\n",
    "Explained here: \\*\n",
    "http://note.yuhc.me/2015/03/graphx-pregel-shortest-path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.reflect.ClassTag\n",
    "import org.apache.spark.graphx._\n",
    "\n",
    "/**\n",
    " * Computes shortest weighted paths to the given set of landmark vertices, returning a graph where each\n",
    " * vertex attribute is a map containing the shortest-path distance to each reachable landmark.\n",
    " * Currently supports only Graph of [VD, Double], where VD is an arbitrary vertex type.\n",
    " *\n",
    " * The object also include a function which transforms the resulting graph into a path_graph between a \n",
    " * specific starting node and goal node. Each edge in the path_grpah is either 1 or 0 depending if it is \n",
    " * the shortest path or not.\n",
    " *\n",
    " */\n",
    "object ShortestPath extends Serializable {\n",
    "\n",
    "  // When finding the shortest path each node stores a map from the itself to each goal node.\n",
    "  // The map returns an array includeing the total distance to the goal node as well as the\n",
    "  // next node pn the shortest path to the goal node. The last value in the array is only \n",
    "  // populated with the nodes own id and is only used for computational convenience. \n",
    "  type SPMap = Map[VertexId, Tuple3[Double, VertexId, VertexId]]\n",
    "  \n",
    "  // PN holds the information of the path nodes which are used for creating a path graph\n",
    "  // PN = ('Distance left to goal node', 'Next path node id', 'Goal node', 'Is on path')\n",
    "  type PN = Tuple4[Double, VertexId, VertexId, Boolean] \n",
    "  \n",
    "  private val INITIAL_DIST = 0.0\n",
    "  private val DEFAULT_ID = -1L\n",
    "  private val INFINITY = Int.MaxValue.toDouble\n",
    "\n",
    "  private def makeMap(x: (VertexId, Tuple3[Double, VertexId, VertexId])*) = Map(x: _*)\n",
    "  \n",
    "  private def incrementMap(spmap: SPMap, delta: Double, id: VertexId): SPMap = { \n",
    "    spmap.map { case (v, d) => v -> (Tuple3(d._1 + delta, d._3, id)) }\n",
    "  }\n",
    "\n",
    "  private def addMaps(spmap1: SPMap, spmap2: SPMap): SPMap = {\n",
    "    (spmap1.keySet ++ spmap2.keySet).map {\n",
    "    k =>{\n",
    "        if (spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1 < spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1) \n",
    "                k -> (Tuple3(spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1, \n",
    "                             spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._2, \n",
    "                             spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._3))\n",
    "        else \n",
    "                k -> (Tuple3(spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1, \n",
    "                             spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._2, \n",
    "                             spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._3))\n",
    "        }\n",
    "    }.toMap\n",
    "  }\n",
    "  \n",
    "  // at this point it does not really matter what vertex type is\n",
    "  def run[VD](graph: Graph[VD, Double], landmarks: Seq[VertexId]): Graph[SPMap, Double] = {\n",
    "    val spGraph = graph.mapVertices { (vid, attr) =>\n",
    "      // initial value for itself is 0.0 as Double\n",
    "      if (landmarks.contains(vid)) makeMap(vid -> Tuple3(INITIAL_DIST, DEFAULT_ID, DEFAULT_ID)) else makeMap()\n",
    "    }\n",
    "\n",
    "    val initMaps = makeMap()\n",
    "\n",
    "    def vProg(id: VertexId, attr: SPMap, msg: SPMap): SPMap = {\n",
    "      addMaps(attr, msg)\n",
    "    }\n",
    "\n",
    "    def sendMsg(edge: EdgeTriplet[SPMap, Double]): Iterator[(VertexId, SPMap)] = {\n",
    "      val newAttr = incrementMap(edge.dstAttr, edge.attr, edge.srcId)\n",
    "      if (edge.srcAttr != addMaps(newAttr, edge.srcAttr)) Iterator((edge.srcId, newAttr))\n",
    "      else Iterator.empty\n",
    "    }\n",
    "\n",
    "    Pregel(spGraph, initMaps)(vProg, sendMsg, addMaps)\n",
    "  }\n",
    "  \n",
    "  def create_path_graph[VD](graph: Graph[SPMap, Double], goalId: VertexId, startId: VertexId): Graph[PN, Int] = {\n",
    "    // For a given goal node we remove the lookup map and extend the state to a Tuple5 with the goal id and a boolean\n",
    "    val path = graph.mapEdges(e => 0)\n",
    "              .mapVertices((vertixId, attr) => {\n",
    "                if (attr.contains(goalId)) {\n",
    "                  val path_step = attr(goalId)\n",
    "                  if (vertixId == path_step._3 && path_step._2 == -1L)\n",
    "                    (path_step._1, goalId, goalId, false) // while we are at it, we clean up the state a bit\n",
    "                  else  \n",
    "                    (path_step._1, path_step._2, goalId, false)\n",
    "                } else// If the vertice does not have a map to our goal we add a default value to it\n",
    "                    (INFINITY, -1L, -1L, false)\n",
    "              })\n",
    "\n",
    "      def mergeMsg(msg1: VertexId, msg2: VertexId): VertexId = { // we should only get one msg\n",
    "          msg2\n",
    "      }\n",
    "\n",
    "      def vprog(id: VertexId, attr: PN, msg: VertexId): PN = {\n",
    "        // Check that the current node is the one adressed in the message\n",
    "        if (id == msg)\n",
    "          (attr._1, attr._2, attr._3, true)\n",
    "        else // If the message is not addressed to the current node (happens for inital message), use the old value \n",
    "          attr\n",
    "      }\n",
    "      def sendMsg(triplet: EdgeTriplet[PN, Int]): Iterator[(VertexId, VertexId)] = {\n",
    "        // If dstId is the next node on the path and has not yet been activated\n",
    "        if (triplet.srcAttr._2 == triplet.dstId && triplet.srcAttr._4 && !triplet.dstAttr._4) \n",
    "          Iterator((triplet.dstId, triplet.dstId))// Send next msg\n",
    "        else\n",
    "          Iterator.empty// Do nothing\n",
    "      }\n",
    "\n",
    "      Pregel(path, startId)(vprog, sendMsg, mergeMsg).mapTriplets(triplet => {\n",
    "        if(triplet.srcAttr._2 == triplet.dstId && triplet.srcAttr._4)\n",
    "          1\n",
    "        else\n",
    "          0\n",
    "      })\n",
    "  }\n",
    "}\n",
    "\n",
    "println(\"Usage: val result = GraphXShortestWeightedPaths.run(graph, Seq(4L, 0L, 9L))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Usage: val result = GraphXShortestWeightedPaths.run(graph, Seq(4L, 0L, 9L))\n",
    ">     import scala.reflect.ClassTag\n",
    ">     import org.apache.spark.graphx._\n",
    ">     defined object ShortestPath\n",
    "\n",
    "  \n",
    "\n",
    "### Generate test graph\n",
    "\n",
    "Generate simple graph with double weights for edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Random\n",
    "\n",
    "import org.apache.spark.graphx.{Graph, VertexId}\n",
    "import org.apache.spark.graphx.util.GraphGenerators\n",
    "\n",
    "// A graph with edge attributes containing distances\n",
    "val graph: Graph[Long, Double] = GraphGenerators.logNormalGraph(sc, numVertices = 150, seed=123L).mapEdges { e => \n",
    "  // to make things nicer we assign 0 distance to itself\n",
    "  if (e.srcId == e.dstId) 0.0 else Random.nextDouble()\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import scala.util.Random\n",
    ">     import org.apache.spark.graphx.{Graph, VertexId}\n",
    ">     import org.apache.spark.graphx.util.GraphGenerators\n",
    ">     graph: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@7d63ae0f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create an RDD for the vertices\n",
    "val nodes: RDD[(VertexId, String)] =\n",
    "  sc.parallelize(Seq((0L, \"0\"), \n",
    "                     (1L, \"1\"), \n",
    "                     (2L, \"2\"), \n",
    "                     (3L, \"3\"), \n",
    "                     (4L, \"4\"), \n",
    "                     (5L, \"5\"), \n",
    "                     (6L, \"6\"), \n",
    "                     (7L, \"7\")\n",
    "                    )\n",
    "                )\n",
    "\n",
    "\n",
    "// Create an RDD for edges\n",
    "val connections: RDD[Edge[Double]] =\n",
    "  sc.parallelize(Seq(Edge(0L, 1L, 1),   \n",
    "                     Edge(1L, 2L, 1),\n",
    "                     Edge(2L, 3L, 1), \n",
    "                     Edge(3L, 4L, 1),\n",
    "                     Edge(0L, 4L, 100),\n",
    "                     Edge(5L, 6L, 1),\n",
    "                     Edge(6L, 7L, 1),\n",
    "                     Edge(7L, 5L, 1),\n",
    "                     Edge(4L, 2L, 1),\n",
    "                     Edge(2L, 3L, 1)\n",
    "                    )\n",
    "                )\n",
    "// Build the initial Graph\n",
    "val default_node = \"default\"\n",
    "val graph = Graph(nodes, connections, default_node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     nodes: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, String)] = ParallelCollectionRDD[18056] at parallelize at command-1884488408458313:3\n",
    ">     connections: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] = ParallelCollectionRDD[18057] at parallelize at command-1884488408458313:17\n",
    ">     default_node: String = default\n",
    ">     graph: org.apache.spark.graphx.Graph[String,Double] = org.apache.spark.graphx.impl.GraphImpl@6901db86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.vertices.take(150).foreach(println)\n",
    "graph.edges.take(150).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     (0,0)\n",
    ">     (1,1)\n",
    ">     (2,2)\n",
    ">     (3,3)\n",
    ">     (4,4)\n",
    ">     (5,5)\n",
    ">     (6,6)\n",
    ">     (7,7)\n",
    ">     Edge(0,1,1.0)\n",
    ">     Edge(1,2,1.0)\n",
    ">     Edge(2,3,1.0)\n",
    ">     Edge(3,4,1.0)\n",
    ">     Edge(0,4,100.0)\n",
    ">     Edge(5,6,1.0)\n",
    ">     Edge(6,7,1.0)\n",
    ">     Edge(7,5,1.0)\n",
    ">     Edge(3,2,1.0)\n",
    ">     Edge(2,3,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val landMarkVertexIds = Seq(4L)\n",
    "val result = ShortestPath.run(graph, landMarkVertexIds)\n",
    "// Found shortest paths\n",
    "println(result.vertices.collect.mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     (0,Map(4 -> (4.0,1,0)))\n",
    ">     (1,Map(4 -> (3.0,2,1)))\n",
    ">     (2,Map(4 -> (2.0,3,2)))\n",
    ">     (3,Map(4 -> (1.0,-1,3)))\n",
    ">     (4,Map(4 -> (0.0,-1,-1)))\n",
    ">     (5,Map())\n",
    ">     (6,Map())\n",
    ">     (7,Map())\n",
    ">     landMarkVertexIds: Seq[Long] = List(4)\n",
    ">     result: org.apache.spark.graphx.Graph[ShortestPath.SPMap,Double] = org.apache.spark.graphx.impl.GraphImpl@139aeb54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " val path_graph = ShortestPath.create_path_graph(result, 4L, 2L)\n",
    "println(path_graph.vertices.collect.mkString(\"\\n\"))\n",
    "println(path_graph.edges.collect.mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     (0,(4.0,1,4,false))\n",
    ">     (1,(3.0,2,4,false))\n",
    ">     (2,(2.0,3,4,true))\n",
    ">     (3,(1.0,4,4,true))\n",
    ">     (4,(0.0,-1,4,true))\n",
    ">     (5,(2.147483647E9,-1,-1,false))\n",
    ">     (6,(2.147483647E9,-1,-1,false))\n",
    ">     (7,(2.147483647E9,-1,-1,false))\n",
    ">     Edge(0,1,0)\n",
    ">     Edge(1,2,0)\n",
    ">     Edge(2,3,1)\n",
    ">     Edge(3,4,1)\n",
    ">     Edge(0,4,0)\n",
    ">     Edge(5,6,0)\n",
    ">     Edge(6,7,0)\n",
    ">     Edge(7,5,0)\n",
    ">     Edge(4,2,0)\n",
    ">     Edge(2,3,1)\n",
    ">     path_graph: org.apache.spark.graphx.Graph[ShortestPath.PN,Int] = org.apache.spark.graphx.impl.GraphImpl@f2499ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(path_graph.edges.toDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val startId = 5L\n",
    "val goalId = 4L\n",
    "val INFINITY = Int.MaxValue.toDouble\n",
    "// set all edges to zero\n",
    "// For a given goal node we remove the lookup map and extend the state to a Tuple5 with the goal id and a boolean\n",
    "val path = result.mapEdges(e => 0)\n",
    "                 .mapVertices((vertixId, attr) => {\n",
    "                   if (attr.contains(goalId)) {\n",
    "                     val path_step = attr(goalId)\n",
    "                     if (vertixId == path_step._3 && path_step._2 == -1L)\n",
    "                       (path_step._1, goalId, goalId, false) // while we are at it, we clean up the state a bit\n",
    "                     else  \n",
    "                       (path_step._1, path_step._2, goalId, false)\n",
    "                   } else// If the vertice does not have a map to our goal we add a default value to it\n",
    "                       (INFINITY, -1L, -1L, false)\n",
    "                 })\n",
    "println(path.vertices.collect.mkString(\"\\n\"))\n",
    "// PN = ('Distance to goal node', 'Next path node id', 'Goal node', 'Is on path')\n",
    "type PN = Tuple4[Double, VertexId, VertexId, Boolean] \n",
    "\n",
    "val initMsg = startId\n",
    "\n",
    "def mergeMsg(msg1: VertexId, msg2: VertexId): VertexId = { // we should only get one msg\n",
    "    msg1\n",
    "}\n",
    "\n",
    "def vprog(id: VertexId, attr: PN, msg: VertexId): PN = {\n",
    "  // Check that the current node is the one adressed in the message\n",
    "  if (id == msg)\n",
    "    (attr._1, attr._2, attr._3, true)\n",
    "  else // If the message is not addressed to the current node (happens for inital message), use the old value \n",
    "    (attr._1, attr._2, attr._3, attr._4)\n",
    "}\n",
    "def sendMsg(triplet: EdgeTriplet[PN, Int]): Iterator[(VertexId, VertexId)] = {\n",
    "  // If dstId is the next node on the path and has not yet been activated\n",
    "  if (triplet.srcAttr._2 == triplet.dstId & triplet.srcAttr._4 == true & triplet.dstAttr._4 != true) \n",
    "    Iterator((triplet.dstId, triplet.dstId))// Send next msg\n",
    "  else\n",
    "    Iterator.empty// Do nothing\n",
    "}\n",
    "\n",
    "val result2 = Pregel(path, initMsg)(vprog, sendMsg, mergeMsg)\n",
    "println(result2.vertices.collect.mkString(\"\\n\"))\n",
    "\n",
    "val finalResult = result2.mapTriplets(triplet => {\n",
    "  if(triplet.srcAttr._2 == triplet.dstId && triplet.srcAttr._4 == true && triplet.dstAttr._4 == true)\n",
    "    1\n",
    "  else\n",
    "    0\n",
    "})\n",
    "\n",
    "println(finalResult.edges.collect.mkString(\"\\n\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     (0,(4.0,1,4,false))\n",
    ">     (1,(3.0,2,4,false))\n",
    ">     (2,(2.0,3,4,false))\n",
    ">     (3,(1.0,4,4,false))\n",
    ">     (4,(0.0,-1,4,false))\n",
    ">     (5,(2.147483647E9,-1,-1,false))\n",
    ">     (6,(2.147483647E9,-1,-1,false))\n",
    ">     (7,(2.147483647E9,-1,-1,false))\n",
    ">     (0,(4.0,1,4,false))\n",
    ">     (1,(3.0,2,4,false))\n",
    ">     (2,(2.0,3,4,false))\n",
    ">     (3,(1.0,4,4,false))\n",
    ">     (4,(0.0,-1,4,false))\n",
    ">     (5,(2.147483647E9,-1,-1,true))\n",
    ">     (6,(2.147483647E9,-1,-1,false))\n",
    ">     (7,(2.147483647E9,-1,-1,false))\n",
    ">     Edge(0,1,0)\n",
    ">     Edge(1,2,0)\n",
    ">     Edge(2,3,0)\n",
    ">     Edge(3,4,0)\n",
    ">     Edge(0,4,0)\n",
    ">     Edge(5,6,0)\n",
    ">     Edge(6,7,0)\n",
    ">     Edge(7,5,0)\n",
    ">     Edge(3,2,0)\n",
    ">     Edge(2,3,0)\n",
    ">     startId: Long = 5\n",
    ">     goalId: Long = 4\n",
    ">     INFINITY: Double = 2.147483647E9\n",
    ">     path: org.apache.spark.graphx.Graph[(Double, Long, Long, Boolean),Int] = org.apache.spark.graphx.impl.GraphImpl@48840ec0\n",
    ">     defined type alias PN\n",
    ">     initMsg: Long = 5\n",
    ">     mergeMsg: (msg1: org.apache.spark.graphx.VertexId, msg2: org.apache.spark.graphx.VertexId)org.apache.spark.graphx.VertexId\n",
    ">     vprog: (id: org.apache.spark.graphx.VertexId, attr: PN, msg: org.apache.spark.graphx.VertexId)PN\n",
    ">     sendMsg: (triplet: org.apache.spark.graphx.EdgeTriplet[PN,Int])Iterator[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)]\n",
    ">     result2: org.apache.spark.graphx.Graph[(Double, Long, Long, Boolean),Int] = org.apache.spark.graphx.impl.GraphImpl@583a0b1e\n",
    ">     finalResult: org.apache.spark.graphx.Graph[(Double, Long, Long, Boolean),Int] = org.apache.spark.graphx.impl.GraphImpl@3d2d339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "println(finalResult.edges.collect.mkString(\"\\n\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Edge(0,1,0)\n",
    ">     Edge(1,2,0)\n",
    ">     Edge(2,3,0)\n",
    ">     Edge(3,4,0)\n",
    ">     Edge(0,4,0)\n",
    ">     Edge(5,6,0)\n",
    ">     Edge(6,7,0)\n",
    ">     Edge(7,5,0)\n",
    ">     Edge(3,2,0)\n",
    ">     Edge(2,3,0)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
