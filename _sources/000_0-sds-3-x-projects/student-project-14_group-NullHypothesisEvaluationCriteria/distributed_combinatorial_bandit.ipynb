{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributional combinatorial bandits\n",
    "====================================\n",
    "\n",
    "Niklas Åkerblom, Jonas Nordlöf, Emilio Jorge\n",
    "\n",
    "### Idea\n",
    "\n",
    "We try to solve a routing problem, that is trying to tell a vechicle\n",
    "what the sportest path is to a destination. The problem is that the\n",
    "dispatcher knows the connections in the graph but not the length of each\n",
    "edge. The dispatcher learns how long it takes to traverse a path when a\n",
    "vehicle travels it. This makes the routing problem an online learning\n",
    "problem such that the dispatcher has to learn which paths to tell the\n",
    "vehicle to take in a way that finds the best path, both in terms of\n",
    "speed and gaining information about future good paths (more on this\n",
    "later). Additionally the edges are stochastic, such that one traversal\n",
    "is no enough to get perfect information.\n",
    "\n",
    "This setting can be seen as a case of a combinatorial bandit where we\n",
    "have to select a set of edges that reach the destination from our start\n",
    "while balancing the need of getting a fast route with obtaining better\n",
    "estimates of edges such that future paths can be more efficient (this is\n",
    "known as exploration-explotation tradeoff).\n",
    "\n",
    "Distributing this task could be an interesting idea, both since multiple\n",
    "dispatchers and vehicles could work in parallell (which we do not\n",
    "consider here) but also that large graphs can be sped up through\n",
    "distributed computations in the shortest path problems that arise.\n",
    "\n",
    "### Practicalities\n",
    "\n",
    "To make our task more realistic we have used data from OpenStreetMap, a\n",
    "collection of real world map data to create a graph consisting of real\n",
    "world roads. We also generate some synthetic data to experiment with.\n",
    "\n",
    "The graph network then goes into our contextual bandit algorithm which\n",
    "samples edge weights from a belief and then selects the shortest path\n",
    "from this sampled graph. This leads to an algorithm with very nice\n",
    "theoretical properties in terms of online learning.\n",
    "\n",
    "A video were we describe this notebook can be found at\n",
    "[VIDEO](https://drive.google.com/file/d/1fOwzMlVK0M4fZ6HynQWBYlavu--hfovd/view)\n",
    "\n",
    "Loading of Openstreetmap data\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import scala.sys.process._\n",
    "import org.apache.spark.sql.functions.{col}\n",
    "\n",
    "def toMap(tupesArray: Seq[Row]): Option[Map[String, String]] = {\n",
    "    if (tupesArray == null) {\n",
    "      None\n",
    "    } else {\n",
    "      val tuples = tupesArray.map(e => {\n",
    "        (\n",
    "          e.getAs[String](\"key\"),\n",
    "          e.getAs[String](\"value\")\n",
    "        )\n",
    "      })\n",
    "      Some(tuples.toMap)\n",
    "    }\n",
    "  }\n",
    "\n",
    "def handleCommon()(df:DataFrame):DataFrame = {\n",
    "  val toMapUDF = udf(toMap _)\n",
    "  df.drop(\"uid\", \"user_sid\", \"changeset\", \"version\", \"timestamp\")\n",
    "    .withColumn(\"tags\", toMapUDF(col(\"tags\")))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql._\n",
    ">     import scala.sys.process._\n",
    ">     import org.apache.spark.sql.functions.col\n",
    ">     toMap: (tupesArray: Seq[org.apache.spark.sql.Row])Option[Map[String,String]]\n",
    ">     handleCommon: ()(df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.setConf(\"spark.sql.parquet.binaryAsString\",\"true\")\n",
    "val nodeDF = sqlContext.read.parquet(\"dbfs:/FileStore/group14/sweden-latest_osm_pbf_node.parquet\").transform(handleCommon())\n",
    "\n",
    "val wayDF = sqlContext.read.parquet(\"dbfs:/FileStore/group14/sweden_latest_osm_pbf_way.parquet\").transform(handleCommon())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     nodeDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, tags: map<string,string> ... 2 more fields]\n",
    ">     wayDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, tags: map<string,string> ... 1 more field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions.{explode,arrays_zip, concat,array, lit}\n",
    "\n",
    "val wayDF_exploded = wayDF.withColumn(\"exploded\", explode(arrays_zip(concat($\"nodes.nodeId\",array(lit(-1L))), concat(array(lit(-1L)),$\"nodes.nodeId\"))))\n",
    "val wayDF_filtered = wayDF_exploded.filter($\"exploded.0\" > 0 && $\"exploded.1\" > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql._\n",
    ">     import org.apache.spark.sql.functions.{explode, arrays_zip, concat, array, lit}\n",
    ">     wayDF_exploded: org.apache.spark.sql.DataFrame = [id: bigint, tags: map<string,string> ... 2 more fields]\n",
    ">     wayDF_filtered: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, tags: map<string,string> ... 2 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val wayNodeDF = wayDF_exploded.select($\"exploded.0\".as(\"start\"), $\"exploded.1\".as(\"end\"),$\"tags.highway\", $\"tags.maxspeed\")\n",
    ".filter($\"highway\" isin (\"motorway\",\"trunk\",\"primary\",\"secondary\", \"tertiary\", \"unclassified\", \"residential\",\"motorway_link\", \"trunk_link\",  \"primary_link\", \"secondary_link\", \"tertiary_link\"))\n",
    "\n",
    "wayNodeDF.createOrReplaceTempView(\"wayHighway\")\n",
    "val wayNodeDF_nonull = wayNodeDF.withColumn(\"maxspeed\", when($\"maxspeed\".isNull && col(\"highway\") == \"motorway\", 110)\n",
    "                     .when($\"maxspeed\".isNull && col(\"highway\")==\"primary\", 50).when($\"maxspeed\".isNull && col(\"highway\")==\"secondary\", 50).when($\"maxspeed\".isNull && col(\"highway\")==\"motorway_link\", 50)\n",
    "                     .when($\"maxspeed\".isNull && col(\"highway\")==\"residential\", 15).when($\"maxspeed\".isNull, 50)\n",
    "                     .otherwise($\"maxspeed\"))\n",
    "wayNodeDF_nonull.createOrReplaceTempView(\"wayHighway\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     wayNodeDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [start: bigint, end: bigint ... 2 more fields]\n",
    ">     wayNodeDF_nonull: org.apache.spark.sql.DataFrame = [start: bigint, end: bigint ... 2 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val nodeLatLonDF = nodeDF\n",
    "  .select($\"id\".as(\"nodeId\"), $\"latitude\".as(\"startLat\"), $\"longitude\".as(\"startLong\"))\n",
    "\n",
    "val endnodeLatLonDF = nodeDF\n",
    "  .select($\"id\".as(\"nodeId2\"), $\"latitude\".as(\"endLat\"), $\"longitude\".as(\"endLong\"))\n",
    "\n",
    "val wayGeometryDF = wayNodeDF_nonull.join(nodeLatLonDF, $\"start\" === $\"nodeId\").join(endnodeLatLonDF, $\"end\" === $\"nodeId2\")\n",
    "\n",
    "val wayGeometry_distDF = wayGeometryDF.withColumn(\"a\", pow(sin(radians($\"endLat\" - $\"startLat\") / 2), 2) + cos(radians($\"startLat\")) * cos(radians($\"endLat\")) * pow(sin(radians($\"endLong\" - $\"startLong\") / 2), 2))\n",
    "  .withColumn(\"distance\", atan2(org.apache.spark.sql.functions.sqrt($\"a\"), org.apache.spark.sql.functions.sqrt(-$\"a\" + 1)) * 2 * 6371)\n",
    "  .filter($\"endLat\"<55.4326186d && $\"endLong\">13.7d) //Small area south of sweden.\n",
    "  .withColumn(\"time\", $\"distance\"/$\"maxspeed\").select(\"time\", \"start\", \"end\", \"distance\", \"maxspeed\")\n",
    "wayGeometry_distDF.createOrReplaceTempView(\"wayGeometry_distDF\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     nodeLatLonDF: org.apache.spark.sql.DataFrame = [nodeId: bigint, startLat: double ... 1 more field]\n",
    ">     endnodeLatLonDF: org.apache.spark.sql.DataFrame = [nodeId2: bigint, endLat: double ... 1 more field]\n",
    ">     wayGeometryDF: org.apache.spark.sql.DataFrame = [start: bigint, end: bigint ... 8 more fields]\n",
    ">     wayGeometry_distDF: org.apache.spark.sql.DataFrame = [time: double, start: bigint ... 3 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "val eps = 0.000001\n",
    "val edges: RDD[Edge[Double]]  =  wayGeometry_distDF\n",
    "    .select(\"start\", \"end\", \"time\").rdd.map(line => Edge(line.getAs(\"start\"), line.getAs(\"end\"), line.getAs(\"time\")))\n",
    "\n",
    "\n",
    "val graph = Graph.fromEdges(edges, \"defaultname\")\n",
    "graph.cache()\n",
    "\n",
    "println(\"Num edges:\")\n",
    "println(graph.edges.toDF.count())\n",
    "println(\"Num vertices:\")\n",
    "println(graph.vertices.toDF.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Num edges:\n",
    ">     4667\n",
    ">     Num vertices:\n",
    ">     4488\n",
    ">     import org.apache.spark.graphx._\n",
    ">     import org.apache.spark.rdd.RDD\n",
    ">     eps: Double = 1.0E-6\n",
    ">     edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] = MapPartitionsRDD[1075908] at map at command-2294440354339724:6\n",
    ">     graph: org.apache.spark.graphx.Graph[String,Double] = org.apache.spark.graphx.impl.GraphImpl@7f4c143a\n",
    "\n",
    "  \n",
    "\n",
    "The Shortest Path algorithm\n",
    "---------------------------\n",
    "\n",
    "The implemented shorthest path alborithm uses the the distributed Pregel\n",
    "algorithm and is divided into two parts.\n",
    "\n",
    "The first part is based on the code in\n",
    "`998_EX_01_GraphXShortestWeightedPaths`. As the original code did not\n",
    "have have all functionality desiered functionality, the algorithm did\n",
    "find the shortest distance but didn't keep track of the path itself, the\n",
    "algorithm was extend with this functionality.\n",
    "\n",
    "The first part takes a graph, where the edges are double values\n",
    "representing the cost of trevelling between its connected nodes, and an\n",
    "array of the ids of each goal node. As output, it provides a graph where\n",
    "each node containse a Map-object of the different landmarks/goal nodes.\n",
    "When a lookup is made in the map from a specific node, a tuple contaning\n",
    "the shortest distance, the id of the next node in the path and the id of\n",
    "the current node. The last element serves no pupose in the final results\n",
    "but is used as a form of stopping critera in the algorithm.\n",
    "\n",
    "The second part transforms the output of the first part to a \"path\n",
    "graph\" where each edge is marked with either a 1 or a 0 depending on if\n",
    "it is used in a path between a starting node and a goal node. Altough\n",
    "this recursion can be performed on a single machine for small examples,\n",
    "this procedure is also implemented using the Pregel algorithm to handle\n",
    "situations of millions of edges.\n",
    "\n",
    "The input of the second part is the graph created in the first part as\n",
    "well as the id of a single goal node and a start node. The goal node has\n",
    "to be in the set of goal nodes used in the first part. This part outputs\n",
    "a \"path graph\" where each edge is given the value 1 or 0 depending on if\n",
    "it is on the shortest path or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.reflect.ClassTag\n",
    "import org.apache.spark.graphx._\n",
    "\n",
    "/**\n",
    " * Computes shortest weighted paths to the given set of goal nodes, returning a graph where each\n",
    " * vertex attribute is a map containing the shortest-path distance to each reachable landmark.\n",
    " * Currently supports only Graph of [VD, Double], where VD is an arbitrary vertex type.\n",
    " *\n",
    " * The object also include a function which transforms the resulting graph into a path_graph between a \n",
    " * specific starting node and goal node. Each edge in the path_grpah is either 1 or 0 depending if it is \n",
    " * the shortest path or not.\n",
    " *\n",
    " */\n",
    "object ShortestPath extends Serializable {\n",
    "\n",
    "  // When finding the shortest path each node stores a map from the itself to each goal node.\n",
    "  // The map returns an array includeing the total distance to the goal node as well as the\n",
    "  // next node pn the shortest path to the goal node. The last value in the array is only \n",
    "  // populated with the nodes own id and is only used for computational convenience. \n",
    "  type SPMap = Map[VertexId, Tuple3[Double, VertexId, VertexId]]\n",
    "  \n",
    "  // PN holds the information of the path nodes which are used for creating a path graph\n",
    "  // PN = ('Distance left to goal node', 'Next path node id', 'Goal node', 'Is on path')\n",
    "  type PN = Tuple4[Double, VertexId, VertexId, Boolean] \n",
    "  \n",
    "  private val INITIAL_DIST = 0.0\n",
    "  private val DEFAULT_ID = -1L\n",
    "  private val INFINITY = Int.MaxValue.toDouble\n",
    "\n",
    "  private def makeMap(x: (VertexId, Tuple3[Double, VertexId, VertexId])*) = Map(x: _*)\n",
    "  \n",
    "  //private def incrementMap(spmap: SPMap, delta: Double, id: VertexId): SPMap = { \n",
    "  //  spmap.map { case (v, d) => v -> (Tuple3(d._1 + delta, d._3, id)) }\n",
    "  //}\n",
    "  private def incrementMap(spmap: SPMap, delta: Double, srcId: VertexId, dstId: VertexId): SPMap = { \n",
    "    spmap.map { case (v, d) => v -> (Tuple3(d._1 + delta, dstId, srcId)) }\n",
    "  }\n",
    "\n",
    "  private def addMaps(spmap1: SPMap, spmap2: SPMap): SPMap = {\n",
    "    (spmap1.keySet ++ spmap2.keySet).map {\n",
    "    k =>{\n",
    "        if (spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1 < spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1) \n",
    "                k -> (Tuple3(spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1, \n",
    "                             spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._2, \n",
    "                             spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._3))\n",
    "        else \n",
    "                k -> (Tuple3(spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1, \n",
    "                             spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._2, \n",
    "                             spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._3))\n",
    "        }\n",
    "    }.toMap\n",
    "  }\n",
    "  \n",
    "  // at this point it does not really matter what vertex type is\n",
    "  def run[VD](graph: Graph[VD, Double], landmarks: Seq[VertexId]): Graph[SPMap, Double] = {\n",
    "    val spGraph = graph.mapVertices { (vid, attr) =>\n",
    "      // initial value for itself is 0.0 as Double\n",
    "      if (landmarks.contains(vid)) makeMap(vid -> Tuple3(INITIAL_DIST, DEFAULT_ID, DEFAULT_ID)) else makeMap()\n",
    "    }\n",
    "\n",
    "    val initMaps = makeMap()\n",
    "\n",
    "    def vProg(id: VertexId, attr: SPMap, msg: SPMap): SPMap = {\n",
    "      addMaps(attr, msg)\n",
    "    }\n",
    "\n",
    "    def sendMsg(edge: EdgeTriplet[SPMap, Double]): Iterator[(VertexId, SPMap)] = {\n",
    "      val newAttr = incrementMap(edge.dstAttr, edge.attr, edge.srcId, edge.dstId)\n",
    "      if (edge.srcAttr != addMaps(newAttr, edge.srcAttr)) Iterator((edge.srcId, newAttr))\n",
    "      else Iterator.empty\n",
    "    }\n",
    "\n",
    "    Pregel(spGraph, initMaps)(vProg, sendMsg, addMaps)\n",
    "  }\n",
    "  \n",
    "  def create_path_graph[VD](graph: Graph[SPMap, Double], goalId: VertexId, startId: VertexId): Graph[PN, Int] = {\n",
    "    // For a given goal node we remove the lookup map and extend the state to a Tuple5 with the goal id and a boolean\n",
    "    val path = graph.mapEdges(e => 0)\n",
    "              .mapVertices((vertixId, attr) => {\n",
    "                if (attr.contains(goalId)) {\n",
    "                  val path_step = attr(goalId)\n",
    "                  if (vertixId == path_step._3 && path_step._2 == -1L)\n",
    "                    (path_step._1, goalId, goalId, false) // while we are at it, we clean up the state a bit\n",
    "                  else  \n",
    "                    (path_step._1, path_step._2, goalId, false)\n",
    "                } else// If the vertice does not have a map to our goal we add a default value to it\n",
    "                    (INFINITY, -1L, -1L, false)\n",
    "              })\n",
    "\n",
    "      def mergeMsg(msg1: VertexId, msg2: VertexId): VertexId = { // we should only get one msg\n",
    "          msg2\n",
    "      }\n",
    "\n",
    "      def vprog(id: VertexId, attr: PN, msg: VertexId): PN = {\n",
    "        // Check that the current node is the one adressed in the message\n",
    "        if (id == msg)\n",
    "          (attr._1, attr._2, attr._3, true)\n",
    "        else // If the message is not addressed to the current node (happens for inital message), use the old value \n",
    "          attr\n",
    "      }\n",
    "      def sendMsg(triplet: EdgeTriplet[PN, Int]): Iterator[(VertexId, VertexId)] = {\n",
    "        // If dstId is the next node on the path and has not yet been activated\n",
    "        if (triplet.srcAttr._2 == triplet.dstId && triplet.srcAttr._4 && !triplet.dstAttr._4) \n",
    "          Iterator((triplet.dstId, triplet.dstId))// Send next msg\n",
    "        else\n",
    "          Iterator.empty// Do nothing\n",
    "      }\n",
    "\n",
    "      Pregel(path, startId)(vprog, sendMsg, mergeMsg).mapTriplets(triplet => {\n",
    "        if(triplet.srcAttr._2 == triplet.dstId && triplet.srcAttr._4)\n",
    "          1\n",
    "        else\n",
    "          0\n",
    "      })\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import scala.reflect.ClassTag\n",
    ">     import org.apache.spark.graphx._\n",
    ">     defined object ShortestPath\n",
    "\n",
    "  \n",
    "\n",
    "To make the code somewhat more accessible, we wrap the execution of the\n",
    "two parts above in a new function called `shortestPath`. This new\n",
    "function takes the id of the start node and a single goal node as well\n",
    "as the input graph as input. The function then ouputs the path graph\n",
    "mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Random\n",
    "\n",
    "def shortestPath(srcId : Long, dstId : Long, graph : Graph[Long, Double], placeholder: Boolean) : Graph[Long, Double] = {\n",
    "  if (placeholder) {\n",
    "    return graph.mapEdges(e => Random.nextInt(2))\n",
    "  } else {\n",
    "    val distanceGraph = ShortestPath.run(graph, Seq(dstId))\n",
    "    val pathGraph = ShortestPath.create_path_graph(distanceGraph, dstId, srcId)\n",
    "    return pathGraph.mapVertices((vid, attr) => 0L).mapEdges(e => e.attr)\n",
    "  }\n",
    "}\n",
    "\n",
    "def shortestPath(srcId : Long, dstId : Long, graph : Graph[Long, Double]) : Graph[Long, Double] = {\n",
    "  return shortestPath(srcId, dstId, graph, false)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import scala.util.Random\n",
    ">     shortestPath: (srcId: Long, dstId: Long, graph: org.apache.spark.graphx.Graph[Long,Double], placeholder: Boolean)org.apache.spark.graphx.Graph[Long,Double] <and> (srcId: Long, dstId: Long, graph: org.apache.spark.graphx.Graph[Long,Double])org.apache.spark.graphx.Graph[Long,Double]\n",
    ">     shortestPath: (srcId: Long, dstId: Long, graph: org.apache.spark.graphx.Graph[Long,Double], placeholder: Boolean)org.apache.spark.graphx.Graph[Long,Double] <and> (srcId: Long, dstId: Long, graph: org.apache.spark.graphx.Graph[Long,Double])org.apache.spark.graphx.Graph[Long,Double]\n",
    "\n",
    "  \n",
    "\n",
    "Since we want to work with edge attributes rather than vertex\n",
    "attributes, we can't work directly with graph joins in GraphX, since\n",
    "they only join on vertices. This is a helper method to merge edge\n",
    "attributes of two graphs with identical structure, through an inner join\n",
    "on the respective edge RDDs and create a new graph with a tuple\n",
    "combining edge attributes from both graphs. This will only work if both\n",
    "graphs have identical partitioning strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.reflect.ClassTag\n",
    "\n",
    "// # Merge edge attributes of two (identical in structure) graphs\n",
    "def mergeEdgeAttributes[ED1 : ClassTag, ED2 : ClassTag](firstGraph : Graph[Long, ED1], secondGraph : Graph[Long, ED2]) : Graph[Long, (ED1, ED2)] = {\n",
    "  return Graph(firstGraph.vertices, firstGraph.edges.innerJoin(secondGraph.edges) {(id1, id2, first, second) => (first, second)}) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import scala.reflect.ClassTag\n",
    ">     mergeEdgeAttributes: [ED1, ED2](firstGraph: org.apache.spark.graphx.Graph[Long,ED1], secondGraph: org.apache.spark.graphx.Graph[Long,ED2])(implicit evidence$1: scala.reflect.ClassTag[ED1], implicit evidence$2: scala.reflect.ClassTag[ED2])org.apache.spark.graphx.Graph[Long,(ED1, ED2)]\n",
    "\n",
    "  \n",
    "\n",
    "In order to perform distributed sampling from Gaussian distributions in\n",
    "Spark in a reproducible way (specifically for stochastic edge weights in\n",
    "graphs), we want to be able to pass a seed to the random number\n",
    "generator. For this to work consistently, we use the Spark SQL `randn`\n",
    "function on the edge RDDs and subsequently build a new graph from the\n",
    "sampled weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Random\n",
    "\n",
    "def graphRandomGaussian(graph : Graph[Long, (Double, Double)], seed : Int, eps : Double, sparkSqlRandom : Boolean) : Graph[Long, Double] = {\n",
    "  if (sparkSqlRandom) {\n",
    "    return Graph(graph.vertices, graph.edges.toDF.select($\"srcId\", $\"dstId\", $\"attr._1\" + org.apache.spark.sql.functions.sqrt($\"attr._2\") * org.apache.spark.sql.functions.randn(seed)).rdd.map(r => Edge(r.getLong(0), r.getLong(1), r.getDouble(2)))).mapEdges(e => scala.math.max(eps, e.attr))\n",
    "  } else {\n",
    "    return graph.mapEdges(e => scala.math.max(eps, e.attr._1 + Random.nextGaussian() * scala.math.sqrt(e.attr._2)))\n",
    "  }\n",
    "} \n",
    "\n",
    "def graphRandomGaussian(graph : Graph[Long, (Double, Double)], seed : Int, eps : Double) : Graph[Long, Double] = {\n",
    "  return graphRandomGaussian(graph, seed, eps, true)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import scala.util.Random\n",
    ">     graphRandomGaussian: (graph: org.apache.spark.graphx.Graph[Long,(Double, Double)], seed: Int, eps: Double, sparkSqlRandom: Boolean)org.apache.spark.graphx.Graph[Long,Double] <and> (graph: org.apache.spark.graphx.Graph[Long,(Double, Double)], seed: Int, eps: Double)org.apache.spark.graphx.Graph[Long,Double]\n",
    ">     graphRandomGaussian: (graph: org.apache.spark.graphx.Graph[Long,(Double, Double)], seed: Int, eps: Double, sparkSqlRandom: Boolean)org.apache.spark.graphx.Graph[Long,Double] <and> (graph: org.apache.spark.graphx.Graph[Long,(Double, Double)], seed: Int, eps: Double)org.apache.spark.graphx.Graph[Long,Double]\n",
    "\n",
    "  \n",
    "\n",
    "RDDs in Spark contain lineage graphs with information about previous\n",
    "operations, for e.g. fault tolerance. These can increase significantly\n",
    "in size after many transformations which may result in reduced\n",
    "performance, especially in iterative algorithms (such as in GraphX). For\n",
    "this reason, we truncate the lineage graph by checkpointing the RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locallyCheckpointedGraph[VD : ClassTag, ED : ClassTag](graph : Graph[VD, ED]) : Graph[VD, ED] = {\n",
    "  val mappedGraph = graph.mapEdges(e => e.attr)\n",
    "  val edgeRdd = mappedGraph.edges.map(x => x)\n",
    "  val vertexRdd = mappedGraph.vertices.map(x => x)\n",
    "  edgeRdd.cache()\n",
    "  edgeRdd.localCheckpoint()\n",
    "  edgeRdd.count() // We need this line to force the RDD to evaluate, otherwise the truncation is not performed\n",
    "  vertexRdd.cache()\n",
    "  vertexRdd.localCheckpoint()\n",
    "  vertexRdd.count() // We need this line to force the RDD to evaluate, otherwise the truncation is not performed\n",
    "  return Graph(vertexRdd, edgeRdd)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     locallyCheckpointedGraph: [VD, ED](graph: org.apache.spark.graphx.Graph[VD,ED])(implicit evidence$1: scala.reflect.ClassTag[VD], implicit evidence$2: scala.reflect.ClassTag[ED])org.apache.spark.graphx.Graph[VD,ED]\n",
    "\n",
    "  \n",
    "\n",
    "Distributed combinatorial bandit algorithm\n",
    "------------------------------------------\n",
    "\n",
    "The cell below contains the distributed combinatorial bandit algorithm,\n",
    "as well as the simulation framework. In a standard (stochastic)\n",
    "multi-armed bandit problem setting, there is a set of actions\n",
    "$\\\\mathcal{A}$ wherein each selected action $a \\\\in \\\\mathcal{A}$\n",
    "results in the agent receiving a random reward $r(a)$ from the\n",
    "environment. The distributions of these rewards are unknown and it is\n",
    "the objective of an agent to select actions to learn enough information\n",
    "about the reward distributions such that the long-term rewards can be\n",
    "maximized.\n",
    "\n",
    "Thompson Sampling is Bayesian bandit algorithm, where the assumption is\n",
    "that the parameters of the reward distributions are drawn from some\n",
    "known prior. By using observed rewards to compute a posterior\n",
    "distribution, the posterior can be used to let the agent explore actions\n",
    "which have a high probability of being optimal. In each iteration,\n",
    "parameters are sampled from the posterior for all actions. The action\n",
    "with the highest (sampled) expected reward is then selected. Thompson\n",
    "Sampling is straightforward to extend to a combinatorial setting, where\n",
    "instead of individual actions, subsets of actions subject to\n",
    "combinatorial constraints are selected in each iteration.\n",
    "\n",
    "Under the assumption that the travel times individual edges in the road\n",
    "network graph are mutually independent, an online learning version of\n",
    "the shortest path problem can be cast into the combinatorial bandit\n",
    "setting. With the same assumption, the above operations can be performed\n",
    "using Spark and GraphX, in an iterative algorithm. With the exception of\n",
    "the step in which the distributed shortest path algorithm (through\n",
    "pregel) is used to find the path with the lowest (sampled from the\n",
    "posterior distribution) expected travel time, the rest of the steps can\n",
    "be performed almost exclusively by using `mapEdges` transformations. In\n",
    "this way, expensive repartitioning can be avoided.\n",
    "\n",
    "As we see it, the main benefit with this approach is that if the road\n",
    "network graph is very large, sub-graphs can be located on different\n",
    "worker nodes.\n",
    "\n",
    "NOTE: We run this with a toy example instead of the actual road network\n",
    "graph, since we had performance issues keeping us from evaluating it in\n",
    "a meaningful way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.math.sqrt\n",
    "import scala.math.max\n",
    "\n",
    "println(\"Starting experiment!\")\n",
    "val startTime = System.currentTimeMillis()\n",
    "\n",
    "val seed = 1000\n",
    "val eps = 0.00001\n",
    "\n",
    "// # Horizon N\n",
    "var N = 10\n",
    "\n",
    "// # Source and destination node IDs\n",
    "//val srcId = 0L\n",
    "var srcId = 2010606671L\n",
    "//val dstId = numVertices - 1L\n",
    "var dstId = 2869293096L\n",
    "\n",
    "var baseGraph: Graph[Long, Double] = null\n",
    "\n",
    "// # Toy example\n",
    "val useToyExample = true\n",
    "if (useToyExample) {\n",
    "  val numVertices = 10\n",
    "  N = 100\n",
    "  srcId = 0L\n",
    "  dstId = numVertices - 1\n",
    "  baseGraph = Graph.fromEdges(spark.sparkContext.parallelize(1 until numVertices-1).flatMap(vid => List(Edge(0L, vid.toLong, 100.0), Edge(vid.toLong, numVertices-1, 100.0))), 0L)\n",
    "} else {\n",
    "  baseGraph = graph.mapVertices((vid, attr) => 0L)\n",
    "}\n",
    "\n",
    "// # Assumption: Gaussian rewards with known variance\n",
    "// # Prior graph (map weight to prior mean and variance)\n",
    "// TODO: Prior mean from real graph\n",
    "val varFactor = 0.01\n",
    "val prior = baseGraph.mapVertices((vid, attr) => 0L).mapEdges(e => (e.attr, varFactor * (e.attr * e.attr)))\n",
    "var posterior = prior\n",
    "\n",
    "// # Environment (sample true environment from prior)\n",
    "val env = mergeEdgeAttributes(prior, graphRandomGaussian(prior, seed, eps)).mapEdges(e => (e.attr._2, e.attr._1._2))\n",
    "\n",
    "// # For regret calculations\n",
    "val optimal = shortestPath(srcId, dstId, env.mapEdges(e => e.attr._1))\n",
    "val optimalExpectedCost = mergeEdgeAttributes(optimal, env).edges.map(e => e.attr._1 * e.attr._2._1).reduce(_ + _)\n",
    "val optimalPathEdges = optimal.edges.filter(e => e.attr == 1).map(e => (e.srcId, e.dstId)).collect()\n",
    "printf(\"Optimal path edges: [%s]\\n\", optimalPathEdges.mkString(\",\"))\n",
    "\n",
    "// # Array with instant regret values\n",
    "var lastAction = optimal\n",
    "var allActions = env.mapEdges(e => Array[Double](0))\n",
    "\n",
    "// # Run experiment for N iterations\n",
    "for (t <- 0 until N) {\n",
    "  printf(\"Iteration %d, elapsed time: %d ms\", t, System.currentTimeMillis() - startTime)\n",
    "  \n",
    "  // # Checkpoint to break lineage graph\n",
    "  allActions = locallyCheckpointedGraph(allActions)\n",
    "  posterior = locallyCheckpointedGraph(posterior)\n",
    "  \n",
    "  // # Find action (super arm) using posterior sampling\n",
    "  val sampledParameters = graphRandomGaussian(posterior, seed+t*2+1, eps)\n",
    "  val action = shortestPath(srcId, dstId, sampledParameters)\n",
    "  lastAction = action\n",
    "  \n",
    "  // # Apply action on environments (assuming path is indicated by 1-valued edge attributes) and observe realized costs\n",
    "  val realizedEnv = graphRandomGaussian(env, seed+t*2+2, eps)\n",
    "  val observation = mergeEdgeAttributes(action, realizedEnv).mapEdges(e => e.attr._1 * e.attr._2)\n",
    "  \n",
    "  // # Update posterior\n",
    "  posterior = mergeEdgeAttributes(env, mergeEdgeAttributes(action, mergeEdgeAttributes(observation, posterior))).mapEdges(e => {\n",
    "    val trueVar = e.attr._1._2\n",
    "    val act = e.attr._2._1\n",
    "    val obs = e.attr._2._2._1\n",
    "    val pMean = e.attr._2._2._2._1\n",
    "    val pVar = e.attr._2._2._2._2\n",
    "    if (act == 1) {\n",
    "      val newVar = (1/(1/trueVar + 1/pVar))\n",
    "      (newVar*(obs/trueVar + pMean/pVar), newVar)\n",
    "    } else {\n",
    "      (pMean, pVar)\n",
    "    }  \n",
    "  })\n",
    "  \n",
    "  // # Calculate regret\n",
    "  allActions = mergeEdgeAttributes(allActions, action).mapEdges(e => e.attr._1 :+ e.attr._2)\n",
    "  printf(\"\\n\")\n",
    "}\n",
    "\n",
    "printf(\"Starting aggregation of regret values, elapsed time: %d ms\\n\", System.currentTimeMillis() - startTime)\n",
    "\n",
    "// # Aggregation of regret values\n",
    "val countActions = allActions.mapEdges(e => e.attr.reduce(_ + _))\n",
    "allActions = allActions.cache()\n",
    "val instantRegretValues = new Array[Double](N)\n",
    "for (t <- 0 until N) {\n",
    "  val action = allActions.mapEdges(e => e.attr(t+1))\n",
    "  val actionExpectedCost = mergeEdgeAttributes(action, env).edges.map(e => e.attr._1 * e.attr._2._1).reduce(_ + _)\n",
    "  val instantRegret = actionExpectedCost - optimalExpectedCost\n",
    "  instantRegretValues(t) = instantRegret\n",
    "}\n",
    "\n",
    "val endTime = System.currentTimeMillis()\n",
    "printf(\"Finished experiment! Elapsed time:%d\\n\", endTime - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Starting experiment!\n",
    ">     Optimal path edges: [(0,1),(1,9)]\n",
    ">     Iteration 0, elapsed time: 15793 ms\n",
    ">     Iteration 1, elapsed time: 37679 ms\n",
    ">     Iteration 2, elapsed time: 78937 ms\n",
    ">     Iteration 3, elapsed time: 96495 ms\n",
    ">     Iteration 4, elapsed time: 114013 ms\n",
    ">     Iteration 5, elapsed time: 129434 ms\n",
    ">     Iteration 6, elapsed time: 149040 ms\n",
    ">     Iteration 7, elapsed time: 161854 ms\n",
    ">     Iteration 8, elapsed time: 178462 ms\n",
    ">     Iteration 9, elapsed time: 193178 ms\n",
    ">     Iteration 10, elapsed time: 208457 ms\n",
    ">     Iteration 11, elapsed time: 228850 ms\n",
    ">     Iteration 12, elapsed time: 249601 ms\n",
    ">     Iteration 13, elapsed time: 277713 ms\n",
    ">     Iteration 14, elapsed time: 300296 ms\n",
    ">     Iteration 15, elapsed time: 315547 ms\n",
    ">     Iteration 16, elapsed time: 334719 ms\n",
    ">     Iteration 17, elapsed time: 352283 ms\n",
    ">     Iteration 18, elapsed time: 370505 ms\n",
    ">     Iteration 19, elapsed time: 384963 ms\n",
    ">     Iteration 20, elapsed time: 400986 ms\n",
    ">     Iteration 21, elapsed time: 419840 ms\n",
    ">     Iteration 22, elapsed time: 437701 ms\n",
    ">     Iteration 23, elapsed time: 463018 ms\n",
    ">     Iteration 24, elapsed time: 494099 ms\n",
    ">     Iteration 25, elapsed time: 515728 ms\n",
    ">     Iteration 26, elapsed time: 553540 ms\n",
    ">     Iteration 27, elapsed time: 575015 ms\n",
    ">     Iteration 28, elapsed time: 592082 ms\n",
    ">     Iteration 29, elapsed time: 624349 ms\n",
    ">     Iteration 30, elapsed time: 645980 ms\n",
    ">     Iteration 31, elapsed time: 686832 ms\n",
    ">     Iteration 32, elapsed time: 716649 ms\n",
    ">     Iteration 33, elapsed time: 746812 ms\n",
    ">     Iteration 34, elapsed time: 763532 ms\n",
    ">     Iteration 35, elapsed time: 786390 ms\n",
    ">     Iteration 36, elapsed time: 813434 ms\n",
    ">     Iteration 37, elapsed time: 835171 ms\n",
    ">     Iteration 38, elapsed time: 866043 ms\n",
    ">     Iteration 39, elapsed time: 892568 ms\n",
    ">     Iteration 40, elapsed time: 918749 ms\n",
    ">     Iteration 41, elapsed time: 942338 ms\n",
    ">     Iteration 42, elapsed time: 956895 ms\n",
    ">     Iteration 43, elapsed time: 972804 ms\n",
    ">     Iteration 44, elapsed time: 986972 ms\n",
    ">     Iteration 45, elapsed time: 1003529 ms\n",
    ">     Iteration 46, elapsed time: 1021157 ms\n",
    ">     Iteration 47, elapsed time: 1053163 ms\n",
    ">     Iteration 48, elapsed time: 1069664 ms\n",
    ">     Iteration 49, elapsed time: 1089713 ms\n",
    ">     Iteration 50, elapsed time: 1119183 ms\n",
    ">     Iteration 51, elapsed time: 1132827 ms\n",
    ">     Iteration 52, elapsed time: 1144452 ms\n",
    ">     Iteration 53, elapsed time: 1159900 ms\n",
    ">     Iteration 54, elapsed time: 1174581 ms\n",
    ">     Iteration 55, elapsed time: 1192365 ms\n",
    ">     Iteration 56, elapsed time: 1206917 ms\n",
    ">     Iteration 57, elapsed time: 1221419 ms\n",
    ">     Iteration 58, elapsed time: 1235554 ms\n",
    ">     Iteration 59, elapsed time: 1253976 ms\n",
    ">     Iteration 60, elapsed time: 1283507 ms\n",
    ">     Iteration 61, elapsed time: 1303086 ms\n",
    ">     Iteration 62, elapsed time: 1318591 ms\n",
    ">     Iteration 63, elapsed time: 1348081 ms\n",
    ">     Iteration 64, elapsed time: 1365186 ms\n",
    ">     Iteration 65, elapsed time: 1385235 ms\n",
    ">     Iteration 66, elapsed time: 1399504 ms\n",
    ">     Iteration 67, elapsed time: 1428328 ms\n",
    ">     Iteration 68, elapsed time: 1443365 ms\n",
    ">     Iteration 69, elapsed time: 1466065 ms\n",
    ">     Iteration 70, elapsed time: 1485121 ms\n",
    ">     Iteration 71, elapsed time: 1501413 ms\n",
    ">     Iteration 72, elapsed time: 1517653 ms\n",
    ">     Iteration 73, elapsed time: 1535818 ms\n",
    ">     Iteration 74, elapsed time: 1550425 ms\n",
    ">     Iteration 75, elapsed time: 1582959 ms\n",
    ">     Iteration 76, elapsed time: 1599048 ms\n",
    ">     Iteration 77, elapsed time: 1617722 ms\n",
    ">     Iteration 78, elapsed time: 1634656 ms\n",
    ">     Iteration 79, elapsed time: 1656438 ms\n",
    ">     Iteration 80, elapsed time: 1676291 ms\n",
    ">     Iteration 81, elapsed time: 1693179 ms\n",
    ">     Iteration 82, elapsed time: 1710943 ms\n",
    ">     Iteration 83, elapsed time: 1725179 ms\n",
    ">     Iteration 84, elapsed time: 1738993 ms\n",
    ">     Iteration 85, elapsed time: 1757378 ms\n",
    ">     Iteration 86, elapsed time: 1772705 ms\n",
    ">     Iteration 87, elapsed time: 1811560 ms\n",
    ">     Iteration 88, elapsed time: 1827914 ms\n",
    ">     Iteration 89, elapsed time: 1848061 ms\n",
    ">     Iteration 90, elapsed time: 1869575 ms\n",
    ">     Iteration 91, elapsed time: 1887501 ms\n",
    ">     Iteration 92, elapsed time: 1902993 ms\n",
    ">     Iteration 93, elapsed time: 1925588 ms\n",
    ">     Iteration 94, elapsed time: 1951213 ms\n",
    ">     Iteration 95, elapsed time: 1969550 ms\n",
    ">     Iteration 96, elapsed time: 1997350 ms\n",
    ">     Iteration 97, elapsed time: 2013487 ms\n",
    ">     Iteration 98, elapsed time: 2027179 ms\n",
    ">     Iteration 99, elapsed time: 2041817 ms\n",
    ">     Starting aggregation of regret values, elapsed time: 2065189 ms\n",
    ">     Finished experiment! Elapsed time:2131615\n",
    ">     import scala.math.sqrt\n",
    ">     import scala.math.max\n",
    ">     startTime: Long = 1610442657284\n",
    ">     seed: Int = 1000\n",
    ">     eps: Double = 1.0E-5\n",
    ">     N: Int = 100\n",
    ">     srcId: Long = 0\n",
    ">     dstId: Long = 9\n",
    ">     baseGraph: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@28b64ba4\n",
    ">     useToyExample: Boolean = true\n",
    ">     varFactor: Double = 0.01\n",
    ">     prior: org.apache.spark.graphx.Graph[Long,(Double, Double)] = org.apache.spark.graphx.impl.GraphImpl@74f06565\n",
    ">     posterior: org.apache.spark.graphx.Graph[Long,(Double, Double)] = org.apache.spark.graphx.impl.GraphImpl@751a558c\n",
    ">     env: org.apache.spark.graphx.Graph[Long,(Double, Double)] = org.apache.spark.graphx.impl.GraphImpl@8ce8e2\n",
    ">     optimal: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@6ee0cb5a\n",
    ">     optimalExpectedCost: Double = 170.67842154901433\n",
    ">     optimalPathEdges: Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = Array((0,1), (1,9))\n",
    ">     lastAction: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@4806b809\n",
    ">     allActions: org.apache.spark.graphx.Graph[Long,Array[Double]] = org.apache.spark.graphx.impl.GraphImpl@3ebc1a58\n",
    ">     countActions: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@6d46efc7\n",
    ">     allActions: org.apache.spark.graphx.Graph[Long,Array[Double]] = org.apache.spark.graphx.impl.GraphImpl@3ebc1a58\n",
    ">     instantRegretValues: Array[Double] = Array(26.636988940342178, 23.984447406903655, 0.0, 0.0, 0.0, 28.749696934220793, 0.0, 43.00788444960253, 0.0, 36.899665259259336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 37.27307360794842, 0.0, 13.28180079789928, 0.0, 13.28180079789928, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.28180079789928, 13.28180079789928, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.28180079789928, 0.0)\n",
    ">     endTime: Long = 1610444788899\n",
    "\n",
    "  \n",
    "\n",
    "Graph visualization\n",
    "-------------------\n",
    "\n",
    "In order to analys the data we visualize a graph which show the number\n",
    "times it has been visited by the exploration algorithm. When visualised,\n",
    "the edges with multiple visits are marked with a thicker line. Note that\n",
    "the graph will get very cluttered if more than 50 nodes are in the\n",
    "graph.\n",
    "\n",
    "But first, we need to initialize the d3 package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package d3\n",
    "\n",
    "\n",
    "// We use a package object so that we can define top level classes like Edge that need to be used in other cells\n",
    "// This was modified by Ivan Sadikov to make sure it is compatible the latest databricks notebook\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "import com.databricks.backend.daemon.driver.EnhancedRDDFunctions.displayHTML\n",
    "\n",
    "case class Edge(src: String, dest: String, count: Long)\n",
    "\n",
    "case class Node(name: String)\n",
    "case class Link(source: Int, target: Int, value: Long)\n",
    "case class Graph(nodes: Seq[Node], links: Seq[Link])\n",
    "\n",
    "object graphs {\n",
    "// val sqlContext = SQLContext.getOrCreate(org.apache.spark.SparkContext.getOrCreate())  /// fix\n",
    "val sqlContext = SparkSession.builder().getOrCreate().sqlContext\n",
    "import sqlContext.implicits._\n",
    "  \n",
    "def force(clicks: Dataset[Edge], height: Int = 100, width: Int = 960): Unit = {\n",
    "  val data = clicks.collect()\n",
    "  val nodes = (data.map(_.src) ++ data.map(_.dest)).map(_.replaceAll(\"_\", \" \")).toSet.toSeq.map(Node)\n",
    "  val links = data.map { t =>\n",
    "    Link(nodes.indexWhere(_.name == t.src.replaceAll(\"_\", \" \")), nodes.indexWhere(_.name == t.dest.replaceAll(\"_\", \" \")), t.count / 20 + 1)\n",
    "  }\n",
    "  showGraph(height, width, Seq(Graph(nodes, links)).toDF().toJSON.first())\n",
    "}\n",
    "\n",
    "/**\n",
    " * Displays a force directed graph using d3\n",
    " * input: {\"nodes\": [{\"name\": \"...\"}], \"links\": [{\"source\": 1, \"target\": 2, \"value\": 0}]}\n",
    " */\n",
    "def showGraph(height: Int, width: Int, graph: String): Unit = {\n",
    "\n",
    "displayHTML(s\"\"\"\n",
    "<style>\n",
    "\n",
    ".node_circle {\n",
    "  stroke: #777;\n",
    "  stroke-width: 1.3px;\n",
    "}\n",
    "\n",
    ".node_label {\n",
    "  pointer-events: none;\n",
    "}\n",
    "\n",
    ".link {\n",
    "  stroke: #777;\n",
    "  stroke-opacity: .2;\n",
    "}\n",
    "\n",
    ".node_count {\n",
    "  stroke: #777;\n",
    "  stroke-width: 1.0px;\n",
    "  fill: #999;\n",
    "}\n",
    "\n",
    "text.legend {\n",
    "  font-family: Verdana;\n",
    "  font-size: 13px;\n",
    "  fill: #000;\n",
    "}\n",
    "\n",
    ".node text {\n",
    "  font-family: \"Helvetica Neue\",\"Helvetica\",\"Arial\",sans-serif;\n",
    "  font-size: 17px;\n",
    "  font-weight: 200;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\n",
    "<div id=\"clicks-graph\">\n",
    "<script src=\"//d3js.org/d3.v3.min.js\"></script>\n",
    "<script>\n",
    "\n",
    "var graph = $graph;\n",
    "\n",
    "var width = $width,\n",
    "    height = $height;\n",
    "\n",
    "var color = d3.scale.category20();\n",
    "\n",
    "var force = d3.layout.force()\n",
    "    .charge(-700)\n",
    "    .linkDistance(180)\n",
    "    .size([width, height]);\n",
    "\n",
    "var svg = d3.select(\"#clicks-graph\").append(\"svg\")\n",
    "    .attr(\"width\", width)\n",
    "    .attr(\"height\", height);\n",
    "    \n",
    "force\n",
    "    .nodes(graph.nodes)\n",
    "    .links(graph.links)\n",
    "    .start();\n",
    "\n",
    "var link = svg.selectAll(\".link\")\n",
    "    .data(graph.links)\n",
    "    .enter().append(\"line\")\n",
    "    .attr(\"class\", \"link\")\n",
    "    .style(\"stroke-width\", function(d) { return Math.sqrt(d.value); });\n",
    "\n",
    "var node = svg.selectAll(\".node\")\n",
    "    .data(graph.nodes)\n",
    "    .enter().append(\"g\")\n",
    "    .attr(\"class\", \"node\")\n",
    "    .call(force.drag);\n",
    "\n",
    "node.append(\"circle\")\n",
    "    .attr(\"r\", 10)\n",
    "    .style(\"fill\", function (d) {\n",
    "    if (d.name.startsWith(\"other\")) { return color(1); } else { return color(2); };\n",
    "})\n",
    "\n",
    "node.append(\"text\")\n",
    "      .attr(\"dx\", 10)\n",
    "      .attr(\"dy\", \".35em\")\n",
    "      .text(function(d) { return d.name });\n",
    "      \n",
    "//Now we are giving the SVGs co-ordinates - the force layout is generating the co-ordinates which this code is using to update the attributes of the SVG elements\n",
    "force.on(\"tick\", function () {\n",
    "    link.attr(\"x1\", function (d) {\n",
    "        return d.source.x;\n",
    "    })\n",
    "        .attr(\"y1\", function (d) {\n",
    "        return d.source.y;\n",
    "    })\n",
    "        .attr(\"x2\", function (d) {\n",
    "        return d.target.x;\n",
    "    })\n",
    "        .attr(\"y2\", function (d) {\n",
    "        return d.target.y;\n",
    "    });\n",
    "    d3.selectAll(\"circle\").attr(\"cx\", function (d) {\n",
    "        return d.x;\n",
    "    })\n",
    "        .attr(\"cy\", function (d) {\n",
    "        return d.y;\n",
    "    });\n",
    "    d3.selectAll(\"text\").attr(\"x\", function (d) {\n",
    "        return d.x;\n",
    "    })\n",
    "        .attr(\"y\", function (d) {\n",
    "        return d.y;\n",
    "    });\n",
    "});\n",
    "</script>\n",
    "</div>\n",
    "\"\"\")\n",
    "}\n",
    "  \n",
    "  def help() = {\n",
    "displayHTML(\"\"\"\n",
    "<p>\n",
    "Produces a force-directed graph given a collection of edges of the following form:</br>\n",
    "<tt><font color=\"#a71d5d\">case class</font> <font color=\"#795da3\">Edge</font>(<font color=\"#ed6a43\">src</font>: <font color=\"#a71d5d\">String</font>, <font color=\"#ed6a43\">dest</font>: <font color=\"#a71d5d\">String</font>, <font color=\"#ed6a43\">count</font>: <font color=\"#a71d5d\">Long</font>)</tt>\n",
    "</p>\n",
    "<p>Usage:<br/>\n",
    "<tt><font color=\"#a71d5d\">import</font> <font color=\"#ed6a43\">d3._</font></tt><br/>\n",
    "<tt><font color=\"#795da3\">graphs.force</font>(</br>\n",
    "&nbsp;&nbsp;<font color=\"#ed6a43\">height</font> = <font color=\"#795da3\">500</font>,<br/>\n",
    "&nbsp;&nbsp;<font color=\"#ed6a43\">width</font> = <font color=\"#795da3\">500</font>,<br/>\n",
    "&nbsp;&nbsp;<font color=\"#ed6a43\">clicks</font>: <font color=\"#795da3\">Dataset</font>[<font color=\"#795da3\">Edge</font>])</tt>\n",
    "</p>\"\"\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Warning: classes defined within packages cannot be redefined without a cluster restart.\n",
    ">     Compilation successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.graphframes.GraphFrame\n",
    "import org.apache.spark.sql.functions.lit // import the lit function in sql\n",
    "\n",
    "val visitedEdges = GraphFrame.fromGraphX(countActions.mapEdges(e => e.attr.toInt))\n",
    "val visits = visitedEdges.edges.select($\"attr\".as(\"count\"))\n",
    "val maxVisits = visits.agg(org.apache.spark.sql.functions.max(visits(\"count\")))\n",
    "d3.graphs.force(\n",
    "  height = 500,\n",
    "  width = 1000,\n",
    "  clicks = visitedEdges.edges.select($\"src\", $\"dst\".as(\"dest\"), $\"attr\".divide(maxVisits.first().getInt(0)).multiply(500).cast(\"int\").as(\"count\")).as[d3.Edge])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "We can also visualize the shortest path in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val posteriorShortestPath = GraphFrame.fromGraphX(lastAction.mapEdges(e => e.attr.toInt*10000))\n",
    "\n",
    "d3.graphs.force(\n",
    "  height = 500,\n",
    "  width = 1000,\n",
    "  clicks = posteriorShortestPath.edges.select($\"src\", $\"dst\".as(\"dest\"), $\"attr\".as(\"count\")).as[d3.Edge])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Now we visualize the instant regret. We should see a lot ot spikes in\n",
    "the beginning of the graph but the general trend should be that the\n",
    "curve decrease to zero while having fewer spikes as the algorithm gets\n",
    "closer to the optimal solutions. Note that there will always be some\n",
    "spikes as these corresponds to \"exploratory\" actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.sparkContext.parallelize((1 to N) zip instantRegretValues).toDF(\"Iteration (t)\",\"Instant regret\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "  \n",
    "\n",
    "We can also show the cumulative regret. As the algorithm reaches a final\n",
    "solution, the instantaneous regret should decrease and the cumulative\n",
    "regret should reach a plateau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cumulativeRegret = instantRegretValues.scanLeft(0.0)(_ + _)\n",
    "val df = spark.sparkContext.parallelize((1 to N) zip cumulativeRegret).toDF(\"Iteration (t)\",\"Cumulative regret\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
