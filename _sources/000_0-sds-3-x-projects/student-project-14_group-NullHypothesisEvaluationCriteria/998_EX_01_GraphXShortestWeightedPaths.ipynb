{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending spark.graphx.lib.ShortestPaths to GraphXShortestWeightedPaths\n",
    "=======================================================================\n",
    "\n",
    "### 2016-2020, Ivan Sadikov and Raazesh Sainudiin\n",
    "\n",
    "We extend Shortest Paths algorithm in Spark's GraphX Library to allow\n",
    "for user-specified edge-weights as an edge attribute.\n",
    "\n",
    "This is part of *Project MEP: Meme Evolution Programme* and supported by\n",
    "databricks academic partners program.\n",
    "\n",
    "The analysis is available in the following databricks notebook: \\*\n",
    "<http://lamastex.org/lmse/mep/src/GraphXShortestWeightedPaths.html>\n",
    "\n",
    "\\`\\`\\` Copyright 2016 Ivan Sadikov and Raazesh Sainudiin\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n",
    "not use this file except in compliance with the License. You may obtain\n",
    "a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License. \\`\\`\\`\n",
    "\n",
    "### Let's modify shortest paths algorithm to allow for user-specified edge-weights\n",
    "\n",
    "Update shortest paths algorithm to work over edge attribute of\n",
    "edge-weights as Double, key concepts are: - we increment map with delta,\n",
    "which is `edge.attr` - edge attribute is anything numeric, tested on\n",
    "Double - infinity value is not infinity, but `Integer.MAX_VALUE`\n",
    "\n",
    "Modifying the following code: \\*\n",
    "https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/ShortestPaths.scala\n",
    "\n",
    "Explained here: \\*\n",
    "http://note.yuhc.me/2015/03/graphx-pregel-shortest-path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.reflect.ClassTag\n",
    "import org.apache.spark.graphx._\n",
    "\n",
    "/**\n",
    " * Computes shortest weighted paths to the given set of landmark vertices, returning a graph where each\n",
    " * vertex attribute is a map containing the shortest-path distance to each reachable landmark.\n",
    " * Currently supports only Graph of [VD, Double], where VD is an arbitrary vertex type.\n",
    " */\n",
    "object GraphXShortestWeightedPaths extends Serializable {\n",
    "  /** Stores a map from the vertex id of a landmark to the distance to that landmark. */\n",
    "  \n",
    "  \n",
    "  //type SPMap = Map[VertexId, Double]\n",
    "  type SPMap = Map[VertexId, Tuple3[Double, VertexId, VertexId]]\n",
    "  \n",
    "  // initial and infinity values, use to relax edges\n",
    "  private val INITIAL = 0.0\n",
    "  private val DEFAULT_ID = -1.toLong\n",
    "  private val INFINITY = Int.MaxValue.toDouble\n",
    "\n",
    "  //private def makeMap(x: (VertexId, Double)*) = Map(x: _*)\n",
    "  private def makeMap(x: (VertexId, Tuple3[Double, VertexId, VertexId])*) = Map(x: _*) // _* -> python \"unpacking\", see https://stackoverflow.com/questions/6051302/what-does-colon-underscore-star-do-in-scala\n",
    "  \n",
    "  \n",
    "  //private def incrementMap(spmap: SPMap, delta: Double): SPMap = {\n",
    "  //  spmap.map { case (v, d) => v -> (d + delta) }\n",
    "  //}\n",
    "  \n",
    "  private def incrementMap(spmap: SPMap, delta: Double, id: VertexId): SPMap = { \n",
    "    spmap.map { case (v, d) => v -> (Tuple3(d._1 + delta, d._3, id)) }\n",
    "  }\n",
    "\n",
    "  private def addMaps(spmap1: SPMap, spmap2: SPMap): SPMap = {\n",
    "    (spmap1.keySet ++ spmap2.keySet).map {\n",
    "      \n",
    "    k =>{\n",
    "        if (spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1 < spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1) \n",
    "                k -> (Tuple3(spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1, \n",
    "                             spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._2, \n",
    "                             spmap1.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._3))\n",
    "        else \n",
    "                k -> (Tuple3(spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._1, \n",
    "                             spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._2, \n",
    "                             spmap2.getOrElse(k, Tuple3(INFINITY, DEFAULT_ID, DEFAULT_ID))._3))\n",
    "        }\n",
    "      /*k => k -> (Tuple2(\n",
    "                  math.min(\n",
    "                    spmap1.getOrElse(k, Tuple2(INFINITY, 0L))._1,\n",
    "                    spmap2.getOrElse(k, Tuple2(INFINITY, 0L))._1),\n",
    "                  0L)\n",
    "                )*/\n",
    "      //k => k -> math.min(spmap1.getOrElse(k, INFINITY), spmap2.getOrElse(k, INFINITY))\n",
    "    }.toMap\n",
    "  }\n",
    "  \n",
    "  // at this point it does not really matter what vertex type is\n",
    "  def run[VD](graph: Graph[VD, Double], landmarks: Seq[VertexId]): Graph[SPMap, Double] = {\n",
    "    val spGraph = graph.mapVertices { (vid, attr) =>\n",
    "      // initial value for itself is 0.0 as Double\n",
    "      if (landmarks.contains(vid)) makeMap(vid -> Tuple3(INITIAL,DEFAULT_ID,DEFAULT_ID)) else makeMap()\n",
    "      //if (landmarks.contains(vid)) makeMap(vid -> INITIAL) else makeMap()\n",
    "    }\n",
    "\n",
    "    val initialMessage = makeMap()\n",
    "\n",
    "    def vertexProgram(id: VertexId, attr: SPMap, msg: SPMap): SPMap = {\n",
    "      addMaps(attr, msg)\n",
    "    }\n",
    "\n",
    "    def sendMessage(edge: EdgeTriplet[SPMap, Double]): Iterator[(VertexId, SPMap)] = {\n",
    "    //def sendMessage(edge: EdgeTriplet[SPMap, Double]): Iterator[(VertexId, SPMap)] = {// Change to Double to VertexId after debugging\n",
    "      val newAttr = incrementMap(edge.dstAttr, edge.attr, edge.srcId)\n",
    "      if (edge.srcAttr != addMaps(newAttr, edge.srcAttr)) Iterator((edge.srcId, newAttr))\n",
    "      else Iterator.empty\n",
    "    }\n",
    "\n",
    "    Pregel(spGraph, initialMessage)(vertexProgram, sendMessage, addMaps)\n",
    "  }\n",
    "}\n",
    "\n",
    "println(\"Usage: val result = GraphXShortestWeightedPaths.run(graph, Seq(4L, 0L, 9L))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Usage: val result = GraphXShortestWeightedPaths.run(graph, Seq(4L, 0L, 9L))\n",
    ">     import scala.reflect.ClassTag\n",
    ">     import org.apache.spark.graphx._\n",
    ">     defined object GraphXShortestWeightedPaths\n",
    "\n",
    "  \n",
    "\n",
    "### Generate test graph\n",
    "\n",
    "Generate simple graph with double weights for edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Random\n",
    "\n",
    "import org.apache.spark.graphx.{Graph, VertexId}\n",
    "import org.apache.spark.graphx.util.GraphGenerators\n",
    "\n",
    "// A graph with edge attributes containing distances\n",
    "val graph: Graph[Long, Double] = GraphGenerators.logNormalGraph(sc, numVertices = 150, seed=123L).mapEdges { e => \n",
    "  // to make things nicer we assign 0 distance to itself\n",
    "  if (e.srcId == e.dstId) 0.0 else Random.nextDouble()\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import scala.util.Random\n",
    ">     import org.apache.spark.graphx.{Graph, VertexId}\n",
    ">     import org.apache.spark.graphx.util.GraphGenerators\n",
    ">     graph: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@e2066f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create an RDD for the vertices\n",
    "val nodes: RDD[(VertexId, String)] =\n",
    "  sc.parallelize(Seq((0L, \"0\"), (1L, \"1\"), (2L, \"2\"), (3L, \"3\")))\n",
    "// Create an RDD for edges\n",
    "val connections: RDD[Edge[Double]] =\n",
    "  sc.parallelize(Seq(Edge(0L, 1L, 1),    Edge(1L, 2L, 1),\n",
    "                       Edge(2L, 3L, 1), Edge(3L, 4L, 1)))\n",
    "// Build the initial Graph\n",
    "val default_node = \"default\"\n",
    "val graph = Graph(nodes, connections, default_node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     nodes: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, String)] = ParallelCollectionRDD[17] at parallelize at command-3289963953973041:3\n",
    ">     connections: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] = ParallelCollectionRDD[18] at parallelize at command-3289963953973041:6\n",
    ">     default_node: String = default\n",
    ">     graph: org.apache.spark.graphx.Graph[String,Double] = org.apache.spark.graphx.impl.GraphImpl@9637035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.vertices.take(150).foreach(println)\n",
    "graph.edges.take(150).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     (0,0)\n",
    ">     (1,1)\n",
    ">     (2,2)\n",
    ">     (3,3)\n",
    ">     (4,default)\n",
    ">     Edge(0,1,1.0)\n",
    ">     Edge(1,2,1.0)\n",
    ">     Edge(2,3,1.0)\n",
    ">     Edge(3,4,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val landMarkVertexIds = Seq(4L)\n",
    "val result = GraphXShortestWeightedPaths.run(graph, landMarkVertexIds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     landMarkVertexIds: Seq[Long] = List(4)\n",
    ">     result: org.apache.spark.graphx.Graph[GraphXShortestWeightedPaths.SPMap,Double] = org.apache.spark.graphx.impl.GraphImpl@4af64c00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Found shortest paths\n",
    "println(result.vertices.collect.mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     (0,Map(4 -> (4.0,1,0)))\n",
    ">     (1,Map(4 -> (3.0,2,1)))\n",
    ">     (2,Map(4 -> (2.0,3,2)))\n",
    ">     (3,Map(4 -> (1.0,-1,3)))\n",
    ">     (4,Map(4 -> (0.0,-1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// set all edges to zero\n",
    "val path = result.mapEdges(e => 0)\n",
    "println(path.edges.collect.mkString(\"\\n\"))\n",
    "println(path.vertices.collect.mkString(\"\\n\"))\n",
    "println(path.triplets.collect.mkString(\"\\n\"))\n",
    "\n",
    "\n",
    "val relations: RDD[Int] = path.triplets.map(triplet => {\n",
    "  if (triplet.srcAttr(4)._2 == -1 ) // If the edge between srcId and dstId is use in the path\n",
    "    -1\n",
    "  else\n",
    "    1\n",
    "})\n",
    "  \n",
    "relations.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Edge(0,1,0)\n",
    ">     Edge(1,2,0)\n",
    ">     Edge(2,3,0)\n",
    ">     Edge(3,4,0)\n",
    ">     (0,Map(4 -> (4.0,1,0)))\n",
    ">     (1,Map(4 -> (3.0,2,1)))\n",
    ">     (2,Map(4 -> (2.0,3,2)))\n",
    ">     (3,Map(4 -> (1.0,-1,3)))\n",
    ">     (4,Map(4 -> (0.0,-1,-1)))\n",
    ">     ((0,Map(4 -> (4.0,1,0))),(1,Map(4 -> (3.0,2,1))),0)\n",
    ">     ((1,Map(4 -> (3.0,2,1))),(2,Map(4 -> (2.0,3,2))),0)\n",
    ">     ((2,Map(4 -> (2.0,3,2))),(3,Map(4 -> (1.0,-1,3))),0)\n",
    ">     ((3,Map(4 -> (1.0,-1,3))),(4,Map(4 -> (0.0,-1,-1))),0)\n",
    ">     1\n",
    ">     1\n",
    ">     1\n",
    ">     -1\n",
    ">     path: org.apache.spark.graphx.Graph[GraphXShortestWeightedPaths.SPMap,Int] = org.apache.spark.graphx.impl.GraphImpl@5f801981\n",
    ">     relations: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[150] at map at command-3289963953973045:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Note on message: the message consists of two variables. \n",
    "// The first varible specifies the goal node, the second specifies the next node which should be executed in the path.\n",
    "// Note on vprog\n",
    "type SPMap = Map[VertexId, Tuple3[Double, VertexId, VertexId]]\n",
    "val goalId = 4L\n",
    "val startId = 0L\n",
    "\n",
    "val initMsg = (goalId, startId) \n",
    "\n",
    "def mergeMsg(msg1: Tuple2[VertexId,VertexId], msg2: Tuple2[VertexId,VertexId]): Tuple2[VertexId,VertexId]) = { // we should only get one msg\n",
    "    msg1\n",
    "}\n",
    "\n",
    "def vprog(id: VertexId, attr: SPMap, msg: Tuple2[VertexId,VertexId]): Map[VertexId, Tuple3[Double, VertexId, VertexId]] = {\n",
    "  // Check that the current node is the one adressed in the next step of the path, i.e. the second variable\n",
    "  if(VertexId == msg._2)\n",
    "    // Do a lookup in the map, attr(msg._1)._2 gives the next node on the path   \n",
    "    val nextStep = attr(msg._1)._2\n",
    "      // Note: if attr(msg._1)._2 is -1 and attr(msg._1)._3 is id then the next node is the goal node, i.e. msg._1\n",
    "\n",
    "    // Now, I need to find a way to send goalId, i.e. msg._1, to the sendMsg function\n",
    "\n",
    "}\n",
    "def sendMsg(edge: EdgeTriplet[SPMap, Double]): Iterator[(VertexId, VertexId)] = {\n",
    "  \n",
    "  // How should this function recieve the goalId variable??\n",
    "  \n",
    "  if (edge.srcAttr(goalId)._2 == -1) // If the edge between srcId and dstId is not used in the path\n",
    "    Iterator.empty// Do nothing\n",
    "  else\n",
    "    Iterator((triplet.dstId, triplet.dstId))// Send xest msg\n",
    "}\n",
    "\n",
    "result.Pregel(initMsg)(vertexProgram, sendMsg, mergeMsg)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// edges with weights, make sure to check couple of shortest paths from above\n",
    "display(result.edges.toDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(graph.vertices) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// now let us collect the shortest distance between every vertex and every landmark vertex\n",
    "// to manipulate scala maps that are vertices of the result see: http://docs.scala-lang.org/overviews/collections/maps.html\n",
    "// a quick point: http://stackoverflow.com/questions/28769367/scala-map-a-map-to-list-of-tuples\n",
    "val shortestDistsVertex2Landmark = result.vertices.flatMap(GxSwpSPMap => {\n",
    "  GxSwpSPMap._2.toSeq.map(x => (GxSwpSPMap._1, x._1, x._2)) // to get triples: vertex, landmarkVertex, shortest_distance\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     shortestDistsVertex2Landmark: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId, (Double, org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId))] = MapPartitionsRDD[2345] at flatMap at command-4443336225095845:4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortestDistsVertex2Landmark.collect.mkString(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res9: String =\n",
    ">     (0,4,(4.0,1,0))\n",
    ">     (1,4,(3.0,2,1))\n",
    ">     (2,4,(2.0,3,2))\n",
    ">     (3,4,(1.0,-1,3))\n",
    ">     (4,4,(0.0,-1,-1))\n",
    "\n",
    "  \n",
    "\n",
    "#### Let's make a DataFrame for visualizing pairwise matrix plots\n",
    "\n",
    "We want to make 4 columns in this example as follows (note actual values\n",
    "change for each realisation of graph!):\n",
    "\n",
    "`landmark_Id1 (\"0\"),   landmarkID2 (\"4\"), landmarkId3 (\"9\"),  srcVertexId ------------------------------------------------------------------------ 0.0,                  0.7425..,          0.8718,                0 0.924...,             1.2464..,          1.0472,                1 ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// http://alvinalexander.com/scala/how-to-sort-map-in-scala-key-value-sortby-sortwith\n",
    "// we need this to make sure that the maps are ordered by the keys for ensuring unique column values\n",
    "import scala.collection.immutable.ListMap\n",
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import scala.collection.immutable.ListMap\n",
    ">     import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " // recall our landmark vertices in landMarkVertexIds. let's use their Strings for names\n",
    "val unorderedNamedLandmarkVertices = landMarkVertexIds.map(id => (id, id.toString) )\n",
    "val orderedNamedLandmarkVertices = ListMap(unorderedNamedLandmarkVertices.sortBy(_._1):_*)\n",
    "val orderedLandmarkVertexNames = orderedNamedLandmarkVertices.toSeq.map(x => x._2)\n",
    "orderedLandmarkVertexNames.mkString(\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     unorderedNamedLandmarkVertices: Seq[(Long, String)] = List((4,4))\n",
    ">     orderedNamedLandmarkVertices: scala.collection.immutable.ListMap[Long,String] = ListMap(4 -> 4)\n",
    ">     orderedLandmarkVertexNames: Seq[String] = Vector(4)\n",
    ">     res7: String = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// this is going to be our column names\n",
    "val columnNames:Seq[String] = orderedLandmarkVertexNames :+ \"srcVertexId\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     columnNames: Seq[String] = Vector(4, srcVertexId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// a case class to make a data-frame quickly from the result\n",
    "case class SeqOfDoublesAndsrcVertexId(shortestDistances: Seq[Double], srcVertexId: VertexId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     defined class SeqOfDoublesAndsrcVertexId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val shortestDistsSeqFromVertex2Landmark2DF = result.vertices.map(GxSwpSPMap => {\n",
    "  //GxSwpSPMap._2.toSeq.map(x => (GxSwpSPMap._1, x._1, x._2)) // from before to get triples: vertex, landmarkVertex, shortest_distance\n",
    "  val v = GxSwpSPMap._1\n",
    "  val a = ListMap(GxSwpSPMap._2.toSeq.sortBy(_._1):_*).toSeq.map(x => x._2)\n",
    "  val d = (a,v)\n",
    "  d\n",
    "}).map(x => SeqOfDoublesAndsrcVertexId(x._1, x._2)).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(shortestDistsSeqFromVertex2Landmark2DF) // but this dataframe needs the first column exploded into 3 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Now we want to make separate columns for each distance in the Sequence\n",
    "in column 'shortestDistances'.\n",
    "\n",
    "Let us use the following ideas for this: \\*\n",
    "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3741049972324885/2662535171379268/4413065072037724/latest.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// this is from https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3741049972324885/2662535171379268/4413065072037724/latest.html\n",
    "import org.apache.spark.sql.{Column, DataFrame}\n",
    "import org.apache.spark.sql.functions.{lit, udf}\n",
    "\n",
    "// UDF to extract i-th element from array column\n",
    "//val elem = udf((x: Seq[Int], y: Int) => x(y))\n",
    "val elem = udf((x: Seq[Double], y: Int) => x(y)) // modified for Sequence of Doubles\n",
    "\n",
    "// Method to apply 'elem' UDF on each element, requires knowing length of sequence in advance\n",
    "def split(col: Column, len: Int): Seq[Column] = {\n",
    "  for (i <- 0 until len) yield { elem(col, lit(i)).as(s\"$col($i)\") }\n",
    "}\n",
    "\n",
    "// Implicit conversion to make things nicer to use, e.g. \n",
    "// select(Column, Seq[Column], Column) is converted into select(Column*) flattening sequences\n",
    "implicit class DataFrameSupport(df: DataFrame) {\n",
    "  def select(cols: Any*): DataFrame = {\n",
    "    var buffer: Seq[Column] = Seq.empty\n",
    "    for (col <- cols) {\n",
    "      if (col.isInstanceOf[Seq[_]]) {\n",
    "        buffer = buffer ++ col.asInstanceOf[Seq[Column]]\n",
    "      } else {\n",
    "        buffer = buffer :+ col.asInstanceOf[Column]\n",
    "      }\n",
    "    }\n",
    "    df.select(buffer:_*)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql.{Column, DataFrame}\n",
    ">     import org.apache.spark.sql.functions.{lit, udf}\n",
    ">     elem: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$9124/2006068772@4ae2323a,DoubleType,List(Some(class[value[0]: array<double>]), Some(class[value[0]: int])),None,false,true)\n",
    ">     split: (col: org.apache.spark.sql.Column, len: Int)Seq[org.apache.spark.sql.Column]\n",
    ">     defined class DataFrameSupport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val shortestDistsFromVertex2Landmark2DF = shortestDistsSeqFromVertex2Landmark2DF.select(split($\"shortestDistances\", 3), $\"srcVertexId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     shortestDistsFromVertex2Landmark2DF: org.apache.spark.sql.DataFrame = [shortestDistances(0): double, shortestDistances(1): double ... 2 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(shortestDistsFromVertex2Landmark2DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// now let's give it our names based on the landmark vertex Ids\n",
    "val shortestDistsFromVertex2Landmark2DF = shortestDistsSeqFromVertex2Landmark2DF.select(split($\"shortestDistances\", 3), $\"srcVertexId\").toDF(columnNames:_*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     shortestDistsFromVertex2Landmark2DF: org.apache.spark.sql.DataFrame = [0: double, 4: double ... 2 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(shortestDistsFromVertex2Landmark2DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(shortestDistsFromVertex2Landmark2DF.select($\"0\",$\"4\",$\"9\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
