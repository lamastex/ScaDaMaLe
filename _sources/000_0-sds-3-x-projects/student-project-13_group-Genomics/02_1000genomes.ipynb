{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genomics Analysis with Glow and Spark\n",
    "=====================================\n",
    "\n",
    "The aim of this notebook is to analyze genomic data in the form of SNPs,\n",
    "and see how different variations of SNPs correlated to ethnicity. This\n",
    "work is inspired by the paper from Huang et al., [Genetic differences\n",
    "among ethnic\n",
    "groups](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-015-2328-0)\n",
    "(2015), and the notebook\n",
    "https://glow.readthedocs.io/en/latest/\\_static/notebooks/tertiary/gwas.html.\n",
    "\n",
    "### Problem background\n",
    "\n",
    "Each person as a unique setup of DNA. The DNA consitst of necleotides,\n",
    "structured as a double helix, where each neucliotide binds to one other.\n",
    "The DNA is split between 22 chromosomes, and each person has 23 pairs of\n",
    "chorosomes. There are four different neucliotides, commonly denoted as\n",
    "A, T, C, and G.\n",
    "\n",
    "Single nucleotide polymorphisms (SNPs) are the most common genetic\n",
    "variation between individuals. Each SNP represents a variation of a\n",
    "specific neucleotide. For example, a SNP may replace the nucleotide\n",
    "cytosine (C) with the nucleotide thymine (T) in a certain stretch of\n",
    "DNA.\n",
    "\n",
    "### Data\n",
    "\n",
    "Genomic data is collected from the [1000 Genomes\n",
    "project](https://www.internationalgenome.org/), with corresponding\n",
    "sample annotations for all individuals in the dataset. For simiplicty,\n",
    "we are only analyzing SNPs assosiated to chromosome 1, however this\n",
    "study can easily be extended to include SNPs from all chromosomes.\n",
    "\n",
    "The data consists of approximatly 6.5 million SNPs from 2504 subjects.\n",
    "\n",
    "### Method\n",
    "\n",
    "After reading the data, we filter low quality SNPs. After this\n",
    "operation, we end up with approx. 400'000 SNPs.\n",
    "\n",
    "By doing a correlation analysis using PCA, we see that different\n",
    "ethnicities cluster together. There is therfore a good reason to suppose\n",
    "that SNPs can be used to predict ethnicity. However, since not all SNPs\n",
    "are correlated to ethnicity, we want to only use the most relevant ones\n",
    "for linear regression analysis.\n",
    "\n",
    "For each SNPs, we calculate the correlation between the values and\n",
    "ethnicity, and take the SNPs with a higher correlation than a threshold\n",
    "value of 0.6 (or maximum 2000 SNPs).\n",
    "\n",
    "Using the selected SNPs as features, we do a linear regression analysis.\n",
    "We make some plots.\n",
    "\n",
    "Load libs and define helper functions\n",
    "====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.functions import array_min, col, monotonically_increasing_id, when, log10\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.linalg import Vector, Vectors, SparseVector, DenseMatrix\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.sql.functions import col,lit\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import mlflow\n",
    "import glow\n",
    "glow.register(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def plot_layout(plot_title, plot_style, xlabel):\n",
    "  plt.style.use(plot_style) #e.g. ggplot, seaborn-colorblind, print(plt.style.available)\n",
    "  plt.title(plot_title)\n",
    "  plt.xlabel(r'${0}$'.format(xlabel))\n",
    "  plt.gca().spines['right'].set_visible(False)\n",
    "  plt.gca().spines['top'].set_visible(False)\n",
    "  plt.gca().yaxis.set_ticks_position('left')\n",
    "  plt.gca().xaxis.set_ticks_position('bottom')\n",
    "  plt.tight_layout()\n",
    "  \n",
    "def plot_histogram(df, col, xlabel, xmin, xmax, nbins, plot_title, plot_style, color, vline, out_path):\n",
    "  plt.close()\n",
    "  plt.figure()\n",
    "  bins = np.linspace(xmin, xmax, nbins)\n",
    "  df = df.toPandas()\n",
    "  plt.hist(df[col], bins, alpha=1, color=color)\n",
    "  if vline:\n",
    "    plt.axvline(x=vline, linestyle='dashed', linewidth=2.0, color='black')\n",
    "  plot_layout(plot_title, plot_style, xlabel)\n",
    "  plt.savefig(out_path)\n",
    "  plt.show()\n",
    "  \n",
    "def calculate_pval_bonferroni_cutoff(df, cutoff=0.05):\n",
    "  bonferroni_p =  cutoff / df.count()\n",
    "  return bonferroni_p\n",
    "\n",
    "def get_sample_info(vcf_df, sample_metadata_df):\n",
    "  \"\"\"\n",
    "  get sample IDs from VCF dataframe, index them, then join to sample metadata dataframe\n",
    "  \"\"\"\n",
    "  sample_id_list = vcf_df.limit(1).select(\"genotypes.sampleId\").collect()[0].__getitem__(\"sampleId\")\n",
    "  sample_id_indexed = spark.createDataFrame(sample_id_list, StringType()). \\\n",
    "                            coalesce(1). \\\n",
    "                            withColumnRenamed(\"value\", \"Sample\"). \\\n",
    "                            withColumn(\"index\", monotonically_increasing_id())\n",
    "  sample_id_annotated = sample_id_indexed.join(sample_metadata_df, \"Sample\")\n",
    "  return sample_id_annotated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to store/find data.\n",
    "# Since a lot of the processing takes a long time, we store intermediate results.\n",
    "vcf_path = \"dbfs:///datasets/sds/genomics/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\"\n",
    "delta_silver_path = \"/mnt/gwas_test/snps.delta\"\n",
    "\n",
    "gwas_results_path = \"/mnt/gwas_test/gwas_results.delta\"\n",
    "phenotype_path = \"/databricks-datasets/genomics/1000G/phenotypes.normalized\"\n",
    "sample_info_path = \"/databricks-datasets/genomics/1000G/samples/populations_1000_genomes_samples.csv\"\n",
    "\n",
    "principal_components_path = \"/dbfs/datasets/sds/genomics/pcs.delta\"\n",
    "hwe_path = \"dbfs:///datasets/sds/genomics/hwe.delta\"\n",
    "vectorized_path = \"dbfs:///datasets/sds/genomics/vectorized.delta\"\n",
    "delta_gold_path = \"dbfs:///datasets/sds/genomics/snps.qced.delta.delta\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Setup and loading data\n",
    "----------------------\n",
    "\n",
    "The data used was dowloaded from\n",
    "ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr1.phase3*shapeit2*mvncall*integrated*v5a.20130502.genotypes.vcf.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Read data\n",
    "=========\n",
    "\n",
    "The data is read using the [Glow](https://projectglow.io/), an\n",
    "open-source library for working with genomics data. It is inlcuded when\n",
    "enabling \"Databricks Runtime for Genomics\", allowing easy read of\n",
    "genomic-specific file formats, and other helper methods.\n",
    "\n",
    "Data exploration and filtering\n",
    "------------------------------\n",
    "\n",
    "Load and view the vcf files. The info fields are combined to one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_view_unsplit = spark.read.format(\"vcf\"). \\\n",
    "   option(\"flattenInfoFields\", \"false\"). \\\n",
    "   load(vcf_path)\n",
    "\n",
    "display(vcf_view_unsplit.withColumn(\"genotypes\", col(\"genotypes\")[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "In the dataframe above, we see that we have columns named\n",
    "\"referenceAllele\" and \"alternateAlleles\". The data so called variations,\n",
    "i.e., genetic sequences which are different between two individuals. The\n",
    "difference may appear differently, and each difference is called an\n",
    "allele. In the data, we have reference genomes, and the alternate\n",
    "alleles are all the variations at a specific position which is found\n",
    "amoung the analyzed subjects.\n",
    "\n",
    "We do not want multiple alternative allelels in one row, so we split\n",
    "them using the `split_miltiallelics` function from Glow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_view = glow.transform(\"split_multiallelics\", vcf_view_unsplit)\n",
    "display(vcf_view.withColumn(\"genotypes\", col(\"genotypes\")[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "We now save our modified dataframe in the Delta format (which compared\n",
    "to VCF is more user friendly). At the same time, we calulcate som\n",
    "neccessary statistics, which we will use later, using the Glow functions\n",
    "`call_summary_stats` and `hardy_weinberg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE - this takes approx 2 hours\n",
    "\n",
    "# vcf_view.selectExpr(\"*\", \"expand_struct(call_summary_stats(genotypes))\", \"expand_struct(hardy_weinberg(genotypes))\"). \\\n",
    "#    write. \\\n",
    "#    mode(\"overwrite\"). \\\n",
    "#    format(\"delta\"). \\\n",
    "#    save(delta_silver_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "The statistics we calculated, as well as the Hardy-Weinberg equilibrium\n",
    "p-values (which basically denotes the probability of a given allele is\n",
    "probable to be true or may be a reading mistake), are used to filter out\n",
    "low quality SNPs.\n",
    "\n",
    "We read the saved dataframe, and filter the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper paramters\n",
    "allele_freq_cutoff = 0.05\n",
    "num_pcs = 5 #number of principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hwe = spark.read.format(\"delta\"). \\\n",
    "#                  load(delta_silver_path). \\\n",
    "#                  where((col(\"alleleFrequencies\").getItem(0) >= allele_freq_cutoff) & \n",
    "#                        (col(\"alleleFrequencies\").getItem(0) <= (1.0 - allele_freq_cutoff))). \\\n",
    "#                  withColumn(\"log10pValueHwe\", when(col(\"pValueHwe\") == 0, 26).otherwise(-log10(col(\"pValueHwe\"))))\n",
    "\n",
    "# hwe.write. \\\n",
    "#    mode(\"overwrite\"). \\\n",
    "#    format(\"delta\").save(hwe_path)\n",
    "\n",
    "hwe = spark.read.format('delta').load(hwe_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hwe_cutoff = calculate_pval_bonferroni_cutoff(hwe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(plot_histogram(df=hwe.select(\"log10pValueHwe\"), \n",
    "                       col=\"log10pValueHwe\",\n",
    "                       xlabel='-log_{10}(P)',\n",
    "                       xmin=0, \n",
    "                       xmax=25, \n",
    "                       nbins=50, \n",
    "                       plot_title=\"hardy-weinberg equilibrium\", \n",
    "                       plot_style=\"ggplot\",\n",
    "                       color='#e41a1c',\n",
    "                       vline = -np.log10(hwe_cutoff),\n",
    "                       out_path = \"/databricks/driver/hwe.png\"\n",
    "                      )\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Filter and save new dataframe, only alleles with in the frequency band,\n",
    "and where the Hardy-Weiburg value is higher than cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.read.format(\"delta\"). \\\n",
    "#    load(hwe_path). \\\n",
    "#    where((col(\"alleleFrequencies\").getItem(0) >= allele_freq_cutoff) & \n",
    "#          (col(\"alleleFrequencies\").getItem(0) <= (1.0 - allele_freq_cutoff)) &\n",
    "#          (col(\"pValueHwe\") >= hwe_cutoff)). \\\n",
    "#    write. \\\n",
    "#    mode(\"overwrite\"). \\\n",
    "#    format(\"delta\"). \\\n",
    "#    save(delta_gold_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hwe_filtered = spark.read.format('delta').load(delta_gold_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "PCA\n",
    "===\n",
    "\n",
    "We perform a PCA analysis for data exploration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized = spark.read.format(\"delta\"). \\\n",
    "#                         load(delta_gold_path). \\\n",
    "#                         selectExpr(\"array_to_sparse_vector(genotype_states(genotypes)) as features\"). \\\n",
    "#                         cache()\n",
    "\n",
    "# vectorized.write. \\\n",
    "#    mode(\"overwrite\"). \\\n",
    "#    format(\"delta\").save(\"dbfs:///datasets/sds/genomics/vectorized.delta\")\n",
    "\n",
    "vectorized = spark.read.format('delta').load(vectorized_path)\n",
    "display(vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - takes approx 30 min\n",
    "\n",
    "# matrix = RowMatrix(MLUtils.convertVectorColumnsFromML(vectorized, \"features\").rdd.map(lambda x: x.features))\n",
    "# pcs = matrix.computeSVD(num_pcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass()\n",
    "# class Covariates:\n",
    "#     covariates: DenseMatrix\n",
    "      \n",
    "# spark.createDataFrame([Covariates(pcs.V.asML())]). \\\n",
    "#       write. \\\n",
    "#       format(\"delta\"). \\\n",
    "#       save(principal_components_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcs_df = spark.createDataFrame(pcs.V.toArray().tolist(), [\"pc\" + str(i) for i in range(num_pcs)])\n",
    "\n",
    "# display(pcs_df)\n",
    "\n",
    "# pcs_df.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"dbfs:///datasets/sds/genomics/pcs_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read already caluclated pca\n",
    "pcs_df = spark.read.format('csv').load(\"dbfs:///datasets/sds/genomics/pcs_df.csv\")\n",
    "display(pcs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Read sample metadata and add to PCA components\n",
    "\n",
    "We load the subject meta data (which includes information about\n",
    "ethnicity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_metadata = spark.read.option(\"header\", True).csv(sample_info_path)\n",
    "sample_info = get_sample_info(vcf_view, sample_metadata)\n",
    "\n",
    "sample_count = sample_info.count()\n",
    "\n",
    "pcs_indexed = pcs_df.coalesce(1).withColumn(\"index\", monotonically_increasing_id())\n",
    "pcs_with_samples = pcs_indexed.join(sample_info, \"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "View 1st and 2nd principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pcs_with_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "We see that there there are some clustering based on ethnicity, showing\n",
    "that it could be possible to tell ethnicity from the SNP information of\n",
    "a subject.\n",
    "\n",
    "Predicting Ethinicity\n",
    "=====================\n",
    "\n",
    "Replication of the paper \"Genetic differences among ethnic groups\",\n",
    "Huang et al.\n",
    "https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-015-2328-0\n",
    "\n",
    "Originally, we had approx. 6.5 milj genetic variations, from 2500\n",
    "individuals. We first did a quality control by filtering only alleles\n",
    "which occur frequently enough, and those above the Hardy-Weinberg P\n",
    "value cutoff.\n",
    "\n",
    "Since we still have over 400 000 varitations, we need to filter further.\n",
    "As in referenced paper, we use the Cramér's V statistic to measure the\n",
    "correlation between ethnicity and genetic variation, with a value\n",
    "between 0 and 1. Those with significantly high correlation are kept.\n",
    "\n",
    "$$V=\\\\sqrt{\\\\frac{\\\\chi^2/N}{ \\\\min \\\\left(k-1,r-1\\\\right)}}$$\n",
    "\n",
    "N is the number of individual samples, k is the number of ethic groups,\n",
    "r is the SNP status (0, 1 or 2 minor alleles).\n",
    "\n",
    "Where the Pearson’s chi-squared statistic can be calculated as:\n",
    "\n",
    "$${\\\\chi}^2={\\\\displaystyle \\\\sum*{i=1}^k{\\\\displaystyle\n",
    "\\\\sum*{j=1}^r\\\\frac{{\\\\left({O}*{i,j}-{E}*{i,j}\\\\right)}^2}{E\\_{i,j}}}}$$\n",
    "\n",
    "Read the dataset that contains the population information and encode it\n",
    "for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sample information\n",
    "sample_metadata = spark.read.option(\"header\", True).csv(sample_info_path)\n",
    "sample_info = get_sample_info(vcf_view, sample_metadata)\n",
    "\n",
    "sample_count = sample_info.count()\n",
    "\n",
    "mlflow.log_param(\"number of samples\", sample_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sample_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of the labels\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# indexer = StringIndexer(inputCol=\"Population\", outputCol=\"Population_index\")\n",
    "indexer = StringIndexer(inputCol=\"super_population\", outputCol=\"Population_index\")\n",
    "model = indexer.fit(sample_info)\n",
    "indexed = model.transform(sample_info)\n",
    "\n",
    "# indexed.select(\"Population\", \"Population_index\").distinct().show(30)\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"Population_index\"],\n",
    "                        outputCols=[\"population_onehot\"])\n",
    "model = encoder.fit(indexed)\n",
    "encoded = model.transform(indexed)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Filtering of SNPs based on chi-squared test\n",
    "===========================================\n",
    "\n",
    "According to [Tao Huang et al.\n",
    "(2015)](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-015-2328-0),\n",
    "85 % of SNPs are the same in all human populations, hence we will apply\n",
    "chi-squared based feature selection to try to identify the approximately\n",
    "15 % of SNPs that are population-specific. We decided to test our\n",
    "classifiers with several sizes of the feature vectors: \\* all 416,005\n",
    "available SNPs from Chromosome 1 \\* most relevant 200,000 SNPs \\* most\n",
    "relevant 20,000 SNPs \\* most relevant 2,000 SNPs \\* most relevant 1,000\n",
    "SNPs \\* most relevant 100 SNPs \\* most relevant 50 SNPs\n",
    "\n",
    "This would give us a better understanding if the ChiSqSelector is\n",
    "appropriate method for selecting the features in genomics. We aklowedge\n",
    "that [Tao Huang et al.\n",
    "(2015)](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-015-2328-0)\n",
    "used different metric based on chi-squared distribution, but\n",
    "ChiSqSelector is the closest already implemented method in spark that we\n",
    "could find.\n",
    "\n",
    "In order to use ChiSqSelector from pyspark.ml.feature, we first need to\n",
    "format the data into a sparce feature vectors and numeric corresponding\n",
    "label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load earlier-filtered data\n",
    "delta_gold_path = \"dbfs:///datasets/sds/genomics/snps.qced.delta.delta\"\n",
    "hwe_filtered = spark.read.format('delta').load(delta_gold_path)\n",
    "vectorized_2 = hwe_filtered.select(glow.genotype_states('genotypes').alias('states')).collect()\n",
    "vectorized_df = spark.createDataFrame(vectorized_2)\n",
    "display(vectorized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Transformation of dataframe to fit ChiSqSelector\n",
    "\n",
    "We use monotonically*increasing*id, poseexplode and collect\\_list\n",
    "methods to achieve the required format of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column that indicates to which SNP the states belong and then explode SNPs\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "from pyspark.sql.functions import explode, posexplode\n",
    "from pyspark.sql.functions import col, concat, desc, first, lit, row_number, collect_list\n",
    "\n",
    "vec_df_dummy = vectorized_df.withColumn(\"SNP\", monotonically_increasing_id())\n",
    "#vec_exploded_states = vec_df_dummy.withColumn(\"expanded_states\", explode(\"states\"))\n",
    "vec_exploded_states = vec_df_dummy.select(\"SNP\",posexplode(\"states\"))\n",
    "vec_exploded_states = vec_exploded_states.withColumnRenamed(\"pos\", \"subjectID\")\n",
    "vec_exploded_states = vec_exploded_states.withColumnRenamed(\"col\", \"expandedState\")\n",
    "features_df = vec_exploded_states.groupBy(\"subjectID\").agg(collect_list(\"expandedState\").alias(\"Features\"))\n",
    "features_df = features_df.join(encoded, features_df.subjectID == encoded.index).select(\"Features\",\"Population_index\", \"population_onehot\")\n",
    "features_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Finally, Glow utility function array*to*sparse\\_vector is used to\n",
    "convert the dense feature vectors into sparse vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.selectExpr(\"array_to_sparse_vector(Features) as features_sparse\",\"Population_index\", \"population_onehot\")\n",
    "features_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### Fitting ChiSqSelector\n",
    "\n",
    "As the computation time of ChiSqSelector takes roughly 3 hours for each\n",
    "subset of features, we are saving them to disc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_200_000 = \"dbfs:///datasets/sds/genomics/selected_feat_200_000.delta\"\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "# selector = ChiSqSelector(featuresCol='features_sparse', outputCol='ChiSq',labelCol='Population_index', numTopFeatures = 200000)\n",
    "# selected_feat_200_000 = selector.fit(features_df).transform(features_df)\n",
    "# selected_feat_200_000.write.format(\"delta\").save(feature_selection_200_000)\n",
    "selected_feat_200_000 = spark.read.format('delta').load(feature_selection_200_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_20_000 = \"dbfs:///datasets/sds/genomics/selected_feat_20_000.delta\"\n",
    "# selector = ChiSqSelector(featuresCol='features_sparse', outputCol='ChiSq',labelCol='Population_index', numTopFeatures = 20000)\n",
    "# selected_feat_20_000 = selector.fit(features_df).transform(features_df)\n",
    "# selected_feat_20_000.write.format(\"delta\").save(feature_selection_20_000)\n",
    "selected_feat_20_000 = spark.read.format('delta').load(feature_selection_20_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_2000 = \"dbfs:///datasets/sds/genomics/selected_feat_2000.delta\"\n",
    "# from pyspark.ml.feature import ChiSqSelector\n",
    "# selector = ChiSqSelector(featuresCol='features_sparse', outputCol='ChiSq',labelCol='Population_index', numTopFeatures = 2000)\n",
    "# selected_feat_2000 = selector.fit(features_df).transform(features_df)\n",
    "# selected_feat_2000.write.format(\"delta\").save(feature_selection_2000)\n",
    "\n",
    "selected_feat_2000 = spark.read.format('delta').load(feature_selection_2000)\n",
    "selected_feat_2000.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_results_1000 = \"dbfs:///datasets/sds/genomics/selected_feat_1000.delta\"\n",
    "# selector = ChiSqSelector(featuresCol='features_sparse', outputCol='ChiSq',labelCol='Population_index', numTopFeatures = 1000)\n",
    "# selected_feat_1000 = selector.fit(features_df).transform(features_df)\n",
    "# selected_feat_1000.write.format(\"delta\").save(feature_selection_results_1000)\n",
    "#result = selector.fit(features_df).transform(features_df)\n",
    "selected_feat_1000 = spark.read.format('delta').load(feature_selection_results_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_results_100 = \"dbfs:///datasets/sds/genomics/selected_feat_100.delta\"\n",
    "# selector = ChiSqSelector(featuresCol='features_sparse', outputCol='ChiSq',labelCol='Population_index', numTopFeatures = 100)\n",
    "# selected_feat_100 = selector.fit(features_df).transform(features_df)\n",
    "# selected_feat_100.write.format(\"delta\").save(feature_selection_results_100)\n",
    "#result = selector.fit(features_df).transform(features_df)\n",
    "selected_feat_100 = spark.read.format('delta').load(feature_selection_results_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_50 = \"dbfs:///datasets/sds/genomics/selected_feat_50.delta\"\n",
    "# selector = ChiSqSelector(featuresCol='features_sparse', outputCol='ChiSq',labelCol='Population_index', numTopFeatures = 50)\n",
    "# selected_feat_50 = selector.fit(features_df).transform(features_df)\n",
    "# selected_feat_50.write.format(\"delta\").save(feature_selection_50)\n",
    "selected_feat_50 = spark.read.format('delta').load(feature_selection_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Train logistic regression and random forest models\n",
    "--------------------------------------------------\n",
    "\n",
    "The code below implements a loop over datasets with different number of\n",
    "SNPs, test-train split and fitting of logistic regression and random\n",
    "forest models. The performance is measured in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "def fit_ML_model(trainSet,testSet, model_in):\n",
    "\n",
    "  # Train model.  \n",
    "  model = model_in.fit(trainSet)\n",
    "\n",
    "  # Make predictions.\n",
    "  predictions = model.transform(testSet)\n",
    "\n",
    "  # Evaluate the classifier based on accuracy\n",
    "  evaluator = MulticlassClassificationEvaluator(\n",
    "      labelCol=\"Population_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "  accuracy = evaluator.evaluate(predictions)\n",
    "  return accuracy\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"Population_index\", featuresCol=\"final_features\", numTrees=20)\n",
    "lr = LogisticRegression(featuresCol=\"final_features\", labelCol=\"Population_index\", maxIter=100)\n",
    "\n",
    "acc_rf = []\n",
    "acc_lr = []\n",
    "\n",
    "# Run the models on full features set (without ChiSqSelector)\n",
    "features_ready = selected_feat_2000.selectExpr(\"features_sparse as final_features\",\"Population_index\")\n",
    "trainSet, testSet = features_ready.randomSplit((0.8, 0.2), seed=123)\n",
    "acc_rf.append(fit_ML_model(trainSet, testSet, rf))\n",
    "acc_lr.append(fit_ML_model(trainSet, testSet, lr))\n",
    "\n",
    "# Run the models on selected features by ChiSqSelector\n",
    "for data_item in [selected_feat_200_000, selected_feat_20_000, selected_feat_2000, selected_feat_1000, selected_feat_100, selected_feat_50]:\n",
    "    # Work around for the bug in ChiSqSelector - otherwise does not work with the random forest model\n",
    "    #ChiSqSelector has a bug that it formats data in a way that RandomForest does not accept. We found this work around to work. Bug reported in https://stackoverflow.com/questions/46269275/spark-ml-issue-in-training-after-using-chisqselector-for-feature-selection\n",
    "    features_ready = data_item.select(glow.vector_to_array('ChiSq').alias('features_dense'), \"Population_index\", \"ChiSq\")\n",
    "    features_ready = features_ready.selectExpr(\"array_to_sparse_vector(features_dense) as final_features\",\"Population_index\")\n",
    "    trainSet, testSet = features_ready.randomSplit((0.8, 0.2), seed=123)\n",
    "    acc_rf.append(fit_ML_model(trainSet, testSet, rf))\n",
    "    acc_lr.append(fit_ML_model(trainSet, testSet, lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Results and Discussion\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rf_plot = plt.scatter(x=['all_feat', '200,000', '20,000', '2,000', '1,000', '100', '50'], y=acc_rf, c='r')\n",
    "lf_plot = plt.scatter(x=['all_feat', '200,000', '20,000', '2,000', '1,000', '100', '50'], y=acc_lr, c='b')\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend((rf_plot, lf_plot),\n",
    "           ('Random forest', 'Logistic regression'),\n",
    "           scatterpoints=1,\n",
    "           bbox_to_anchor=(1.5, 1),\n",
    "           ncol=1,\n",
    "           fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "The plot above shows the accuracy of random forest and logistic\n",
    "regression classifiers on predicting the ethnicity from prepared SNPs.\n",
    "Both classifiers preformed much better than random guessing, hence the\n",
    "ethnicity information is clearly encoded in SNPs. The logistic\n",
    "regression consistently outperformed random forest and reached accuracy\n",
    "over 90 %. We can also see that random forest was sensitive to the\n",
    "feature selection and it's performance droped once fewer that the all\n",
    "available SNPs (416,005 SNPs) were used. In contrast, logistic\n",
    "regression performed equally well with half of the available features\n",
    "(200,000 SNPs). This indicates that having well-tuned classifier and an\n",
    "appropriate feature selector allows us to reduce the required number of\n",
    "features dramatically without compromising the performance. The work\n",
    "could be improved by testing other ways of selecting the relevant SNPs,\n",
    "tuning the hyperparameters in a grid-search manner, building confidence\n",
    "intervals based on bootstrapping or cross-validation and testing other\n",
    "types of classifiers."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
