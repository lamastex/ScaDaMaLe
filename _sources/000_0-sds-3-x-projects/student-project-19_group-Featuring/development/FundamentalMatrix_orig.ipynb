{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem formulation\n",
    "-------------------\n",
    "\n",
    "A common problem in computer vision is estimating the fundamental matrix\n",
    "based on a image pair. The fundamental matrix relates corresponding\n",
    "points in stereo geometry, and is useful as a pre-processing step for\n",
    "example when one wants to perform reconstruction of a captured scene. In\n",
    "this small project we use a scalable distributed algorithm to compute\n",
    "fundamental matrices between a large set of images.\n",
    "\n",
    "#### Short theory section\n",
    "\n",
    "Assume that we want to link points in some image taken by camera &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;P*1\" /&gt; to points in\n",
    "an image taken by another camera &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;P*2\" /&gt;. Let &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;x*i\" /&gt; and &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;x*i'\" /&gt; denote the\n",
    "projections of global point &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;X*i\" /&gt; onto the\n",
    "cameras &lt;img src=\"https://latex.codecogs.com/svg.latex?&space;P*1\"\n",
    "/&gt; and &lt;img src=\"https://latex.codecogs.com/svg.latex?&space;P\\_2\"\n",
    "/&gt;, respectivly. Then the points are related as follows\n",
    "\n",
    "&lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;\\\\begin{cases}\\\\lambda*i\n",
    "x*i = P*1X*i \\\\ \\\\lambda*i' x*i' = P*2X*i \\\\end{cases} \\\\Leftrightarrow\n",
    "\\\\quad \\\\begin{cases}\\\\lambda*i x*i = P*1HH^{-1}X*i \\\\ \\\\lambda*i' x*i'\n",
    "= P*2HH^{-1}X*i \\\\end{cases} \\\\Leftrightarrow \\\\quad\n",
    "\\\\begin{cases}\\\\lambda*i x*i = \\\\tilde{P*1}\\\\tilde{X*i} \\\\ \\\\lambda*i'\n",
    "x*i' = \\\\tilde{P*2}\\\\tilde{X*i} \\\\end{cases}\" /&gt;\n",
    "\n",
    "where &lt;img src=\"https://latex.codecogs.com/svg.latex?&space;\\\\lambda,\n",
    "\\\\lambda'\" /&gt; are scale factors. Since we always can apply a\n",
    "projective transformation &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;H\" /&gt; to set one of\n",
    "the cameras to &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;P*1 = \\[I \\\\quad 0\\]\"\n",
    "/&gt; and the other to some &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;P*2 = \\[A \\\\quad t\\]\"\n",
    "/&gt; we can parameterize the global point &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;X*i\" /&gt; by &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;X*i(\\\\lambda) =\n",
    "\\[\\\\lambda x*i \\\\quad 1\\]^T\" /&gt;. Thus the projected point onto camera\n",
    "&lt;img src=\"https://latex.codecogs.com/svg.latex?&space;P*2\" /&gt; is\n",
    "represented by the line &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;P*2X*i(\\\\lambda) =\n",
    "\\\\lambda Ax*i + t \" /&gt;. This line is called the epipolar line to the\n",
    "point &lt;img src=\"https://latex.codecogs.com/svg.latex?&space;x*i\"\n",
    "/&gt; in epipolar geomtry, and descirbes how the point &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;x*i\" /&gt; in image 1\n",
    "is related to points on in image 2. Since all scene points that can\n",
    "project to &lt;img src=\"https://latex.codecogs.com/svg.latex?&space;x*i\"\n",
    "/&gt; are on the viewing ray, all points in the second image that can\n",
    "correspond &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;x\\_i\" /&gt; have to be\n",
    "on the epipolar line. This condition is called the epipolar constraint.\n",
    "\n",
    "![plot](http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT10/img17.gif)\n",
    "\n",
    "Taking two points on this line (one of them being &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;e'\" /&gt; using &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;\\\\lambda = 0\" /&gt;),\n",
    "we can derive an expression of this line &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;\\\\ell\" /&gt;, as any\n",
    "point x on the line &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;\\\\ell\" /&gt; must\n",
    "fulfill &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;\\\\ell^Tx = 0\" /&gt;.\n",
    "Thus the line is thus given by\n",
    "\n",
    "&lt;img src=\"https://latex.codecogs.com/svg.latex?&space;\\\\ell = t\n",
    "\\\\times (Ax +t ) = t \\\\times (Ax) = e' \\\\times Ax\\_i.\\\\\" /&gt;\n",
    "\n",
    "Let &lt;img src=\"https://latex.codecogs.com/svg.latex?&space;F = e'\n",
    "\\\\times A \" /&gt;, this is called the fundamental matrix. The\n",
    "fundamental matrix thus is a mathematical formulation which links points\n",
    "in image 1 to lines in image 2 (and vice versa). If &lt;img\n",
    "src=\"https://latex.codecogs.com/svg.latex?&space;x'\" /&gt; corresponds\n",
    "to &lt;img src=\"https://latex.codecogs.com/svg.latex?&space;x\" /&gt;\n",
    "then the epipolar constraint can be written\n",
    "\n",
    "&lt;img src=\"https://latex.codecogs.com/svg.latex?&space;x'^T\\\\ell =\n",
    "x'^T F x = 0 \" /&gt;\n",
    "\n",
    "F is a 3x 3 matrix with 9 entiers and has 7 degrees of freedom. It can\n",
    "be estimated using 7 points using the 7-point algorithm.\n",
    "\n",
    "Before we have assumed the the correspndeces between points in the\n",
    "imagaes are known, however these are found by first extracting features\n",
    "in the images using some form of feature extractor (e.g. SIFT) and\n",
    "subsequently finding matches using some mathcing criterion/algorithm\n",
    "(e.g. using Lowes criterion or in our case FLANN based matcher)\n",
    "\n",
    "#### SIFT\n",
    "\n",
    "Scale-invariant feature transform (SIFT) is a feature detection\n",
    "algorithm which detect and describe local features in images, see\n",
    "examples of detected SIFT features in the two images (a) and (b). SIFT\n",
    "finds local features present in the image and compute desriptors and\n",
    "locations of these features. Next we need to link the features present\n",
    "in image 1 to the features in image 2, which can be done using e.g. a\n",
    "FLANN (Fast Library for Approximate Nearest Neighbors) based matcher. In\n",
    "short the features in the images are compared and the matches are found\n",
    "using a nearest neighbor search. After a matching algorithm is used we\n",
    "have correspandence between the detected points in image 1 and image 2,\n",
    "see example in image (c) below. Note that there is still a high\n",
    "probaility that some of these matches are incorrect.\n",
    "\n",
    "![plot](https://www.researchgate.net/profile/Hieu_Nguyen144/publication/259952213/figure/fig1/AS:614330479439873@1523479208356/Scale-invariant-feature-transform-SIFT-matching-result-of-a-few-objects-placed-in.png)\n",
    "\n",
    "### RANSAC\n",
    "\n",
    "Some matches found by the FLANN may be incorrect, and a common robust\n",
    "method used for reducing the influence of these outliers in the\n",
    "estimation of F is RANSAC (RANdom SAmpling Consensus). In short, it\n",
    "relies on the fact that the inliers will tend to a consesus regarding\n",
    "the correct estimation, whereas the outlier estimation will show greater\n",
    "variation. By sampling random sets of points with size corresponding to\n",
    "the degrees of freedom of the model, calculating their corresponding\n",
    "estimations, and grouping all estimations with a difference below a set\n",
    "threshold, the largest consesus group is found. This set is then lastly\n",
    "used for the final estimate of F.\n",
    "\n",
    "### For a more joyful presentation of the theory, listed to The Fundamental Matrix Song! (link)\n",
    "\n",
    "[![The Fundamental\n",
    "matrix](https://img.youtube.com/vi/DgGV3l82NTk/0.jpg)](https://www.youtube.com/watch?v=DgGV3l82NTk)\n",
    "\n",
    "OpenCV is an well-known open-source library for computer vision, machine\n",
    "learning, and image processing tasks. In this project we will use it for\n",
    "feature extraction (SIFT), feature matching (FLANN) and the estimation\n",
    "of the fundamental matrix (using the 7-point algorithm). Let us install\n",
    "opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Also we need to download a dataset that we can work with, this dataset\n",
    "is collected by Carl Olsson from LTH. This is achieved by the bash shell\n",
    "script below. The dataset is placed in the /tmp folder using the -P\n",
    "\"prefix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget -P /tmp  vision.maths.lth.se/calledataset/door/door.zip\n",
    "\n",
    "unzip /tmp/door.zip -d /tmp/0019/\n",
    "\n",
    "rm -r /tmp/door.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "//\"wget -P /tmp vision.maths.lth.se/calledataset/door/door.zip\" !!\n",
    "//\"unzip /tmp/door.zip -d /tmp/door/\"!!\n",
    "\n",
    "//move downloaded dataset to dbfs\n",
    "\n",
    "val localpath=\"file:/tmp/0019/\"\n",
    "\n",
    "dbutils.fs.rm(\"dbfs:/datasets/0019/images\", true)\n",
    "\n",
    "dbutils.fs.mkdirs(\"dbfs:/datasets/0019/images\")\n",
    "\n",
    "dbutils.fs.cp(localpath, \"dbfs:/datasets/0019/images\", true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import sys.process._\n",
    ">     localpath: String = file:/tmp/0019/\n",
    ">     res0: Boolean = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -r /tmp/0019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/datasets/0019/images\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading one image from the dataset for testing\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_img(figtitle,img):\n",
    "  \n",
    "  #create figure with std size\n",
    "  fig = plt.figure(figtitle, figsize=(10, 5))\n",
    "    \n",
    "  plt.imshow(img)\n",
    "\n",
    "  display(plt.show())\n",
    "\n",
    "img = cv2.imread(\"/dbfs/datasets/0019/images/DSC_0007.JPG\")\n",
    "\n",
    "plot_img(\"test\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Read Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "dataset_path  = \"/dbfs/datasets/0019/images/\"\n",
    "\n",
    "#get all filenames in folder\n",
    "files = glob.glob(os.path.join(dataset_path,\"*.JPG\"))\n",
    "\n",
    "dataset = []\n",
    "\n",
    "#load all images\n",
    "for file in files:\n",
    "  # Load an color image\n",
    "  img = cv2.imread(file)\n",
    " \n",
    "  #add image and image name as a tupel to the list\n",
    "  dataset.append((file,img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Define maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_features = 1000\n",
    "\n",
    "def plot_img(figtitle,img):\n",
    "  \n",
    "  #create figure with std size\n",
    "  fig = plt.figure(figtitle, figsize=(10, 5))\n",
    "    \n",
    "  plt.imshow(img)\n",
    "\n",
    "  display(plt.show())\n",
    "  \n",
    "  \n",
    "def extract_features(s): \n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  \n",
    "  #convert to gray scale\n",
    "  gray= cv2.cvtColor(s[1],cv2.COLOR_BGR2GRAY)\n",
    "  \n",
    "  sift = cv2.SIFT_create(max_features)\n",
    "\n",
    "  #extract sift features and descriptors\n",
    "  kp, des = sift.detectAndCompute(gray, None)\n",
    "  \n",
    "  #convert keypoint class to list of feature locations (for serialization)\n",
    "  points=[]\n",
    "  for i in range(len(kp)): \n",
    "    points.append(kp[i].pt)\n",
    "  \n",
    "  #return a tuple of image name, image, feature points, descriptors, called a feature tuple\n",
    "  return (s[0], s[1], points, des)\n",
    "\n",
    "def estimate_fundamental_matrix(s): \n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  \n",
    "  # s[0] is a feature tuple for the first image, s[1] is the same for the second image\n",
    "  a = s[0]\n",
    "  b = s[1]\n",
    "  \n",
    "  # unpacks the tuples\n",
    "  name1, im1, kp1, desc1 = a\n",
    "  name2, im2, kp2, desc2 = b\n",
    "  \n",
    "  # Create FLANN matcher object \n",
    "  FLANN_INDEX_KDTREE = 0\n",
    "  indexParams = dict(algorithm=FLANN_INDEX_KDTREE, \n",
    "                   trees=5) \n",
    "  searchParams = dict(checks=50) \n",
    "  flann = cv2.FlannBasedMatcher(indexParams, \n",
    "                              searchParams) \n",
    "  \n",
    "  # matches the descriptors, for each query descriptor it finds the two best matches among the train descriptors\n",
    "  matches = flann.knnMatch(desc1, desc2, k=2)\n",
    "  \n",
    "  goodMatches = [] \n",
    "  pts1 = [] \n",
    "  pts2 = [] \n",
    "\n",
    "  # compares the best with the second best match and only adds those where the best match is significantly better than the next best.\n",
    "  for i,(m,n) in enumerate(matches):\n",
    "    if m.distance < 0.8*n.distance:\n",
    "        goodMatches.append([m.queryIdx, m.trainIdx])\n",
    "        pts2.append(kp2[m.trainIdx])\n",
    "        pts1.append(kp1[m.queryIdx])\n",
    "        \n",
    "  pts1 = np.array(pts1, dtype=np.float32)\n",
    "  pts2 = np.array(pts2, dtype=np.float32)\n",
    "\n",
    "  # finds the fundamental matrix using ransac: \n",
    "  # selects minimal sub-set of the matches, \n",
    "  # estimates the fundamental matrix, \n",
    "  # checks how many of the matches satisfy the epipolar geometry (the inlier set)\n",
    "  # iterates this for a number of iterations,\n",
    "  # returns the fundamental matrix and mask with the largest number of inliers.\n",
    "  F, mask = cv2.findFundamentalMat(pts1, pts2, cv2.FM_RANSAC)\n",
    "   \n",
    "  inlier_matches = []\n",
    "  \n",
    "  # removes all matches that are not inliers\n",
    "  if mask is not None:  \n",
    "    for i, el in enumerate(mask):\n",
    "      if el == 1:\n",
    "        inlier_matches.append(goodMatches[i])\n",
    "  \n",
    "  # returns a tuple containing the feature tuple of image one and image two, the fundamental matrix and the inlier matches\n",
    "  return (a, b, F, inlier_matches)\n",
    "\n",
    "def display_data(data):\n",
    "  for el in data:\n",
    "  \n",
    "    print(el[2])\n",
    "  \n",
    "    print(\"#######################################################\")\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Perform Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates an rdd from the loaded images (im_name, image)\n",
    "rdd = sc.parallelize(dataset)\n",
    "\n",
    "# applys the feature extraction to the images\n",
    "rdd_features = rdd.map(extract_features)\n",
    "\n",
    "# forms pairs of images by applying the cartisian product and filtering away the identity pair\n",
    "rdd_pairs = rdd_features.cartesian(rdd_features).filter(lambda s: s[0][0] != s[1][0])\n",
    "\n",
    "# applys the fundamental matrix estimation function on the pairs formed in the previous step and filters away all pairs with a low inlier set.\n",
    "rdd_fundamental_matrix = rdd_pairs.map(estimate_fundamental_matrix).filter(lambda s: len(s[3]) > 50)\n",
    "\n",
    "# collects the result from the nodes\n",
    "data = rdd_fundamental_matrix.collect()\n",
    "\n",
    "# displays the fundamental matrices\n",
    "display_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Now we have computed the fundamental matrices, let us have a look at\n",
    "them by present the epipolar lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def drawlines(img1,img2,lines,pts1,pts2):\n",
    "  #from opencv tutorial\n",
    "    ''' img1 - image on which we draw the epilines for the points in img2\n",
    "        lines - corresponding epilines '''\n",
    "    r,c,_ = img1.shape\n",
    "    for r,pt1,pt2 in zip(lines,pts1,pts2):\n",
    "        color = tuple(np.random.randint(0,255,3).tolist())\n",
    "        x0,y0 = map(int, [0, -r[2]/r[1] ])\n",
    "        x1,y1 = map(int, [c, -(r[2]+r[0]*c)/r[1] ])\n",
    "        img1 = cv2.line(img1, (x0,y0), (x1,y1), color,3)\n",
    "        img1 = cv2.circle(img1,tuple(pt1),10,color,-1)\n",
    "        img2 = cv2.circle(img2,tuple(pt2),10,color,-1)\n",
    "    return img1,img2\n",
    "\n",
    "# draws a random subset of the data\n",
    "sampling = random.choices(data, k=4)\n",
    "  \n",
    "#plotts the inlier features in the first image and the corresponding epipolar lines in the second image\n",
    "i = 0\n",
    "fig, axs = plt.subplots(1, 8, figsize=(25, 5))\n",
    "for el in sampling:\n",
    "    \n",
    "  a, b, F, matches = el;\n",
    "  \n",
    "  if F is None:\n",
    "    continue\n",
    "\n",
    "  name1, im1, kp1, desc1 = a\n",
    "  name2, im2, kp2, desc2 = b\n",
    "  \n",
    "  pts1 = [] \n",
    "  pts2 = [] \n",
    "  \n",
    "  for m in matches:\n",
    "    pts1.append(kp1[m[0]]);\n",
    "    pts2.append(kp2[m[1]]);\n",
    "    \n",
    "  pts1 = np.array(pts1, dtype=np.float32)\n",
    "  pts2 = np.array(pts2, dtype=np.float32)\n",
    "  \n",
    "  lines1 = cv2.computeCorrespondEpilines(pts2.reshape(-1,1,2), 2, F)\n",
    "  lines1 = lines1.reshape(-1,3)\n",
    "  \n",
    "  im1 = im1.copy()\n",
    "  im2 = im2.copy()\n",
    "  \n",
    "  img1, img2 = drawlines(im1,im2,lines1,pts1,pts2)\n",
    "  \n",
    "  \n",
    "\n",
    "  axs[i].imshow(img2), axs[i].set_title('Image pair '+str(i+1)+': Features')\n",
    "  axs[i+1].imshow(img1), axs[i+1].set_title('Image pair '+str(i+1)+': Epipolar lines')\n",
    "\n",
    "  i += 2\n",
    "  #plt.subplot(121),plt.imshow(img1), plt.title('Epipolar lines')\n",
    "  #plt.subplot(122),plt.imshow(img2), plt.title('Points')\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "Present Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# draws a random subset of the data\n",
    "sampling = random.choices(data, k=4)\n",
    "  \n",
    "j = 0\n",
    "fig, axs = plt.subplots(1, 4, figsize=(25, 5))\n",
    "# draws lines between the matched feature in the two images (not epipolar lines!)\n",
    "for el in sampling:\n",
    "    \n",
    "  a, b, F, matches = el;\n",
    "  \n",
    "  if F is None:\n",
    "    continue\n",
    "\n",
    "  name1, im1, kp1, desc1 = a\n",
    "  name2, im2, kp2, desc2 = b\n",
    "  \n",
    "  kp1_vec = [] \n",
    "  kp2_vec = [] \n",
    "  matches_vec = []\n",
    "  \n",
    "  for i,m in enumerate(matches):\n",
    "    kp1_vec.append(cv2.KeyPoint(kp1[m[0]][0], kp1[m[0]][1],1))\n",
    "    kp2_vec.append(cv2.KeyPoint(kp2[m[1]][0], kp2[m[1]][1],1))\n",
    "                   \n",
    "    matches_vec.append(cv2.DMatch(i, i, 1))    \n",
    "    \n",
    "  matched_image = im1.copy()\n",
    "      \n",
    "  matched_image = cv2.drawMatches(im1, kp1_vec, im2, kp2_vec, matches_vec, matched_image)\n",
    "    \n",
    "  axs[j].imshow(matched_image), axs[j].set_title('Image pair '+str(j+1)+': Matches')\n",
    "  j += 1\n",
    "  #plot_img(\"matches\", matched_image)\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions: \n",
    "# Pics of different resolutions/sizes: Yes\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
