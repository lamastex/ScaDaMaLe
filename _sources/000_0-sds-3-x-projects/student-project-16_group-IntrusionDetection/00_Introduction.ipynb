{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrusion Detection\n",
    "\n",
    "links URLs\n",
    "\n",
    "Problem Definition\n",
    "==================\n",
    "\n",
    "With the evolution and pervasive usage of the computer networks and\n",
    "cloud environments, Cyber-attacks such as Distributed Denial of Service\n",
    "(DDoS) become major threads for such environments. For an example, DDoS\n",
    "attacks can prohibit normal usage of the web services through saturating\n",
    "their underlying system’s resources and even in most recent type of\n",
    "attacks namely Low-rate DDoS attacks, they drop the Quality of Service\n",
    "(QoS) of the cloud service providers significantly and bypass the\n",
    "detection systems by behaving similar to the normal users. Modern\n",
    "networked business environments require a high level of security to\n",
    "ensure safe and trusted communication of information between various\n",
    "organizations and to counter such attacks. An Intrusion Detection System\n",
    "is the foremost important step against such threads happening in the\n",
    "network and act as an adaptable safeguard technology for system security\n",
    "after traditional technologies fail. As the network attacks become more\n",
    "sophisticated, it is crucial to equip the system with the\n",
    "state-of-the-art intrusion detection systems. In this project, we\n",
    "investigate different types of learning-based Intrusion detection\n",
    "systems and evaluate them based on different metrics on a large\n",
    "benchmark dataset in a distributed manner using Apache Spark, which is\n",
    "an open-source distributed general-purpose cluster-computing framework.\n",
    "\n",
    "Loading and Preprocessing Data\n",
    "==============================\n",
    "\n",
    "The raw data of UNSW-NB15 Dataset is a pcap file of network traffic with\n",
    "the size of 100gb, that 49 features (including labels) is extracted from\n",
    "the dataset using Argus, Bro-IDS tools and twelve other algorithms. The\n",
    "extracted features desription is given below.\n",
    "\n",
    "|No.|Name |Type |Description |\n",
    "|---|----------------|---------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "|0 |srcip |nominal |Source IP address | |1 |sport |integer |Source port\n",
    "number | |2 |dstip |nominal |Destination IP address | |3 |dsport\n",
    "|integer |Destination port number | |4 |proto |nominal |Transaction\n",
    "protocol | |5 |state |nominal |Indicates to the state and its dependent\n",
    "protocol, e.g. ACC, CLO, CON, ECO, ECR, FIN, INT, MAS, PAR, REQ, RST,\n",
    "TST, TXD, URH, URN, and (-) (if not used state) | |6 |dur |Float |Record\n",
    "total duration | |7 |sbytes |Integer |Source to destination transaction\n",
    "bytes | |8 |dbytes |Integer |Destination to source transaction bytes |\n",
    "|9 |sttl |Integer |Source to destination time to live value | |10 |dttl\n",
    "|Integer |Destination to source time to live value | |11 |sloss |Integer\n",
    "|Source packets retransmitted or dropped | |12 |dloss |Integer\n",
    "|Destination packets retransmitted or dropped | |13 |service |nominal\n",
    "|http, ftp, smtp, ssh, dns, ftp-data ,irc and (-) if not much used\n",
    "service | |14 |Sload |Float |Source bits per second | |15 |Dload |Float\n",
    "|Destination bits per second | |16 |Spkts |integer |Source to\n",
    "destination packet count | |17 |Dpkts |integer |Destination to source\n",
    "packet count | |18 |swin |integer |Source TCP window advertisement value\n",
    "| |19 |dwin |integer |Destination TCP window advertisement value | |20\n",
    "|stcpb |integer |Source TCP base sequence number | |21 |dtcpb |integer\n",
    "|Destination TCP base sequence number | |22 |smeansz |integer |Mean of\n",
    "the ?ow packet size transmitted by the src | |23 |dmeansz |integer |Mean\n",
    "of the ?ow packet size transmitted by the dst | |24 |trans*depth\n",
    "|integer |Represents the pipelined depth into the connection of http\n",
    "request/response transaction | |25 |res*bdy*len |integer |Actual\n",
    "uncompressed content size of the data transferred from the server’s http\n",
    "service. | |26 |Sjit |Float |Source jitter (mSec) | |27 |Djit |Float\n",
    "|Destination jitter (mSec) | |28 |Stime |Timestamp|record start time |\n",
    "|29 |Ltime |Timestamp|record last time | |30 |Sintpkt |Float |Source\n",
    "interpacket arrival time (mSec) | |31 |Dintpkt |Float |Destination\n",
    "interpacket arrival time (mSec) | |32 |tcprtt |Float |TCP connection\n",
    "setup round-trip time, the sum of ’synack’ and ’ackdat’. | |33 |synack\n",
    "|Float |TCP connection setup time, the time between the SYN and the\n",
    "SYN*ACK packets. | |34 |ackdat |Float |TCP connection setup time, the\n",
    "time between the SYN*ACK and the ACK packets. | |35 |is*sm*ips*ports\n",
    "|Binary |If source (1) and destination (3)IP addresses equal and port\n",
    "numbers (2)(4) equal then, this variable takes value 1 else 0 | |36\n",
    "|ct*state*ttl |Integer |No. for each state (6) according to specific\n",
    "range of values for source/destination time to live (10) (11). | |37\n",
    "|ct*flw*http*mthd|Integer |No. of flows that has methods such as Get and\n",
    "Post in http service. | |38 |is*ftp*login |Binary |If the ftp session is\n",
    "accessed by user and password then 1 else 0. | |39 |ct*ftp*cmd |integer\n",
    "|No of flows that has a command in ftp session. | |40 |ct*srv*src\n",
    "|integer |No. of connections that contain the same service (14) and\n",
    "source address (1) in 100 connections according to the last time (26). |\n",
    "|41 |ct*srv*dst |integer |No. of connections that contain the same\n",
    "service (14) and destination address (3) in 100 connections according to\n",
    "the last time (26). | |42 |ct*dst*ltm |integer |No. of connections of\n",
    "the same destination address (3) in 100 connections according to the\n",
    "last time (26). | |43 |ct*src\\_ ltm |integer |No. of connections of the\n",
    "same source address (1) in 100 connections according to the last time\n",
    "(26). | |44 |ct*src*dport*ltm|integer |No of connections of the same\n",
    "source address (1) and the destination port (4) in 100 connections\n",
    "according to the last time (26). | |45 |ct*dst*sport*ltm|integer |No of\n",
    "connections of the same destination address (3) and the source port (2)\n",
    "in 100 connections according to the last time (26). | |46\n",
    "|ct*dst*src*ltm |integer |No of connections of the same source (1) and\n",
    "the destination (3) address in in 100 connections according to the last\n",
    "time (26). | |47 |attack*cat |nominal |The name of each attack category.\n",
    "In this data set , nine categories e.g. Fuzzers, Analysis, Backdoors,\n",
    "DoS Exploits, Generic, Reconnaissance, Shellcode and Worms| |48 |Label\n",
    "|binary |0 for normal and 1 for attack records |\n",
    "\n",
    "The data of accessible from the [dataset\n",
    "source](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/).\n",
    "\n",
    "We used Spark csv for reading the dataset. In the following cell a spark\n",
    "dataframe from csv is created. In this dataset, the last label is the\n",
    "label, indicating whether an attack happened or not. The problem is a\n",
    "binary classification problem, which the machine learning algorithm has\n",
    "to predict the attack record label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv with pyspark\n",
    "# File location and type\n",
    "file_location = \"/FileStore/tables/IDdataset.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "### Removing unnecessary data\n",
    "\n",
    "In the dataaset, ip and port of source and destination are not useful,\n",
    "so we drop those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping ip and port of source and destination \n",
    "df= df.drop(\"_c0\",\"_c1\",\"_c2\", \"_c3\", \"_c47\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "Truncated to 30 rows\n",
    "\n",
    "Truncated to 12 cols\n",
    "\n",
    "  \n",
    "\n",
    "### Numerization\n",
    "\n",
    "Now we have to change categorical data (that are columns 4,5,13) to\n",
    "number, that is called ordinal encoding, and we can do it by\n",
    "StringIndexer.\n",
    "\n",
    "The next step is to convert all columns types to Double. It is\n",
    "neccesssary, as it seems pyspark returned a string dataframe from csv,\n",
    "and doesnot change data types for numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#handling categorical data\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "\n",
    "indexer = StringIndexer(inputCols=[\"_c4\", \"_c5\", \"_c13\"], outputCols=[\"c4\", \"c5\", \"c13\"])\n",
    "\n",
    "dff = indexer.fit(df).transform(df)\n",
    "\n",
    "dff = dff.drop(\"_c4\", \"_c5\", \"_c13\")\n",
    "\n",
    "\n",
    "#changing type to double\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "\n",
    "for col in dff.columns:\n",
    "  dff = dff.withColumn(col, dff[col].cast(DoubleType()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "### Handling null values\n",
    "\n",
    "#### Check which cells have null values\n",
    "\n",
    "One another step in preprocessing is to check if the data has null\n",
    "values. For this, we use .isNull() over rows of each column, and count\n",
    "null values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if it has missing data\n",
    "\n",
    "#showing number of null data  \n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "for cl in dff.columns:\n",
    "  dff.select([count(when(col(cl).isNull(),True))]).show()\n",
    "\n",
    "  \n",
    "#for c in df.columns:\n",
    "#  df.select(sum(df.select(c).na.fill(0))).show(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +--------------------------------------------+\n",
    ">     |count(CASE WHEN (_c6 IS NULL) THEN true END)|\n",
    ">     +--------------------------------------------+\n",
    ">     |                                           0|\n",
    ">     +--------------------------------------------+\n",
    ">\n",
    ">     +--------------------------------------------+\n",
    ">     |count(CASE WHEN (_c7 IS NULL) THEN true END)|\n",
    ">     +--------------------------------------------+\n",
    ">     |                                           0|\n",
    ">     +--------------------------------------------+\n",
    ">\n",
    ">     +--------------------------------------------+\n",
    ">     |count(CASE WHEN (_c8 IS NULL) THEN true END)|\n",
    ">     +--------------------------------------------+\n",
    ">     |                                           0|\n",
    ">     +--------------------------------------------+\n",
    ">\n",
    ">     +--------------------------------------------+\n",
    ">     |count(CASE WHEN (_c9 IS NULL) THEN true END)|\n",
    ">     +--------------------------------------------+\n",
    ">     |                                           0|\n",
    ">     +--------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c10 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c11 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c12 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c14 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c15 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c16 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c17 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c18 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c19 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c20 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c21 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c22 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c23 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c24 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c25 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c26 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c27 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c28 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c29 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c30 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c31 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c32 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c33 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c34 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c35 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c36 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c37 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                      1348145|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c38 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                      1429879|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c39 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                      1429879|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c40 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c41 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c42 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c43 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c44 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c45 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c46 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +---------------------------------------------+\n",
    ">     |count(CASE WHEN (_c48 IS NULL) THEN true END)|\n",
    ">     +---------------------------------------------+\n",
    ">     |                                            0|\n",
    ">     +---------------------------------------------+\n",
    ">\n",
    ">     +-------------------------------------------+\n",
    ">     |count(CASE WHEN (c4 IS NULL) THEN true END)|\n",
    ">     +-------------------------------------------+\n",
    ">     |                                          0|\n",
    ">     +-------------------------------------------+\n",
    ">\n",
    ">     +-------------------------------------------+\n",
    ">     |count(CASE WHEN (c5 IS NULL) THEN true END)|\n",
    ">     +-------------------------------------------+\n",
    ">     |                                          0|\n",
    ">     +-------------------------------------------+\n",
    ">\n",
    ">     +--------------------------------------------+\n",
    ">     |count(CASE WHEN (c13 IS NULL) THEN true END)|\n",
    ">     +--------------------------------------------+\n",
    ">     |                                           0|\n",
    ">     +--------------------------------------------+\n",
    "\n",
    "  \n",
    "\n",
    "#### Filling null values\n",
    "\n",
    "We noticed that column 37(ct*flw*http*mthd) has 1348145, column\n",
    "38(is*ftp*login) has 1429879, and column 39(ct*ftp\\_cmd) has 1429879\n",
    "null values. So we fill them by using Imputer function of pyspark. This\n",
    "function fill the missing values with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling null data\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "dff= Imputer(inputCols= dff.columns, outputCols=dff.columns).fit(dff).transform(dff)\n",
    "\n",
    "#for cl in dff.columns:\n",
    "#  dff.select([count(when(col(cl).isNull(),True))]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "### Creating dataset\n",
    "\n",
    "For creating dataset, we take the following steps:\n",
    "\n",
    "#### Vectorizing\n",
    "\n",
    "We firstly have to put all features into one big vector, using\n",
    "VectorAssembler. We take all the columns of data, except the first 4\n",
    "columns(are irrelevant) an the last two (are the labels) into \"features\"\n",
    "column. Notice that VectorAssembler generates either Sparse, or Dense\n",
    "vectors, in favour of the memory.\n",
    "\n",
    "#### Normalization\n",
    "\n",
    "Next, we normalize data that is vectorized in one column. For this\n",
    "dataset, VEctorAssembler returned a sparse vector, and we chose a\n",
    "normalizer that is compatible with sparse vectors. So we used\n",
    "ml.feature.Normalizer for normalizing data.\n",
    "\n",
    "#### Sparse to Dense\n",
    "\n",
    "After normalization, we convert the sparse vectors of features to dense\n",
    "vectors. For this we defined a UDF function.\n",
    "\n",
    "#### Selecting columns of features and labels\n",
    "\n",
    "Finally, we select two columns in the dataframe to use in further steps,\n",
    "that are \"labels\", and \"features\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector,VectorUDT, Vector\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "\n",
    "numCols = ['c4', 'c5', '_c6', '_c7', '_c8', '_c9', '_c10', '_c11', '_c12', 'c13', '_c14', '_c15', '_c16', '_c17', '_c18', '_c19', '_c20', '_c21', '_c22', '_c23', '_c24', '_c25', '_c26', '_c27', '_c28', '_c29', '_c30', '_c31', '_c32', '_c33', '_c34', '_c35', '_c36', '_c37', '_c38', '_c39', '_c40', '_c41', '_c42', '_c43', '_c44', '_c45', '_c46']\n",
    "\n",
    "\n",
    "dfff = VectorAssembler(inputCols=numCols, outputCol=\"features\").transform(dff)\n",
    "#VA.select(\"numFeatures\").show(1000)\n",
    "\n",
    "nrm = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1).transform(dfff)\n",
    "\n",
    "\n",
    "\n",
    "#df4 = StandardScaler(inputCol=\"features\", outputCol = \"scaledFeatures\", withStd=False,withMean=False).fit(dfff).transform(dfff)\n",
    "\n",
    "\n",
    "def sparse_to_array(v):\n",
    "  v = DenseVector(v)\n",
    "  new_array = list([float(x) for x in v])\n",
    "  return new_array\n",
    "\n",
    "\n",
    "sparse_to_array_udf = F.udf(sparse_to_array, T.ArrayType(T.FloatType()))\n",
    "featArr = nrm.withColumn('featuresArray', sparse_to_array_udf('features_norm'))\n",
    "\n",
    "\n",
    "featArr=featArr.withColumnRenamed(\"_c48\", \"labels\")\n",
    "\n",
    "#display(featArr)\n",
    "\n",
    "trainSet = featArr.select('labels', \"featuresArray\")\n",
    "\n",
    "display(trainSet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_assemble = []\n",
    "for f in range(2,45):\n",
    "  features_to_assemble.append('_'+str(f))\n",
    "print(features_to_assemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     ['_2', '_3', '_4', '_5', '_6', '_7', '_8', '_9', '_10', '_11', '_12', '_13', '_14', '_15', '_16', '_17', '_18', '_19', '_20', '_21', '_22', '_23', '_24', '_25', '_26', '_27', '_28', '_29', '_30', '_31', '_32', '_33', '_34', '_35', '_36', '_37', '_38', '_39', '_40', '_41', '_42', '_43', '_44']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest implementation\n",
    "def functionForRdd(r):\n",
    "  l = []\n",
    "  l.append(r[0])\n",
    "  l = l+list(r[1])\n",
    "  return l\n",
    "\n",
    "trainSetRdd = trainSet.rdd.map(functionForRdd)\n",
    "randomForestDf = trainSetRdd.toDF()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[9]: 2540047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=features_to_assemble,\n",
    "    outputCol=\"features\")\n",
    "\n",
    "randomForestDf = assembler.transform(randomForestDf)\n",
    "train, test = randomForestDf.randomSplit([0.7, 0.3], seed = 2018)\n",
    "newtrainSet = train.sample(fraction=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[11]: 1777296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = '_1')\n",
    "rfModel = rf.fit(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "prediction = rfModel.transform(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionAndLabels = prediction.rdd.map(lambda r: (r.prediction, r._1))\n",
    "\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "metrics2 = MulticlassMetrics(predictionAndLabels)\n",
    "print(\"Precions = %s\" % metrics2.precision(1.0))\n",
    "print(\"Recall = %s\" % metrics2.recall(1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Area under ROC = 0.9893749378235688\n",
    ">     Precions = 0.921533690334014\n",
    ">     Recall = 0.9909789543458447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "metrics2 = MulticlassMetrics(predictionAndLabels)\n",
    "print(\"Precions = %s\" % metrics2.precision(1.0))\n",
    "print(\"Recall = %s\" % metrics2.recall(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy = %s\" % metrics2.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "#### Logistic Regression classifier\n",
    "\n",
    "We have trained and tested the Logistic Regression classifer on our\n",
    "training and testing set respectively as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Classification\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "logr = LogisticRegression(featuresCol = 'features', labelCol = '_1')\n",
    "logrmodel = logr.fit(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "lr_prediction = logrmodel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "#### Logistic Regression classifier evaluation\n",
    "\n",
    "We have evaluated the Logistic Regression classifier using binary and\n",
    "also multi-class evaluation metrics as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Evaluation\n",
    "lr_predictionAndLabels = lr_prediction.rdd.map(lambda r: (r.prediction, r._1))\n",
    "\n",
    "\n",
    "metrics = BinaryClassificationMetrics(lr_predictionAndLabels)\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "metrics2 = MulticlassMetrics(lr_predictionAndLabels)\n",
    "print(\"Precions = %s\" % metrics2.precision(1.0))\n",
    "print(\"Recall = %s\" % metrics2.recall(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Area under ROC = 0.9814394501866226\n",
    ">     Precions = 0.9305169153391484\n",
    ">     Recall = 0.9734132902477421\n",
    "\n",
    "  \n",
    "\n",
    "#### Gradient-Boosted Trees (GBTs) classifier\n",
    "\n",
    "We have trained and tested the GBTs classifer on our training and\n",
    "testing set respectively as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(featuresCol = 'features', labelCol = '_1', maxDepth=5)\n",
    "gbtmodel = gbt.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "gbt_prediction = gbtmodel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "#### Gradient-Boosted Trees (GBTs) classifier evaluation\n",
    "\n",
    "We have evaluated the GBTs classifier using binary and also multi-class\n",
    "evaluation metrics as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_predictionAndLabels = gbt_prediction.rdd.map(lambda r: (r.prediction, r._1))\n",
    "\n",
    "metrics = BinaryClassificationMetrics(gbt_predictionAndLabels)\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "metrics2 = MulticlassMetrics(gbt_predictionAndLabels)\n",
    "print(\"Precions = %s\" % metrics2.precision(1.0))\n",
    "print(\"Recall = %s\" % metrics2.recall(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Area under ROC = 0.9801886786459773\n",
    ">     Precions = 0.962630715074687\n",
    ">     Recall = 0.9658111691109454"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install tensorflow\n",
    "# !{sys.executable} -m pip uninstall keras\n",
    "# !{sys.executable} -m pip install keras\n",
    "# !{sys.executable} -m pip install dist-keras\n",
    "# !{sys.executable} -m pip install elephas\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.optimizers import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "from distkeras.trainers import *\n",
    "from distkeras.predictors import *\n",
    "from distkeras.transformers import *\n",
    "from distkeras.evaluators import *\n",
    "from distkeras.utils import *\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from elephas.utils.rdd_utils import to_simple_rdd\n",
    "from elephas.spark_model import SparkModel\n",
    "\n",
    "# trainSett = trainSet.rdd.map(lambda row: Row(\n",
    "#     labels=row[\"labels\"], \n",
    "#     featuresArray=Vectors.dense(row[\"featuresArray\"])\n",
    "# )).toDF()\n",
    "\n",
    "# inpDim= len(trainSett.select(\"featuresArray\").first()[0])\n",
    "\n",
    "inpDim= len(train.select(\"features\").first()[0])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim = inpDim,activation='relu',use_bias=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation='sigmoid',use_bias=True)) #The number of neurons is equal to the number of classes\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"compile done\")\n",
    "# model.build()\n",
    "print(model.summary())\n",
    "trainer = SingleTrainer(keras_model=model, worker_optimizer='adam', loss='binary_crossentropy', num_epoch=1)\n",
    "print(\"trainer done\")  \n",
    "trained_model = trainer.train(train)#newtrainSet\n",
    "print(\"training done\")\n",
    "predictor = ModelPredictor(keras_model=trained_model)\n",
    "ff=predictor.predict(test.take(50)[15].features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     compile done\n",
    ">     Model: \"sequential_2\"\n",
    ">     _________________________________________________________________\n",
    ">     Layer (type)                 Output Shape              Param #   \n",
    ">     =================================================================\n",
    ">     dense_3 (Dense)              (None, 128)               5632      \n",
    ">     _________________________________________________________________\n",
    ">     dropout_2 (Dropout)          (None, 128)               0         \n",
    ">     _________________________________________________________________\n",
    ">     dense_4 (Dense)              (None, 1)                 129       \n",
    ">     =================================================================\n",
    ">     Total params: 5,761\n",
    ">     Trainable params: 5,761\n",
    ">     Non-trainable params: 0\n",
    ">     _________________________________________________________________\n",
    ">     None\n",
    ">     trainer done\n",
    "\n",
    "  \n",
    "\n",
    "### Multilayer Perceptron Classifier\n",
    "\n",
    "MLPC consists of multiple layers of nodes. Each layer is fully connected\n",
    "to the next layer in the network. Following code snippet is the\n",
    "implementation of such a model in pyspark. The input layer and the\n",
    "output layer have the size of 43 and 2, with two hidden layers of 5 and\n",
    "4 neurons respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Perceptron Classifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 43 (features), two intermediate of size 5 and 4\n",
    "# and output of size 2 (classes)\n",
    "layers = [43, 5, 4, 2]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "mlpc = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234, featuresCol = 'features', labelCol = '_1')\n",
    "\n",
    "# train the model\n",
    "mlpcModel = mlpc.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "Finally, a trained MLPC model is returned that is ready to evaluate on\n",
    "the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy on the test set\n",
    "testLabeled = test.withColumnRenamed( '_1', \"label\")\n",
    "result = mlpcModel.transform(testLabeled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "The model is tested using MulticlassClassificationEvaluator with\n",
    "accuracy as an evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Test set accuracy = 0.9290567957301924\n",
    "\n",
    "  \n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "The spark.ml implementation supports decision trees for binary and\n",
    "multiclass classification and for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = '_1')\n",
    "dtModel = dt.fit(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "The model is tested using BinaryClassificationMetrics and\n",
    "MulticlassMetrics. For BinaryClassificationMetrics the area under ROC is\n",
    "0.9738, for MulticlassMetrics precision is 0.9626 and recall is 0.9531."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Metrics\n",
    "prediction = dtModel.transform(test)\n",
    "predictionAndLabels = prediction.rdd.map(lambda r: (r.prediction, r._1))\n",
    "\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "metrics2 = MulticlassMetrics(predictionAndLabels)\n",
    "print(\"Precisions = %s\" % metrics2.precision(1.0))\n",
    "print(\"Recall = %s\" % metrics2.recall(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Area under ROC = 0.9738847246751252\n",
    ">     Precions = 0.9626855522893936\n",
    ">     Recall = 0.9531237053608418"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
