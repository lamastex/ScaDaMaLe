{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SDS-2.x, Scalable Data Engineering Science](https://lamastex.github.io/scalable-data-science/sds/2/x/)\n",
    "=======================================================================================================\n",
    "\n",
    "This is a 2019 augmentation and update of [Adam\n",
    "Breindel](https://www.linkedin.com/in/adbreind)'s initial notebooks.\n",
    "\n",
    "#### We can also implement the model with mini-batches -- this will let us see matrix ops in action:\n",
    "\n",
    "(N.b., feed*dict is intended for small data / experimentation. For more\n",
    "info on ingesting data at scale, see\n",
    "https://www.tensorflow.org/api*guides/python/reading\\_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know these params, but we're making TF learn them\n",
    "\n",
    "REAL_SLOPE_X1 = 2 # slope along axis 1 (x-axis)\n",
    "REAL_SLOPE_X2 = 3 # slope along axis 2 (y-axis)\n",
    "REAL_INTERCEPT = 5 # intercept along axis 3 (z-axis), think of (x,y,z) axes in the usual way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# GENERATE a batch of true data, with a little Gaussian noise added\n",
    "\n",
    "def make_mini_batch(size=10):\n",
    "  X = np.random.rand(size, 2) # \n",
    "  Y = np.matmul(X, [REAL_SLOPE_X1, REAL_SLOPE_X2]) + REAL_INTERCEPT + 0.2 * np.random.randn(size) \n",
    "  return X.reshape(size,2), Y.reshape(size,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "  \n",
    "\n",
    "To digest what's going on inside the function above, let's take it step\n",
    "by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Xex = np.random.rand(10, 2) # Xex is simulating PRNGs from independent Uniform [0,1] RVs\n",
    " Xex # visualize these as 10 orddered pairs of points in the x-y plane that makes up our x-axis and y-axis (or x1 and x2 axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[10]: \n",
    ">     array([[0.21757443, 0.01815727],\n",
    ">            [0.55624387, 0.51717902],\n",
    ">            [0.50736386, 0.81707665],\n",
    ">            [0.85695071, 0.08247471],\n",
    ">            [0.45759568, 0.75775923],\n",
    ">            [0.47513327, 0.35025263],\n",
    ">            [0.80005626, 0.33802171],\n",
    ">            [0.21462401, 0.14219813],\n",
    ">            [0.11143285, 0.89456028],\n",
    ">            [0.89588183, 0.95661233]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yex = np.matmul(Xex, [REAL_SLOPE_X1, REAL_SLOPE_X2]) #+ REAL_INTERCEPT #+ 0.2 * np.random.randn(10) \n",
    "Yex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[11]: \n",
    ">     array([0.48962067, 2.66402479, 3.46595769, 1.96132554, 3.18846906,\n",
    ">            2.00102441, 2.61417766, 0.85584243, 2.90654655, 4.66160066])\n",
    "\n",
    "  \n",
    "\n",
    "The first entry in Yex is obtained as follows (change the numbers in the\n",
    "produc below if you reevaluated the cells above) and geometrically it is\n",
    "the location in z-axis of the plane with slopes given by REAL*SLOPE*X1\n",
    "in the x-axis and REAL*SLOPE*X2 in the y-aixs with intercept 0 at the\n",
    "point in the x-y or x1-x2 plane given by (0.68729439, 0.58462379)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.21757443*REAL_SLOPE_X1 +  0.01815727*REAL_SLOPE_X2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[12]: 0.48962067000000004\n",
    "\n",
    "  \n",
    "\n",
    "The next steps are adding an intercept term to translate the plane in\n",
    "the z-axis and then a scaled (the multiplication by 0.2 here) gaussian\n",
    "noise from independetly drawn pseudo-random samples from the standard\n",
    "normal or Normal(0,1) random variable via `np.random.randn(size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yex = np.matmul(Xex, [REAL_SLOPE_X1, REAL_SLOPE_X2]) + REAL_INTERCEPT # + 0.2 * np.random.randn(10) \n",
    "Yex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[13]: \n",
    ">     array([5.48962067, 7.66402479, 8.46595769, 6.96132554, 8.18846906,\n",
    ">            7.00102441, 7.61417766, 5.85584243, 7.90654655, 9.66160066])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yex = np.matmul(Xex, [REAL_SLOPE_X1, REAL_SLOPE_X2])  + REAL_INTERCEPT + 0.2 * np.random.randn(10) \n",
    "Yex # note how each entry in Yex is jiggled independently a bit by 0.2 * np.random.randn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[14]: \n",
    ">     array([5.51604324, 7.77768412, 8.44899552, 7.20338209, 8.40246384,\n",
    ">            7.07592727, 7.26620664, 5.85385249, 8.18290919, 9.69088502])\n",
    "\n",
    "  \n",
    "\n",
    "Thus we can now fully appreciate what is going on in `make_mini_batch`.\n",
    "This is meant to substitute for pulling random sub-samples of batches of\n",
    "the real data during stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_mini_batch() # our mini-batch of Xx and Ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[15]: \n",
    ">     (array([[0.53055592, 0.62512968],\n",
    ">             [0.71641671, 0.16258358],\n",
    ">             [0.98501818, 0.3434015 ],\n",
    ">             [0.00145872, 0.42206563],\n",
    ">             [0.13963167, 0.49958068],\n",
    ">             [0.58965079, 0.69253778],\n",
    ">             [0.1125337 , 0.7821038 ],\n",
    ">             [0.53812365, 0.72680835],\n",
    ">             [0.0228825 , 0.48261083],\n",
    ">             [0.83198857, 0.64067338]]), array([[7.72924566],\n",
    ">             [6.83058688],\n",
    ">             [8.17826698],\n",
    ">             [6.21726091],\n",
    ">             [6.66331251],\n",
    ">             [8.48564693],\n",
    ">             [7.59010849],\n",
    ">             [7.87368412],\n",
    ">             [6.70226285],\n",
    ">             [8.55898236]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "batch = 10 # size of batch\n",
    "\n",
    "tf.reset_default_graph() # this is important to do before you do something new in TF\n",
    "\n",
    "# we will work with single floating point precision and this is specified in the tf.float32 type argument to each tf object/method\n",
    "x = tf.placeholder(tf.float32, shape=(batch, 2)) # placeholder node for the pairs of x variables (predictors) in batches of size batch\n",
    "x_aug = tf.concat( (x, tf.ones((batch, 1))), 1 ) # x_aug is a concatenation of a vector of 1`s along the first dimension\n",
    "\n",
    "y = tf.placeholder(tf.float32, shape=(batch, 1)) # placeholder node for the univariate response y with batch many rows and 1 column\n",
    "model_params = tf.get_variable(\"model_params\", [3,1]) # these are the x1 slope, x2 slope and the intercept (3 rows and 1 column)\n",
    "y_model = tf.matmul(x_aug, model_params) # our two-factor regression model is defined by this matrix multiplication\n",
    "# note that the noise is formally part of the model and what we are actually modeling is the mean response...\n",
    "\n",
    "error = tf.reduce_sum(tf.square(y - y_model))/batch # this is mean square error where the sum is computed by a reduce call on addition\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(0.02).minimize(error) # learning rate is set to 0.02\n",
    "\n",
    "init = tf.global_variables_initializer() # our way into running the TF session\n",
    "\n",
    "errors = [] # list to track errors over iterations\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)    \n",
    "    for i in range(1000):\n",
    "      x_data, y_data = make_mini_batch(batch) # simulate the mini-batch of data x1,x2 and response y with noise\n",
    "      _, error_val = session.run([train_op, error], feed_dict={x: x_data, y: y_data})\n",
    "      errors.append(error_val)\n",
    "\n",
    "    out = session.run(model_params)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     [[1.9413265]\n",
    ">      [2.8955398]\n",
    ">      [5.085459 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_SLOPE_X1, REAL_SLOPE_X2, REAL_INTERCEPT # compare with rue parameter values - it's not too far from the estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     Out[17]: (2, 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((4,3))\n",
    "plt.plot(errors)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
