{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SDS-2.x, Scalable Data Engineering Science](https://lamastex.github.io/scalable-data-science/sds/2/x/)\n",
    "=======================================================================================================\n",
    "\n",
    "This is a 2019 augmentation and update of [Adam\n",
    "Breindel](https://www.linkedin.com/in/adbreind)'s initial notebooks.\n",
    "\n",
    "Operations in the Real World\n",
    "============================\n",
    "\n",
    "Practical Options, Tools, Patterns, and Considerations for Deep Learning\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "There are various ways to use deep learning in an enterprise setting\n",
    "that may not require designing your own networks!\n",
    "\n",
    "### Ways to Use Deep Learning\n",
    "\n",
    "(in order from least complex/expensive investment to most)\n",
    "\n",
    "\\[1\\] Load and use a pretrained model\n",
    "\n",
    "Many of the existing toolkit projects offer models pretrained on\n",
    "datasets, including\n",
    "\n",
    "-   natural language corpus models\n",
    "-   image datasets like ImageNet (http://www.image-net.org/) or Google's\n",
    "    Open Image Dataset\n",
    "    (https://research.googleblog.com/2016/09/introducing-open-images-dataset.html)\n",
    "-   video datasets like the YouTube 8 million video dataset\n",
    "    (https://research.googleblog.com/2016/09/announcing-youtube-8m-large-and-diverse.html)\n",
    "\n",
    "\\[2\\] Augmenting a pretrained model with new training data, or using it\n",
    "in a related context (see Transfer Learning)\n",
    "\n",
    "\\[3\\] Use a known, established network type (topology) but train on your\n",
    "own data\n",
    "\n",
    "\\[4\\] Modify established network models for your specific problem\n",
    "\n",
    "\\[5\\] Research and experiment with new types of models\n",
    "\n",
    "**Just because Google DeepMind, Facebook, and Microsoft are getting\n",
    "press for doing a lot of new research doesn't mean you have to do it\n",
    "too.**\n",
    "\n",
    "&lt;img src=\"http://i.imgur.com/XczCfNR.png\" width=500&gt; &lt;img\n",
    "src=\"http://i.imgur.com/vcaj99I.jpg\" width=500&gt;\n",
    "\n",
    "Data science and machine learning is challenging in general for\n",
    "enterprises (though some industries, such as pharma, have been doing it\n",
    "for a long time). Deep learning takes that even further, since deep\n",
    "learning experiments may require new kinds of hardware ... in some ways,\n",
    "it's more like chemistry than the average IT project!\n",
    "\n",
    "### Tools and Processes for your Deep Learning Pipeline\n",
    "\n",
    "#### Data Munging\n",
    "\n",
    "Most of the deep learning toolkits are focused on model-building\n",
    "performance or flexibility, less on production data processing.\n",
    "\n",
    "However, Google recently introduced `tf.Transform`, a data processing\n",
    "pipeline project: https://github.com/tensorflow/transform\n",
    "\n",
    "and Dataset, an API for data processing:\n",
    "https://www.tensorflow.org/api\\_docs/python/tf/contrib/data/Dataset\n",
    "\n",
    "TensorFlow can read from HDFS and run on Hadoop although it *does not*\n",
    "scale out automatically on a Hadoop/Spark cluster:\n",
    "https://www.tensorflow.org/deploy/hadoop\n",
    "\n",
    "Falling back to \"regular\" tools, we have Apache Spark for big data, and\n",
    "the Python family of pandas, sklearn, scipy, numpy.\n",
    "\n",
    "#### Experimenting and Training\n",
    "\n",
    "Once you want to scale beyond your laptop, there are few options...\n",
    "\n",
    "-   AWS GPU-enabled instances\n",
    "-   Deep-learning-infrastructure as a Service\n",
    "    -   EASY\n",
    "        -   \"Floyd aims to be the Heroku of Deep Learning\"\n",
    "            https://www.floydhub.com/\n",
    "        -   \"Effortless infrastructure for deep learning\"\n",
    "            https://www.crestle.com/\n",
    "        -   \"GitHub of Machine Learning / We provide machine learning\n",
    "            platform-as-a-service.\" https://valohai.com/  \n",
    "        -   \"Machine Learning for Everyone\" (may be in closed beta)\n",
    "            https://machinelabs.ai\n",
    "        -   Algorithms as a service / model deployment\n",
    "            https://algorithmia.com/  \n",
    "    -   MEDIUM Google Cloud Platform \"Cloud Machine Learning Engine\"\n",
    "        https://cloud.google.com/ml-engine/\n",
    "    -   HARDER Amazon Deep Learning AMI + CloudFormation\n",
    "        https://aws.amazon.com/blogs/compute/distributed-deep-learning-made-easy/\n",
    "-   On your own infrastructure or VMs\n",
    "    -   Distributed TensorFlow is free, OSS\n",
    "    -   Apache Spark combined with Intel BigDL (CPU) or DeepLearning4J\n",
    "        (GPU)\n",
    "    -   TensorFlowOnSpark\n",
    "    -   CERN Dist Keras (Spark + Keras)\n",
    "        https://github.com/cerndb/dist-keras\n",
    "\n",
    "#### Frameworks\n",
    "\n",
    "We've focused on TensorFlow and Keras, because that's where the \"center\n",
    "of mass\" is at the moment.\n",
    "\n",
    "But there are lots of others. Major ones include:\n",
    "\n",
    "-   Caffe\n",
    "-   PaddlePaddle\n",
    "-   Theano\n",
    "-   CNTK\n",
    "-   MXNet\n",
    "-   DeepLearning4J\n",
    "-   BigDL\n",
    "-   Torch/PyTorch\n",
    "-   NVIDIA Digits\n",
    "\n",
    "and there are at least a dozen more minor ones.\n",
    "\n",
    "#### Taking Your Trained Model to Production\n",
    "\n",
    "Most trained models can predict in production in near-zero time. (Recall\n",
    "the forward pass is just a bunch of multiplication and addition with a\n",
    "few other calculations thrown in.)\n",
    "\n",
    "For a neat example, you can persist Keras models and load them to run\n",
    "live in a browser with Keras.js\n",
    "\n",
    "See Keras.js for code and demos:\n",
    "https://github.com/transcranial/keras-js\n",
    "\n",
    "&lt;img src=\"http://i.imgur.com/5xx62zw.png\" width=700&gt;\n",
    "\n",
    "TensorFlow has an Android example at\n",
    "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android\n",
    "\n",
    "and Apple CoreML supports Keras models:\n",
    "https://developer.apple.com/documentation/coreml/converting*trained*models*to*core\\_ml\n",
    "\n",
    "(remember, the model is already trained, we're just predicting here)\n",
    "\n",
    "#### And for your server-side model update-and-serve tasks, or bulk prediction at scale...\n",
    "\n",
    "(imagine classifying huge batches of images, or analyzing millions of\n",
    "chat messages or emails)\n",
    "\n",
    "-   TensorFlow has a project called TensorFlow Serving:\n",
    "    https://tensorflow.github.io/serving/\n",
    "\n",
    "-   Spark Deep Learning Pipelines (bulk/SQL inference)\n",
    "    https://github.com/databricks/spark-deep-learning\n",
    "\n",
    "-   Apache Spark + (DL4J | BigDL | TensorFlowOnSpark)\n",
    "\n",
    "-   DeepLearning4J can import your Keras model:\n",
    "    https://deeplearning4j.org/model-import-keras\n",
    "\n",
    "    -   (which is a really nice contribution, but not magic -- remember\n",
    "        the model is just a pile of weights, convolution kernels, etc.\n",
    "        ... in the worst case, many thousands of floats)\n",
    "\n",
    "-   http://pipeline.io/ by Netflix and Databricks alum Chris Fregly\n",
    "\n",
    "-   MLeap http://mleap-docs.combust.ml/\n",
    "\n",
    "### Security and Robustness\n",
    "\n",
    "A recent (3/2017) paper on general key failure modes is **Failures of\n",
    "Deep Learning**: https://arxiv.org/abs/1703.07950\n",
    "\n",
    "Deep learning models are subject to a variety of unexpected\n",
    "perturbations and adversarial data -- even when they seem to\n",
    "\"understand,\" they definitely don't understand in a way that is similar\n",
    "to us.\n",
    "\n",
    "&lt;img src=\"http://i.imgur.com/3LjF9xl.png\"&gt;\n",
    "\n",
    "Ian Goodfellow has distilled and referenced some of the research here:\n",
    "https://openai.com/blog/adversarial-example-research/\n",
    "\n",
    "-   He is also maintainer of an open-source project to measure\n",
    "    robustness to adversarial examples, Clever Hans:\n",
    "    https://github.com/tensorflow/cleverhans\n",
    "-   Another good project in that space is Foolbox:\n",
    "    https://github.com/bethgelab/foolbox\n",
    "\n",
    "##### It's all fun and games until a few tiny stickers that a human won't even notice ... turn a stop sign into a \"go\" sign for your self-driving car ... **and that's exactly what this team of researchers has done** in *Robust Physical-World Attacks on Machine Learning Models*: https://arxiv.org/pdf/1707.08945v1.pdf\n",
    "\n",
    "Final Notes\n",
    "===========\n",
    "\n",
    "The research and projects are coming so fast that this will probably be\n",
    "outdated by the time you see it ...\n",
    "\n",
    "#### 2017 is the last ILSVRC! http://image-net.org/challenges/beyond\\_ilsvrc.php\n",
    "\n",
    "Try visualizing principal components of high-dimensional data with\n",
    "**TensorFlow Embedding Projector** http://projector.tensorflow.org/\n",
    "\n",
    "Or explore with Google / PAIR's Facets tool:\n",
    "https://pair-code.github.io/facets/\n",
    "\n",
    "Visualize the behavior of Keras models with keras-vis:\n",
    "https://raghakot.github.io/keras-vis/\n",
    "\n",
    "Want more out of Keras without coding it yourself? See if your needs are\n",
    "covered in the extension repo for keras, keras-contrib:\n",
    "https://github.com/farizrahman4u/keras-contrib\n",
    "\n",
    "Interested in a slightly different approach to APIs, featuring\n",
    "interactive (imperative) execution? In the past year, a lot of people\n",
    "have started using PyTorch: http://pytorch.org/\n",
    "\n",
    "**XLA**, an experimental compiler to make TensorFlow even faster:\n",
    "https://www.tensorflow.org/versions/master/experimental/xla/\n",
    "\n",
    "...and in addition to refinements of what we've already talked about,\n",
    "there is bleeding-edge work in\n",
    "\n",
    "-   Neural Turing Machines\n",
    "-   Code-generating Networks\n",
    "-   Network-designing Networks\n",
    "-   Evolution Strategies (ES) as an alternative to DQL / PG:\n",
    "    https://arxiv.org/abs/1703.03864\n",
    "\n",
    "Books\n",
    "=====\n",
    "\n",
    "To fill in gaps, refresh your memory, gain deeper intuition and\n",
    "understanding, and explore theoretical underpinnings of deep learning...\n",
    "\n",
    "#### Easier intro books (less math)\n",
    "\n",
    "*Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts,\n",
    "Tools, and Techniques to Build Intelligent Systems* by Aurélien Géron\n",
    "\n",
    "*Deep Learning with Python* by Francois Chollet\n",
    "\n",
    "*Fundamentals of Machine Learning for Predictive Data Analytics:\n",
    "Algorithms, Worked Examples, and Case Studies* by John D. Kelleher,\n",
    "Brian Mac Namee, Aoife D'Arcy\n",
    "\n",
    "#### More thorough books (more math)\n",
    "\n",
    "*Deep Learning* by Ian Goodfellow, Yoshua Bengio, Aaron Courville\n",
    "\n",
    "*Information Theory, Inference and Learning Algorithms 1st Edition* by\n",
    "David J. C. MacKay"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
