{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signed Triads in Social Media\n",
    "=============================\n",
    "\n",
    "By **Guangyi Zhang** (guaz@kth.se)\n",
    "\n",
    "Please click\n",
    "[HERE](https://drive.google.com/file/d/1TrxhdSxsU1qKk_SywKf2nUA8mUAO4CG4/view?usp=sharing)\n",
    "to watch the accompanying video.\n",
    "\n",
    "Introduction\n",
    "------------\n",
    "\n",
    "This project aims to verify the friend-foe motifs in a large-scale\n",
    "signed social network.\n",
    "\n",
    "A signed network is a graph that contains both positive and negative\n",
    "links. The sign of a link contains rich semantics in different\n",
    "appliations. For example, in a social network, positive links can\n",
    "indicate friendly relationships, while negative ones indicate\n",
    "antagonistic interactions.\n",
    "\n",
    "In on-line discussion sites such as Slashdot, users can tag other users\n",
    "as “friends” and “foes”. These provide us exemplary datasets to study a\n",
    "online signed network. In this notebook we explore the a dataset from\n",
    "Epinions, which contains up to 119,217 nodes, 841,200 edges, and\n",
    "millions of motifs. Epinions is the trust network of the Epinions\n",
    "product review web site, where users can indicate their trust or\n",
    "distrust of the reviews of others. We analyze the network data in an\n",
    "undirected representation.\n",
    "\n",
    "References:\n",
    "\n",
    "Leskovec, Jure, Daniel Huttenlocher, and Jon Kleinberg. \"Signed networks\n",
    "in social media.\" Proceedings of the SIGCHI conference on human factors\n",
    "in computing systems. 2010.\n",
    "\n",
    "Regarding the motifs, we investigate several interesting triads that are\n",
    "related to *structural balance theory* in an online social signed\n",
    "network. Structural balance originates in social psychology in the\n",
    "mid-20th-century, and considers the possible ways in which triangles on\n",
    "three individuals can be signed.\n",
    "\n",
    "Let us explain different types of triads, which is shown in the figure\n",
    "below,\n",
    "\n",
    "-   T3: “the friend of my friend is my friend”\n",
    "-   T1: “the friend of my enemy is my enemy,” “the enemy of my friend is\n",
    "    my enemy” and “the enemy of my enemy is my friend”\n",
    "-   T2 and T0: does not quite make sense in social network. For example,\n",
    "    two friends of mine are unlikely to be enemy to each other.\n",
    "\n",
    "Our goal is to compare the numbers of different triads in our appointed\n",
    "dataset.\n",
    "\n",
    "![triads](https://drive.google.com/uc?export=view&id=1QY9ouqxbVqpH3KLl-x-QyR72yxtE0vSX)\n",
    "\n",
    "Download dataset\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     /databricks/driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget http://snap.stanford.edu/data/soc-sign-epinions.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     --2020-11-24 16:15:45--  http://snap.stanford.edu/data/soc-sign-epinions.txt.gz\n",
    ">     Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
    ">     Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
    ">     HTTP request sent, awaiting response... 200 OK\n",
    ">     Length: 2972840 (2.8M) [application/x-gzip]\n",
    ">     Saving to: ‘soc-sign-epinions.txt.gz’\n",
    ">\n",
    ">          0K .......... .......... .......... .......... ..........  1% 1.13M 2s\n",
    ">         50K .......... .......... .......... .......... ..........  3% 1.12M 2s\n",
    ">        100K .......... .......... .......... .......... ..........  5% 2.22M 2s\n",
    ">        150K .......... .......... .......... .......... ..........  6% 2.23M 2s\n",
    ">        200K .......... .......... .......... .......... ..........  8%  131M 1s\n",
    ">        250K .......... .......... .......... .......... .......... 10% 2.18M 1s\n",
    ">        300K .......... .......... .......... .......... .......... 12% 2.25M 1s\n",
    ">        350K .......... .......... .......... .......... .......... 13%  101M 1s\n",
    ">        400K .......... .......... .......... .......... .......... 15% 2.27M 1s\n",
    ">        450K .......... .......... .......... .......... .......... 17%  110M 1s\n",
    ">        500K .......... .......... .......... .......... .......... 18%  118M 1s\n",
    ">        550K .......... .......... .......... .......... .......... 20% 2.29M 1s\n",
    ">        600K .......... .......... .......... .......... .......... 22%  119M 1s\n",
    ">        650K .......... .......... .......... .......... .......... 24% 2.29M 1s\n",
    ">        700K .......... .......... .......... .......... .......... 25%  109M 1s\n",
    ">        750K .......... .......... .......... .......... .......... 27% 93.2M 1s\n",
    ">        800K .......... .......... .......... .......... .......... 29% 89.2M 1s\n",
    ">        850K .......... .......... .......... .......... .......... 31% 2.38M 1s\n",
    ">        900K .......... .......... .......... .......... .......... 32%  138M 1s\n",
    ">        950K .......... .......... .......... .......... .......... 34%  105M 1s\n",
    ">       1000K .......... .......... .......... .......... .......... 36%  107M 0s\n",
    ">       1050K .......... .......... .......... .......... .......... 37%  121M 0s\n",
    ">       1100K .......... .......... .......... .......... .......... 39% 2.38M 0s\n",
    ">       1150K .......... .......... .......... .......... .......... 41%  135M 0s\n",
    ">       1200K .......... .......... .......... .......... .......... 43%  125M 0s\n",
    ">       1250K .......... .......... .......... .......... .......... 44%  113M 0s\n",
    ">       1300K .......... .......... .......... .......... .......... 46%  101M 0s\n",
    ">       1350K .......... .......... .......... .......... .......... 48% 2.44M 0s\n",
    ">       1400K .......... .......... .......... .......... .......... 49%  140M 0s\n",
    ">       1450K .......... .......... .......... .......... .......... 51% 74.7M 0s\n",
    ">       1500K .......... .......... .......... .......... .......... 53% 6.68M 0s\n",
    ">       1550K .......... .......... .......... .......... .......... 55% 96.7M 0s\n",
    ">       1600K .......... .......... .......... .......... .......... 56% 61.3M 0s\n",
    ">       1650K .......... .......... .......... .......... .......... 58%  140M 0s\n",
    ">       1700K .......... .......... .......... .......... .......... 60%  101M 0s\n",
    ">       1750K .......... .......... .......... .......... .......... 62% 4.13M 0s\n",
    ">       1800K .......... .......... .......... .......... .......... 63%  117M 0s\n",
    ">       1850K .......... .......... .......... .......... .......... 65%  117M 0s\n",
    ">       1900K .......... .......... .......... .......... .......... 67% 6.82M 0s\n",
    ">       1950K .......... .......... .......... .......... .......... 68% 91.3M 0s\n",
    ">       2000K .......... .......... .......... .......... .......... 70%  104M 0s\n",
    ">       2050K .......... .......... .......... .......... .......... 72% 98.2M 0s\n",
    ">       2100K .......... .......... .......... .......... .......... 74%  134M 0s\n",
    ">       2150K .......... .......... .......... .......... .......... 75%  101M 0s\n",
    ">       2200K .......... .......... .......... .......... .......... 77% 4.24M 0s\n",
    ">       2250K .......... .......... .......... .......... .......... 79% 96.9M 0s\n",
    ">       2300K .......... .......... .......... .......... .......... 80%  124M 0s\n",
    ">       2350K .......... .......... .......... .......... .......... 82%  117M 0s\n",
    ">       2400K .......... .......... .......... .......... .......... 84% 7.51M 0s\n",
    ">       2450K .......... .......... .......... .......... .......... 86% 86.3M 0s\n",
    ">       2500K .......... .......... .......... .......... .......... 87%  139M 0s\n",
    ">       2550K .......... .......... .......... .......... .......... 89%  127M 0s\n",
    ">       2600K .......... .......... .......... .......... .......... 91%  103M 0s\n",
    ">       2650K .......... .......... .......... .......... .......... 93%  122M 0s\n",
    ">       2700K .......... .......... .......... .......... .......... 94%  101M 0s\n",
    ">       2750K .......... .......... .......... .......... .......... 96%  123M 0s\n",
    ">       2800K .......... .......... .......... .......... .......... 98% 4.44M 0s\n",
    ">       2850K .......... .......... .......... .......... .......... 99% 81.3M 0s\n",
    ">       2900K ...                                                   100% 6035G=0.4s\n",
    ">\n",
    ">     2020-11-24 16:15:45 (7.59 MB/s) - ‘soc-sign-epinions.txt.gz’ saved [2972840/2972840]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     total 2924\n",
    ">     drwxr-xr-x  2 root root    4096 Jan  1  1970 conf\n",
    ">     -rw-r--r--  1 root root     733 Nov 24 15:24 derby.log\n",
    ">     drwxr-xr-x 10 root root    4096 Nov 24 15:24 eventlogs\n",
    ">     drwxr-xr-x  2 root root    4096 Nov 24 16:15 ganglia\n",
    ">     drwxr-xr-x  2 root root    4096 Nov 24 16:04 logs\n",
    ">     -rw-r--r--  1 root root 2972840 Dec  3  2009 soc-sign-epinions.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gunzip soc-sign-epinions.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     total 11000\n",
    ">     drwxr-xr-x  2 root root     4096 Jan  1  1970 conf\n",
    ">     -rw-r--r--  1 root root      733 Nov 24 15:24 derby.log\n",
    ">     drwxr-xr-x 10 root root     4096 Nov 24 15:24 eventlogs\n",
    ">     drwxr-xr-x  2 root root     4096 Nov 24 16:15 ganglia\n",
    ">     drwxr-xr-x  2 root root     4096 Nov 24 16:04 logs\n",
    ">     -rw-r--r--  1 root root 11243141 Dec  3  2009 soc-sign-epinions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head soc-sign-epinions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     # Directed graph: soc-sign-epinions\n",
    ">     # Epinions signed social network\n",
    ">     # Nodes: 131828 Edges: 841372\n",
    ">     # FromNodeId\tToNodeId\tSign\n",
    ">     0\t1\t-1\n",
    ">     1\t128552\t-1\n",
    ">     2\t3\t1\n",
    ">     4\t5\t-1\n",
    ">     4\t155\t-1\n",
    ">     4\t558\t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p epinions\n",
    "mv soc-sign-epinions.txt epinions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l /dbfs/FileStore\n",
    "mv epinions /dbfs/FileStore/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     total 33\n",
    ">     drwxrwxrwx 2 root root   24 May  1  2018 datasets_magellan\n",
    ">     drwxrwxrwx 2 root root 4096 Nov 24 11:14 DIGSUM-files\n",
    ">     drwxrwxrwx 2 root root 4096 Nov 24 11:14 import-stage\n",
    ">     drwxrwxrwx 2 root root 4096 Nov 24 11:14 jars\n",
    ">     drwxrwxrwx 2 root root 4096 Nov 24 11:14 plots\n",
    ">     drwxrwxrwx 2 root root 4096 Nov 24 11:14 shared_uploads\n",
    ">     drwxrwxrwx 2 root root 4096 Nov 24 11:14 simon_temp_files_feel_free_to_delete_any_time\n",
    ">     drwxrwxrwx 2 root root 4096 Nov 24 11:14 tables\n",
    ">     drwxrwxrwx 2 root root 4096 Nov 24 11:14 timelinesOfInterest\n",
    ">     mv: preserving permissions for ‘/dbfs/FileStore/epinions/soc-sign-epinions.txt’: Operation not permitted\n",
    ">     mv: preserving permissions for ‘/dbfs/FileStore/epinions’: Operation not permitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /FileStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls file:/databricks/driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//%fs mv file:///databricks/driver/epinions /FileStore/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /FileStore/epinions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]\n",
    "\n",
    "  \n",
    "\n",
    "Preprocess dataset\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "import org.graphframes._\n",
    "\n",
    "// This import is needed to use the $-notation\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     import org.apache.spark.sql._\n",
    ">     import org.apache.spark.sql.functions._\n",
    ">     import org.graphframes._\n",
    ">     import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var df = spark.read.format(\"csv\")\n",
    "//   .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"comment\", \"#\")\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .load(\"/FileStore/epinions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     df: org.apache.spark.sql.DataFrame = [_c0: int, _c1: int ... 1 more field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res1: Long = 841372"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res36: Int = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res37: Array[org.apache.spark.sql.Row] = Array([0,1,-1], [1,128552,-1], [2,3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     root\n",
    ">      |-- _c0: integer (nullable = true)\n",
    ">      |-- _c1: integer (nullable = true)\n",
    ">      |-- _c2: integer (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val newNames = Seq(\"src\", \"dst\", \"rela\")\n",
    "val e = df.toDF(newNames: _*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     newNames: Seq[String] = List(src, dst, rela)\n",
    ">     e: org.apache.spark.sql.DataFrame = [src: int, dst: int ... 1 more field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     root\n",
    ">      |-- src: integer (nullable = true)\n",
    ">      |-- dst: integer (nullable = true)\n",
    ">      |-- rela: integer (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Vertex DataFrame\n",
    "val v = spark.range(1, 131827).toDF(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     v: org.apache.spark.sql.DataFrame = [id: bigint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val g = GraphFrame(v, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     g: org.graphframes.GraphFrame = GraphFrame(v:[id: bigint], e:[src: int, dst: int ... 1 more field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.edges.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     res15: Array[org.apache.spark.sql.Row] = Array([0,1,-1], [1,128552,-1], [2,3,1])\n",
    "\n",
    "  \n",
    "\n",
    "Count triads\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val results = g.triangleCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "We can not make use of the convenient API `triangleCount()` because it\n",
    "does not take the sign of edges into consideration. We need to write our\n",
    "own code to find triads.\n",
    "\n",
    "First, a triad should be undirected, but our graph concists of only\n",
    "directed edges.\n",
    "\n",
    "One strategy is to keep only bi-direction edges of the same sign. But we\n",
    "need to examine how large is the proportion of edges we will lose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Search for pairs of vertices with edges in both directions between them, i.e., find undirected or bidirected edges.\n",
    "val pair = g.find(\"(a)-[e1]->(b); (b)-[e2]->(a)\")\n",
    "println(pair.count())\n",
    "val filtered = pair.filter(\"e1.rela == e2.rela\")\n",
    "println(filtered.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     259751\n",
    ">     254345\n",
    ">     pair: org.apache.spark.sql.DataFrame = [a: struct<id: bigint>, e1: struct<src: int, dst: int ... 1 more field> ... 2 more fields]\n",
    ">     filtered: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [a: struct<id: bigint>, e1: struct<src: int, dst: int ... 1 more field> ... 2 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "Fortunately, we only lose a very small amount of edges.\n",
    "\n",
    "It also makes sense for this dataset, because if A trusts B, then it is\n",
    "quite unlikely that B does not trust A.\n",
    "\n",
    "In order to count different triads, first we have to find all triads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val triad = g.find(\"(a)-[eab]->(b); (b)-[eba]->(a); (b)-[ebc]->(c); (c)-[ecb]->(b); (c)-[eca]->(a); (a)-[eac]->(c)\")\n",
    "println(triad.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     3314925\n",
    ">     triad: org.apache.spark.sql.DataFrame = [a: struct<id: bigint>, eab: struct<src: int, dst: int ... 1 more field> ... 7 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "After finding all triads, we find each type by filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val t111 = triad.filter(\"eab.rela = 1 AND eab.rela = ebc.rela AND ebc.rela = eca.rela\")\n",
    "println(t111.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     3232357\n",
    ">     t111: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [a: struct<id: bigint>, eab: struct<src: int, dst: int ... 1 more field> ... 7 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val t000 = triad.filter(\"eab.rela = -1 AND eab.rela = ebc.rela AND ebc.rela = eca.rela\")\n",
    "println(t000.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     1610\n",
    ">     t000: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [a: struct<id: bigint>, eab: struct<src: int, dst: int ... 1 more field> ... 7 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val t110 = triad.filter(\"eab.rela + ebc.rela + eca.rela = 1\")\n",
    "println(t110.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     62634\n",
    ">     t110: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [a: struct<id: bigint>, eab: struct<src: int, dst: int ... 1 more field> ... 7 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val t001 = triad.filter(\"eab.rela + ebc.rela + eca.rela = -1\")\n",
    "println(t001.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     18324\n",
    ">     t001: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [a: struct<id: bigint>, eab: struct<src: int, dst: int ... 1 more field> ... 7 more fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val n111 = t111.count()\n",
    "val n001 = t001.count()\n",
    "val n000 = t000.count() \n",
    "val n110 = t110.count()\n",
    "val imbalanced = n000 + n110\n",
    "val balanced = n111 + n001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     n111: Long = 3232357\n",
    ">     n001: Long = 18324\n",
    ">     n000: Long = 1610\n",
    ">     n110: Long = 62634\n",
    ">     imbalanced: Long = 64244\n",
    ">     balanced: Long = 3250681\n",
    "\n",
    "  \n",
    "\n",
    "As we can see, the number of balanced triads overwhelms the number of\n",
    "imbalanced ones, which verifies the effectiveness of structural balance\n",
    "theory.\n",
    "\n",
    "Duplicates\n",
    "----------\n",
    "\n",
    "Some tests about duplicated motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val g: GraphFrame = examples.Graphs.friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     g: org.graphframes.GraphFrame = GraphFrame(v:[id: string, name: string ... 1 more field], e:[src: string, dst: string ... 1 more field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(g.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "[TABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val motifs = g.find(\"(a)-[e]->(b); (b)-[e2]->(a)\")\n",
    "motifs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    ">     +----------------+--------------+----------------+--------------+\n",
    ">     |               a|             e|               b|            e2|\n",
    ">     +----------------+--------------+----------------+--------------+\n",
    ">     |    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|[c, b, follow]|\n",
    ">     |[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|[b, c, follow]|\n",
    ">     +----------------+--------------+----------------+--------------+\n",
    ">\n",
    ">     motifs: org.apache.spark.sql.DataFrame = [a: struct<id: string, name: string ... 1 more field>, e: struct<src: string, dst: string ... 1 more field> ... 2 more fields]\n",
    "\n",
    "  \n",
    "\n",
    "As shown above, bi-direction edges are reported twice. Therefore, each\n",
    "triad is counted three times. However, this does not matter in our\n",
    "project, because the ratios between different triads remain the same."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
