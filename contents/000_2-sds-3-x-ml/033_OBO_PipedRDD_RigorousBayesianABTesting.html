<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>033_OBO_PipedRDD_RigorousBayesianABTesting - ScaDaMaLe/sds-3.x</title>
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="../../favicon.svg">
        
        
        <link rel="shortcut icon" href="../../favicon.png">
        
        <link rel="stylesheet" href="../../css/variables.css">
        <link rel="stylesheet" href="../../css/general.css">
        <link rel="stylesheet" href="../../css/chrome.css">
        
        <link rel="stylesheet" href="../../css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="../../FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="../../fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../highlight.css">
        <link rel="stylesheet" href="../../tomorrow-night.css">
        <link rel="stylesheet" href="../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
        <link rel="stylesheet" href="../../scroll-mdbook-outputs.css">
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/000_ScaDaMaLe.html">000_ScaDaMaLe</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/001_whySpark.html">001_whySpark</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/002_00_loginToDatabricks.html">002_00_loginToDatabricks</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/002_01_multiLingualNotebooks.html">002_01_multiLingualNotebooks</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/003_00_scalaCrashCourse.html">003_00_scalaCrashCourse</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/003_01_scalaCrashCourse.html">003_01_scalaCrashCourse</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/004_RDDsTransformationsActions.html">004_RDDsTransformationsActions</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/005_RDDsTransformationsActionsHOMEWORK.html">005_RDDsTransformationsActionsHOMEWORK</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/006a_PipedRDD.html">006a_PipedRDD</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/006_WordCount.html">006_WordCount</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/007a_SparkSQLProgGuide_HW.html">007a_SparkSQLProgGuide_HW</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/007b_SparkSQLProgGuide_HW.html">007b_SparkSQLProgGuide_HW</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/007c_SparkSQLProgGuide_HW.html">007c_SparkSQLProgGuide_HW</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/007d_SparkSQLProgGuide_HW.html">007d_SparkSQLProgGuide_HW</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/007e_SparkSQLProgGuide_HW.html">007e_SparkSQLProgGuide_HW</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/007f_SparkSQLProgGuide_HW.html">007f_SparkSQLProgGuide_HW</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/007g_PivotInSQL.html">007g_PivotInSQL</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/007_SparkSQLIntroBasics.html">007_SparkSQLIntroBasics</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/008_DiamondsPipeline_01ETLEDA.html">008_DiamondsPipeline_01ETLEDA</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/009_PowerPlantPipeline_01ETLEDA.html">009_PowerPlantPipeline_01ETLEDA</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/010_wikipediaClickStream_01ETLEDA.html">010_wikipediaClickStream_01ETLEDA</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/033_OBO_LoadExtract.html">033_OBO_LoadExtract</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html">033_OBO_PipedRDD_RigorousBayesianABTesting</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/010a_packageCells.html">010a_packageCells</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/011_02_IntroToSimulation.html">011_02_IntroToSimulation</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/011_03_IntroToML.html">011_03_IntroToML</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/012_UnsupervisedClustering_1MSongsKMeans_Intro.html">012_UnsupervisedClustering_1MSongsKMeans_Intro</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/013_UnsupervisedClustering_1MSongsKMeans_Stage1ETL.html">013_UnsupervisedClustering_1MSongsKMeans_Stage1ETL</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/014_UnsupervisedClustering_1MSongsKMeans_Stage2Explore.html">014_UnsupervisedClustering_1MSongsKMeans_Stage2Explore</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/015_UnsupervisedClustering_1MSongsKMeans_Stage3Model.html">015_UnsupervisedClustering_1MSongsKMeans_Stage3Model</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/016_SupervisedClustering_DecisionTrees_HandWrittenDigitRecognition.html">016_SupervisedClustering_DecisionTrees_HandWrittenDigitRecognition</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/017_LAlgIntro.html">017_LAlgIntro</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/018_LinRegIntro.html">018_LinRegIntro</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/019_DistLAlgForLinRegIntro.html">019_DistLAlgForLinRegIntro</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/019x_000_dataTypesProgGuide.html">019x_000_dataTypesProgGuide</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/019x_001_LocalVector.html">019x_001_LocalVector</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/019x_002_LabeledPoint.html">019x_002_LabeledPoint</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/019x_003_LocalMatrix.html">019x_003_LocalMatrix</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/019x_004_DistributedMatrix.html">019x_004_DistributedMatrix</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/019x_005_RowMatrix.html">019x_005_RowMatrix</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/019x_006_IndexedRowMatrix.html">019x_006_IndexedRowMatrix</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/019x_007_CoordinateMatrix.html">019x_007_CoordinateMatrix</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/019x_008_BlockMatrix.html">019x_008_BlockMatrix</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/020_PowerPlantPipeline_02ModelTuneEvaluate.html">020_PowerPlantPipeline_02ModelTuneEvaluate</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/021_recognizeActivityByRandomForest.html">021_recognizeActivityByRandomForest</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/022_GraphFramesUserGuide.html">022_GraphFramesUserGuide</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/023_OnTimeFlightPerformance.html">023_OnTimeFlightPerformance</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/030_PowerPlantPipeline_03ModelTuneEvaluateDeploy.html">030_PowerPlantPipeline_03ModelTuneEvaluateDeploy</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/033_OBO_LoadExtract.html">033_OBO_LoadExtract</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/033_OBO_PipedRDD_RigorousBayesianABTesting.html" class="active">033_OBO_PipedRDD_RigorousBayesianABTesting</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/034_LDA_20NewsGroupsSmall.html">034_LDA_20NewsGroupsSmall</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/035_LDA_CornellMovieDialogs.html">035_LDA_CornellMovieDialogs</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/036_ALS_MovieRecommender.html">036_ALS_MovieRecommender</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/998_EX_01_GraphXShortestWeightedPaths.html">998_EX_01_GraphXShortestWeightedPaths</a></li><li class="chapter-item expanded affix "><a href="../../contents/000_2-sds-3-x-ml/999_YT_01_FinancialFraudDetectionUsingDecisionTreeMachineLearningModels.html">999_YT_01_FinancialFraudDetectionUsingDecisionTreeMachineLearningModels</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">ScaDaMaLe/sds-3.x</h1>

                    <div class="right-buttons">
                        
                        <a href="../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        

                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="piped-rdds-and-bayesian-ab-testing"><a class="header" href="#piped-rdds-and-bayesian-ab-testing">Piped RDDs and Bayesian AB Testing</a></h1>
<p><strong>Continued with Application to Old Bailey Online Data</strong></p>
</div>
<div class="cell markdown">
<p>This is a recall/repeat of <code>006a_PipedRDD</code>. After the recall, we continue with applying Bayesian A/B Testing to the data extracted from Old Bailey Online counts of crimes and punishments.</p>
</div>
<div class="cell markdown">
<p>Here we will first take excerpts with minor modifications from the end of <strong>Chapter 12. Resilient Distributed Datasets (RDDs)</strong> of <em>Spark: The Definitive Guide</em>:</p>
<ul>
<li>https://learning.oreilly.com/library/view/spark-the-definitive/9781491912201/ch12.html</li>
</ul>
<p>Next, we will do Bayesian AB Testing using PipedRDDs.</p>
</div>
<div class="cell markdown">
<p>First, we create the toy RDDs as in <em>The Definitive Guide</em>:</p>
<blockquote>
<h1 id="from-a-local-collection"><a class="header" href="#from-a-local-collection">From a Local Collection</a></h1>
</blockquote>
<p>To create an RDD from a collection, you will need to use the parallelize method on a SparkContext (within a SparkSession). This turns a single node collection into a parallel collection. When creating this parallel collection, you can also explicitly state the number of partitions into which you would like to distribute this array. In this case, we are creating two partitions:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// in Scala
val myCollection = &quot;Spark The Definitive Guide : Big Data Processing Made Simple&quot;  .split(&quot; &quot;)
val words = spark.sparkContext.parallelize(myCollection, 2)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>myCollection: Array[String] = Array(Spark, The, Definitive, Guide, :, Big, Data, Processing, Made, Simple)
words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[353] at parallelize at command-685894176422621:3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># in Python
myCollection = &quot;Spark The Definitive Guide : Big Data Processing Made Simple&quot;\
  .split(&quot; &quot;)
words = spark.sparkContext.parallelize(myCollection, 2)
words
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="glom"><a class="header" href="#glom">glom</a></h1>
</blockquote>
<blockquote>
<p><code>glom</code> is an interesting function that takes every partition in your dataset and converts them to arrays. This can be useful if you’re going to collect the data to the driver and want to have an array for each partition. However, this can cause serious stability issues because if you have large partitions or a large number of partitions, it’s simple to crash the driver.</p>
</blockquote>
</div>
<div class="cell markdown">
<p>Let's use <code>glom</code> to see how our <code>words</code> are distributed among the two partitions we used explicitly.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">words.glom.collect 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: Array[Array[String]] = Array(Array(Spark, The, Definitive, Guide, :), Array(Big, Data, Processing, Made, Simple))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">words.glom().collect()
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="checkpointing"><a class="header" href="#checkpointing">Checkpointing</a></h1>
<p>One feature not available in the DataFrame API is the concept of checkpointing. Checkpointing is the act of saving an RDD to disk so that future references to this RDD point to those intermediate partitions on disk rather than recomputing the RDD from its original source. This is similar to caching except that it’s not stored in memory, only disk. This can be helpful when performing iterative computation, similar to the use cases for caching:</p>
</blockquote>
<p>Let's create a directory in <code>dbfs:///</code> for checkpointing of RDDs in the sequel. The following <code>%fs mkdirs /path_to_dir</code> is a shortcut to create a directory in <code>dbfs:///</code></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">mkdirs /datasets/ScaDaMaLe/checkpointing/
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res1: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">spark.sparkContext.setCheckpointDir(&quot;dbfs:///datasets/ScaDaMaLe/checkpointing&quot;)
words.checkpoint()
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p>Now, when we reference this RDD, it will derive from the checkpoint instead of the source data. This can be a helpful optimization.</p>
</blockquote>
</div>
<div class="cell markdown">
<h2 id="youtry"><a class="header" href="#youtry">YouTry</a></h2>
<p>Just some more words in <code>haha_words</code> with <code>\n</code>, the End-Of-Line (EOL) characters, in-place.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val haha_words = sc.parallelize(Seq(&quot;ha\nha&quot;, &quot;he\nhe\nhe&quot;, &quot;ho\nho\nho\nho&quot;),3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>haha_words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[357] at parallelize at command-685894176422632:1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's use <code>glom</code> to see how our <code>haha_words</code> are distributed among the partitions</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">haha_words.glom.collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res3: Array[Array[String]] =
Array(Array(ha
ha), Array(he
he
he), Array(ho
ho
ho
ho))
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="pipe-rdds-to-system-commands"><a class="header" href="#pipe-rdds-to-system-commands">Pipe RDDs to System Commands</a></h1>
</blockquote>
<blockquote>
<p>The pipe method is probably one of Spark’s more interesting methods. With pipe, you can return an RDD created by piping elements to a forked external process. The resulting RDD is computed by executing the given process once per partition. All elements of each input partition are written to a process’s stdin as lines of input separated by a newline. The resulting partition consists of the process’s stdout output, with each line of stdout resulting in one element of the output partition. A process is invoked even for empty partitions.</p>
</blockquote>
<blockquote>
<p>The print behavior can be customized by providing two functions.</p>
</blockquote>
<p>We can use a simple example and pipe each partition to the command wc. Each row will be passed in as a new line, so if we perform a line count, we will get the number of lines, one per partition:</p>
</div>
<div class="cell markdown">
<p>The following produces a <code>PipedRDD</code>:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wc_l_PipedRDD = words.pipe(&quot;wc -l&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wc_l_PipedRDD: org.apache.spark.rdd.RDD[String] = PipedRDD[359] at pipe at command-685894176422637:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">wc_l_PipedRDD = words.pipe(&quot;wc -l&quot;)
wc_l_PipedRDD
</code></pre>
</div>
<div class="cell markdown">
<p>Now, we take an action via <code>collect</code> to bring the results to the Driver.</p>
<p>NOTE: Be careful what you collect! You can always write the output to parquet of binary files in <code>dbfs:///</code> if the returned output is large.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">wc_l_PipedRDD.collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res4: Array[String] = Array(5, 5)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">wc_l_PipedRDD.collect()
</code></pre>
</div>
<div class="cell markdown">
<p>In this case, we got the number of lines returned by <code>wc -l</code> per partition.</p>
</div>
<div class="cell markdown">
<h2 id="youtry-1"><a class="header" href="#youtry-1">YouTry</a></h2>
<p>Try to make sense of the next few cells where we do NOT specifiy the number of partitions explicitly and let Spark decide on the number of partitions automatically.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val haha_words = sc.parallelize(Seq(&quot;ha\nha&quot;, &quot;he\nhe\nhe&quot;, &quot;ho\nho\nho\nho&quot;),3)
haha_words.glom.collect
val wc_l_PipedRDD_haha_words = haha_words.pipe(&quot;wc -l&quot;)
wc_l_PipedRDD_haha_words.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>haha_words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[361] at parallelize at command-685894176422644:1
wc_l_PipedRDD_haha_words: org.apache.spark.rdd.RDD[String] = PipedRDD[363] at pipe at command-685894176422644:3
res5: Array[String] = Array(2, 3, 4)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Do you understand why the above <code>collect</code> statement returns what it does?</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val haha_words_again = sc.parallelize(Seq(&quot;ha\nha&quot;, &quot;he\nhe\nhe&quot;, &quot;ho\nho\nho\nho&quot;))
haha_words_again.glom.collect
val wc_l_PipedRDD_haha_words_again = haha_words_again.pipe(&quot;wc -l&quot;)
wc_l_PipedRDD_haha_words_again.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>haha_words_again: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[364] at parallelize at command-685894176422646:1
wc_l_PipedRDD_haha_words_again: org.apache.spark.rdd.RDD[String] = PipedRDD[366] at pipe at command-685894176422646:3
res6: Array[String] = Array(0, 0, 2, 0, 0, 3, 0, 4)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Did you understand why some of the results are <code>0</code> in the last <code>collect</code> statement?</p>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="mappartitions"><a class="header" href="#mappartitions">mapPartitions</a></h1>
</blockquote>
<blockquote>
<p>The previous command revealed that Spark operates on a per-partition basis when it comes to actually executing code. You also might have noticed earlier that the return signature of a map function on an RDD is actually <code>MapPartitionsRDD</code>.</p>
</blockquote>
<p>Or <code>ParallelCollectionRDD</code> in our case.</p>
<blockquote>
<p>This is because map is just a row-wise alias for <code>mapPartitions</code>, which makes it possible for you to map an individual partition (represented as an iterator). That’s because physically on the cluster we operate on each partition individually (and not a specific row). A simple example creates the value “1” for every partition in our data, and the sum of the following expression will count the number of partitions we have:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// in Scala
words.mapPartitions(part =&gt; Iterator[Int](1)).sum() // 2.0
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res7: Double = 2.0
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># in Python
words.mapPartitions(lambda part: [1]).sum() # 2
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p>Naturally, this means that we operate on a per-partition basis and therefore it allows us to perform an operation on that <em>entire</em> partition. This is valuable for performing something on an entire subdataset of your RDD. You can gather all values of a partition class or group into one partition and then operate on that entire group using arbitrary functions and controls. An example use case of this would be that you could pipe this through some custom machine learning algorithm and train an individual model for that company’s portion of the dataset. A Facebook engineer has an interesting demonstration of their particular implementation of the pipe operator with a similar use case demonstrated at <a href="https://spark-summit.org/east-2017/events/experiences-with-sparks-rdd-apis-for-complex-custom-applications/">Spark Summit East 2017</a>.</p>
</blockquote>
<blockquote>
<p>Other functions similar to <code>mapPartitions</code> include <code>mapPartitionsWithIndex</code>. With this you specify a function that accepts an index (within the partition) and an iterator that goes through all items within the partition. The partition index is the partition number in your RDD, which identifies where each record in our dataset sits (and potentially allows you to debug). You might use this to test whether your map functions are behaving correctly:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// in Scala
def indexedFunc(partitionIndex:Int, withinPartIterator: Iterator[String]) = {  withinPartIterator.toList.map(    
  value =&gt; s&quot;Partition: $partitionIndex =&gt; $value&quot;).iterator
                                                                            }
words.mapPartitionsWithIndex(indexedFunc).collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>indexedFunc: (partitionIndex: Int, withinPartIterator: Iterator[String])Iterator[String]
res8: Array[String] = Array(Partition: 0 =&gt; Spark, Partition: 0 =&gt; The, Partition: 0 =&gt; Definitive, Partition: 0 =&gt; Guide, Partition: 0 =&gt; :, Partition: 1 =&gt; Big, Partition: 1 =&gt; Data, Partition: 1 =&gt; Processing, Partition: 1 =&gt; Made, Partition: 1 =&gt; Simple)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># in Python
def indexedFunc(partitionIndex, withinPartIterator):  
  return [&quot;partition: {} =&gt; {}&quot;.format(partitionIndex,    x) for x in withinPartIterator]
words.mapPartitionsWithIndex(indexedFunc).collect()
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="foreachpartition"><a class="header" href="#foreachpartition">foreachPartition</a></h1>
</blockquote>
<blockquote>
<p>Although <code>mapPartitions</code> needs a return value to work properly, this next function does not. <code>foreachPartition</code> simply iterates over all the partitions of the data. The difference is that the function has no return value. This makes it great for doing something with each partition like writing it out to a database. In fact, this is how many data source connectors are written. You can create</p>
</blockquote>
<p>your</p>
<blockquote>
<p>own text file source if you want by specifying outputs to the temp directory with a random ID:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">words.foreachPartition { iter =&gt;  
  import java.io._  
  import scala.util.Random  
  val randomFileName = new Random().nextInt()  
  val pw = new PrintWriter(new File(s&quot;/tmp/random-file-${randomFileName}.txt&quot;))  
  while (iter.hasNext) {
    pw.write(iter.next())  
  }  
  pw.close()
}
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p>You’ll find these two files if you scan your /tmp directory.</p>
</blockquote>
<p>You need to scan for the file across all the nodes. As the file may not be in the Driver node's <code>/tmp/</code> directory but in those of the executors that hosted the partition.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">pwd
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>/databricks/driver
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">ls /tmp/random-file-*.txt
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>ls: cannot access '/tmp/random-file-*.txt': No such file or directory
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell markdown">
<h1 id="numerically-rigorous-bayesian-ab-testing"><a class="header" href="#numerically-rigorous-bayesian-ab-testing">Numerically Rigorous Bayesian AB Testing</a></h1>
<p>This is an example of Bayesian AB Testing with computer-aided proofs for the posterior samples.</p>
<p>The main learning goal for you is to use pipedRDDs to distribute, in an embarassingly paralle way, across all the worker nodes in the Spark cluster an executible <code>IsIt1or2Coins</code>.</p>
<h3 id="what-does-isit1or2coins-do"><a class="header" href="#what-does-isit1or2coins-do">What does <code>IsIt1or2Coins</code> do?</a></h3>
<p>At a very high-level, to understand what <code>IsIt1or2Coins</code> does, imagine the following simple experiment.</p>
<p>We are given</p>
<ul>
<li>the number of heads that result from a first sequence of independent and identical tosses of a coin and then</li>
<li>we are given the number of heads that result from a second sequence of independent and identical tosses of a coin</li>
</ul>
<p>Our decision problem is to do help shed light on whether both sequence of tosses came from the same coin or not (whatever the bias may be).</p>
<p><code>IsIt1or2Coins</code> tries to help us decide if the two sequence of coin-tosses are based on one coin with an unknown bias or two coins with different biases.</p>
<p>If you are curious about details feel free to see:</p>
<ul>
<li>Exact Bayesian A/B testing using distributed fault-tolerant Moore rejection sampler, Benny Avelin and Raazesh Sainudiin, Extended Abstract, 2 pages, 2018 <a href="http://lamastex.org/preprints/20180507_ABTestingViaDistributedMRS.pdf">(PDF 104KB)</a>.</li>
<li>which builds on: An auto-validating, trans-dimensional, universal rejection sampler for locally Lipschitz arithmetical expressions, Raazesh Sainudiin and Thomas York, <a href="http://interval.louisiana.edu/reliable-computing-journal/volume-18/reliable-computing-18-pp-015-054.pdf">Reliable Computing, vol.18, pp.15-54, 2013</a> (<a href="http://lamastex.org/preprints/avs_rc_2013.pdf">preprint: PDF 2612KB</a>)</li>
</ul>
<p><strong>See first about <code>PipedRDDs</code> excerpt from <em>Spark The Definitive Guide</em> earlier.</strong></p>
<h3 id="getting-the-executible-isit1or2coins-into-our-spark-cluster"><a class="header" href="#getting-the-executible-isit1or2coins-into-our-spark-cluster">Getting the executible <code>IsIt1or2Coins</code> into our Spark Cluster</a></h3>
<p><strong>This has already been done in the project-shard. You need not do it again for this executible!</strong></p>
<p>You need to upload the C++ executible <code>IsIt1or2Coins</code> from: - https://github.com/lamastex/mrs2</p>
<p>Here, suppose you have an executible for linux x86 64 bit processor with all dependencies pre-compiled into one executibe.</p>
<p>Say this executible is <code>IsIt10r2Coins</code>.</p>
<p>This executible comes from the following dockerised build:</p>
<ul>
<li>https://github.com/lamastex/mrs2/tree/master/docker</li>
<li>by statically compiling inside the docerised environment for mrs2:
<ul>
<li>https://github.com/lamastex/mrs2/tree/master/mrs-2.0/examples/MooreRejSam/IsIt1or2Coins</li>
</ul>
</li>
</ul>
<p>You can replace the executible with any other executible with appropriate I/O to it.</p>
<p>Then you upload the executible to databricks' <code>FileStore</code>.</p>
</div>
<div class="cell markdown">
<p>Just note the path to the file and DO NOT click <code>Create Table</code> or other buttons!</p>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/images/2020/ScaDaMaLe/screenShotOfUploadingStaticExecutibleIsIt1or2CoinsViaFileStore.png" alt="creenShotOfUploadingStaticExecutibleIsIt1or2CoinsViaFileStore" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">ls &quot;/FileStore/tables/IsIt1or2Coins&quot;
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/FileStore/tables/IsIt1or2Coins</td>
<td>IsIt1or2Coins</td>
<td>6559480.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell markdown">
<p>Now copy the file from <code>dbfs://FileStore</code> that you just uploaded into the local file system of the Driver.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.cp(&quot;dbfs:/FileStore/tables/IsIt1or2Coins&quot;, &quot;file:/tmp/IsIt1or2Coins&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res11: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">ls -al /tmp/IsIt1or2Coins
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>-rw-r--r-- 1 root root 6559480 May 31 23:33 /tmp/IsIt1or2Coins
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Note it is a big static executible with all dependencies inbuilt (it uses GNU Scientific Library and a specialized C++ Library called C-XSC or C Extended for Scientific Computing to do hard-ware optimized rigorous numerical proofs using Interval-Extended Hessian Differentiation Arithmetics over Rounding-Controlled Hardware-Specified Machine Intervals).</p>
<p>Just note it is over 6.5MB. Also we need to change the permissions so it is indeed executible.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">chmod +x /tmp/IsIt1or2Coins
</code></pre>
</div>
<div class="cell markdown">
<h1 id="usage-instructions-for-isit1or2coins"><a class="header" href="#usage-instructions-for-isit1or2coins">Usage instructions for IsIt1or2Coins</a></h1>
<p><code>./IsIt1or2Coins numboxes numiter seed numtosses1 heads1 numtosses2 heads2 logScale</code> - numboxes = Number of boxes for Moore Rejection Sampling (Rigorous von Neumann Rejection Sampler) - numiter = Number of samples drawn from posterior distribution to estimate the model probabilities - seed = a random number seed - numtosses1 = number of tosses for the first coin - heads1 = number of heads shown up on the first coin - numtosses2 = number of tosses for the second coin - heads2 = number of heads shown up on the second coin - logscale = True/False as Int</p>
<p>Don't worry about the details of what the executible <code>IsIt1or2Coins</code> is doing for now. Just realise that this executible takes some input on command-line and gives some output.</p>
</div>
<div class="cell markdown">
<p>Let's make sure the executible takes input and returns output string on the Driver node.</p>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-sh">/tmp/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>theSeed: 234565432
N1: 1000
n1: 500
N2: 1200
n2: 600
UseLogPi: 1
  n_boxes: 1000  n_samples: 100  rng_seed = 234565432
  N1=number of first coin tosses          : 1000
  n1=number of heads in first coin tosses : 500
  N2=number of second coin tosses         : 1200
  n2=number of heads in second coin tosses: 600
Ldomain.L: 0
Ldomain.L: 1
end of FIsIt1or2Coins constructor. 
in FirstBox, before getBoxREInfo. k: 0
0 [1.000000000000000E-300,   1.000000000000000] RE: [-1.519706161376072E+006,   0.000000000000000] BoxIntegral: [-1.519706161376072E+006,   0.000000000000000]
in FirstBox, after getBoxREInfo 
0 [1.000000000000000E-300,   1.000000000000000]
in FirstBox, before getBoxREInfo. k: 1
1 [1.000000000000000E-300,   1.000000000000000] [1.000000000000000E-300,   1.000000000000000] RE: [-1.519706161376072E+006,   0.000000000000000] BoxIntegral: [-1.519706161376072E+006,   0.000000000000000]
in FirstBox, after getBoxREInfo 
1 [1.000000000000000E-300,   1.000000000000000] [1.000000000000000E-300,   1.000000000000000]
Umax:    0.000000000000000
UmaxMAX, Umax, f_scale_local: 1e+200    0.000000000000000 -460.517018598809159
f_scale: -460.517018598809159  -460.517018598809159
bottom of updateUmax 
in FirstBox, after updateUmax 
bottom of FirstBox. 
after FirstBox, before Refine 
Umax: -1.512624733218932E+003
UmaxMAX, Umax, f_scale_local: 1e+200 -1.512624733218932E+003 -1.973141751817741E+003
f_scale: -1.973141751817741E+003  -1.973141751817741E+003
bottom of updateUmax 
in AdaptPartition after updateUmax2 
in updateIntegral. IL, IU:    0.000000000000000 1.000000000000362E+200
# Adaptive partitioning complete. Boxes: 1000  Lower bound on Acceptance Prob.: 4.88326e-07 IL, IU: 1.41768e+191   2.90314e+197
#Using log(pi)? 1
#No. of Boxes with proposal mass function &lt;= 1e-16 48
#No. of Boxes with proposal mass function &lt;= 1e-10 112
#No. of Boxes with proposal mass function &gt;= 1e-6 776
#No. of Boxes with proposal mass function &gt;= 1e-3 230
after Refine 
before Rej..SampleMany 
n_samples: 100
after Rej..SampleMany 
rs_sample IU, N, Nrs: 2.903136091244425E+197 100 100
RSSampleMany, integral est: 1.2113e+193
RSSampleMany mean: 
   Number of labels or topologies = 2
label: 0  proportion:    0.970000000000000
Labelled Mean:
   0.500284830473953

label: 1  proportion:    0.030000000000000
Labelled Mean:
   0.510581317236150
   0.489823533395186

n interval function calls: 1998
n real function calls: 2396703
# CPU Time (seconds). Partitioning: 0.006637  Sampling: 0.737085  Total: 0.743722
# CPU time (secods) per estimate: 0.00743722
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-sh"># You can also do it like this

/dbfs/FileStore/tables/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>theSeed: 234565432
N1: 1000
n1: 500
N2: 1200
n2: 600
UseLogPi: 1
  n_boxes: 1000  n_samples: 100  rng_seed = 234565432
  N1=number of first coin tosses          : 1000
  n1=number of heads in first coin tosses : 500
  N2=number of second coin tosses         : 1200
  n2=number of heads in second coin tosses: 600
Ldomain.L: 0
Ldomain.L: 1
end of FIsIt1or2Coins constructor. 
in FirstBox, before getBoxREInfo. k: 0
0 [1.000000000000000E-300,   1.000000000000000] RE: [-1.519706161376072E+006,   0.000000000000000] BoxIntegral: [-1.519706161376072E+006,   0.000000000000000]
in FirstBox, after getBoxREInfo 
0 [1.000000000000000E-300,   1.000000000000000]
in FirstBox, before getBoxREInfo. k: 1
1 [1.000000000000000E-300,   1.000000000000000] [1.000000000000000E-300,   1.000000000000000] RE: [-1.519706161376072E+006,   0.000000000000000] BoxIntegral: [-1.519706161376072E+006,   0.000000000000000]
in FirstBox, after getBoxREInfo 
1 [1.000000000000000E-300,   1.000000000000000] [1.000000000000000E-300,   1.000000000000000]
Umax:    0.000000000000000
UmaxMAX, Umax, f_scale_local: 1e+200    0.000000000000000 -460.517018598809159
f_scale: -460.517018598809159  -460.517018598809159
bottom of updateUmax 
in FirstBox, after updateUmax 
bottom of FirstBox. 
after FirstBox, before Refine 
Umax: -1.512624733218932E+003
UmaxMAX, Umax, f_scale_local: 1e+200 -1.512624733218932E+003 -1.973141751817741E+003
f_scale: -1.973141751817741E+003  -1.973141751817741E+003
bottom of updateUmax 
in AdaptPartition after updateUmax2 
in updateIntegral. IL, IU:    0.000000000000000 1.000000000000362E+200
# Adaptive partitioning complete. Boxes: 1000  Lower bound on Acceptance Prob.: 4.88326e-07 IL, IU: 1.41768e+191   2.90314e+197
#Using log(pi)? 1
#No. of Boxes with proposal mass function &lt;= 1e-16 48
#No. of Boxes with proposal mass function &lt;= 1e-10 112
#No. of Boxes with proposal mass function &gt;= 1e-6 776
#No. of Boxes with proposal mass function &gt;= 1e-3 230
after Refine 
before Rej..SampleMany 
n_samples: 100
after Rej..SampleMany 
rs_sample IU, N, Nrs: 2.903136091244425E+197 100 100
RSSampleMany, integral est: 1.2113e+193
RSSampleMany mean: 
   Number of labels or topologies = 2
label: 0  proportion:    0.970000000000000
Labelled Mean:
   0.500284830473953

label: 1  proportion:    0.030000000000000
Labelled Mean:
   0.510581317236150
   0.489823533395186

n interval function calls: 1998
n real function calls: 2396703
# CPU Time (seconds). Partitioning: 0.006415  Sampling: 0.759395  Total: 0.76581
# CPU time (secods) per estimate: 0.0076581
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="moving-the-executables-to-the-worker-nodes"><a class="header" href="#moving-the-executables-to-the-worker-nodes">Moving the executables to the worker nodes</a></h2>
</div>
<div class="cell markdown">
<p>To copy the executible from <code>dbfs</code> to the local drive of each executor you can use the following helper function.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import scala.sys.process._
import scala.concurrent.duration._
// from Ivan Sadikov

def copyFile(): Unit = {
  &quot;mkdir -p /tmp/executor/bin&quot;.!!
  &quot;cp /dbfs/FileStore/tables/IsIt1or2Coins /tmp/executor/bin/&quot;.!!
}

sc.runOnEachExecutor(copyFile, new FiniteDuration(1, HOURS))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import scala.sys.process._
import scala.concurrent.duration._
copyFile: ()Unit
res12: scala.collection.Map[String,scala.util.Try[Unit]] = Map(0 -&gt; Success(()), 2 -&gt; Success(()))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Now, let us use piped RDDs via <code>bash</code> to execute the given command in each partition as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val input = Seq(&quot;/tmp/executor/bin/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1&quot;, &quot;/tmp/executor/bin/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1&quot;)

val output = sc
  .parallelize(input)
  .repartition(2)
  .pipe(&quot;bash&quot;)
  .collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>input: Seq[String] = List(/tmp/executor/bin/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1, /tmp/executor/bin/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1)
output: Array[String] = Array(theSeed: 234565432, N1: 1000, n1: 500, N2: 1200, n2: 600, UseLogPi: 1, &quot;  n_boxes: 1000  n_samples: 100  rng_seed = 234565432&quot;, &quot;  N1=number of first coin tosses          : 1000&quot;, &quot;  n1=number of heads in first coin tosses : 500&quot;, &quot;  N2=number of second coin tosses         : 1200&quot;, &quot;  n2=number of heads in second coin tosses: 600&quot;, Ldomain.L: 0, Ldomain.L: 1, &quot;end of FIsIt1or2Coins constructor. &quot;, in FirstBox, before getBoxREInfo. k: 0, 0 [1.000000000000000E-300,   1.000000000000000] RE: [-1.519706161376072E+006,   0.000000000000000] BoxIntegral: [-1.519706161376072E+006,   0.000000000000000], &quot;in FirstBox, after getBoxREInfo &quot;, 0 [1.000000000000000E-300,   1.000000000000000], in FirstBox, before getBoxREInfo. k: 1, 1 [1.000000000000000E-300,   1.000000000000000] [1.000000000000000E-300,   1.000000000000000] RE: [-1.519706161376072E+006,   0.000000000000000] BoxIntegral: [-1.519706161376072E+006,   0.000000000000000], &quot;in FirstBox, after getBoxREInfo &quot;, 1 [1.000000000000000E-300,   1.000000000000000] [1.000000000000000E-300,   1.000000000000000], Umax:    0.000000000000000, UmaxMAX, Umax, f_scale_local: 1e+200    0.000000000000000 -460.517018598809159, f_scale: -460.517018598809159  -460.517018598809159, &quot;bottom of updateUmax &quot;, &quot;in FirstBox, after updateUmax &quot;, &quot;bottom of FirstBox. &quot;, &quot;after FirstBox, before Refine &quot;, Umax: -1.512624733218932E+003, UmaxMAX, Umax, f_scale_local: 1e+200 -1.512624733218932E+003 -1.973141751817741E+003, f_scale: -1.973141751817741E+003  -1.973141751817741E+003, &quot;bottom of updateUmax &quot;, &quot;in AdaptPartition after updateUmax2 &quot;, in updateIntegral. IL, IU:    0.000000000000000 1.000000000000362E+200, # Adaptive partitioning complete. Boxes: 1000  Lower bound on Acceptance Prob.: 4.88326e-07 IL, IU: 1.41768e+191   2.90314e+197, #Using log(pi)? 1, #No. of Boxes with proposal mass function &lt;= 1e-16 48, #No. of Boxes with proposal mass function &lt;= 1e-10 112, #No. of Boxes with proposal mass function &gt;= 1e-6 776, #No. of Boxes with proposal mass function &gt;= 1e-3 230, &quot;after Refine &quot;, &quot;before Rej..SampleMany &quot;, n_samples: 100, &quot;after Rej..SampleMany &quot;, rs_sample IU, N, Nrs: 2.903136091244425E+197 100 100, RSSampleMany, integral est: 1.2113e+193, &quot;RSSampleMany mean: &quot;, &quot;   Number of labels or topologies = 2&quot;, label: 0  proportion:    0.970000000000000, Labelled Mean:, &quot;   0.500284830473953&quot;, &quot;&quot;, label: 1  proportion:    0.030000000000000, Labelled Mean:, &quot;   0.510581317236150&quot;, &quot;   0.489823533395186&quot;, &quot;&quot;, n interval function calls: 1998, n real function calls: 2396703, # CPU Time (seconds). Partitioning: 0.005925  Sampling: 0.726124  Total: 0.732049, # CPU time (secods) per estimate: 0.00732049, theSeed: 234565432, N1: 1000, n1: 500, N2: 1200, n2: 600, UseLogPi: 1, &quot;  n_boxes: 1000  n_samples: 100  rng_seed = 234565432&quot;, &quot;  N1=number of first coin tosses          : 1000&quot;, &quot;  n1=number of heads in first coin tosses : 500&quot;, &quot;  N2=number of second coin tosses         : 1200&quot;, &quot;  n2=number of heads in second coin tosses: 600&quot;, Ldomain.L: 0, Ldomain.L: 1, &quot;end of FIsIt1or2Coins constructor. &quot;, in FirstBox, before getBoxREInfo. k: 0, 0 [1.000000000000000E-300,   1.000000000000000] RE: [-1.519706161376072E+006,   0.000000000000000] BoxIntegral: [-1.519706161376072E+006,   0.000000000000000], &quot;in FirstBox, after getBoxREInfo &quot;, 0 [1.000000000000000E-300,   1.000000000000000], in FirstBox, before getBoxREInfo. k: 1, 1 [1.000000000000000E-300,   1.000000000000000] [1.000000000000000E-300,   1.000000000000000] RE: [-1.519706161376072E+006,   0.000000000000000] BoxIntegral: [-1.519706161376072E+006,   0.000000000000000], &quot;in FirstBox, after getBoxREInfo &quot;, 1 [1.000000000000000E-300,   1.000000000000000] [1.000000000000000E-300,   1.000000000000000], Umax:    0.000000000000000, UmaxMAX, Umax, f_scale_local: 1e+200    0.000000000000000 -460.517018598809159, f_scale: -460.517018598809159  -460.517018598809159, &quot;bottom of updateUmax &quot;, &quot;in FirstBox, after updateUmax &quot;, &quot;bottom of FirstBox. &quot;, &quot;after FirstBox, before Refine &quot;, Umax: -1.512624733218932E+003, UmaxMAX, Umax, f_scale_local: 1e+200 -1.512624733218932E+003 -1.973141751817741E+003, f_scale: -1.973141751817741E+003  -1.973141751817741E+003, &quot;bottom of updateUmax &quot;, &quot;in AdaptPartition after updateUmax2 &quot;, in updateIntegral. IL, IU:    0.000000000000000 1.000000000000362E+200, # Adaptive partitioning complete. Boxes: 1000  Lower bound on Acceptance Prob.: 4.88326e-07 IL, IU: 1.41768e+191   2.90314e+197, #Using log(pi)? 1, #No. of Boxes with proposal mass function &lt;= 1e-16 48, #No. of Boxes with proposal mass function &lt;= 1e-10 112, #No. of Boxes with proposal mass function &gt;= 1e-6 776, #No. of Boxes with proposal mass function &gt;= 1e-3 230, &quot;after Refine &quot;, &quot;before Rej..SampleMany &quot;, n_samples: 100, &quot;after Rej..SampleMany &quot;, rs_sample IU, N, Nrs: 2.903136091244425E+197 100 100, RSSampleMany, integral est: 1.2113e+193, &quot;RSSampleMany mean: &quot;, &quot;   Number of labels or topologies = 2&quot;, label: 0  proportion:    0.970000000000000, Labelled Mean:, &quot;   0.500284830473953&quot;, &quot;&quot;, label: 1  proportion:    0.030000000000000, Labelled Mean:, &quot;   0.510581317236150&quot;, &quot;   0.489823533395186&quot;, &quot;&quot;, n interval function calls: 1998, n real function calls: 2396703, # CPU Time (seconds). Partitioning: 0.005478  Sampling: 0.729789  Total: 0.735267, # CPU time (secods) per estimate: 0.00735267)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>In fact, you can just use <code>DBFS FUSE</code> to run the commands without any file copy in databricks-provisioned Spark clusters we are on here:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val isIt1or2StaticExecutible = &quot;/dbfs/FileStore/tables/IsIt1or2Coins&quot;
val same_input = Seq(s&quot;$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1&quot;, 
                     s&quot;$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1&quot;)

val same_output = sc
  .parallelize(same_input)
  .repartition(2)
  .pipe(&quot;bash&quot;)
  .collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>isIt1or2StaticExecutible: String = /dbfs/FileStore/tables/IsIt1or2Coins
same_input: Seq[String] = List(/dbfs/FileStore/tables/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1, /dbfs/FileStore/tables/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1)
same_output: Array[String] = Array(theSeed: 234565432, N1: 1000, n1: 500, N2: 1200, n2: 600, UseLogPi: 1, &quot;  n_boxes: 1000  n_samples: 100  rng_seed = 234565432&quot;, &quot;  N1=number of first coin tosses          : 1000&quot;, &quot;  n1=number of heads in first coin tosses : 500&quot;, &quot;  N2=number of second coin tosses         : 1200&quot;, &quot;  n2=number of heads in second coin tosses: 600&quot;, Ldomain.L: 0, Ldomain.L: 1, &quot;end of FIsIt1or2Coins constructor. &quot;, in FirstBox, before getBoxREInfo. k: 0, 0 [1.000000000000000E-300,   1.000000000000000] RE: [-1.519706161376072E+006,   0.000000000000000] BoxIntegral: [-1.519706161376072E+006,   0.000000000000000], &quot;in FirstBox, after getBoxREInfo &quot;, 0 [1.000000000000000E-300,   1.000000000000000], in FirstBox, before getBoxREInfo. k: 1, 1 [1.000000000000000E-300,   1.000000000000000] [1.000000000000000E-300,   1.000000000000000] RE: [-1.519706161376072E+006,   0.000000000000000] BoxIntegral: [-1.519706161376072E+006,   0.000000000000000], &quot;in FirstBox, after getBoxREInfo &quot;, 1 [1.000000000000000E-300,   1.000000000000000] [1.000000000000000E-300,   1.000000000000000], Umax:    0.000000000000000, UmaxMAX, Umax, f_scale_local: 1e+200    0.000000000000000 -460.517018598809159, f_scale: -460.517018598809159  -460.517018598809159, &quot;bottom of updateUmax &quot;, &quot;in FirstBox, after updateUmax &quot;, &quot;bottom of FirstBox. &quot;, &quot;after FirstBox, before Refine &quot;, Umax: -1.512624733218932E+003, UmaxMAX, Umax, f_scale_local: 1e+200 -1.512624733218932E+003 -1.973141751817741E+003, f_scale: -1.973141751817741E+003  -1.973141751817741E+003, &quot;bottom of updateUmax &quot;, &quot;in AdaptPartition after updateUmax2 &quot;, in updateIntegral. IL, IU:    0.000000000000000 1.000000000000362E+200, # Adaptive partitioning complete. Boxes: 1000  Lower bound on Acceptance Prob.: 4.88326e-07 IL, IU: 1.41768e+191   2.90314e+197, #Using log(pi)? 1, #No. of Boxes with proposal mass function &lt;= 1e-16 48, #No. of Boxes with proposal mass function &lt;= 1e-10 112, #No. of Boxes with proposal mass function &gt;= 1e-6 776, #No. of Boxes with proposal mass function &gt;= 1e-3 230, &quot;after Refine &quot;, &quot;before Rej..SampleMany &quot;, n_samples: 100, &quot;after Rej..SampleMany &quot;, rs_sample IU, N, Nrs: 2.903136091244425E+197 100 100, RSSampleMany, integral est: 1.2113e+193, &quot;RSSampleMany mean: &quot;, &quot;   Number of labels or topologies = 2&quot;, label: 0  proportion:    0.970000000000000, Labelled Mean:, &quot;   0.500284830473953&quot;, &quot;&quot;, label: 1  proportion:    0.030000000000000, Labelled Mean:, &quot;   0.510581317236150&quot;, &quot;   0.489823533395186&quot;, &quot;&quot;, n interval function calls: 1998, n real function calls: 2396703, # CPU Time (seconds). Partitioning: 0.006454  Sampling: 0.717612  Total: 0.724066, # CPU time (secods) per estimate: 0.00724066, theSeed: 234565432, N1: 1000, n1: 500, N2: 1200, n2: 600, UseLogPi: 1, &quot;  n_boxes: 1000  n_samples: 100  rng_seed = 234565432&quot;, &quot;  N1=number of first coin tosses          : 1000&quot;, &quot;  n1=number of heads in first coin tosses : 500&quot;, &quot;  N2=number of second coin tosses         : 1200&quot;, &quot;  n2=number of heads in second coin tosses: 600&quot;, Ldomain.L: 0, Ldomain.L: 1, &quot;end of FIsIt1or2Coins constructor. &quot;, in FirstBox, before getBoxREInfo. k: 0, 0 [1.000000000000000E-300,   1.000000000000000] RE: [-1.519706161376072E+006,   0.000000000000000] BoxIntegral: [-1.519706161376072E+006,   0.000000000000000], &quot;in FirstBox, after getBoxREInfo &quot;, 0 [1.000000000000000E-300,   1.000000000000000], in FirstBox, before getBoxREInfo. k: 1, 1 [1.000000000000000E-300,   1.000000000000000] [1.000000000000000E-300,   1.000000000000000] RE: [-1.519706161376072E+006,   0.000000000000000] BoxIntegral: [-1.519706161376072E+006,   0.000000000000000], &quot;in FirstBox, after getBoxREInfo &quot;, 1 [1.000000000000000E-300,   1.000000000000000] [1.000000000000000E-300,   1.000000000000000], Umax:    0.000000000000000, UmaxMAX, Umax, f_scale_local: 1e+200    0.000000000000000 -460.517018598809159, f_scale: -460.517018598809159  -460.517018598809159, &quot;bottom of updateUmax &quot;, &quot;in FirstBox, after updateUmax &quot;, &quot;bottom of FirstBox. &quot;, &quot;after FirstBox, before Refine &quot;, Umax: -1.512624733218932E+003, UmaxMAX, Umax, f_scale_local: 1e+200 -1.512624733218932E+003 -1.973141751817741E+003, f_scale: -1.973141751817741E+003  -1.973141751817741E+003, &quot;bottom of updateUmax &quot;, &quot;in AdaptPartition after updateUmax2 &quot;, in updateIntegral. IL, IU:    0.000000000000000 1.000000000000362E+200, # Adaptive partitioning complete. Boxes: 1000  Lower bound on Acceptance Prob.: 4.88326e-07 IL, IU: 1.41768e+191   2.90314e+197, #Using log(pi)? 1, #No. of Boxes with proposal mass function &lt;= 1e-16 48, #No. of Boxes with proposal mass function &lt;= 1e-10 112, #No. of Boxes with proposal mass function &gt;= 1e-6 776, #No. of Boxes with proposal mass function &gt;= 1e-3 230, &quot;after Refine &quot;, &quot;before Rej..SampleMany &quot;, n_samples: 100, &quot;after Rej..SampleMany &quot;, rs_sample IU, N, Nrs: 2.903136091244425E+197 100 100, RSSampleMany, integral est: 1.2113e+193, &quot;RSSampleMany mean: &quot;, &quot;   Number of labels or topologies = 2&quot;, label: 0  proportion:    0.970000000000000, Labelled Mean:, &quot;   0.500284830473953&quot;, &quot;&quot;, label: 1  proportion:    0.030000000000000, Labelled Mean:, &quot;   0.510581317236150&quot;, &quot;   0.489823533395186&quot;, &quot;&quot;, n interval function calls: 1998, n real function calls: 2396703, # CPU Time (seconds). Partitioning: 0.005981  Sampling: 0.70349  Total: 0.709471, # CPU time (secods) per estimate: 0.00709471)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Thus by mixing several different executibles that are statically compiled for linux 64 bit machine, we can mix and match multiple executibles with appropriate inputs.</p>
<p>The resulting outputs can themselves be re-processed in Spark to feed into toher pipedRDDs or normal RDDs or DataFrames and DataSets.</p>
</div>
<div class="cell markdown">
<p>Finally, we can have more than one command per partition and then use <code>mapPartitions</code> to send all the executible commands within the input partition that is to be run by the executor in which that partition resides as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-scala">val isIt1or2StaticExecutible = &quot;/dbfs/FileStore/tables/IsIt1or2Coins&quot;

// let us make 2 commands in each of the 2 input partitions
val same_input_mp = Seq(s&quot;$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1&quot;, 
                        s&quot;$isIt1or2StaticExecutible 1000 100 123456789 1000 500 1200 600 1&quot;,
                        s&quot;$isIt1or2StaticExecutible 1000 100 123456789 1000 500 1200 600 1&quot;,
                        s&quot;$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1&quot;)

val same_output_mp = sc
  .parallelize(same_input)
  .repartition(2)
  .pipe(&quot;bash&quot;)
  .mapPartitions(x =&gt; Seq(x.mkString(&quot;\n&quot;)).iterator)
  .collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>isIt1or2StaticExecutible: String = /dbfs/FileStore/tables/IsIt1or2Coins
same_input_mp: Seq[String] = List(/dbfs/FileStore/tables/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1, /dbfs/FileStore/tables/IsIt1or2Coins 1000 100 123456789 1000 500 1200 600 1, /dbfs/FileStore/tables/IsIt1or2Coins 1000 100 123456789 1000 500 1200 600 1, /dbfs/FileStore/tables/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1)
same_output_mp: Array[String] =
Array(theSeed: 234565432
N1: 1000
n1: 500
N2: 1200
n2: 600
UseLogPi: 1
  n_boxes: 1000  n_samples: 100  rng_seed = 234565432
  N1=number of first coin tosses          : 1000
  n1=number of heads in first coin tosses : 500
  N2=number of second coin tosses         : 1200
  n2=number of heads in second coin tosses: 600
Ldomain.L: 0
Ldomain.L: 1
end of FIsIt1or2Coins constructor.
in FirstBox, before getBoxREInfo. k: 0
0 [1.000000000000000E-300,   1.000000000000000] RE: [-1.519706161376072E+006,   0.000000000000000] BoxIntegral: [-1.519706161376072E+006,   0.000000000000000]
in FirstBox, after getBoxREInfo
0 [1.000000000000000E-300,   1.000000000000000]
in FirstBox, before getBoxREInfo. k: 1
1 [1.000000000000000E-300,   1.000000000000000] [1.000000000000000E-300,   1.000000000000000] RE: [-1.519706161376072E+006,   0.000000000000000] BoxIntegral: [-1.519706161376072E+006,   0.000000000000000]
in FirstBox, after getBoxREInfo
1 [1.000000000000000E-300,   1.000000000000000] [1.000000000000000E-300,   1.000000000000000]
Umax:    0.000000000000000
UmaxMAX, Umax, f_scale_local: 1e+200    0.000000000000000 -460.517018598809159
f_scale: -460.517018598809159  -460.517018598809159
bottom of updateUmax
in FirstBox, after updateUmax
bottom of FirstBox.
after FirstBox, before Refine
Umax: -1.512624733218932E+003
UmaxMAX, Umax, f_scale_local: 1e+200 -1.512624733218932E+003 -1.973141751817741E+003
f_scale: -1.973141751817741E+003  -1.973141751817741E+003
bottom of updateUmax
in AdaptPartition after updateUmax2
in updateIntegral. IL, IU:    0.000000000000000 1.000000000000362E+200
# Adaptive partitioning complete. Boxes: 1000  Lower bound on Acceptance Prob.: 4.88326e-07 IL, IU: 1.41768e+191   2.90314e+197
#Using log(pi)? 1
#No. of Boxes with proposal mass function &lt;= 1e-16 48
#No. of Boxes with proposal mass function &lt;= 1e-10 112
#No. of Boxes with proposal mass function &gt;= 1e-6 776
#No. of Boxes with proposal mass function &gt;= 1e-3 230
after Refine
before Rej..SampleMany
n_samples: 100
after Rej..SampleMany
rs_sample IU, N, Nrs: 2.903136091244425E+197 100 100
RSSampleMany, integral est: 1.2113e+193
RSSampleMany mean:
   Number of labels or topologies = 2
label: 0  proportion:    0.970000000000000
Labelled Mean:
   0.500284830473953

label: 1  proportion:    0.030000000000000
Labelled Mean:
   0.510581317236150
   0.489823533395186

n interval function calls: 1998
n real function calls: 2396703
# CPU Time (seconds). Partitioning: 0.006053  Sampling: 0.71035  Total: 0.716403
# CPU time (secods) per estimate: 0.00716403, theSeed: 234565432
N1: 1000
n1: 500
N2: 1200
n2: 600
UseLogPi: 1
  n_boxes: 1000  n_samples: 100  rng_seed = 234565432
  N1=number of first coin tosses          : 1000
  n1=number of heads in first coin tosses : 500
  N2=number of second coin tosses         : 1200
  n2=number of heads in second coin tosses: 600
Ldomain.L: 0
Ldomain.L: 1
end of FIsIt1or2Coins constructor.
in FirstBox, before getBoxREInfo. k: 0
0 [1.000000000000000E-300,   1.000000000000000] RE: [-1.519706161376072E+006,   0.000000000000000] BoxIntegral: [-1.519706161376072E+006,   0.000000000000000]
in FirstBox, after getBoxREInfo
0 [1.000000000000000E-300,   1.000000000000000]
in FirstBox, before getBoxREInfo. k: 1
1 [1.000000000000000E-300,   1.000000000000000] [1.000000000000000E-300,   1.000000000000000] RE: [-1.519706161376072E+006,   0.000000000000000] BoxIntegral: [-1.519706161376072E+006,   0.000000000000000]
in FirstBox, after getBoxREInfo
1 [1.000000000000000E-300,   1.000000000000000] [1.000000000000000E-300,   1.000000000000000]
Umax:    0.000000000000000
UmaxMAX, Umax, f_scale_local: 1e+200    0.000000000000000 -460.517018598809159
f_scale: -460.517018598809159  -460.517018598809159
bottom of updateUmax
in FirstBox, after updateUmax
bottom of FirstBox.
after FirstBox, before Refine
Umax: -1.512624733218932E+003
UmaxMAX, Umax, f_scale_local: 1e+200 -1.512624733218932E+003 -1.973141751817741E+003
f_scale: -1.973141751817741E+003  -1.973141751817741E+003
bottom of updateUmax
in AdaptPartition after updateUmax2
in updateIntegral. IL, IU:    0.000000000000000 1.000000000000362E+200
# Adaptive partitioning complete. Boxes: 1000  Lower bound on Acceptance Prob.: 4.88326e-07 IL, IU: 1.41768e+191   2.90314e+197
#Using log(pi)? 1
#No. of Boxes with proposal mass function &lt;= 1e-16 48
#No. of Boxes with proposal mass function &lt;= 1e-10 112
#No. of Boxes with proposal mass function &gt;= 1e-6 776
#No. of Boxes with proposal mass function &gt;= 1e-3 230
after Refine
before Rej..SampleMany
n_samples: 100
after Rej..SampleMany
rs_sample IU, N, Nrs: 2.903136091244425E+197 100 100
RSSampleMany, integral est: 1.2113e+193
RSSampleMany mean:
   Number of labels or topologies = 2
label: 0  proportion:    0.970000000000000
Labelled Mean:
   0.500284830473953

label: 1  proportion:    0.030000000000000
Labelled Mean:
   0.510581317236150
   0.489823533395186

n interval function calls: 1998
n real function calls: 2396703
# CPU Time (seconds). Partitioning: 0.006481  Sampling: 0.716364  Total: 0.722845
# CPU time (secods) per estimate: 0.00722845)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>allCatch is a useful tool to use as a filtering function when testing if a command will work without error.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import scala.util.control.Exception.allCatch
(allCatch opt &quot; 12 &quot;.trim.toLong).isDefined
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import scala.util.control.Exception.allCatch
res13: Boolean = true
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>The following should only be done after you have been introduced to Notebook: <code>033_OBO_PipedRDD_RigorousBayesianABTesting</code> and the Old Bailey Online Data.</p>
<p><strong>TODO</strong>: The below needs redo with DBFS FUSE.</p>
</div>
<div class="cell markdown">
<h1 id="parsing-the-output-from-isit1or2coins"><a class="header" href="#parsing-the-output-from-isit1or2coins">Parsing the output from <code>IsIt1or2Coins</code></a></h1>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">/**
 * Returns the label proportions from the output of IsIt1or2Coins
 *
 * This function takes an array of Strings, where each element
 * contains the whole output of one execution of IsIt1or2Coins.
 * It returns an Array[Array[Double]], where the first index denotes which
 * execution it belonged to, the second index is whether it is label0 or label1
 * i.e. it is a numExec x 2 array.
 */

def getLabelProps(input : Array[String]):Array[Array[Double]] = {
  input
  .map(out =&gt; out
       .split(&quot;\n&quot;)
       .filter(line =&gt; line.contains(&quot;label:&quot;))
       .map(filtLine =&gt; filtLine
            .split(&quot; &quot;)
            .filter(line =&gt; (allCatch opt line.trim.toDouble).isDefined)
            .map(filtFiltLine =&gt; filtFiltLine.toDouble)))
  .map(trial =&gt; trial.map(labels =&gt; labels(1)))
}

/**
 * Returns the label means from the output of IsIt1or2Coins
 *
 * This function takes an array of Strings, where each element
 * contains the whole output of one execution of IsIt1or2Coins.
 * It returns an Array[Array[Double]], where the first index denotes which
 * execution it belonged to, the second index is whether it is label0 Mean 
 * or label1 Mean1 or label1 Mean2, i.e. it is a numExec x 3 array.
 */

def getLabelMeans(input : Array[String]):Array[Array[Double]] = {
  val output_pre = input
  .map(out =&gt; out.split(&quot;\n&quot;)
       .filter(line =&gt; (allCatch opt line.trim.toDouble).isDefined))
  .map(arr =&gt; arr.map(num =&gt; num.toDouble))
  // Some runs have such a low probability for a label that some end up being only 2 in length instead of three
  // That means we should pad with a 0 to fix this
  output_pre.map(trial =&gt; if (trial.length == 2) Array(0,trial(0),trial(1)) else trial)
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>getLabelProps: (input: Array[String])Array[Array[Double]]
getLabelMeans: (input: Array[String])Array[Array[Double]]
</code></pre>
</div>
</div>
<div class="cell markdown">
<h1 id="providing-case-classes-for-input-and-output-for-easy-spark-communication"><a class="header" href="#providing-case-classes-for-input-and-output-for-easy-spark-communication">Providing case classes for input and output for easy spark communication</a></h1>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.sql.DataFrame

case class OutputRow(ID:Long, 
                     NumTosses1:Int, NumHeads1:Int,
                     NumTosses2:Int, NumHeads2:Int,
                     Label0Prob:Double,
                     Label1Prob:Double,
                     Label0Mean0:Double, Label0Mean1:Double, 
                     Label1Mean0:Double, Label1Mean1:Double)

case class InputOpts(ID:Long,NumBoxes:Int, NumIter:Int, Seed:Long, 
                     NumTosses1:Int, NumHeads1:Int,
                     NumTosses2:Int, NumHeads2:Int, 
                     LogScaling:Int) {
  def toExecutableString:String = {
    &quot;/tmp/IsIt1or2Coins &quot;+Array(NumBoxes, NumIter, Seed, NumTosses1, NumHeads1, NumTosses2, NumHeads2, LogScaling).mkString(&quot; &quot;)
  }
}

/**
 * Returns the result of running all trials in the array of InputOpts
 *
 * This function takes an Array[InputOpts], creates executable strings
 * via suckAs and runs all in a pipedRDD, after it creates this it will parse the output
 * and assemble it into the case class OutputRow, i.e. it returns an Array of OutputRow
 */

def execute(trials : Array[InputOpts]) = {
  sc.parallelize(trials.map(trial =&gt; trial.toExecutableString))
    .repartition(trials.length)
    .pipe(&quot;/tmp/suckAs.sh&quot;)
    .mapPartitions(x =&gt; Seq(x.mkString(&quot;\n&quot;).split(&quot;theSeed&quot;)).iterator) // We know that theSeed is included once in every output
    .collect
    .flatMap(x =&gt; x.filter(y =&gt; y.length &gt; 0)) // Since each collection of outputs are split at theSeed we need to remove all empty strings and flatmap
}

def parseOutput(trials: Array[InputOpts], res:Array[String]) = {
  val labProp = getLabelProps(res)
  val labMean = getLabelMeans(res)
  (trials zip labProp zip labMean)
  .map(trial =&gt; (trial._1._1,trial._1._2,trial._2))
  .map(trial =&gt; OutputRow(trial._1.ID,
                          trial._1.NumTosses1,trial._1.NumHeads1,
                          trial._1.NumTosses2,trial._1.NumHeads2,
                          trial._2(0),trial._2(1),
                          trial._3(0),trial._3(0),
                          trial._3(1),trial._3(2)
                         ))
}

/* Returns a DataFrame of the Array[OutputRow] for ease of displaying in Databricks */

def resultAsDF(result:Array[OutputRow]):DataFrame = {
  sc.parallelize(result).toDF
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.sql.DataFrame
defined class OutputRow
defined class InputOpts
execute: (trials: Array[InputOpts])Array[String]
parseOutput: (trials: Array[InputOpts], res: Array[String])Array[OutputRow]
resultAsDF: (result: Array[OutputRow])org.apache.spark.sql.DataFrame
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val inputOpts = Array(InputOpts(1680,1000,1000,100,792,245,151,63,1), InputOpts(1690,1000,1000,100,1805,526,215,68,1), InputOpts(1700,1000,1000,100,1060,242,57,26,1),InputOpts(1710,1000,1000,100,1060,243,57,26,1),InputOpts(1720,1000,1000,100,1060,245,57,26,1),InputOpts(1730,1000,1000,100,1060,245,57,26,1))
val res = execute(inputOpts)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(resultAsDF(parseOutput(inputOpts,res)))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="../../contents/000_2-sds-3-x-ml/033_OBO_LoadExtract.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="../../contents/000_2-sds-3-x-ml/034_LDA_20NewsGroupsSmall.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="../../contents/000_2-sds-3-x-ml/033_OBO_LoadExtract.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a rel="next" href="../../contents/000_2-sds-3-x-ml/034_LDA_20NewsGroupsSmall.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        
        <!-- Livereload script (if served using the cli tool) -->
        <script type="text/javascript">
            var socket = new WebSocket("ws://localhost:3000/__livereload");
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>
        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="../../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="../../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>
