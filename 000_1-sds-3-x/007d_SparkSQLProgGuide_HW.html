
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Data Sources &#8212; Scalable Data Science &amp; Distributed Machine Learning</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Performance Tuning" href="007e_SparkSQLProgGuide_HW.html" />
    <link rel="prev" title="Getting Started - Exercise" href="007c_SparkSQLProgGuide_HW.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Scalable Data Science & Distributed Machine Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="000_ScaDaMaLe.html">
   Introduction
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction to Apache Spark Core
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="001_whySpark.html">
   Why Apache Spark?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002_00_loginToDatabricks.html">
   databricks community edition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002_00_loginToDatabricks.html#essentials-of-databricks-cloud-dbc-in-a-big-hurry">
   Essentials of Databricks Cloud (DBC) in a Big Hurry
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002_00_loginToDatabricks.html#dbc-essentials-what-is-databricks-cloud">
   DBC Essentials: What is Databricks Cloud?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002_00_loginToDatabricks.html#dbc-essentials-shard-cluster-notebook-and-dashboard">
   DBC Essentials: Shard, Cluster, Notebook and Dashboard
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002_00_loginToDatabricks.html#dbc-essentials-team-state-collaboration-elastic-resources">
   DBC Essentials: Team, State, Collaboration, Elastic Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002_00_loginToDatabricks.html#you-should-all-have-databricks-community-edition-account-by-now">
   You Should All Have databricks community edition account by now!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002_00_loginToDatabricks.html#import-course-content-now">
   Import Course Content Now!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002_00_loginToDatabricks.html#cloud-free-computing-environment-optional-but-recommended">
   Cloud-free Computing Environment (Optional but recommended)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002_01_multiLingualNotebooks.html">
   Notebooks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002_01_multiLingualNotebooks.html#further-reference-homework-recurrrent-points-of-reference">
   Further Reference / Homework / Recurrrent Points of Reference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_00_scalaCrashCourse.html">
   Introduction to Scala through Scala Notebook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_00_scalaCrashCourse.html#scala-in-your-own-computer">
   Scala in your own computer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_00_scalaCrashCourse.html#scala-resources">
   Scala Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_00_scalaCrashCourse.html#introduction-to-scala">
   Introduction to Scala
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_00_scalaCrashCourse.html#let-s-get-our-hands-dirty-in-scala">
   Let’s get our hands dirty in Scala
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_00_scalaCrashCourse.html#scala-types">
   Scala Types
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_00_scalaCrashCourse.html#expressions">
   Expressions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_00_scalaCrashCourse.html#blocks">
   Blocks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_00_scalaCrashCourse.html#functions">
   Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_00_scalaCrashCourse.html#methods">
   Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_00_scalaCrashCourse.html#classes">
   Classes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_00_scalaCrashCourse.html#case-classes">
   Case Classes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_00_scalaCrashCourse.html#objects">
   Objects
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_00_scalaCrashCourse.html#traits">
   Traits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_00_scalaCrashCourse.html#main-method">
   Main Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_00_scalaCrashCourse.html#what-i-try-not-do-while-learning-a-new-language">
   What I try not do while learning a new language?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_01_scalaCrashCourse.html">
   Scala Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_01_scalaCrashCourse.html#let-s-continue-to-get-our-hands-dirty-in-scala">
   Let’s continue to get our hands dirty in Scala
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_01_scalaCrashCourse.html#scala-type-hierarchy">
   Scala Type Hierarchy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_01_scalaCrashCourse.html#scala-collections">
   Scala Collections
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_01_scalaCrashCourse.html#exercise-in-functional-programming">
   Exercise in Functional Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_01_scalaCrashCourse.html#lazy-evaluation">
   Lazy Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003_01_scalaCrashCourse.html#recursions">
   Recursions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="004_RDDsTransformationsActions.html">
   Introduction to Spark
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="004_RDDsTransformationsActions.html#spark-cluster-overview">
   Spark Cluster Overview:
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="005_RDDsTransformationsActionsHOMEWORK.html">
   HOMEWORK notebook - RDDs Transformations and Actions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="006a_PipedRDD.html">
   Piped RDDs and Bayesian AB Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="006a_PipedRDD.html#from-a-local-collection">
   From a Local Collection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="006a_PipedRDD.html#glom">
   glom
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="006a_PipedRDD.html#checkpointing">
   Checkpointing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="006a_PipedRDD.html#pipe-rdds-to-system-commands">
   Pipe RDDs to System Commands
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="006a_PipedRDD.html#mappartitions">
   mapPartitions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="006a_PipedRDD.html#foreachpartition">
   foreachPartition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="006a_PipedRDD.html#numerically-rigorous-bayesian-ab-testing">
   Numerically Rigorous Bayesian AB Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="006a_PipedRDD.html#usage-instructions-for-isit1or2coins">
   Usage instructions for IsIt1or2Coins
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="006_WordCount.html">
   Word Count on US State of the Union (SoU) Addresses
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction to Apache Spark SQL
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="007_SparkSQLIntroBasics.html">
   Introduction to Spark SQL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007_SparkSQLIntroBasics.html#overview">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007_SparkSQLIntroBasics.html#datasets-and-dataframes">
   Datasets and DataFrames
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007_SparkSQLIntroBasics.html#getting-started-in-spark-2-x">
   Getting Started in Spark 2.x
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007a_SparkSQLProgGuide_HW.html">
   Spark Sql Programming Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007b_SparkSQLProgGuide_HW.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007b_SparkSQLProgGuide_HW.html#id1">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007c_SparkSQLProgGuide_HW.html">
   Getting Started - Exercise
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Data Sources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="#id1">
   Data Sources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007e_SparkSQLProgGuide_HW.html">
   Performance Tuning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007f_SparkSQLProgGuide_HW.html">
   Distributed SQL Engine
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007f_SparkSQLProgGuide_HW.html#id1">
   Distributed SQL Engine
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007g_PivotInSQL.html">
   ScaDaMaLe, Scalable Data Science and Distributed Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007g_PivotInSQL.html#sql-pivoting-since-spark-2-4">
   SQL Pivoting since Spark 2.4
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007g_PivotInSQL.html#load-data">
   Load Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007g_PivotInSQL.html#pivoting-in-sql">
   Pivoting in SQL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007g_PivotInSQL.html#pivoting-with-multiple-aggregate-expressions">
   Pivoting with Multiple Aggregate Expressions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007g_PivotInSQL.html#pivoting-with-multiple-grouping-columns">
   Pivoting with Multiple Grouping Columns
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="007g_PivotInSQL.html#pivoting-with-multiple-pivot-columns">
   Pivoting with Multiple Pivot Columns
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="008_DiamondsPipeline_01ETLEDA.html">
   Diamonds ML Pipeline Workflow - DataFrame ETL and EDA Part
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="008_DiamondsPipeline_01ETLEDA.html#step-1-load-data-as-dataframe">
   Step 1. Load data as DataFrame
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="008_DiamondsPipeline_01ETLEDA.html#step-2-understand-the-data">
   Step 2. Understand the data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="008_DiamondsPipeline_01ETLEDA.html#let-us-run-through-some-basic-inteactive-sql-queries-next">
   Let us run through some basic inteactive SQL queries next
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="008_DiamondsPipeline_01ETLEDA.html#why-do-we-need-to-know-these-interactive-sql-queries">
   Why do we need to know these interactive SQL queries?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="008_DiamondsPipeline_01ETLEDA.html#we-will-continue-later-with-ml-pipelines-to-do-prediction-with-a-fitted-model-from-this-dataset">
   We will continue later with ML pipelines to do prediction with a fitted model from this dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="009_PowerPlantPipeline_01ETLEDA.html">
   Power Plant ML Pipeline Application - DataFrame Part
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="009_PowerPlantPipeline_01ETLEDA.html#step-1-business-understanding">
   Step 1: Business Understanding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="009_PowerPlantPipeline_01ETLEDA.html#step-2-load-your-data">
   Step 2: Load Your Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="009_PowerPlantPipeline_01ETLEDA.html#use-this-for-other-smallish-datasets-you-want-to-import-to-your-ce">
   USE THIS FOR OTHER SMALLish DataSets you want to import to your CE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="009_PowerPlantPipeline_01ETLEDA.html#step-3-explore-your-data">
   Step 3: Explore Your Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="009_PowerPlantPipeline_01ETLEDA.html#step-4-visualize-your-data">
   Step 4: Visualize Your Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="010_wikipediaClickStream_01ETLEDA.html">
   Wiki Clickstream Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="010_wikipediaClickStream_01ETLEDA.html#wikipedia-logo">
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="033_OBO_LoadExtract.html">
   Old Bailey Online Data Analysis in Apache Spark
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="033_OBO_LoadExtract.html#analysing-the-full-old-bailey-online-sessions-papers-dataset">
   Analysing the Full Old Bailey Online Sessions Papers Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="033_OBO_PipedRDD_RigorousBayesianABTesting.html">
   Piped RDDs and Bayesian AB Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="033_OBO_PipedRDD_RigorousBayesianABTesting.html#from-a-local-collection">
   From a Local Collection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="033_OBO_PipedRDD_RigorousBayesianABTesting.html#glom">
   glom
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="033_OBO_PipedRDD_RigorousBayesianABTesting.html#checkpointing">
   Checkpointing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="033_OBO_PipedRDD_RigorousBayesianABTesting.html#pipe-rdds-to-system-commands">
   Pipe RDDs to System Commands
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="033_OBO_PipedRDD_RigorousBayesianABTesting.html#mappartitions">
   mapPartitions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="033_OBO_PipedRDD_RigorousBayesianABTesting.html#foreachpartition">
   foreachPartition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="033_OBO_PipedRDD_RigorousBayesianABTesting.html#numerically-rigorous-bayesian-ab-testing">
   Numerically Rigorous Bayesian AB Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="033_OBO_PipedRDD_RigorousBayesianABTesting.html#usage-instructions-for-isit1or2coins">
   Usage instructions for IsIt1or2Coins
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="033_OBO_PipedRDD_RigorousBayesianABTesting.html#parsing-the-output-from-isit1or2coins">
   Parsing the output from
   <code class="docutils literal notranslate">
    <span class="pre">
     IsIt1or2Coins
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="033_OBO_PipedRDD_RigorousBayesianABTesting.html#providing-case-classes-for-input-and-output-for-easy-spark-communication">
   Providing case classes for input and output for easy spark communication
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="035_LDA_CornellMovieDialogs.html">
   Topic Modeling of Movie Dialogs with Latent Dirichlet Allocation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Distribute Deep Learning with Horovod
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../000_7-sds-3-x-ddl/00_DDL_Introduction.html">
   Introduction to Distributed Deep Learning (DDL) with Horovod over Tensorflow/keras and Pytorch
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../000_7-sds-3-x-ddl/0x_mnist-tensorflow-keras.html">
     Distributed deep learning training using TensorFlow and Keras with HorovodRunner
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../000_7-sds-3-x-ddl/0y_mnist-pytorch.html">
     Distributed deep learning training using PyTorch with HorovodRunner for MNIST
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Voluntary Student Projects
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="030_Spark_GDELT_project.html">
   Plugging into GDELT Streams - TODO - IN PROGRESS
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="030_Spark_GDELT_project.html#this-is-just-dipping-our-pinky-toe-in-this-ocean-of-information">
   This is just dipping our pinky toe in this ocean of information!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="030_Spark_GDELT_project.html#download-from-gdelt-project">
   Download from gdelt-project
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/000_1-sds-3-x/007d_SparkSQLProgGuide_HW.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://lamastex.github.io/ScaDaMaLe/index.html"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://lamastex.github.io/ScaDaMaLe/index.html/issues/new?title=Issue%20on%20page%20%2F000_1-sds-3-x/007d_SparkSQLProgGuide_HW.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Data Sources
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spark-sql-programming-guide">
     Spark Sql Programming Guide
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Data Sources
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overview">
     Overview
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#third-party-datasource-packages">
     Third-party datasource packages
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generic-load-save-functions">
     Generic Load/Save functions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#manually-specifying-options">
       Manually Specifying Options
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#run-sql-on-files-directly">
       Run SQL on files directly
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#save-modes">
       Save Modes
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#saving-to-persistent-tables">
       Saving to Persistent Tables
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parquet-files">
     Parquet Files
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#more-on-parquet">
       More on Parquet
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#loading-data-programmatically">
       Loading Data Programmatically
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bucketing-sorting-and-partitioning">
     Bucketing, Sorting and Partitioning
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#partition-discovery">
       Partition Discovery
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#schema-merging">
       Schema Merging
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#hive-metastore-parquet-table-conversion">
       Hive metastore Parquet table conversion
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#hive-parquet-schema-reconciliation">
         Hive/Parquet Schema Reconciliation
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#metadata-refreshing">
         Metadata Refreshing
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#configuration">
       Configuration
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#json-datasets">
     JSON Datasets
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hive-tables">
     Hive Tables
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#interacting-with-different-versions-of-hive-metastore">
       Interacting with Different Versions of Hive Metastore
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jdbc-to-other-databases">
     JDBC To Other Databases
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#troubleshooting">
       Troubleshooting
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p>ScaDaMaLe Course
<a class="reference external" href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and
<a class="reference external" href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
<p>This is an elaboration of the
<a class="reference external" href="http://spark.apache.org/docs/latest/sql-programming-guide.html">http://spark.apache.org/docs/latest/sql-programming-guide.html</a> by Ivan
Sadikov and Raazesh Sainudiin.</p>
<div class="section" id="data-sources">
<h1>Data Sources<a class="headerlink" href="#data-sources" title="Permalink to this headline">¶</a></h1>
<div class="section" id="spark-sql-programming-guide">
<h2>Spark Sql Programming Guide<a class="headerlink" href="#spark-sql-programming-guide" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Data Sources</p>
<ul>
<li><p>Generic Load/Save Functions</p>
<ul>
<li><p>Manually Specifying Options</p></li>
<li><p>Run SQL on files directly</p></li>
<li><p>Save Modes</p></li>
<li><p>Saving to Persistent Tables</p></li>
</ul>
</li>
<li><p>Parquet Files</p>
<ul>
<li><p>Loading Data Programmatically</p></li>
<li><p>Partition Discovery</p></li>
<li><p>Schema Merging</p></li>
<li><p>Hive metastore Parquet table conversion</p>
<ul>
<li><p>Hive/Parquet Schema Reconciliation</p></li>
<li><p>Metadata Refreshing</p></li>
</ul>
</li>
<li><p>Configuration</p></li>
</ul>
</li>
<li><p>JSON Datasets</p></li>
<li><p>Hive Tables</p>
<ul>
<li><p>Interacting with Different Versions of Hive Metastore</p></li>
</ul>
</li>
<li><p>JDBC To Other Databases</p></li>
<li><p>Troubleshooting</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="id1">
<h1>Data Sources<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>Spark SQL supports operating on a variety of data sources through the
<code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> or <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> interfaces. A Dataset can be operated on as
normal RDDs and can also be registered as a temporary table. Registering
a Dataset as a table allows you to run SQL queries over its data. But
from time to time you would need to either load or save Dataset. Spark
SQL provides built-in data sources as well as Data Source API to define
your own data source and use it read / write data into Spark.</p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Spark provides some built-in datasources that you can use straight out
of the box, such as <a class="reference external" href="https://parquet.apache.org/">Parquet</a>,
<a class="reference external" href="http://www.json.org/">JSON</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/Java_Database_Connectivity">JDBC</a>,
<a class="reference external" href="https://orc.apache.org/">ORC</a> (available with enabled Hive Support, but
this is changing, and ORC will not require Hive support and will work
with default Spark session starting from next release), and Text (since
Spark 1.6) and CSV (since Spark 2.0, before that it is accessible as a
package).</p>
</div>
<div class="section" id="third-party-datasource-packages">
<h2>Third-party datasource packages<a class="headerlink" href="#third-party-datasource-packages" title="Permalink to this headline">¶</a></h2>
<p>Community also have built quite a few datasource packages to provide
easy access to the data from other formats. You can find list of those
packages on http://spark-packages.org/, e.g.
<a class="reference external" href="http://spark-packages.org/package/databricks/spark-avro">Avro</a>,
<a class="reference external" href="http://spark-packages.org/package/databricks/spark-csv">CSV</a>, <a class="reference external" href="http://spark-packages.org/package/databricks/spark-redshift">Amazon
Redshit</a>
(for Spark &lt; 2.0),
<a class="reference external" href="http://spark-packages.org/package/HyukjinKwon/spark-xml">XML</a>,
<a class="reference external" href="http://spark-packages.org/package/sadikovi/spark-netflow">NetFlow</a> and
many others.</p>
</div>
<div class="section" id="generic-load-save-functions">
<h2>Generic Load/Save functions<a class="headerlink" href="#generic-load-save-functions" title="Permalink to this headline">¶</a></h2>
<p>In order to load or save DataFrame you have to call either <code class="docutils literal notranslate"><span class="pre">read</span></code> or
<code class="docutils literal notranslate"><span class="pre">write</span></code>. This will return
<a class="reference external" href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader">DataFrameReader</a>
or
<a class="reference external" href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter">DataFrameWriter</a>
depending on what you are trying to achieve. Essentially these classes
are entry points to the reading / writing actions. They allow you to
specify writing mode or provide additional options to read data source.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">This</span> <span class="n">will</span> <span class="k">return</span> <span class="n">DataFrameReader</span> <span class="n">to</span> <span class="n">read</span> <span class="n">data</span> <span class="n">source</span>
<span class="n">println</span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="p">)</span>

<span class="n">val</span> <span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="o">//</span> <span class="n">This</span> <span class="n">will</span> <span class="k">return</span> <span class="n">DataFrameWriter</span> <span class="n">to</span> <span class="n">save</span> <span class="n">DataFrame</span>
<span class="n">println</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">Saving</span> <span class="n">Parquet</span> <span class="n">table</span> <span class="ow">in</span> <span class="n">Scala</span>
<span class="o">//</span> <span class="n">DataFrames</span> <span class="ow">and</span> <span class="n">tables</span> <span class="n">can</span> <span class="n">be</span> <span class="n">saved</span> <span class="k">as</span> <span class="n">Parquet</span> <span class="n">files</span><span class="p">,</span> <span class="n">maintaining</span> <span class="n">the</span> <span class="n">schema</span> <span class="n">information</span>
<span class="n">val</span> <span class="n">df_save</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;social_media_usage&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;platform&quot;</span><span class="p">,</span> <span class="s2">&quot;visits&quot;</span><span class="p">)</span> <span class="o">//</span> <span class="n">assuming</span> <span class="n">you</span> <span class="n">made</span> <span class="n">the</span> <span class="n">social_media_usage</span> <span class="n">table</span> <span class="n">permanent</span> <span class="ow">in</span> <span class="n">previous</span> <span class="n">notebook</span>
<span class="n">df_save</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&quot;overwrite&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;/tmp/platforms.parquet&quot;</span><span class="p">)</span>

<span class="o">//</span> <span class="n">Read</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">parquet</span> <span class="n">file</span> <span class="n">created</span> <span class="n">above</span>
<span class="o">//</span> <span class="n">Parquet</span> <span class="n">files</span> <span class="n">are</span> <span class="bp">self</span><span class="o">-</span><span class="n">describing</span> <span class="n">so</span> <span class="n">the</span> <span class="n">schema</span> <span class="ow">is</span> <span class="n">preserved</span>
<span class="o">//</span> <span class="n">The</span> <span class="n">result</span> <span class="n">of</span> <span class="n">loading</span> <span class="n">a</span> <span class="n">Parquet</span> <span class="n">file</span> <span class="ow">is</span> <span class="n">also</span> <span class="n">a</span> <span class="n">DataFrame</span>
<span class="n">val</span> <span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;/tmp/platforms.parquet&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="ow">in</span> <span class="n">databricks</span> <span class="s1">&#39;/tmp/...&#39;</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">same</span> <span class="k">as</span> <span class="s1">&#39;dbfs:///tmp/...&#39;</span>
<span class="n">display</span><span class="p">(</span><span class="n">dbutils</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">ls</span><span class="p">(</span><span class="s2">&quot;/tmp/&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">dbutils</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">ls</span><span class="p">(</span><span class="s2">&quot;/tmp/platforms.parquet/&quot;</span><span class="p">))</span> <span class="o">//</span> <span class="n">note</span> <span class="n">this</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">directory</span> <span class="k">with</span> <span class="n">many</span> <span class="n">files</span> <span class="ow">in</span> <span class="n">it</span><span class="o">...</span> <span class="n">files</span> <span class="n">beginning</span> <span class="k">with</span> <span class="n">part</span> <span class="n">have</span> <span class="n">content</span> <span class="ow">in</span> <span class="n">possibly</span> <span class="n">many</span> <span class="n">partitions</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Loading Parquet table in Python</span>
<span class="n">dfPy</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;/tmp/platforms.parquet&quot;</span><span class="p">)</span>
<span class="n">dfPy</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">Saving</span> <span class="n">JSON</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="n">Scala</span>
<span class="n">val</span> <span class="n">df_save</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;social_media_usage&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;platform&quot;</span><span class="p">,</span> <span class="s2">&quot;visits&quot;</span><span class="p">)</span>
<span class="n">df_save</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&quot;overwrite&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s2">&quot;/tmp/platforms.json&quot;</span><span class="p">)</span>

<span class="o">//</span> <span class="n">Loading</span> <span class="n">JSON</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="n">Scala</span>
<span class="n">val</span> <span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s2">&quot;/tmp/platforms.json&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Loading JSON dataset in Python</span>
<span class="n">dfPy</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s2">&quot;/tmp/platforms.json&quot;</span><span class="p">)</span>
<span class="n">dfPy</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="manually-specifying-options">
<h3>Manually Specifying Options<a class="headerlink" href="#manually-specifying-options" title="Permalink to this headline">¶</a></h3>
<p>You can also manually specify the data source that will be used along
with any extra options that you would like to pass to the data source.
Data sources are specified by their fully qualified name (i.e.,
<code class="docutils literal notranslate"><span class="pre">org.apache.spark.sql.parquet</span></code>), but for built-in sources you can also
use their short names (<code class="docutils literal notranslate"><span class="pre">json</span></code>, <code class="docutils literal notranslate"><span class="pre">parquet</span></code>, <code class="docutils literal notranslate"><span class="pre">jdbc</span></code>). DataFrames of any
type can be converted into other types using this syntax.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">val</span> <span class="n">json</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;/tmp/platforms.json&quot;</span><span class="p">)</span>
<span class="n">json</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;platform&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="n">val</span> <span class="n">parquet</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;parquet&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;/tmp/platforms.parquet&quot;</span><span class="p">)</span>
<span class="n">parquet</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;platform&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="run-sql-on-files-directly">
<h3>Run SQL on files directly<a class="headerlink" href="#run-sql-on-files-directly" title="Permalink to this headline">¶</a></h3>
<p>Instead of using read API to load a file into DataFrame and query it,
you can also query that file directly with SQL.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">val</span> <span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM parquet.`/tmp/platforms.parquet`&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="save-modes">
<h3>Save Modes<a class="headerlink" href="#save-modes" title="Permalink to this headline">¶</a></h3>
<p>Save operations can optionally take a <code class="docutils literal notranslate"><span class="pre">SaveMode</span></code>, that specifies how to
handle existing data if present. It is important to realize that these
save modes do not utilize any locking and are not atomic. Additionally,
when performing a <code class="docutils literal notranslate"><span class="pre">Overwrite</span></code>, the data will be deleted before writing
out the new data.</p>
<p>| Scala/Java | Any language | Meaning | | — | — | — | |
<code class="docutils literal notranslate"><span class="pre">SaveMode.ErrorIfExists</span></code> (default) | <code class="docutils literal notranslate"><span class="pre">&quot;error&quot;</span></code> (default) | When saving a
DataFrame to a data source, if data already exists, an exception is
expected to be thrown. | | <code class="docutils literal notranslate"><span class="pre">SaveMode.Append</span></code> | <code class="docutils literal notranslate"><span class="pre">&quot;append&quot;</span></code> | When saving
a DataFrame to a data source, if data/table already exists, contents of
the DataFrame are expected to be appended to existing data. | |
<code class="docutils literal notranslate"><span class="pre">SaveMode.Overwrite</span></code> | <code class="docutils literal notranslate"><span class="pre">&quot;overwrite&quot;</span></code> | Overwrite mode means that when
saving a DataFrame to a data source, if data/table already exists,
existing data is expected to be overwritten by the contents of the
DataFrame. | | <code class="docutils literal notranslate"><span class="pre">SaveMode.Ignore</span></code> | <code class="docutils literal notranslate"><span class="pre">&quot;ignore&quot;</span></code> | Ignore mode means that
when saving a DataFrame to a data source, if data already exists, the
save operation is expected to not save the contents of the DataFrame and
to not change the existing data. This is similar to a
<code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">TABLE</span> <span class="pre">IF</span> <span class="pre">NOT</span> <span class="pre">EXISTS</span></code> in SQL. |</p>
</div>
<div class="section" id="saving-to-persistent-tables">
<h3>Saving to Persistent Tables<a class="headerlink" href="#saving-to-persistent-tables" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> and <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> can also be saved as persistent tables using
the <code class="docutils literal notranslate"><span class="pre">saveAsTable</span></code> command. Unlike the <code class="docutils literal notranslate"><span class="pre">createOrReplaceTempView</span></code> command,
<code class="docutils literal notranslate"><span class="pre">saveAsTable</span></code> will materialize the contents of the dataframe and create
a pointer to the data in the metastore. Persistent tables will still
exist even after your Spark program has restarted, as long as you
maintain your connection to the same metastore. A DataFrame for a
persistent table can be created by calling the <code class="docutils literal notranslate"><span class="pre">table</span></code> method on a
<code class="docutils literal notranslate"><span class="pre">SparkSession</span></code> with the name of the table.</p>
<p>By default <code class="docutils literal notranslate"><span class="pre">saveAsTable</span></code> will create a “managed table”, meaning that the
location of the data will be controlled by the metastore. Managed tables
will also have their data deleted automatically when a table is dropped.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">First</span> <span class="n">of</span> <span class="nb">all</span> <span class="nb">list</span> <span class="n">tables</span> <span class="n">to</span> <span class="n">see</span> <span class="n">that</span> <span class="n">table</span> <span class="n">we</span> <span class="n">are</span> <span class="n">about</span> <span class="n">to</span> <span class="n">create</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">exist</span>
<span class="n">spark</span><span class="o">.</span><span class="n">catalog</span><span class="o">.</span><span class="n">listTables</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">drop</span> <span class="n">table</span> <span class="k">if</span> <span class="n">exists</span> <span class="n">simple_range</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">val</span> <span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s2">&quot;simple_range&quot;</span><span class="p">)</span>

<span class="o">//</span> <span class="n">Verify</span> <span class="n">that</span> <span class="n">table</span> <span class="ow">is</span> <span class="n">saved</span> <span class="ow">and</span> <span class="n">it</span> <span class="ow">is</span> <span class="n">marked</span> <span class="k">as</span> <span class="n">persistent</span> <span class="p">(</span><span class="s2">&quot;isTemporary&quot;</span> <span class="n">value</span> <span class="n">should</span> <span class="n">be</span> <span class="s2">&quot;false&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">catalog</span><span class="o">.</span><span class="n">listTables</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="parquet-files">
<h2>Parquet Files<a class="headerlink" href="#parquet-files" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://parquet.io">Parquet</a> is a columnar format that is supported by
many other data processing systems. Spark SQL provides support for both
reading and writing Parquet files that automatically preserves the
schema of the original data. When writing Parquet files, all columns are
automatically converted to be nullable for compatibility reasons.</p>
<div class="section" id="more-on-parquet">
<h3>More on Parquet<a class="headerlink" href="#more-on-parquet" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://parquet.apache.org/">Apache Parquet</a> is a <a class="reference external" href="http://en.wikipedia.org/wiki/Column-oriented_DBMS">columnar
storage</a> format
available to any project in the Hadoop ecosystem, regardless of the
choice of data processing framework, data model or programming language.
It is a more efficient way to store data frames.</p>
<ul class="simple">
<li><p>To understand the ideas read <a class="reference external" href="http://research.google.com/pubs/pub36632.html">Dremel: Interactive Analysis of
Web-Scale Datasets, Sergey Melnik, Andrey Gubarev, Jing Jing Long,
Geoffrey Romer, Shiva Shivakumar, Matt Tolton and Theo
Vassilakis,Proc. of the 36th Int’l Conf on Very Large Data Bases
(2010), pp. 330-339</a>,
whose Abstract is as follows:</p>
<ul>
<li><p>Dremel is a scalable, interactive ad-hoc query system for
analysis of read-only nested data. By combining multi-level
execution trees and columnar data layouts it is <strong>capable of
running aggregation queries over trillion-row tables in
seconds</strong>. The system <strong>scales to thousands of CPUs and
petabytes of data, and has thousands of users at Google</strong>. In
this paper, we describe the architecture and implementation of
Dremel, and explain how it complements MapReduce-based
computing. We present a novel columnar storage representation
for nested records and discuss experiments on few-thousand node
instances of the system.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span><span class="n">This</span> <span class="n">allows</span> <span class="n">easy</span> <span class="n">embedding</span> <span class="n">of</span> <span class="n">publicly</span> <span class="n">available</span> <span class="n">information</span> <span class="n">into</span> <span class="nb">any</span> <span class="n">other</span> <span class="n">notebook</span>
<span class="o">//</span><span class="n">when</span> <span class="n">viewing</span> <span class="ow">in</span> <span class="n">git</span><span class="o">-</span><span class="n">book</span> <span class="n">just</span> <span class="n">ignore</span> <span class="n">this</span> <span class="n">block</span> <span class="o">-</span> <span class="n">you</span> <span class="n">may</span> <span class="n">have</span> <span class="n">to</span> <span class="n">manually</span> <span class="n">chase</span> <span class="n">the</span> <span class="n">URL</span> <span class="ow">in</span> <span class="n">frameIt</span><span class="p">(</span><span class="s2">&quot;URL&quot;</span><span class="p">)</span><span class="o">.</span>
<span class="o">//</span><span class="n">Example</span> <span class="n">usage</span><span class="p">:</span>
<span class="o">//</span> <span class="n">displayHTML</span><span class="p">(</span><span class="n">frameIt</span><span class="p">(</span><span class="s2">&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;</span><span class="p">,</span><span class="mi">250</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">frameIt</span><span class="p">(</span> <span class="n">u</span><span class="p">:</span><span class="n">String</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span><span class="n">Int</span> <span class="p">)</span> <span class="p">:</span> <span class="n">String</span> <span class="o">=</span> <span class="p">{</span>
      <span class="sd">&quot;&quot;&quot;&lt;iframe </span>
<span class="sd"> src=&quot;&quot;&quot;</span><span class="s2">&quot;+ u+&quot;&quot;&quot;&quot;</span>
<span class="s2"> width=&quot;95%&quot; height=&quot;&quot;&quot;&quot; + h + &quot;&quot;&quot;&quot;</span>
<span class="s2"> sandbox&gt;</span>
<span class="s2">  &lt;p&gt;</span>
<span class="s2">    &lt;a href=&quot;http://spark.apache.org/docs/latest/index.html&quot;&gt;</span>
<span class="s2">      Fallback link for browsers that, unlikely, don&#39;t support frames</span>
<span class="s2">    &lt;/a&gt;</span>
<span class="s2">  &lt;/p&gt;</span>
<span class="s2">&lt;/iframe&gt;&quot;&quot;&quot;</span>
   <span class="p">}</span>
<span class="n">displayHTML</span><span class="p">(</span><span class="n">frameIt</span><span class="p">(</span><span class="s2">&quot;https://parquet.apache.org/documentation/latest/&quot;</span><span class="p">,</span><span class="mi">500</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="loading-data-programmatically">
<h3>Loading Data Programmatically<a class="headerlink" href="#loading-data-programmatically" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">Read</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">parquet</span> <span class="n">file</span> <span class="n">created</span> <span class="n">above</span><span class="o">.</span> <span class="n">Parquet</span> <span class="n">files</span> <span class="n">are</span> <span class="bp">self</span><span class="o">-</span><span class="n">describing</span> <span class="n">so</span> <span class="n">the</span> <span class="n">schema</span> <span class="ow">is</span> <span class="n">preserved</span><span class="o">.</span>
<span class="o">//</span> <span class="n">The</span> <span class="n">result</span> <span class="n">of</span> <span class="n">loading</span> <span class="n">a</span> <span class="n">Parquet</span> <span class="n">file</span> <span class="ow">is</span> <span class="n">also</span> <span class="n">a</span> <span class="n">DataFrame</span><span class="o">.</span>
<span class="n">val</span> <span class="n">parquetFile</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;/tmp/platforms.parquet&quot;</span><span class="p">)</span>

<span class="o">//</span> <span class="n">Parquet</span> <span class="n">files</span> <span class="n">can</span> <span class="n">also</span> <span class="n">be</span> <span class="n">registered</span> <span class="k">as</span> <span class="n">tables</span> <span class="ow">and</span> <span class="n">then</span> <span class="n">used</span> <span class="ow">in</span> <span class="n">SQL</span> <span class="n">statements</span><span class="o">.</span>
<span class="n">parquetFile</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;parquetFile&quot;</span><span class="p">)</span>
<span class="n">val</span> <span class="n">platforms</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT platform FROM parquetFile WHERE visits &gt; 0&quot;</span><span class="p">)</span>
<span class="n">platforms</span><span class="o">.</span><span class="n">distinct</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">t</span> <span class="o">=&gt;</span> <span class="s2">&quot;Name: &quot;</span> <span class="o">+</span> <span class="n">t</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">println</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="bucketing-sorting-and-partitioning">
<h2>Bucketing, Sorting and Partitioning<a class="headerlink" href="#bucketing-sorting-and-partitioning" title="Permalink to this headline">¶</a></h2>
<p>For file-based data source, it is also possible to bucket and sort or
partition the output. Bucketing and sorting are applicable only to
persistent tables:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">val</span> <span class="n">social_media_usage_DF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;social_media_usage&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Find full example code at -
https://raw.githubusercontent.com/apache/spark/master/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala
in the Spark repo.</p>
<p>Note that partitioning can be used with both save and saveAsTable when
using the Dataset APIs.</p>
<p><code class="docutils literal notranslate"><span class="pre">partitionBy</span></code> creates a directory structure as described in the
Partition Discovery section. Thus, it has limited applicability to
columns with high cardinality. In contrast <code class="docutils literal notranslate"><span class="pre">bucketBy</span></code> distributes data
across a fixed number of buckets and can be used when the number of
unique values is unbounded. One can use <code class="docutils literal notranslate"><span class="pre">partitionBy</span></code> by itself or along
with `bucketBy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">social_media_usage_DF</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&quot;overwrite&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;/tmp/social_media_usage.parquet&quot;</span><span class="p">)</span> <span class="o">//</span> <span class="n">write</span> <span class="n">to</span> <span class="n">parquet</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">dbutils</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">ls</span><span class="p">(</span><span class="s2">&quot;/tmp/social_media_usage.parquet&quot;</span><span class="p">))</span> <span class="o">//</span> <span class="n">there</span> <span class="ow">is</span> <span class="n">one</span> <span class="n">part</span><span class="o">-</span><span class="mi">00000</span> <span class="n">file</span> <span class="n">inside</span> <span class="n">the</span> <span class="n">parquet</span> <span class="n">folder</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">val</span> <span class="n">social_media_usage_readFromParquet_DF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;/tmp/social_media_usage.parquet&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">social_media_usage_readFromParquet_DF</span><span class="o">.</span><span class="n">count</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">social_media_usage_readFromParquet_DF</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">social_media_usage_readFromParquet_DF</span><span class="o">.</span><span class="n">printSchema</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">social_media_usage_readFromParquet_DF</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;platform&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">distinct</span><span class="o">.</span><span class="n">count</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">social_media_usage_readFromParquet_DF</span>
  <span class="o">.</span><span class="n">write</span>
  <span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s2">&quot;platform&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&quot;overwrite&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;/tmp/social_media_usage_partitionedByPlatform.parquet&quot;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">dbutils</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">ls</span><span class="p">(</span><span class="s2">&quot;/tmp/social_media_usage_partitionedByPlatform.parquet&quot;</span><span class="p">))</span> <span class="o">//</span> <span class="n">there</span> <span class="n">are</span> <span class="n">many</span> <span class="n">platform</span><span class="o">=*</span> <span class="n">folders</span> <span class="n">inside</span> <span class="n">the</span> <span class="n">parquet</span> <span class="n">folder</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">dbutils</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">ls</span><span class="p">(</span><span class="s2">&quot;/tmp/social_media_usage_partitionedByPlatform.parquet/platform=Android&quot;</span><span class="p">))</span> <span class="o">//</span> <span class="n">threre</span> <span class="n">are</span> <span class="n">part</span><span class="o">-</span><span class="mi">00000</span><span class="o">-</span> <span class="n">files</span> <span class="k">with</span> <span class="n">contents</span> <span class="n">inside</span> <span class="n">each</span> <span class="n">platform</span><span class="o">=*</span> <span class="n">folder</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">parquet</span> <span class="n">folder</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;/tmp/social_media_usage_partitionedByPlatform.parquet&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span>
</pre></div>
</div>
</div>
</div>
<p>We can also use a fixed number of buckets and sort by a column within
each partition. Such finer control of the dataframe written as a parquet
file can help with optimizing downstream operations on the dataframe.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">social_media_usage_readFromParquet_DF</span>
  <span class="o">.</span><span class="n">write</span>
  <span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s2">&quot;platform&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">bucketBy</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="s2">&quot;date&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">sortBy</span><span class="p">(</span><span class="s2">&quot;date&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&quot;overwrite&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s2">&quot;social_media_usage_table_partitionedByPlatformBucketedByDate&quot;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">catalog</span><span class="o">.</span><span class="n">listTables</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">val</span> <span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;social_media_usage_table_partitionedByPlatformBucketedByDate&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="partition-discovery">
<h3>Partition Discovery<a class="headerlink" href="#partition-discovery" title="Permalink to this headline">¶</a></h3>
<p>Table partitioning is a common optimization approach used in systems
like Hive. In a partitioned table, data are usually stored in different
directories, with partitioning column values encoded in the path of each
partition directory. The Parquet data source is now able to discover and
infer partitioning information automatically. For example, we can store
all our previously used population data (from the programming guide
example!) into a partitioned table using the following directory
structure, with two extra columns, <code class="docutils literal notranslate"><span class="pre">gender</span></code> and <code class="docutils literal notranslate"><span class="pre">country</span></code> as
partitioning columns:
<code class="docutils literal notranslate"><span class="pre">path</span>&#160;&#160;&#160;&#160; <span class="pre">└──</span> <span class="pre">to</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">└──</span> <span class="pre">table</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">├──</span> <span class="pre">gender=male</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">│</span>&#160;&#160; <span class="pre">├──</span> <span class="pre">...</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">│</span>&#160;&#160; <span class="pre">│</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">│</span>&#160;&#160; <span class="pre">├──</span> <span class="pre">country=US</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">│</span>&#160;&#160; <span class="pre">│</span>&#160;&#160; <span class="pre">└──</span> <span class="pre">data.parquet</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">│</span>&#160;&#160; <span class="pre">├──</span> <span class="pre">country=CN</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">│</span>&#160;&#160; <span class="pre">│</span>&#160;&#160; <span class="pre">└──</span> <span class="pre">data.parquet</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">│</span>&#160;&#160; <span class="pre">└──</span> <span class="pre">...</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">└──</span> <span class="pre">gender=female</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">├──</span> <span class="pre">...</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">│</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">├──</span> <span class="pre">country=US</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">│</span>&#160;&#160; <span class="pre">└──</span> <span class="pre">data.parquet</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">├──</span> <span class="pre">country=CN</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">│</span>&#160;&#160; <span class="pre">└──</span> <span class="pre">data.parquet</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">└──</span> <span class="pre">...</span></code>
By passing <code class="docutils literal notranslate"><span class="pre">path/to/table</span></code> to either <code class="docutils literal notranslate"><span class="pre">SparkSession.read.parquet</span></code> or
<code class="docutils literal notranslate"><span class="pre">SparkSession.read.load</span></code>, Spark SQL will automatically extract the
partitioning information from the paths. Now the schema of the returned
DataFrame becomes:
<code class="docutils literal notranslate"><span class="pre">root</span>&#160;&#160;&#160;&#160; <span class="pre">|--</span> <span class="pre">name:</span> <span class="pre">string</span> <span class="pre">(nullable</span> <span class="pre">=</span> <span class="pre">true)</span>&#160;&#160;&#160;&#160; <span class="pre">|--</span> <span class="pre">age:</span> <span class="pre">long</span> <span class="pre">(nullable</span> <span class="pre">=</span> <span class="pre">true)</span>&#160;&#160;&#160;&#160; <span class="pre">|--</span> <span class="pre">gender:</span> <span class="pre">string</span> <span class="pre">(nullable</span> <span class="pre">=</span> <span class="pre">true)</span>&#160;&#160;&#160;&#160; <span class="pre">|--</span> <span class="pre">country:</span> <span class="pre">string</span> <span class="pre">(nullable</span> <span class="pre">=</span> <span class="pre">true)</span></code>
Notice that the data types of the partitioning columns are automatically
inferred. Currently, numeric data types and string type are supported.
Sometimes users may not want to automatically infer the data types of
the partitioning columns. For these use cases, the automatic type
inference can be configured by
<code class="docutils literal notranslate"><span class="pre">spark.sql.sources.partitionColumnTypeInference.enabled</span></code>, which is
default to <code class="docutils literal notranslate"><span class="pre">true</span></code>. When type inference is disabled, string type will be
used for the partitioning columns.</p>
<p>Starting from Spark 1.6.0, partition discovery only finds partitions
under the given paths by default. For the above example, if users pass
<code class="docutils literal notranslate"><span class="pre">path/to/table/gender=male</span></code> to either <code class="docutils literal notranslate"><span class="pre">SparkSession.read.parquet</span></code> or
<code class="docutils literal notranslate"><span class="pre">SparkSession.read.load</span></code>, <code class="docutils literal notranslate"><span class="pre">gender</span></code> will not be considered as a
partitioning column. If users need to specify the base path that
partition discovery should start with, they can set <code class="docutils literal notranslate"><span class="pre">basePath</span></code> in the
data source options. For example, when <code class="docutils literal notranslate"><span class="pre">path/to/table/gender=male</span></code> is
the path of the data and users set <code class="docutils literal notranslate"><span class="pre">basePath</span></code> to <code class="docutils literal notranslate"><span class="pre">path/to/table/</span></code>,
<code class="docutils literal notranslate"><span class="pre">gender</span></code> will be a partitioning column.</p>
</div>
<div class="section" id="schema-merging">
<h3>Schema Merging<a class="headerlink" href="#schema-merging" title="Permalink to this headline">¶</a></h3>
<p>Like ProtocolBuffer, Avro, and Thrift, Parquet also supports schema
evolution. Users can start with a simple schema, and gradually add more
columns to the schema as needed. In this way, users may end up with
multiple Parquet files with different but mutually compatible schemas.
The Parquet data source is now able to automatically detect this case
and merge schemas of all these files.</p>
<p>Since schema merging is a relatively expensive operation, and is not a
necessity in most cases, we turned it off by default starting from
1.5.0. You may enable it by:</p>
<ol class="simple">
<li><p>setting data source option <code class="docutils literal notranslate"><span class="pre">mergeSchema</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code> when reading
Parquet files (as shown in the examples below), or</p></li>
<li><p>setting the global SQL option <code class="docutils literal notranslate"><span class="pre">spark.sql.parquet.mergeSchema</span></code> to
<code class="docutils literal notranslate"><span class="pre">true</span></code>.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">Create</span> <span class="n">a</span> <span class="n">simple</span> <span class="n">DataFrame</span><span class="p">,</span> <span class="n">stored</span> <span class="n">into</span> <span class="n">a</span> <span class="n">partition</span> <span class="n">directory</span>
<span class="n">val</span> <span class="n">df1</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="mi">1</span> <span class="n">to</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">i</span> <span class="o">=&gt;</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="s2">&quot;single&quot;</span><span class="p">,</span> <span class="s2">&quot;double&quot;</span><span class="p">)</span>
<span class="n">df1</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&quot;overwrite&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;/tmp/data/test_table/key=1&quot;</span><span class="p">)</span>

<span class="o">//</span> <span class="n">Create</span> <span class="n">another</span> <span class="n">DataFrame</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">new</span> <span class="n">partition</span> <span class="n">directory</span><span class="p">,</span> <span class="n">adding</span> <span class="n">a</span> <span class="n">new</span> <span class="n">column</span> <span class="ow">and</span> <span class="n">dropping</span> <span class="n">an</span> <span class="n">existing</span> <span class="n">column</span>
<span class="n">val</span> <span class="n">df2</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="mi">6</span> <span class="n">to</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">i</span> <span class="o">=&gt;</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="s2">&quot;single&quot;</span><span class="p">,</span> <span class="s2">&quot;triple&quot;</span><span class="p">)</span>
<span class="n">df2</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&quot;overwrite&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;/tmp/data/test_table/key=2&quot;</span><span class="p">)</span>

<span class="o">//</span> <span class="n">Read</span> <span class="n">the</span> <span class="n">partitioned</span> <span class="n">table</span>
<span class="n">val</span> <span class="n">df3</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;mergeSchema&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;/tmp/data/test_table&quot;</span><span class="p">)</span>
<span class="n">df3</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>

<span class="o">//</span> <span class="n">The</span> <span class="n">final</span> <span class="n">schema</span> <span class="n">consists</span> <span class="n">of</span> <span class="nb">all</span> <span class="mi">3</span> <span class="n">columns</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">Parquet</span> <span class="n">files</span> <span class="n">together</span>
<span class="o">//</span> <span class="k">with</span> <span class="n">the</span> <span class="n">partitioning</span> <span class="n">column</span> <span class="n">appeared</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">partition</span> <span class="n">directory</span> <span class="n">paths</span><span class="o">.</span>
<span class="o">//</span> <span class="n">root</span>
<span class="o">//</span>  <span class="o">|--</span> <span class="n">single</span><span class="p">:</span> <span class="n">integer</span> <span class="p">(</span><span class="n">nullable</span> <span class="o">=</span> <span class="n">true</span><span class="p">)</span>
<span class="o">//</span>  <span class="o">|--</span> <span class="n">double</span><span class="p">:</span> <span class="n">integer</span> <span class="p">(</span><span class="n">nullable</span> <span class="o">=</span> <span class="n">true</span><span class="p">)</span>
<span class="o">//</span>  <span class="o">|--</span> <span class="n">triple</span><span class="p">:</span> <span class="n">integer</span> <span class="p">(</span><span class="n">nullable</span> <span class="o">=</span> <span class="n">true</span><span class="p">)</span>
<span class="o">//</span>  <span class="o">|--</span> <span class="n">key</span><span class="p">:</span> <span class="n">integer</span> <span class="p">(</span><span class="n">nullable</span> <span class="o">=</span> <span class="n">true</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">df3</span><span class="o">.</span><span class="n">show</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="hive-metastore-parquet-table-conversion">
<h3>Hive metastore Parquet table conversion<a class="headerlink" href="#hive-metastore-parquet-table-conversion" title="Permalink to this headline">¶</a></h3>
<p>When reading from and writing to Hive metastore Parquet tables, Spark
SQL will try to use its own Parquet support instead of Hive SerDe for
better performance. This behavior is controlled by the
<code class="docutils literal notranslate"><span class="pre">spark.sql.hive.convertMetastoreParquet</span></code> configuration, and is turned on
by default.</p>
<div class="section" id="hive-parquet-schema-reconciliation">
<h4>Hive/Parquet Schema Reconciliation<a class="headerlink" href="#hive-parquet-schema-reconciliation" title="Permalink to this headline">¶</a></h4>
<p>There are two key differences between Hive and Parquet from the
perspective of table schema processing.</p>
<ol class="simple">
<li><p>Hive is case insensitive, while Parquet is not</p></li>
<li><p>Hive considers all columns nullable, while nullability in Parquet is
significant</p></li>
</ol>
<p>Due to this reason, we must reconcile Hive metastore schema with Parquet
schema when converting a Hive metastore Parquet table to a Spark SQL
Parquet table. The reconciliation rules are:</p>
<ol class="simple">
<li><p>Fields that have the same name in both schema must have the same
data type regardless of nullability. The reconciled field should
have the data type of the Parquet side, so that nullability is
respected.</p></li>
<li><p>The reconciled schema contains exactly those fields defined in Hive
metastore schema.</p></li>
</ol>
<ul class="simple">
<li><p>Any fields that only appear in the Parquet schema are dropped in the
reconciled schema.</p></li>
<li><p>Any fileds that only appear in the Hive metastore schema are added
as nullable field in the reconciled schema.</p></li>
</ul>
</div>
<div class="section" id="metadata-refreshing">
<h4>Metadata Refreshing<a class="headerlink" href="#metadata-refreshing" title="Permalink to this headline">¶</a></h4>
<p>Spark SQL caches Parquet metadata for better performance. When Hive
metastore Parquet table conversion is enabled, metadata of those
converted tables are also cached. If these tables are updated by Hive or
other external tools, you need to refresh them manually to ensure
consistent metadata.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">should</span> <span class="n">refresh</span> <span class="n">table</span> <span class="n">metadata</span>
<span class="n">spark</span><span class="o">.</span><span class="n">catalog</span><span class="o">.</span><span class="n">refreshTable</span><span class="p">(</span><span class="s2">&quot;simple_range&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span> <span class="n">Or</span> <span class="n">you</span> <span class="n">can</span> <span class="n">use</span> <span class="n">SQL</span> <span class="n">to</span> <span class="n">refresh</span> <span class="n">table</span>
<span class="n">REFRESH</span> <span class="n">TABLE</span> <span class="n">simple_range</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="configuration">
<h3>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline">¶</a></h3>
<p>Configuration of Parquet can be done using the <code class="docutils literal notranslate"><span class="pre">setConf</span></code> method on
<code class="docutils literal notranslate"><span class="pre">SQLContext</span></code> or by running <code class="docutils literal notranslate"><span class="pre">SET</span> <span class="pre">key=value</span></code> commands using SQL.</p>
<p>| Property Name | Default | Meaning | | — | — | — | — | |
<code class="docutils literal notranslate"><span class="pre">spark.sql.parquet.binaryAsString</span></code> | false | Some other
Parquet-producing systems, in particular Impala, Hive, and older
versions of Spark SQL, do not differentiate between binary data and
strings when writing out the Parquet schema. This flag tells Spark SQL
to interpret binary data as a string to provide compatibility with these
systems. | | <code class="docutils literal notranslate"><span class="pre">spark.sql.parquet.int96AsTimestamp</span></code> | true | Some
Parquet-producing systems, in particular Impala and Hive, store
Timestamp into INT96. This flag tells Spark SQL to interpret INT96 data
as a timestamp to provide compatibility with these systems. | |
<code class="docutils literal notranslate"><span class="pre">spark.sql.parquet.cacheMetadata</span></code> | true | Turns on caching of Parquet
schema metadata. Can speed up querying of static data. | |
<code class="docutils literal notranslate"><span class="pre">spark.sql.parquet.compression.codec</span></code> | gzip | Sets the compression
codec use when writing Parquet files. Acceptable values include:
uncompressed, snappy, gzip, lzo. | | <code class="docutils literal notranslate"><span class="pre">spark.sql.parquet.filterPushdown</span></code>
| true | Enables Parquet filter push-down optimization when set to true.
| | <code class="docutils literal notranslate"><span class="pre">spark.sql.hive.convertMetastoreParquet</span></code> | true | When set to false,
Spark SQL will use the Hive SerDe for parquet tables instead of the
built in support. | | <code class="docutils literal notranslate"><span class="pre">spark.sql.parquet.output.committer.class</span></code> |
<code class="docutils literal notranslate"><span class="pre">org.apache.parquet.hadoop.ParquetOutputCommitter</span></code> | The output
committer class used by Parquet. The specified class needs to be a
subclass of <code class="docutils literal notranslate"><span class="pre">org.apache.hadoop.mapreduce.OutputCommitter</span></code>. Typically,
it’s also a subclass of
<code class="docutils literal notranslate"><span class="pre">org.apache.parquet.hadoop.ParquetOutputCommitter</span></code>. Spark SQL comes with
a builtin <code class="docutils literal notranslate"><span class="pre">org.apache.spark.sql.parquet.DirectParquetOutputCommitter</span></code>,
which can be more efficient then the default Parquet output committer
when writing data to S3. | | <code class="docutils literal notranslate"><span class="pre">spark.sql.parquet.mergeSchema</span></code> | <code class="docutils literal notranslate"><span class="pre">false</span></code> |
When true, the Parquet data source merges schemas collected from all
data files, otherwise the schema is picked from the summary file or a
random data file if no summary file is available. |</p>
</div>
</div>
<div class="section" id="json-datasets">
<h2>JSON Datasets<a class="headerlink" href="#json-datasets" title="Permalink to this headline">¶</a></h2>
<p>Spark SQL can automatically infer the schema of a JSON dataset and load
it as a DataFrame. This conversion can be done using
<code class="docutils literal notranslate"><span class="pre">SparkSession.read.json()</span></code> on either an RDD of String, or a JSON file.</p>
<p>Note that the file that is offered as <em>a json file</em> is not a typical
JSON file. Each line must contain a separate, self-contained valid JSON
object. As a consequence, a regular multi-line JSON file will most often
fail.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">A</span> <span class="n">JSON</span> <span class="n">dataset</span> <span class="ow">is</span> <span class="n">pointed</span> <span class="n">to</span> <span class="n">by</span> <span class="n">path</span><span class="o">.</span>
<span class="o">//</span> <span class="n">The</span> <span class="n">path</span> <span class="n">can</span> <span class="n">be</span> <span class="n">either</span> <span class="n">a</span> <span class="n">single</span> <span class="n">text</span> <span class="n">file</span> <span class="ow">or</span> <span class="n">a</span> <span class="n">directory</span> <span class="n">storing</span> <span class="n">text</span> <span class="n">files</span><span class="o">.</span>
<span class="n">val</span> <span class="n">path</span> <span class="o">=</span> <span class="s2">&quot;/tmp/platforms.json&quot;</span>
<span class="n">val</span> <span class="n">platforms</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

<span class="o">//</span> <span class="n">The</span> <span class="n">inferred</span> <span class="n">schema</span> <span class="n">can</span> <span class="n">be</span> <span class="n">visualized</span> <span class="n">using</span> <span class="n">the</span> <span class="n">printSchema</span><span class="p">()</span> <span class="n">method</span><span class="o">.</span>
<span class="n">platforms</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
<span class="o">//</span> <span class="n">root</span>
<span class="o">//</span>  <span class="o">|--</span> <span class="n">platform</span><span class="p">:</span> <span class="n">string</span> <span class="p">(</span><span class="n">nullable</span> <span class="o">=</span> <span class="n">true</span><span class="p">)</span>
<span class="o">//</span>  <span class="o">|--</span> <span class="n">visits</span><span class="p">:</span> <span class="n">long</span> <span class="p">(</span><span class="n">nullable</span> <span class="o">=</span> <span class="n">true</span><span class="p">)</span>

<span class="o">//</span> <span class="n">Register</span> <span class="n">this</span> <span class="n">DataFrame</span> <span class="k">as</span> <span class="n">a</span> <span class="n">table</span><span class="o">.</span>
<span class="n">platforms</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;platforms&quot;</span><span class="p">)</span>

<span class="o">//</span> <span class="n">SQL</span> <span class="n">statements</span> <span class="n">can</span> <span class="n">be</span> <span class="n">run</span> <span class="n">by</span> <span class="n">using</span> <span class="n">the</span> <span class="n">sql</span> <span class="n">methods</span> <span class="n">provided</span> <span class="n">by</span> <span class="n">sqlContext</span><span class="o">.</span>
<span class="n">val</span> <span class="n">facebook</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT platform, visits FROM platforms WHERE platform like &#39;Face%k&#39;&quot;</span><span class="p">)</span>
<span class="n">facebook</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">//</span> <span class="n">Alternatively</span><span class="p">,</span> <span class="n">a</span> <span class="n">DataFrame</span> <span class="n">can</span> <span class="n">be</span> <span class="n">created</span> <span class="k">for</span> <span class="n">a</span> <span class="n">JSON</span> <span class="n">dataset</span> <span class="n">represented</span> <span class="n">by</span>
<span class="o">//</span> <span class="n">an</span> <span class="n">RDD</span><span class="p">[</span><span class="n">String</span><span class="p">]</span> <span class="n">storing</span> <span class="n">one</span> <span class="n">JSON</span> <span class="nb">object</span> <span class="n">per</span> <span class="n">string</span><span class="o">.</span>
<span class="n">val</span> <span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;{&quot;name&quot;:&quot;IWyn&quot;,&quot;address&quot;:{&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;}}&quot;&quot;&quot;</span> <span class="p">::</span> <span class="n">Nil</span><span class="p">)</span>
<span class="n">val</span> <span class="n">anotherPlatforms</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span>
<span class="n">anotherPlatforms</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="hive-tables">
<h2>Hive Tables<a class="headerlink" href="#hive-tables" title="Permalink to this headline">¶</a></h2>
<p>Spark SQL also supports reading and writing data stored in <a class="reference external" href="http://hive.apache.org/">Apache
Hive</a>. However, since Hive has a large number
of dependencies, it is not included in the default Spark assembly. Hive
support is enabled by adding the <code class="docutils literal notranslate"><span class="pre">-Phive</span></code> and <code class="docutils literal notranslate"><span class="pre">-Phive-thriftserver</span></code>
flags to Spark’s build. This command builds a new assembly jar that
includes Hive. Note that this Hive assembly jar must also be present on
all of the worker nodes, as they will need access to the Hive
serialization and deserialization libraries (SerDes) in order to access
data stored in Hive.</p>
<p>Configuration of Hive is done by placing your <code class="docutils literal notranslate"><span class="pre">hive-site.xml</span></code>,
<code class="docutils literal notranslate"><span class="pre">core-site.xml</span></code> (for security configuration), <code class="docutils literal notranslate"><span class="pre">hdfs-site.xml</span></code> (for HDFS
configuration) file in <code class="docutils literal notranslate"><span class="pre">conf/</span></code>. Please note when running the query on a
YARN cluster (<code class="docutils literal notranslate"><span class="pre">cluster</span></code> mode), the <code class="docutils literal notranslate"><span class="pre">datanucleus</span></code> jars under the
<code class="docutils literal notranslate"><span class="pre">lib_managed/jars</span></code> directory and <code class="docutils literal notranslate"><span class="pre">hive-site.xml</span></code> under <code class="docutils literal notranslate"><span class="pre">conf/</span></code> directory
need to be available on the driver and all executors launched by the
YARN cluster. The convenient way to do this is adding them through the
<code class="docutils literal notranslate"><span class="pre">--jars</span></code> option and <code class="docutils literal notranslate"><span class="pre">--file</span></code> option of the <code class="docutils literal notranslate"><span class="pre">spark-submit</span></code> command.</p>
<p>When working with Hive one must construct a <code class="docutils literal notranslate"><span class="pre">HiveContext</span></code>, which
inherits from <code class="docutils literal notranslate"><span class="pre">SQLContext</span></code>, and adds support for finding tables in the
MetaStore and writing queries using HiveQL. Users who do not have an
existing Hive deployment can still create a <code class="docutils literal notranslate"><span class="pre">HiveContext</span></code>. When not
configured by the hive-site.xml, the context automatically creates
<code class="docutils literal notranslate"><span class="pre">metastore_db</span></code> in the current directory and creates <code class="docutils literal notranslate"><span class="pre">warehouse</span></code>
directory indicated by HiveConf, which defaults to
<code class="docutils literal notranslate"><span class="pre">/user/hive/warehouse</span></code>. Note that you may need to grant write privilege
on <code class="docutils literal notranslate"><span class="pre">/user/hive/warehouse</span></code> to the user who starts the spark application.</p>
<p>```scala val spark =
SparkSession.builder.enableHiveSupport().getOrCreate()</p>
<p>spark.sql(“CREATE TABLE IF NOT EXISTS src (key INT, value STRING)”)
spark.sql(“LOAD DATA LOCAL INPATH ‘examples/src/main/resources/kv1.txt’
INTO TABLE src”)</p>
<p>// Queries are expressed in HiveQL spark.sql(“FROM src SELECT key,
value”).collect().foreach(println) ```</p>
<div class="section" id="interacting-with-different-versions-of-hive-metastore">
<h3>Interacting with Different Versions of Hive Metastore<a class="headerlink" href="#interacting-with-different-versions-of-hive-metastore" title="Permalink to this headline">¶</a></h3>
<p>One of the most important pieces of Spark SQL’s Hive support is
interaction with Hive metastore, which enables Spark SQL to access
metadata of Hive tables. Starting from Spark 1.4.0, a single binary
build of Spark SQL can be used to query different versions of Hive
metastores, using the configuration described below. Note that
independent of the version of Hive that is being used to talk to the
metastore, internally Spark SQL will compile against Hive 1.2.1 and use
those classes for internal execution (serdes, UDFs, UDAFs, etc).</p>
<p>The following options can be used to configure the version of Hive that
is used to retrieve metadata:</p>
<p>| Property Name | Default | Meaning | | — | — | — | |
<code class="docutils literal notranslate"><span class="pre">spark.sql.hive.metastore.version</span></code> | <code class="docutils literal notranslate"><span class="pre">1.2.1</span></code> | Version of the Hive
metastore. Available options are <code class="docutils literal notranslate"><span class="pre">0.12.0</span></code> through <code class="docutils literal notranslate"><span class="pre">1.2.1</span></code>. | |
<code class="docutils literal notranslate"><span class="pre">spark.sql.hive.metastore.jars</span></code> | <code class="docutils literal notranslate"><span class="pre">builtin</span></code> | Location of the jars that
should be used to instantiate the HiveMetastoreClient. This property can
be one of three options: <code class="docutils literal notranslate"><span class="pre">builtin</span></code>, <code class="docutils literal notranslate"><span class="pre">maven</span></code>, a classpath in the standard
format for the JVM. This classpath must include all of Hive and its
dependencies, including the correct version of Hadoop. These jars only
need to be present on the driver, but if you are running in yarn cluster
mode then you must ensure they are packaged with you application. | |
<code class="docutils literal notranslate"><span class="pre">spark.sql.hive.metastore.sharedPrefixes</span></code> |
<code class="docutils literal notranslate"><span class="pre">com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc</span></code> | A
comma separated list of class prefixes that should be loaded using the
classloader that is shared between Spark SQL and a specific version of
Hive. An example of classes that should be shared is JDBC drivers that
are needed to talk to the metastore. Other classes that need to be
shared are those that interact with classes that are already shared. For
example, custom appenders that are used by log4j. | |
<code class="docutils literal notranslate"><span class="pre">spark.sql.hive.metastore.barrierPrefixes</span></code> | <code class="docutils literal notranslate"><span class="pre">(empty)</span></code> | A comma
separated list of class prefixes that should explicitly be reloaded for
each version of Hive that Spark SQL is communicating with. For example,
Hive UDFs that are declared in a prefix that typically would be shared
(i.e. <code class="docutils literal notranslate"><span class="pre">org.apache.spark.*</span></code>). |</p>
</div>
</div>
<div class="section" id="jdbc-to-other-databases">
<h2>JDBC To Other Databases<a class="headerlink" href="#jdbc-to-other-databases" title="Permalink to this headline">¶</a></h2>
<p>Spark SQL also includes a data source that can read data from other
databases using JDBC. This functionality should be preferred over using
<a class="reference external" href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.JdbcRDD">JdbcRDD</a>.
This is because the results are returned as a DataFrame and they can
easily be processed in Spark SQL or joined with other data sources. The
JDBC data source is also easier to use from Java or Python as it does
not require the user to provide a ClassTag. (Note that this is different
than the Spark SQL JDBC server, which allows other applications to run
queries using Spark SQL).</p>
<p>To get started you will need to include the JDBC driver for you
particular database on the spark classpath. For example, to connect to
postgres from the Spark Shell you would run the following command:</p>
<p><code class="docutils literal notranslate"><span class="pre">SPARK_CLASSPATH=postgresql-9.3-1102-jdbc41.jar</span> <span class="pre">bin/spark-shell</span></code></p>
<p>Tables from the remote database can be loaded as a DataFrame or Spark
SQL Temporary table using the Data Sources API. The following options
are supported:</p>
<p>| Property Name | Meaning | | — | — | — | | <code class="docutils literal notranslate"><span class="pre">url</span></code> | The JDBC URL
to connect to. | | <code class="docutils literal notranslate"><span class="pre">dbtable</span></code> | The JDBC table that should be read. Note
that anything that is valid in a <code class="docutils literal notranslate"><span class="pre">FROM</span></code> clause of a SQL query can be
used. For example, instead of a full table you could also use a subquery
in parentheses. | | <code class="docutils literal notranslate"><span class="pre">driver</span></code> | The class name of the JDBC driver needed
to connect to this URL. This class will be loaded on the master and
workers before running an JDBC commands to allow the driver to register
itself with the JDBC subsystem. | |
<code class="docutils literal notranslate"><span class="pre">partitionColumn,</span> <span class="pre">lowerBound,</span> <span class="pre">upperBound,</span> <span class="pre">numPartitions</span></code> | These options
must all be specified if any of them is specified. They describe how to
partition the table when reading in parallel from multiple workers.
<code class="docutils literal notranslate"><span class="pre">partitionColumn</span></code> must be a numeric column from the table in question.
Notice that <code class="docutils literal notranslate"><span class="pre">lowerBound</span></code> and <code class="docutils literal notranslate"><span class="pre">upperBound</span></code> are just used to decide the
partition stride, not for filtering the rows in table. So all rows in
the table will be partitioned and returned. | | <code class="docutils literal notranslate"><span class="pre">fetchSize</span></code> | The JDBC
fetch size, which determines how many rows to fetch per round trip. This
can help performance on JDBC drivers which default to low fetch size
(eg. Oracle with 10 rows). |</p>
<p><code class="docutils literal notranslate"><span class="pre">//</span> <span class="pre">Example</span> <span class="pre">of</span> <span class="pre">using</span> <span class="pre">JDBC</span> <span class="pre">datasource</span> <span class="pre">val</span> <span class="pre">jdbcDF</span> <span class="pre">=</span> <span class="pre">spark.read.format(&quot;jdbc&quot;).options(Map(&quot;url&quot;</span> <span class="pre">-&gt;</span> <span class="pre">&quot;jdbc:postgresql:dbserver&quot;,</span> <span class="pre">&quot;dbtable&quot;</span> <span class="pre">-&gt;</span> <span class="pre">&quot;schema.tablename&quot;)).load()</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">--</span> <span class="pre">Or</span> <span class="pre">using</span> <span class="pre">JDBC</span> <span class="pre">datasource</span> <span class="pre">in</span> <span class="pre">SQL</span> <span class="pre">CREATE</span> <span class="pre">TEMPORARY</span> <span class="pre">TABLE</span> <span class="pre">jdbcTable</span> <span class="pre">USING</span> <span class="pre">org.apache.spark.sql.jdbc</span> <span class="pre">OPTIONS</span> <span class="pre">(</span>&#160;&#160; <span class="pre">url</span> <span class="pre">&quot;jdbc:postgresql:dbserver&quot;,</span>&#160;&#160; <span class="pre">dbtable</span> <span class="pre">&quot;schema.tablename&quot;</span> <span class="pre">)</span></code></p>
<div class="section" id="troubleshooting">
<h3>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>The JDBC driver class must be visible to the primordial class loader
on the client session and on all executors. This is because Java’s
DriverManager class does a security check that results in it
ignoring all drivers not visible to the primordial class loader when
one goes to open a connection. One convenient way to do this is to
modify compute_classpath.sh on all worker nodes to include your
driver JARs.</p></li>
<li><p>Some databases, such as H2, convert all names to upper case. You’ll
need to use upper case to refer to those names in Spark SQL.</p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./000_1-sds-3-x"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="007c_SparkSQLProgGuide_HW.html" title="previous page">Getting Started - Exercise</a>
    <a class='right-next' id="next-link" href="007e_SparkSQLProgGuide_HW.html" title="next page">Performance Tuning</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By ScaDaMaLe Team<br/>
        
            &copy; Copyright 2020 Creative Commons Zero v1.0 Universal.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>