<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>sds-3.x/ScaDaMaLe</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
        <link rel="stylesheet" href="scroll-mdbook-outputs.css">
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/033_OBO_LoadExtract.html">033_OBO_LoadExtract</a></li><li class="chapter-item expanded affix "><a href="contents/000_2-sds-3-x-ml/033_OBO_LoadExtract.html">033_OBO_LoadExtract</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/LinearAlgebra/LAlgCheatSheet.html">LAlgCheatSheet</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/000_MLlibProgGuide.html">000_MLlibProgGuide</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/000_dataTypesProgGuide.html">000_dataTypesProgGuide</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/001_LocalVector.html">001_LocalVector</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/002_LabeledPoint.html">002_LabeledPoint</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/003_LocalMatrix.html">003_LocalMatrix</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/004_DistributedMatrix.html">004_DistributedMatrix</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/005_RowMatrix.html">005_RowMatrix</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/006_IndexedRowMatrix.html">006_IndexedRowMatrix</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/007_CoordinateMatrix.html">007_CoordinateMatrix</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/008_BlockMatrix.html">008_BlockMatrix</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide/000_sqlProgGuide.html">000_sqlProgGuide</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide/001_overview_sqlProgGuide.html">001_overview_sqlProgGuide</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide/002_gettingStarted_sqlProgGuide.html">002_gettingStarted_sqlProgGuide</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide/003_dataSources_sqlProgGuide.html">003_dataSources_sqlProgGuide</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide/004_performanceTuning_sqlProgGuide.html">004_performanceTuning_sqlProgGuide</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide/005_distributedSqlEngine_sqlProgGuide.html">005_distributedSqlEngine_sqlProgGuide</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/sdsDatasets/scraperUSStateofUnionAddresses.html">scraperUSStateofUnionAddresses</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/support/sdsFunctions.html">sdsFunctions</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/guide.html">guide</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/actions/collect.html">collect</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/actions/getNumPartitions.html">getNumPartitions</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/actions/reduce.html">reduce</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/transformations/coalesce.html">coalesce</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/transformations/distinct.html">distinct</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/transformations/filter.html">filter</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/transformations/flatMap.html">flatMap</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/transformations/groupByKey.html">groupByKey</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/transformations/groupBy.html">groupBy</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/transformations/join.html">join</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/transformations/keyBy.html">keyBy</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/transformations/map.html">map</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/transformations/mapPartitions.html">mapPartitions</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/transformations/mapPartitionsWithIndex.html">mapPartitionsWithIndex</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/transformations/partitionBy.html">partitionBy</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/transformations/reduceByKey.html">reduceByKey</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/transformations/sample.html">sample</a></li><li class="chapter-item expanded affix "><a href="contents/xtraResources/visualRDDApi/recall/transformations/union.html">union</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">sds-3.x/ScaDaMaLe</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        

                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="033_obo_loadextract"><a class="header" href="#033_obo_loadextract">033_OBO_LoadExtract</a></h1>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="033_obo_loadextract-1"><a class="header" href="#033_obo_loadextract-1">033_OBO_LoadExtract</a></h1>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
</div>
<div class="cell markdown">
<p>This is from * <a href="https://github.com/scalanlp/breeze/wiki/Linear-Algebra-Cheat-Sheet">https://github.com/scalanlp/breeze/wiki/Linear-Algebra-Cheat-Sheet</a></p>
</div>
<div class="cell markdown">
<h2 id="core-concepts"><a class="header" href="#core-concepts">Core Concepts</a></h2>
<p>Compared to other numerical computing environments, Breeze matrices default to column major ordering, like Matlab, but indexing is 0-based, like Numpy. Breeze has as its core concepts matrices and column vectors. Row vectors are normally stored as matrices with a single row. This allows for greater type safety with the downside that conversion of row vectors to column vectors is performed using a transpose-slice (<code>a.t(::,0)</code>) instead of a simple transpose (<code>a.t</code>).</p>
<p>[[UFunc|Universal Functions]]s are very important in Breeze. Once you get a feel for the syntax (i.e. what's in this section), it might be worthwhile to read the first half of the UFunc wiki page. (You can skip the last half that involves implementing your own UFuncs...until you're ready to contribute to Breeze!)</p>
</div>
<div class="cell markdown">
<h2 id="quick-reference"><a class="header" href="#quick-reference">Quick Reference</a></h2>
<p>The following table assumes that Numpy is used with <code>from numpy import *</code> and Breeze with:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import breeze.linalg._
import breeze.numerics._
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import breeze.linalg._
import breeze.numerics._
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="creation"><a class="header" href="#creation">Creation</a></h3>
<table><thead><tr><th>Operation</th><th>Breeze</th><th>Matlab</th><th>Numpy</th><th>R</th></tr></thead><tbody>
<tr><td>Zeroed matrix</td><td><code>DenseMatrix.zeros[Double](n,m)</code></td><td><code>zeros(n,m)</code></td><td><code>zeros((n,m))</code></td><td><code>mat.or.vec(n, m)</code></td></tr>
<tr><td>Zeroed vector</td><td><code>DenseVector.zeros[Double](n)</code></td><td><code>zeros(n,1)</code></td><td><code>zeros(n)</code></td><td><code>mat.or.vec(n, 1)</code></td></tr>
<tr><td>Vector of ones</td><td><code>DenseVector.ones[Double](n)</code></td><td><code>ones(n,1)</code></td><td><code>ones(n)</code></td><td><code>mat.or.vec(n, 1) + 1</code></td></tr>
<tr><td>Vector of particular number</td><td><code>DenseVector.fill(n){5.0}</code></td><td><code>ones(n,1) * 5</code></td><td><code>ones(n) * 5</code></td><td><code>(mat.or.vec(5, 1) + 1) * 5</code></td></tr>
<tr><td>range given stepsize</td><td><code>DenseVector.range(start,stop,step)</code> or <code>Vector.rangeD(start,stop,step)</code></td><td></td><td></td><td><code>seq(start,stop,step)</code></td></tr>
<tr><td>n element range</td><td><code>linspace(start,stop,numvals)</code></td><td><code>linspace(0,20,15)</code></td><td></td><td></td></tr>
<tr><td>Identity matrix</td><td><code>DenseMatrix.eye[Double](n)</code></td><td><code>eye(n)</code></td><td><code>eye(n)</code></td><td><code>identity(n)</code></td></tr>
<tr><td>Diagonal matrix</td><td><code>diag(DenseVector(1.0,2.0,3.0))</code></td><td><code>diag([1 2 3])</code></td><td><code>diag((1,2,3))</code></td><td><code>diag(c(1,2,3))</code></td></tr>
<tr><td>Matrix inline creation</td><td><code>DenseMatrix((1.0,2.0), (3.0,4.0))</code></td><td><code>[1 2; 3 4]</code></td><td><code>array([ [1,2], [3,4] ])</code></td><td><code>matrix(c(1,2,3,4), nrow = 2, ncol = 2)</code></td></tr>
<tr><td>Column vector inline creation</td><td><code>DenseVector(1,2,3,4)</code></td><td><code>[1 2 3 4]</code></td><td><code>array([1,2,3,4])</code></td><td><code>c(1,2,3,4)</code></td></tr>
<tr><td>Row vector inline creation</td><td><code>DenseVector(1,2,3,4).t</code></td><td><code>[1 2 3 4]'</code></td><td><code>array([1,2,3]).reshape(-1,1)</code></td><td><code>t(c(1,2,3,4))</code></td></tr>
<tr><td>Vector from function</td><td><code>DenseVector.tabulate(3){i =&gt; 2*i}</code></td><td></td><td></td><td></td></tr>
<tr><td>Matrix from function</td><td><code>DenseMatrix.tabulate(3, 2){case (i, j) =&gt; i+j}</code></td><td></td><td></td><td></td></tr>
<tr><td>Vector creation from array</td><td><code>new DenseVector(Array(1, 2, 3, 4))</code></td><td></td><td></td><td></td></tr>
<tr><td>Matrix creation from array</td><td><code>new DenseMatrix(2, 3, Array(11, 12, 13, 21, 22, 23))</code></td><td></td><td></td><td></td></tr>
<tr><td>Vector of random elements from 0 to 1</td><td><code>DenseVector.rand(4)</code></td><td></td><td></td><td><code>runif(4)</code> (requires stats library)</td></tr>
<tr><td>Matrix of random elements from 0 to 1</td><td><code>DenseMatrix.rand(2, 3)</code></td><td></td><td></td><td><code>matrix(runif(6),2)</code> (requires stats library)</td></tr>
</tbody></table>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">DenseMatrix.zeros[Double](2,3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: breeze.linalg.DenseMatrix[Double] =
0.0  0.0  0.0
0.0  0.0  0.0
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">import numpy as np
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">np.zeros((2,3))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>&lt;span class=&quot;ansired&quot;&gt;Out[&lt;/span&gt;&lt;span class=&quot;ansired&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;ansired&quot;&gt;]: &lt;/span&gt;
array([[ 0.,  0.,  0.],
       [ 0.,  0.,  0.]])
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-r">mat.or.vec(2,3)
</code></pre>
</div>
<div class="cell markdown">
<h4 id="reading-and-writing-matrices"><a class="header" href="#reading-and-writing-matrices">Reading and writing Matrices</a></h4>
<p>Currently, Breeze supports IO for Matrices in two ways: Java serialization and csv. The latter comes from two functions: <code>breeze.linalg.csvread</code> and <code>breeze.linalg.csvwrite</code>. <code>csvread</code> takes a File, and optionally parameters for how the CSV file is delimited (e.g. if it is actually a tsv file, you can set tabs as the field delimiter.) and returns a <a href="contents/xtraResources/LinearAlgebra/Data-Structures#densematrix">DenseMatrix</a>. Similarly, <code>csvwrite</code> takes a File and a DenseMatrix, and writes the contents of a matrix to a file.</p>
<h3 id="indexing-and-slicing"><a class="header" href="#indexing-and-slicing">Indexing and Slicing</a></h3>
<table><thead><tr><th>Operation</th><th>Breeze</th><th>Matlab</th><th>Numpy</th><th>R</th></tr></thead><tbody>
<tr><td>Basic Indexing</td><td><code>a(0,1)</code></td><td><code>a(1,2)</code></td><td><code>a[0,1]</code></td><td><code>a[1,2]</code></td></tr>
<tr><td>Extract subset of vector</td><td><code>a(1 to 4)</code> or <code>a(1 until 5)</code> or <code>a.slice(1,5)</code></td><td><code>a(2:5)</code></td><td><code>a[1:5]</code></td><td><code>a[2:5]</code></td></tr>
<tr><td>(negative steps)</td><td><code>a(5 to 0 by -1)</code></td><td><code>a(6:-1:1)</code></td><td><code>a[5:0:-1]</code></td><td></td></tr>
<tr><td>(tail)</td><td><code>a(1 to -1)</code></td><td><code>a(2:end)</code></td><td><code>a[1:]</code></td><td><code>a[2:length(a)]</code> or <code>tail(a,n=length(a)-1)</code></td></tr>
<tr><td>(last element)</td><td><code>a( -1 )</code></td><td><code>a(end)</code></td><td><code>a[-1]</code></td><td><code>tail(a, n=1)</code></td></tr>
<tr><td>Extract column of matrix</td><td><code>a(::, 2)</code></td><td><code>a(:,3)</code></td><td><code>a[:,2]</code></td><td><code>a[,2]</code></td></tr>
</tbody></table>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val matrix = DenseMatrix.rand(2, 3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>matrix: breeze.linalg.DenseMatrix[Double] =
0.31972455666138666  0.20027601169839704  0.5602628276401904
0.4299695123127245   0.9935026349626817   0.0383067428009598
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val two_one = matrix(1, 0) // Remember the index starts from zero
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>two_one: Double = 0.4299695123127245
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="other-manipulation"><a class="header" href="#other-manipulation">Other Manipulation</a></h3>
<table><thead><tr><th>Operation</th><th>Breeze</th><th>Matlab</th><th>Numpy</th><th>R</th></tr></thead><tbody>
<tr><td>Reshaping</td><td><code>a.reshape(3, 2)</code></td><td><code>reshape(a, 3, 2)</code></td><td><code>a.reshape(3,2)</code></td><td><code>matrix(a,nrow=3,byrow=T)</code></td></tr>
<tr><td>Flatten matrix</td><td><code>a.toDenseVector</code> (Makes copy)</td><td><code>a(:)</code></td><td><code>a.flatten()</code></td><td><code>as.vector(a)</code></td></tr>
<tr><td>Copy lower triangle</td><td><code>lowerTriangular(a)</code></td><td><code>tril(a)</code></td><td><code>tril(a)</code></td><td><code>a[upper.tri(a)] &lt;- 0</code></td></tr>
<tr><td>Copy upper triangle</td><td><code>upperTriangular(a)</code></td><td><code>triu(a)</code></td><td><code>triu(a)</code></td><td><code>a[lower.tri(a)] &lt;- 0</code></td></tr>
<tr><td>Copy (note, no parens!!)</td><td><code>a.copy</code></td><td></td><td><code>np.copy(a)</code></td><td></td></tr>
<tr><td>Create view of matrix diagonal</td><td><code>diag(a)</code></td><td>NA</td><td><code>diagonal(a)</code> (Numpy &gt;= 1.9)</td><td></td></tr>
<tr><td>Vector Assignment to subset</td><td><code>a(1 to 4) := 5.0</code></td><td><code>a(2:5) = 5</code></td><td><code>a[1:4] = 5</code></td><td><code>a[2:5] = 5</code></td></tr>
<tr><td>Vector Assignment to subset</td><td><code>a(1 to 4) := DenseVector(1.0,2.0,3.0)</code></td><td><code>a(2:5) = [1 2 3]</code></td><td><code>a[1:4] = array([1,2,3])</code></td><td><code>a[2:5] = c(1,2,3)</code></td></tr>
<tr><td>Matrix Assignment to subset</td><td><code>a(1 to 3,1 to 3) := 5.0</code></td><td><code>a(2:4,2:4) = 5</code></td><td><code>a[1:3,1:3] = 5</code></td><td><code>a[2:4,2:4] = 5</code></td></tr>
<tr><td>Matrix Assignment to column</td><td><code>a(::, 2) := 5.0</code></td><td><code>a(:,3) = 5</code></td><td><code>a[:,2] = 5</code></td><td><code>a[,3] = 5</code></td></tr>
<tr><td>Matrix vertical concatenate</td><td><code>DenseMatrix.vertcat(a,b)</code></td><td><code>[a ; b]</code></td><td><code>vstack((a,b))</code></td><td><code>rbind(a, b)</code></td></tr>
<tr><td>Matrix horizontal concatenate</td><td><code>DenseMatrix.horzcat(d,e)</code></td><td><code>[d , e]</code></td><td><code>hstack((d,e))</code></td><td><code>cbind(d, e)</code></td></tr>
<tr><td>Vector concatenate</td><td><code>DenseVector.vertcat(a,b)</code></td><td><code>[a b]</code></td><td><code>concatenate((a,b))</code></td><td><code>c(a, b)</code></td></tr>
</tbody></table>
<h3 id="operations"><a class="header" href="#operations">Operations</a></h3>
<table><thead><tr><th>Operation</th><th>Breeze</th><th>Matlab</th><th>Numpy</th><th>R</th></tr></thead><tbody>
<tr><td>Elementwise addition</td><td><code>a + b</code></td><td><code>a + b</code></td><td><code>a + b</code></td><td><code>a + b</code></td></tr>
<tr><td>Shaped/Matrix multiplication</td><td><code>a * b</code></td><td><code>a * b</code></td><td><code>dot(a, b)</code></td><td><code>a %*% b</code></td></tr>
<tr><td>Elementwise multiplication</td><td><code>a :* b</code></td><td><code>a .* b</code></td><td><code>a * b</code></td><td><code>a * b</code></td></tr>
<tr><td>Elementwise division</td><td><code>a :/ b</code></td><td><code>a ./ b</code></td><td><code>a / b</code></td><td><code>a / b</code></td></tr>
<tr><td>Elementwise comparison</td><td><code>a :&lt; b</code></td><td><code>a &lt; b</code> (gives matrix of 1/0 instead of true/false)</td><td><code>a &lt; b</code></td><td><code>a &lt; b</code></td></tr>
<tr><td>Elementwise equals</td><td><code>a :== b</code></td><td><code>a == b</code> (gives matrix of 1/0 instead of true/false)</td><td><code>a == b</code></td><td><code>a == b</code></td></tr>
<tr><td>Inplace addition</td><td><code>a :+= 1.0</code></td><td><code>a += 1</code></td><td><code>a += 1</code></td><td><code>a = a + 1</code></td></tr>
<tr><td>Inplace elementwise multiplication</td><td><code>a :*= 2.0</code></td><td><code>a *= 2</code></td><td><code>a *= 2</code></td><td><code>a = a * 2</code></td></tr>
<tr><td>Vector dot product</td><td><code>a dot b</code>, <code>a.t * b</code><sup>†</sup></td><td><code>dot(a,b)</code></td><td><code>dot(a,b)</code></td><td><code>crossprod(a,b)</code></td></tr>
<tr><td>Elementwise max</td><td><code>max(a)</code></td><td><code>max(a)</code></td><td><code>a.max()</code></td><td><code>max(a)</code></td></tr>
<tr><td>Elementwise argmax</td><td><code>argmax(a)</code></td><td><code>[v i] = max(a); i</code></td><td><code>a.argmax()</code></td><td><code>which.max(a)</code></td></tr>
</tbody></table>
<h3 id="sum"><a class="header" href="#sum">Sum</a></h3>
<table><thead><tr><th>Operation</th><th>Breeze</th><th>Matlab</th><th>Numpy</th><th>R</th></tr></thead><tbody>
<tr><td>Elementwise sum</td><td><code>sum(a)</code></td><td><code>sum(sum(a))</code></td><td><code>a.sum()</code></td><td><code>sum(a)</code></td></tr>
<tr><td>Sum down each column (giving a row vector)</td><td><code>sum(a, Axis._0)</code> or <code>sum(a(::, *))</code></td><td><code>sum(a)</code></td><td><code>sum(a,0)</code></td><td><code>apply(a,2,sum)</code></td></tr>
<tr><td>Sum across each row (giving a column vector)</td><td><code>sum(a, Axis._1)</code> or <code>sum(a(*, ::))</code></td><td><code>sum(a')</code></td><td><code>sum(a,1)</code></td><td><code>apply(a,1,sum)</code></td></tr>
<tr><td>Trace (sum of diagonal elements)</td><td><code>trace(a)</code></td><td><code>trace(a)</code></td><td><code>a.trace()</code></td><td><code>sum(diag(a))</code></td></tr>
<tr><td>Cumulative sum</td><td><code>accumulate(a)</code></td><td><code>cumsum(a)</code></td><td><code>a.cumsum()</code></td><td><code>apply(a,2,cumsum)</code></td></tr>
</tbody></table>
<h3 id="boolean-operators"><a class="header" href="#boolean-operators">Boolean Operators</a></h3>
<table><thead><tr><th>Operation</th><th>Breeze</th><th>Matlab</th><th>Numpy</th><th>R</th></tr></thead><tbody>
<tr><td>Elementwise and</td><td><code>a :&amp; b</code></td><td><code>a &amp;&amp; b</code></td><td><code>a &amp; b</code></td><td><code>a &amp; b</code></td></tr>
<tr><td>Elementwise or</td><td>`a :</td><td>b`</td><td>`a</td><td></td></tr>
<tr><td>Elementwise not</td><td><code>!a</code></td><td><code>~a</code></td><td><code>~a</code></td><td><code>!a</code></td></tr>
<tr><td>True if any element is nonzero</td><td><code>any(a)</code></td><td><code>any(a)</code></td><td>any(a)</td><td></td></tr>
<tr><td>True if all elements are nonzero</td><td><code>all(a)</code></td><td><code>all(a)</code></td><td>all(a)</td><td></td></tr>
</tbody></table>
<h3 id="linear-algebra-functions"><a class="header" href="#linear-algebra-functions">Linear Algebra Functions</a></h3>
<table><thead><tr><th>Operation</th><th>Breeze</th><th>Matlab</th><th>Numpy</th><th>R</th></tr></thead><tbody>
<tr><td>Linear solve</td><td><code>a \ b</code></td><td><code>a \ b</code></td><td><code>linalg.solve(a,b)</code></td><td><code>solve(a,b)</code></td></tr>
<tr><td>Transpose</td><td><code>a.t</code></td><td><code>a'</code></td><td><code>a.conj.transpose()</code></td><td><code>t(a)</code></td></tr>
<tr><td>Determinant</td><td><code>det(a)</code></td><td><code>det(a)</code></td><td><code>linalg.det(a)</code></td><td><code>det(a)</code></td></tr>
<tr><td>Inverse</td><td><code>inv(a)</code></td><td><code>inv(a)</code></td><td><code>linalg.inv(a)</code></td><td><code>solve(a)</code></td></tr>
<tr><td>Moore-Penrose Pseudoinverse</td><td><code>pinv(a)</code></td><td><code>pinv(a)</code></td><td><code>linalg.pinv(a)</code></td><td></td></tr>
<tr><td>Vector Frobenius Norm</td><td><code>norm(a)</code></td><td><code>norm(a)</code></td><td><code>norm(a)</code></td><td></td></tr>
<tr><td>Eigenvalues (Symmetric)</td><td><code>eigSym(a)</code></td><td><code>[v,l] = eig(a)</code></td><td><code>linalg.eig(a)[0]</code></td><td></td></tr>
<tr><td>Eigenvalues</td><td><code>val (er, ei, _) = eig(a)</code> (separate real &amp; imaginary part)</td><td><code>eig(a)</code></td><td><code>linalg.eig(a)[0]</code></td><td><code>eigen(a)$values</code></td></tr>
<tr><td>Eigenvectors</td><td><code>eig(a)._3</code></td><td><code>[v,l] = eig(a)</code></td><td><code>linalg.eig(a)[1]</code></td><td><code>eigen(a)$vectors</code></td></tr>
<tr><td>Singular Value Decomposition</td><td><code>val svd.SVD(u,s,v) = svd(a)</code></td><td><code>svd(a)</code></td><td><code>linalg.svd(a)</code></td><td><code>svd(a)$d</code></td></tr>
<tr><td>Rank</td><td><code>rank(a)</code></td><td><code>rank(a)</code></td><td><code>rank(a)</code></td><td><code>rank(a)</code></td></tr>
<tr><td>Vector length</td><td><code>a.length</code></td><td><code>size(a)</code></td><td><code>a.size</code></td><td><code>length(a)</code></td></tr>
<tr><td>Matrix rows</td><td><code>a.rows</code></td><td><code>size(a,1)</code></td><td><code>a.shape[0]</code></td><td><code>nrow(a)</code></td></tr>
<tr><td>Matrix columns</td><td><code>a.cols</code></td><td><code>size(a,2)</code></td><td><code>a.shape[1]</code></td><td><code>ncol(a)</code></td></tr>
</tbody></table>
<h3 id="rounding-and-signs"><a class="header" href="#rounding-and-signs">Rounding and Signs</a></h3>
<table><thead><tr><th>Operation</th><th>Breeze</th><th>Matlab</th><th>Numpy</th><th>R</th></tr></thead><tbody>
<tr><td>Round</td><td><code>round(a)</code></td><td><code>round(a)</code></td><td><code>around(a)</code></td><td><code>round(a)</code></td></tr>
<tr><td>Ceiling</td><td><code>ceil(a)</code></td><td><code>ceil(a)</code></td><td><code>ceil(a)</code></td><td><code>ceiling(a)</code></td></tr>
<tr><td>Floor</td><td><code>floor(a)</code></td><td><code>floor(a)</code></td><td><code>floor(a)</code></td><td><code>floor(a)</code></td></tr>
<tr><td>Sign</td><td><code>signum(a)</code></td><td><code>sign(a)</code></td><td><code>sign(a)</code></td><td><code>sign(a)</code></td></tr>
<tr><td>Absolute Value</td><td><code>abs(a)</code></td><td><code>abs(a)</code></td><td><code>abs(a)</code></td><td><code>abs(a)</code></td></tr>
</tbody></table>
<h3 id="constants"><a class="header" href="#constants">Constants</a></h3>
<table><thead><tr><th>Operation</th><th>Breeze</th><th>Matlab</th><th>Numpy</th><th>R</th></tr></thead><tbody>
<tr><td>Not a Number</td><td><code>NaN</code> or <code>nan</code></td><td><code>NaN</code></td><td><code>nan</code></td><td><code>NA</code></td></tr>
<tr><td>Infinity</td><td><code>Inf</code> or <code>inf</code></td><td><code>Inf</code></td><td><code>inf</code></td><td><code>Inf</code></td></tr>
<tr><td>Pi</td><td><code>Constants.Pi</code></td><td><code>pi</code></td><td><code>math.pi</code></td><td><code>pi</code></td></tr>
<tr><td>e</td><td><code>Constants.E</code></td><td><code>exp(1)</code></td><td><code>math.e</code></td><td><code>exp(1)</code></td></tr>
</tbody></table>
<h2 id="complex-numbers"><a class="header" href="#complex-numbers">Complex numbers</a></h2>
<p>If you make use of complex numbers, you will want to include a <code>breeze.math._</code> import. This declares a <code>i</code> variable, and provides implicit conversions from Scala’s basic types to complex types.</p>
<table><thead><tr><th>Operation</th><th>Breeze</th><th>Matlab</th><th>Numpy</th><th>R</th></tr></thead><tbody>
<tr><td>Imaginary unit</td><td><code>i</code></td><td><code>i</code></td><td><code>z = 1j</code></td><td><code>1i</code></td></tr>
<tr><td>Complex numbers</td><td><code>3 + 4 * i</code> or <code>Complex(3,4)</code></td><td><code>3 + 4i</code></td><td><code>z = 3 + 4j</code></td><td><code>3 + 4i</code></td></tr>
<tr><td>Absolute Value</td><td><code>abs(z)</code> or <code>z.abs</code></td><td><code>abs(z)</code></td><td><code>abs(z)</code></td><td><code>abs(z)</code></td></tr>
<tr><td>Real Component</td><td><code>z.real</code></td><td><code>real(z)</code></td><td><code>z.real</code></td><td><code>Re(z)</code></td></tr>
<tr><td>Imaginary Component</td><td><code>z.imag</code></td><td><code>imag(z)</code></td><td><code>z.imag()</code></td><td><code>Im(z)</code></td></tr>
<tr><td>Imaginary Conjugate</td><td><code>z.conjugate</code></td><td><code>conj(z)</code></td><td><code>z.conj()</code> or <code>z.conjugate()</code></td><td><code>Conj(z)</code></td></tr>
</tbody></table>
<h2 id="numeric-functions"><a class="header" href="#numeric-functions">Numeric functions</a></h2>
<p>Breeze contains a fairly comprehensive set of special functions under the <code>breeze.numerics._</code> import. These functions can be applied to single elements, vectors or matrices of Doubles. This includes versions of the special functions from <code>scala.math</code> that can be applied to vectors and matrices. Any function acting on a basic numeric type can “vectorized”, to a [[UFunc|Universal Functions]] function, which can act elementwise on vectors and matrices:</p>
<pre><code class="language-scala">val v = DenseVector(1.0,2.0,3.0)
exp(v) // == DenseVector(2.7182818284590455, 7.38905609893065, 20.085536923187668)
</code></pre>
<p>UFuncs can also be used in-place on Vectors and Matrices:</p>
<pre><code class="language-scala">val v = DenseVector(1.0,2.0,3.0)
exp.inPlace(v) // == DenseVector(2.7182818284590455, 7.38905609893065, 20.085536923187668)
</code></pre>
<p>See [[Universal Functions]] for more information.</p>
<p>Here is a (non-exhaustive) list of UFuncs in Breeze:</p>
<h3 id="trigonometry"><a class="header" href="#trigonometry">Trigonometry</a></h3>
<ul>
<li><code>sin</code>, <code>sinh</code>, <code>asin</code>, <code>asinh</code></li>
<li><code>cos</code>, <code>cosh</code>, <code>acos</code>, <code>acosh</code></li>
<li><code>tan</code>, <code>tanh</code>, <code>atan</code>, <code>atanh</code></li>
<li><code>atan2</code></li>
<li><code>sinc(x) == sin(x)/x</code></li>
<li><code>sincpi(x) == sinc(x * Pi)</code></li>
</ul>
<h3 id="logarithm-roots-and-exponentials"><a class="header" href="#logarithm-roots-and-exponentials">Logarithm, Roots, and Exponentials</a></h3>
<ul>
<li><code>log</code>, <code>exp</code> <code>log10</code></li>
<li><code>log1p</code>, <code>expm1</code></li>
<li><code>sqrt</code>, <code>sbrt</code></li>
<li><code>pow</code></li>
</ul>
<h3 id="gamma-function-and-its-cousins"><a class="header" href="#gamma-function-and-its-cousins">Gamma Function and its cousins</a></h3>
<p>The <a href="http://en.wikipedia.org/wiki/Gamma_function">gamma function</a> is the extension of the factorial function to the reals. Numpy needs <code>from scipy.special import *</code> for this and subsequent sections.</p>
<table><thead><tr><th>Operation</th><th>Breeze</th><th>Matlab</th><th>Numpy</th><th>R</th></tr></thead><tbody>
<tr><td>Gamma function</td><td><code>exp(lgamma(a))</code></td><td><code>gamma(a)</code></td><td><code>gamma(a)</code></td><td><code>gamma(a)</code></td></tr>
<tr><td>log Gamma function</td><td><code>lgamma(a)</code></td><td><code>gammaln(a)</code></td><td><code>gammaln(a)</code></td><td><code>lgamma(a)</code></td></tr>
<tr><td>Incomplete gamma function</td><td><code>gammp(a, x)</code></td><td><code>gammainc(a, x)</code></td><td><code>gammainc(a, x)</code></td><td><code>pgamma(a, x)</code> (requires stats library)</td></tr>
<tr><td>Upper incomplete gamma function</td><td><code>gammq(a, x)</code></td><td><code>gammainc(a, x, tail)</code></td><td><code>gammaincc(a, x)</code></td><td><code>pgamma(x, a, lower = FALSE) * gamma(a)</code> (requires stats library)</td></tr>
<tr><td>derivative of lgamma</td><td><code>digamma(a)</code></td><td><code>psi(a)</code></td><td><code>polygamma(0, a)</code></td><td><code>digamma(a)</code></td></tr>
<tr><td>derivative of digamma</td><td><code>trigamma(a)</code></td><td><code>psi(1, a)</code></td><td><code>polygamma(1, a)</code></td><td><code>trigama(a)</code></td></tr>
<tr><td>nth derivative of digamma</td><td>na</td><td><code>psi(n, a)</code></td><td><code>polygamma(n, a)</code></td><td><code>psigamma(a, deriv = n)</code></td></tr>
<tr><td>Log <a href="http://en.wikipedia.org/wiki/Beta_function">Beta function</a></td><td>lbeta(a,b)</td><td><code>betaln(a, b)</code></td><td><code>betaln(a,b)</code></td><td><code>lbeta(a, b)</code></td></tr>
<tr><td>Generalized Log <a href="http://en.wikipedia.org/wiki/Beta_function">Beta function</a></td><td>lbeta(a)</td><td>na</td><td>na</td><td></td></tr>
</tbody></table>
<h3 id="error-function"><a class="header" href="#error-function">Error Function</a></h3>
<p>The <a href="http://en.wikipedia.org/wiki/Error_function">error function</a>...</p>
<table><thead><tr><th>Operation</th><th>Breeze</th><th>Matlab</th><th>Numpy</th><th>R</th></tr></thead><tbody>
<tr><td>error function</td><td><code>erf(a)</code></td><td><code>erf(a)</code></td><td><code>erf(a)</code></td><td><code>2 * pnorm(a * sqrt(2)) - 1</code></td></tr>
<tr><td>1 - erf(a)</td><td><code>erfc(a)</code></td><td><code>erfc(a)</code></td><td><code>erfc(a)</code></td><td><code>2 * pnorm(a * sqrt(2), lower = FALSE)</code></td></tr>
<tr><td>inverse error function</td><td><code>erfinv(a)</code></td><td><code>erfinv(a)</code></td><td><code>erfinv(a)</code></td><td><code>qnorm((1 + a) / 2) / sqrt(2)</code></td></tr>
<tr><td>inverse erfc</td><td><code>erfcinv(a)</code></td><td><code>erfcinv(a)</code></td><td><code>erfcinv(a)</code></td><td><code>qnorm(a / 2, lower = FALSE) / sqrt(2)</code></td></tr>
</tbody></table>
<h3 id="other-functions"><a class="header" href="#other-functions">Other functions</a></h3>
<table><thead><tr><th>Operation</th><th>Breeze</th><th>Matlab</th><th>Numpy</th><th>R</th></tr></thead><tbody>
<tr><td>logistic sigmoid</td><td><code>sigmoid(a)</code></td><td>na</td><td><code>expit(a)</code></td><td><code>sigmoid(a)</code> (requires pracma library)</td></tr>
<tr><td>Indicator function</td><td><code>I(a)</code></td><td>not needed</td><td><code>where(cond, 1, 0)</code></td><td><code>0 + (a &gt; 0)</code></td></tr>
<tr><td>Polynominal evaluation</td><td><code>polyval(coef,x)</code></td><td></td><td></td><td></td></tr>
</tbody></table>
<h3 id="map-and-reduce"><a class="header" href="#map-and-reduce">Map and Reduce</a></h3>
<p>For most simple mapping tasks, one can simply use vectorized, or universal functions. Given a vector <code>v</code>, we can simply take the log of each element of a vector with <code>log(v)</code>. Sometimes, however, we want to apply a somewhat idiosyncratic function to each element of a vector. For this, we can use the map function:</p>
<pre><code class="language-scala">val v = DenseVector(1.0,2.0,3.0)
v.map( xi =&gt; foobar(xi) )
</code></pre>
<p>Breeze provides a number of built in reduction functions such as sum, mean. You can implement a custom reduction using the higher order function <code>reduce</code>. For instance, we can sum the first 9 integers as follows:</p>
<pre><code class="language-scala">val v = linspace(0,9,10)
val s = v.reduce( _ + _ )
</code></pre>
<h2 id="broadcasting"><a class="header" href="#broadcasting">Broadcasting</a></h2>
<p>Sometimes we want to apply an operation to every row or column of a matrix, as a unit. For instance, you might want to compute the mean of each row, or add a vector to every column. Adapting a matrix so that operations can be applied columnwise or rowwise is called <strong>broadcasting</strong>. Languages like R and numpy automatically and implicitly do broadcasting, meaning they won’t stop you if you accidentally add a matrix and a vector. In Breeze, you have to signal your intent using the broadcasting operator <code>*</code>. The <code>*</code> is meant to evoke “foreach” visually. Here are some examples:</p>
<pre><code class="language-scala">    val dm = DenseMatrix((1.0,2.0,3.0),
                         (4.0,5.0,6.0))

    val res = dm(::, *) + DenseVector(3.0, 4.0)
    assert(res === DenseMatrix((4.0, 5.0, 6.0), (8.0, 9.0, 10.0)))

    res(::, *) := DenseVector(3.0, 4.0)
    assert(res === DenseMatrix((3.0, 3.0, 3.0), (4.0, 4.0, 4.0)))

    val m = DenseMatrix((1.0, 3.0), (4.0, 4.0))
    // unbroadcasted sums all elements
    assert(sum(m) === 12.0)
    assert(mean(m) === 3.0)

    assert(sum(m(*, ::)) === DenseVector(4.0, 8.0))
    assert(sum(m(::, *)) === DenseMatrix((5.0, 7.0)))

    assert(mean(m(*, ::)) === DenseVector(2.0, 4.0))
    assert(mean(m(::, *)) === DenseMatrix((2.5, 3.5)))

</code></pre>
<p>The UFunc trait is similar to numpy’s ufunc. See [[Universal Functions]] for more information on Breeze UFuncs.</p>
<h2 id="casting-and-type-safety"><a class="header" href="#casting-and-type-safety">Casting and type safety</a></h2>
<p>Compared to Numpy and Matlab, Breeze requires you to be more explicit about the types of your variables. When you create a new vector for example, you must specify a type (such as in <code>DenseVector.zeros[Double](n)</code>) in cases where a type can not be inferred automatically. Automatic inference will occur when you create a vector by passing its initial values in (<code>DenseVector</code>). A common mistake is using integers for initialisation (e.g. <code>DenseVector</code>), which would give a matrix of integers instead of doubles. Both Numpy and Matlab would default to doubles instead.</p>
<p>Breeze will not convert integers to doubles for you in most expressions. Simple operations like <code>a :+ 3</code> when <code>a</code> is a <code>DenseVector[Double]</code> will not compile. Breeze provides a convert function, which can be used to explicitly cast. You can also use <code>v.mapValues(_.toDouble)</code>.</p>
<h3 id="casting"><a class="header" href="#casting">Casting</a></h3>
<table><thead><tr><th>Operation</th><th>Breeze</th><th>Matlab</th><th>Numpy</th><th>R</th></tr></thead><tbody>
<tr><td>Convert to Int</td><td><code>convert(a, Int)</code></td><td><code>int(a)</code></td><td><code>a.astype(int)</code></td><td><code>as.integer(a)</code></td></tr>
</tbody></table>
<h2 id="performance"><a class="header" href="#performance">Performance</a></h2>
<p>Breeze uses <a href="https://github.com/fommil/netlib-java/">netlib-java</a> for its core linear algebra routines. This includes all the cubic time operations, matrix-matrix and matrix-vector multiplication. Special efforts are taken to ensure that arrays are not copied.</p>
<p>Netlib-java will attempt to load system optimised BLAS/LAPACK if they are installed, falling back to the reference natives, falling back to pure Java. Set your logger settings to <code>ALL</code> for the <code>com.github.fommil.netlib</code> package to check the status, and to <code>com.github.fommil.jniloader</code> for a more detailed breakdown. Read the netlib-java project page for more details.</p>
<p>Currently vectors and matrices over types other than <code>Double</code>, <code>Float</code> and <code>Int</code> are boxed, so they will typically be a lot slower. If you find yourself needing other AnyVal types like <code>Long</code> or <code>Short</code>, please ask on the list about possibly adding support for them.</p>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-1"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-1"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
</div>
<div class="cell markdown">
<p>This is an elaboration of the small subset of <a href="http://spark.apache.org/docs/latest/mllib-guide.html">Apache Spark 2.2 mllib-progamming-guide</a> that one needs to dive a bit deeper into distributed linear algebra.</p>
<p>This is a huge task to complete for the entire mlib-programming-guide. Perhaps worth continuing for Spark 2.2. Any contributions in this 'databricksification' of the programming guide are most welcome. Please feel free to send pull-requests or just fork and push yourself at <a href="https://github.com/lamastex/scalable-data-science">https://github.com/raazesh-sainudiin/scalable-data-science</a>.</p>
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/000_MLlibProgGuide">Overview</a></a></h1>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/000_dataTypesProgGuide">Data Types - MLlib Programming Guide</a>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/001_LocalVector">Local vector</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-vector">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/002_LabeledPoint">Labeled point</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/003_LocalMatrix">Local matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-matrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/004_DistributedMatrix">Distributed matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#distributed-matrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/005_RowMatrix">RowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#rowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/006_IndexedRowMatrix">IndexedRowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#indexedrowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/007_CoordinateMatrix">CoordinateMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#coordinatematrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/008_BlockMatrix">BlockMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix">URL</a></li>
</ul>
</li>
</ul>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-2"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-2"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/mllib-data-types.html">Apache Spark mllib-progamming-guide on mllib-data-types</a>.</p>
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/000_MLlibProgGuide">Overview</a></a></h1>
<h2 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/000_dataTypesProgGuide">Data Types - MLlib Programming Guide</a></a></h2>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/001_LocalVector">Local vector</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-vector">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/002_LabeledPoint">Labeled point</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/003_LocalMatrix">Local matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-matrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/004_DistributedMatrix">Distributed matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#distributed-matrix">URL</a>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/005_RowMatrix">RowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#rowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/006_IndexedRowMatrix">IndexedRowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#indexedrowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/007_CoordinateMatrix">CoordinateMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#coordinatematrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/008_BlockMatrix">BlockMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix">URL</a></li>
</ul>
</li>
</ul>
<p>MLlib supports local vectors and matrices stored on a single machine, as well as distributed matrices backed by one or more RDDs. Local vectors and local matrices are simple data models that serve as public interfaces. The underlying linear algebra operations are provided by <a href="http://www.scalanlp.org/">Breeze</a> and <a href="http://jblas.org/">jblas</a>. A training example used in supervised learning is called a “labeled point” in MLlib.</p>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-3"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-3"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/mllib-data-types.html">Apache Spark mllib-progamming-guide on mllib-data-types</a>.</p>
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa-1"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa-1"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/000_MLlibProgGuide">Overview</a></a></h1>
<h2 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea-1"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea-1"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/000_dataTypesProgGuide">Data Types - MLlib Programming Guide</a></a></h2>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/001_LocalVector">Local vector</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-vector">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/002_LabeledPoint">Labeled point</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/003_LocalMatrix">Local matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-matrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/004_DistributedMatrix">Distributed matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#distributed-matrix">URL</a>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/005_RowMatrix">RowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#rowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/006_IndexedRowMatrix">IndexedRowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#indexedrowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/007_CoordinateMatrix">CoordinateMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#coordinatematrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/008_BlockMatrix">BlockMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix">URL</a></li>
</ul>
</li>
</ul>
<p>MLlib supports local vectors and matrices stored on a single machine, as well as distributed matrices backed by one or more RDDs. Local vectors and local matrices are simple data models that serve as public interfaces. The underlying linear algebra operations are provided by <a href="http://www.scalanlp.org/">Breeze</a> and <a href="http://jblas.org/">jblas</a>. A training example used in supervised learning is called a “labeled point” in MLlib.</p>
</div>
<div class="cell markdown">
<h2 id="local-vector-in-scala"><a class="header" href="#local-vector-in-scala">Local vector in Scala</a></h2>
<p>A local vector has integer-typed and 0-based indices and double-typed values, stored on a single machine.</p>
<p>MLlib supports two types of local vectors: * dense and * sparse.</p>
<p>A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values.</p>
<p>For example, a vector <code>(1.0, 0.0, 3.0)</code> can be represented: * in dense format as <code>[1.0, 0.0, 3.0]</code> or * in sparse format as <code>(3, [0, 2], [1.0, 3.0])</code>, where <code>3</code> is the size of the vector.</p>
<p>The base class of local vectors is <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.Vector"><code>Vector</code></a>, and we provide two implementations: <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.DenseVector"><code>DenseVector</code></a> and <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SparseVector"><code>SparseVector</code></a>. We recommend using the factory methods implemented in <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.Vectors$"><code>Vectors</code></a> to create local vectors. Refer to the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.Vector"><code>Vector</code> Scala docs</a> and <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.Vectors$"><code>Vectors</code> Scala docs</a> for details on the API.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.mllib.linalg.{Vector, Vectors}

// Create a dense vector (1.0, 0.0, 3.0).
val dv: Vector = Vectors.dense(1.0, 0.0, 3.0)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.mllib.linalg.{Vector, Vectors}
dv: org.apache.spark.mllib.linalg.Vector = [1.0,0.0,3.0]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create a sparse vector (1.0, 0.0, 3.0) by specifying its indices and values corresponding to nonzero entries.
val sv1: Vector = Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>sv1: org.apache.spark.mllib.linalg.Vector = (3,[0,2],[1.0,3.0])
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create a sparse vector (1.0, 0.0, 3.0) by specifying its nonzero entries.
val sv2: Vector = Vectors.sparse(3, Seq((0, 1.0), (2, 3.0)))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>sv2: org.apache.spark.mllib.linalg.Vector = (3,[0,2],[1.0,3.0])
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><em><strong>Note:</strong></em> Scala imports <code>scala.collection.immutable.Vector</code> by default, so you have to import <code>org.apache.spark.mllib.linalg.Vector</code> explicitly to use MLlib’s <code>Vector</code>.</p>
</div>
<div class="cell markdown">
<hr />
<hr />
<h2 id="local-vector-in-python"><a class="header" href="#local-vector-in-python">Local Vector in Python</a></h2>
</div>
<div class="cell markdown">
<p><strong>python</strong>: MLlib recognizes the following types as dense vectors:</p>
<ul>
<li>NumPy’s <a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html"><code>array</code></a></li>
<li>Python’s list, e.g., <code>[1, 2, 3]</code></li>
</ul>
<p>and the following as sparse vectors:</p>
<ul>
<li>MLlib’s <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector"><code>SparseVector</code></a>.</li>
<li>SciPy’s <a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html#scipy.sparse.csc_matrix"><code>csc_matrix</code></a> with a single column</li>
</ul>
<p>We recommend using NumPy arrays over lists for efficiency, and using the factory methods implemented in <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vectors"><code>Vectors</code></a> to create sparse vectors.</p>
<p>Refer to the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vectors"><code>Vectors</code> Python docs</a> for more details on the API.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">import numpy as np
import scipy.sparse as sps
from pyspark.mllib.linalg import Vectors

# Use a NumPy array as a dense vector.
dv1 = np.array([1.0, 0.0, 3.0])
# Use a Python list as a dense vector.
dv2 = [1.0, 0.0, 3.0]
# Create a SparseVector.
sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])
# Use a single-column SciPy csc_matrix as a sparse vector.
sv2 = sps.csc_matrix((np.array([1.0, 3.0]), np.array([0, 2]), np.array([0, 2])), shape = (3, 1))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">print dv1
print dv2
print sv1
print sv2
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>[ 1.  0.  3.]
[1.0, 0.0, 3.0]
(3,[0,2],[1.0,3.0])
  (0, 0)	1.0
  (2, 0)	3.0
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-4"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-4"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/mllib-data-types.html">Apache Spark mllib-progamming-guide on mllib-data-types</a>.</p>
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa-2"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa-2"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/000_MLlibProgGuide">Overview</a></a></h1>
<h2 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea-2"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea-2"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/000_dataTypesProgGuide">Data Types - MLlib Programming Guide</a></a></h2>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/001_LocalVector">Local vector</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-vector">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/002_LabeledPoint">Labeled point</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/003_LocalMatrix">Local matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-matrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/004_DistributedMatrix">Distributed matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#distributed-matrix">URL</a>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/005_RowMatrix">RowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#rowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/006_IndexedRowMatrix">IndexedRowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#indexedrowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/007_CoordinateMatrix">CoordinateMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#coordinatematrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/008_BlockMatrix">BlockMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix">URL</a></li>
</ul>
</li>
</ul>
<p>MLlib supports local vectors and matrices stored on a single machine, as well as distributed matrices backed by one or more RDDs. Local vectors and local matrices are simple data models that serve as public interfaces. The underlying linear algebra operations are provided by <a href="http://www.scalanlp.org/">Breeze</a> and <a href="http://jblas.org/">jblas</a>. A training example used in supervised learning is called a “labeled point” in MLlib.</p>
</div>
<div class="cell markdown">
<h2 id="labeled-point-in-scala"><a class="header" href="#labeled-point-in-scala">Labeled point in Scala</a></h2>
<p>A labeled point is a local vector, either dense or sparse, associated with a label/response. In MLlib, labeled points are used in supervised learning algorithms.</p>
<p>We use a double to store a label, so we can use labeled points in both regression and classification.</p>
<p>For binary classification, a label should be either <code>0</code> (negative) or <code>1</code> (positive). For multiclass classification, labels should be class indices starting from zero: <code>0, 1, 2, ...</code>.</p>
<p>A labeled point is represented by the case class <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.LabeledPoint"><code>LabeledPoint</code></a>.</p>
<p>Refer to the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.LabeledPoint"><code>LabeledPoint</code> Scala docs</a> for details on the API.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//import first
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create a labeled point with a &quot;positive&quot; label and a dense feature vector.
val pos = LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>pos: org.apache.spark.mllib.regression.LabeledPoint = (1.0,[1.0,0.0,3.0])
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create a labeled point with a &quot;negative&quot; label and a sparse feature vector.
val neg = LabeledPoint(0.0, Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0)))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>neg: org.apache.spark.mllib.regression.LabeledPoint = (0.0,(3,[0,2],[1.0,3.0]))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><em><strong>Sparse data in Scala</strong></em></p>
<p>It is very common in practice to have sparse training data. MLlib supports reading training examples stored in <code>LIBSVM</code> format, which is the default format used by <a href="http://www.csie.ntu.edu.tw/%7Ecjlin/libsvm/"><code>LIBSVM</code></a> and <a href="http://www.csie.ntu.edu.tw/%7Ecjlin/liblinear/"><code>LIBLINEAR</code></a>. It is a text format in which each line represents a labeled sparse feature vector using the following format:</p>
<pre><code>label index1:value1 index2:value2 ...
</code></pre>
<p>where the indices are one-based and in ascending order. After loading, the feature indices are converted to zero-based.</p>
<p><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.util.MLUtils$"><code>MLUtils.loadLibSVMFile</code></a> reads training examples stored in LIBSVM format.</p>
<p>Refer to the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.util.MLUtils"><code>MLUtils</code> Scala docs</a> for details on the API.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.rdd.RDD

//val examples: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, &quot;data/mllib/sample_libsvm_data.txt&quot;) // from prog guide but no such data here - can wget from github 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.rdd.RDD
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="load-mnist-training-and-test-datasets"><a class="header" href="#load-mnist-training-and-test-datasets">Load MNIST training and test datasets</a></h2>
<p>Our datasets are vectors of pixels representing images of handwritten digits. For example:</p>
<p><img src="http://training.databricks.com/databricks_guide/digit.png" alt="Image of a digit" /> <img src="http://training.databricks.com/databricks_guide/MNIST-small.png" alt="Image of all 10 digits" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt&quot;))
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt</td>
<td>mnist-digits-train.txt</td>
<td>6.9430283e7</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val examples: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, &quot;/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>examples: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[69] at map at MLUtils.scala:84
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">examples.take(1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res1: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((5.0,(780,[152,153,154,155,156,157,158,159,160,161,162,163,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,231,232,233,234,235,236,237,238,239,240,241,260,261,262,263,264,265,266,268,269,289,290,291,292,293,319,320,321,322,347,348,349,350,376,377,378,379,380,381,405,406,407,408,409,410,434,435,436,437,438,439,463,464,465,466,467,493,494,495,496,518,519,520,521,522,523,524,544,545,546,547,548,549,550,551,570,571,572,573,574,575,576,577,578,596,597,598,599,600,601,602,603,604,605,622,623,624,625,626,627,628,629,630,631,648,649,650,651,652,653,654,655,656,657,676,677,678,679,680,681,682,683],[3.0,18.0,18.0,18.0,126.0,136.0,175.0,26.0,166.0,255.0,247.0,127.0,30.0,36.0,94.0,154.0,170.0,253.0,253.0,253.0,253.0,253.0,225.0,172.0,253.0,242.0,195.0,64.0,49.0,238.0,253.0,253.0,253.0,253.0,253.0,253.0,253.0,253.0,251.0,93.0,82.0,82.0,56.0,39.0,18.0,219.0,253.0,253.0,253.0,253.0,253.0,198.0,182.0,247.0,241.0,80.0,156.0,107.0,253.0,253.0,205.0,11.0,43.0,154.0,14.0,1.0,154.0,253.0,90.0,139.0,253.0,190.0,2.0,11.0,190.0,253.0,70.0,35.0,241.0,225.0,160.0,108.0,1.0,81.0,240.0,253.0,253.0,119.0,25.0,45.0,186.0,253.0,253.0,150.0,27.0,16.0,93.0,252.0,253.0,187.0,249.0,253.0,249.0,64.0,46.0,130.0,183.0,253.0,253.0,207.0,2.0,39.0,148.0,229.0,253.0,253.0,253.0,250.0,182.0,24.0,114.0,221.0,253.0,253.0,253.0,253.0,201.0,78.0,23.0,66.0,213.0,253.0,253.0,253.0,253.0,198.0,81.0,2.0,18.0,171.0,219.0,253.0,253.0,253.0,253.0,195.0,80.0,9.0,55.0,172.0,226.0,253.0,253.0,253.0,253.0,244.0,133.0,11.0,136.0,253.0,253.0,253.0,212.0,135.0,132.0,16.0])))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Display our data. Each image has the true label (the <code>label</code> column) and a vector of <code>features</code> which represent pixel intensities (see below for details of what is in <code>training</code>).</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(examples.toDF) // covert to DataFrame and display for convenient db visualization
</code></pre>
</div>
<div class="cell markdown">
<p>The pixel intensities are represented in <code>features</code> as a sparse vector, for example the first observation, as seen in row 1 of the output to <code>display(training)</code> below, has <code>label</code> as <code>5</code>, i.e. the hand-written image is for the number 5. And this hand-written image is the following sparse vector (just click the triangle to the left of the feature in first row to see the following):</p>
<pre><code>type: 0
size: 780
indices: [152,153,155,...,682,683]
values: [3, 18, 18,18,126,...,132,16]
</code></pre>
<p>Here * <code>type: 0</code> says we hve a sparse vector. * <code>size: 780</code> says the vector has 780 indices in total * these indices from 0,...,779 are a unidimensional indexing of the two-dimensional array of pixels in the image * <code>indices: [152,153,155,...,682,683]</code> are the indices from the <code>[0,1,...,779]</code> possible indices with non-zero values * a value is an integer encoding the gray-level at the pixel index * <code>values: [3, 18, 18,18,126,...,132,16]</code> are the actual gray level values, for example: * at pixed index <code>152</code> the gray-level value is <code>3</code>, * at index <code>153</code> the gray-level value is <code>18</code>, * ..., and finally at * at index <code>683</code> the gray-level value is <code>18</code></p>
</div>
<div class="cell markdown">
<p>We could also use the following method as done in notebook <code>016_*</code> already.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val training = spark.read.format(&quot;libsvm&quot;)
                    .option(&quot;numFeatures&quot;, &quot;780&quot;)
                    .load(&quot;/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>training: org.apache.spark.sql.DataFrame = [label: double, features: vector]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(training)
</code></pre>
</div>
<div class="cell markdown">
<hr />
<hr />
<h2 id="labeled-point-in-python"><a class="header" href="#labeled-point-in-python">Labeled point in Python</a></h2>
<p>A labeled point is represented by <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint"><code>LabeledPoint</code></a>.</p>
<p>Refer to the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint"><code>LabeledPoint</code> Python docs</a> for more details on the API.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># import first
from pyspark.mllib.linalg import SparseVector
from pyspark.mllib.regression import LabeledPoint

# Create a labeled point with a positive label and a dense feature vector.
pos = LabeledPoint(1.0, [1.0, 0.0, 3.0])

# Create a labeled point with a negative label and a sparse feature vector.
neg = LabeledPoint(0.0, SparseVector(3, [0, 2], [1.0, 3.0]))
</code></pre>
</div>
<div class="cell markdown">
<p><em><strong>Sparse data in Python</strong></em></p>
<p><a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.util.MLUtils"><code>MLUtils.loadLibSVMFile</code></a> reads training examples stored in LIBSVM format.</p>
<p>Refer to the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.util.MLUtils"><code>MLUtils</code> Python docs</a> for more details on the API.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from pyspark.mllib.util import MLUtils

# examples = MLUtils.loadLibSVMFile(sc, &quot;data/mllib/sample_libsvm_data.txt&quot;) #from prog guide but no such data here - can wget from github 
examples = MLUtils.loadLibSVMFile(sc, &quot;/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">examples.take(1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res4: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((5.0,(780,[152,153,154,155,156,157,158,159,160,161,162,163,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,231,232,233,234,235,236,237,238,239,240,241,260,261,262,263,264,265,266,268,269,289,290,291,292,293,319,320,321,322,347,348,349,350,376,377,378,379,380,381,405,406,407,408,409,410,434,435,436,437,438,439,463,464,465,466,467,493,494,495,496,518,519,520,521,522,523,524,544,545,546,547,548,549,550,551,570,571,572,573,574,575,576,577,578,596,597,598,599,600,601,602,603,604,605,622,623,624,625,626,627,628,629,630,631,648,649,650,651,652,653,654,655,656,657,676,677,678,679,680,681,682,683],[3.0,18.0,18.0,18.0,126.0,136.0,175.0,26.0,166.0,255.0,247.0,127.0,30.0,36.0,94.0,154.0,170.0,253.0,253.0,253.0,253.0,253.0,225.0,172.0,253.0,242.0,195.0,64.0,49.0,238.0,253.0,253.0,253.0,253.0,253.0,253.0,253.0,253.0,251.0,93.0,82.0,82.0,56.0,39.0,18.0,219.0,253.0,253.0,253.0,253.0,253.0,198.0,182.0,247.0,241.0,80.0,156.0,107.0,253.0,253.0,205.0,11.0,43.0,154.0,14.0,1.0,154.0,253.0,90.0,139.0,253.0,190.0,2.0,11.0,190.0,253.0,70.0,35.0,241.0,225.0,160.0,108.0,1.0,81.0,240.0,253.0,253.0,119.0,25.0,45.0,186.0,253.0,253.0,150.0,27.0,16.0,93.0,252.0,253.0,187.0,249.0,253.0,249.0,64.0,46.0,130.0,183.0,253.0,253.0,207.0,2.0,39.0,148.0,229.0,253.0,253.0,253.0,250.0,182.0,24.0,114.0,221.0,253.0,253.0,253.0,253.0,201.0,78.0,23.0,66.0,213.0,253.0,253.0,253.0,253.0,198.0,81.0,2.0,18.0,171.0,219.0,253.0,253.0,253.0,253.0,195.0,80.0,9.0,55.0,172.0,226.0,253.0,253.0,253.0,253.0,244.0,133.0,11.0,136.0,253.0,253.0,253.0,212.0,135.0,132.0,16.0])))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-5"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-5"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/mllib-data-types.html">Apache Spark mllib-progamming-guide on mllib-data-types</a>.</p>
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa-3"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa-3"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/000_MLlibProgGuide">Overview</a></a></h1>
<h2 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea-3"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea-3"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/000_dataTypesProgGuide">Data Types - MLlib Programming Guide</a></a></h2>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/001_LocalVector">Local vector</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-vector">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/002_LabeledPoint">Labeled point</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/003_LocalMatrix">Local matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-matrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/004_DistributedMatrix">Distributed matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#distributed-matrix">URL</a>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/005_RowMatrix">RowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#rowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/006_IndexedRowMatrix">IndexedRowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#indexedrowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/007_CoordinateMatrix">CoordinateMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#coordinatematrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/008_BlockMatrix">BlockMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix">URL</a></li>
</ul>
</li>
</ul>
<p>MLlib supports local vectors and matrices stored on a single machine, as well as distributed matrices backed by one or more RDDs. Local vectors and local matrices are simple data models that serve as public interfaces. The underlying linear algebra operations are provided by <a href="http://www.scalanlp.org/">Breeze</a> and <a href="http://jblas.org/">jblas</a>. A training example used in supervised learning is called a “labeled point” in MLlib.</p>
</div>
<div class="cell markdown">
<h2 id="local-matrix-in-scala"><a class="header" href="#local-matrix-in-scala">Local Matrix in Scala</a></h2>
<p>A local matrix has integer-typed row and column indices and double-typed values, <strong>stored on a single machine</strong>. MLlib supports: * dense matrices, whose entry values are stored in a single double array in column-major order, and * sparse matrices, whose non-zero entry values are stored in the Compressed Sparse Column (CSC) format in column-major order.</p>
<p>For example, the following dense matrix: \[ \begin{pmatrix} 1.0 &amp; 2.0 \\ 3.0 &amp; 4.0 \\ 5.0 &amp; 6.0 \end{pmatrix} \] is stored in a one-dimensional array <code>[1.0, 3.0, 5.0, 2.0, 4.0, 6.0]</code> with the matrix size <code>(3, 2)</code>.</p>
<p>The base class of local matrices is <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.Matrix"><code>Matrix</code></a>, and we provide two implementations: <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.DenseMatrix"><code>DenseMatrix</code></a>, and <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SparseMatrix"><code>SparseMatrix</code></a>. We recommend using the factory methods implemented in <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.Matrices$"><code>Matrices</code></a> to create local matrices. Remember, local matrices in MLlib are stored in column-major order.</p>
<p>Refer to the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.Matrix"><code>Matrix</code> Scala docs</a> and <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.Matrices"><code>Matrices</code> Scala docs</a> for details on the API.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">Int.MaxValue // note the largest value an index can take
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: Int = 2147483647
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.mllib.linalg.{Matrix, Matrices}

// Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))
val dm: Matrix = Matrices.dense(3, 2, Array(1.0, 3.0, 5.0, 2.0, 4.0, 6.0))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.mllib.linalg.{Matrix, Matrices}
dm: org.apache.spark.mllib.linalg.Matrix =
1.0  2.0
3.0  4.0
5.0  6.0
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Next, let us create the following sparse local matrix: \[ \begin{pmatrix} 9.0 &amp; 0.0 \\ 0.0 &amp; 8.0 \\ 0.0 &amp; 6.0 \end{pmatrix} \]</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))
val sm: Matrix = Matrices.sparse(3, 2, Array(0, 1, 3), Array(0, 2, 1), Array(9, 6, 8))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>sm: org.apache.spark.mllib.linalg.Matrix =
3 x 2 CSCMatrix
(0,0) 9.0
(2,1) 6.0
(1,1) 8.0
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="local-matrix-in-python"><a class="header" href="#local-matrix-in-python">Local Matrix in Python</a></h2>
<p>The base class of local matrices is <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Matrix"><code>Matrix</code></a>, and we provide two implementations: <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.DenseMatrix"><code>DenseMatrix</code></a>, and <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseMatrix"><code>SparseMatrix</code></a>. We recommend using the factory methods implemented in <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Matrices"><code>Matrices</code></a> to create local matrices. Remember, local matrices in MLlib are stored in column-major order.</p>
<p>Refer to the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Matrix"><code>Matrix</code> Python docs</a> and <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Matrices"><code>Matrices</code> Python docs</a> for more details on the API.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from pyspark.mllib.linalg import Matrix, Matrices

# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))
dm2 = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])
dm2
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>&lt;span class=&quot;ansired&quot;&gt;Out[&lt;/span&gt;&lt;span class=&quot;ansired&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;ansired&quot;&gt;]: &lt;/span&gt;DenseMatrix(3, 2, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], False)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))
sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])
sm
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>&lt;span class=&quot;ansired&quot;&gt;Out[&lt;/span&gt;&lt;span class=&quot;ansired&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;ansired&quot;&gt;]: &lt;/span&gt;SparseMatrix(3, 2, [0, 1, 3], [0, 2, 1], [9.0, 6.0, 8.0], False)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-6"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-6"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/mllib-data-types.html">Apache Spark mllib-progamming-guide on mllib-data-types</a>.</p>
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa-4"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa-4"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/000_MLlibProgGuide">Overview</a></a></h1>
<h2 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea-4"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea-4"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/000_dataTypesProgGuide">Data Types - MLlib Programming Guide</a></a></h2>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/001_LocalVector">Local vector</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-vector">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/002_LabeledPoint">Labeled point</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/003_LocalMatrix">Local matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-matrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/004_DistributedMatrix">Distributed matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#distributed-matrix">URL</a>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/005_RowMatrix">RowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#rowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/006_IndexedRowMatrix">IndexedRowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#indexedrowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/007_CoordinateMatrix">CoordinateMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#coordinatematrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/008_BlockMatrix">BlockMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix">URL</a></li>
</ul>
</li>
</ul>
<p>MLlib supports local vectors and matrices stored on a single machine, as well as distributed matrices backed by one or more RDDs. Local vectors and local matrices are simple data models that serve as public interfaces. The underlying linear algebra operations are provided by <a href="http://www.scalanlp.org/">Breeze</a> and <a href="http://jblas.org/">jblas</a>. A training example used in supervised learning is called a “labeled point” in MLlib.</p>
</div>
<div class="cell markdown">
<h2 id="distributed-matrix-in-scala"><a class="header" href="#distributed-matrix-in-scala">Distributed matrix in Scala</a></h2>
<p>A distributed matrix has <strong>long-typed row and column indices</strong> and <strong>double-typed values</strong>, stored distributively in one or more RDDs.</p>
<p>It is <strong>very important to choose the right format to store large and distributed matrices</strong>. Converting a distributed matrix to a different format may require a global shuffle, which is quite expensive.</p>
<p>Three types of distributed matrices have been implemented so far.</p>
<ol>
<li>The basic type is called <code>RowMatrix</code>.</li>
</ol>
<ul>
<li>A <code>RowMatrix</code> is a row-oriented distributed matrix without meaningful row indices, e.g., a collection of feature vectors. It is backed by an RDD of its rows, where each row is a local vector.</li>
<li>We assume that the number of columns is not huge for a <code>RowMatrix</code> so that a single local vector can be reasonably communicated to the driver and can also be stored / operated on using a single node.</li>
<li>An <code>IndexedRowMatrix</code> is similar to a <code>RowMatrix</code> but with row indices, which can be used for identifying rows and executing joins.</li>
<li>A <code>CoordinateMatrix</code> is a distributed matrix stored in <a href="https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_.28COO.29">coordinate list (COO)</a> format, backed by an RDD of its entries.</li>
</ul>
<p><em><strong>Note</strong></em></p>
<p>The underlying RDDs of a distributed matrix must be deterministic, because we cache the matrix size. In general the use of non-deterministic RDDs can lead to errors.</p>
</div>
<div class="cell markdown">
<p><em><strong>Remark:</strong></em> there is a huge difference in the orders of magnitude between the maximum size of local versus distributed matrices!</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">print(Long.MaxValue.toDouble, Int.MaxValue.toDouble, Long.MaxValue.toDouble / Int.MaxValue.toDouble) // index ranges and ratio for local and distributed matrices
</code></pre>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-7"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-7"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/mllib-data-types.html">Apache Spark mllib-progamming-guide on mllib-data-types</a>.</p>
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa-5"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa-5"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/000_MLlibProgGuide">Overview</a></a></h1>
<h2 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea-5"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea-5"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/000_dataTypesProgGuide">Data Types - MLlib Programming Guide</a></a></h2>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/001_LocalVector">Local vector</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-vector">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/002_LabeledPoint">Labeled point</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/003_LocalMatrix">Local matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-matrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/004_DistributedMatrix">Distributed matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#distributed-matrix">URL</a>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/005_RowMatrix">RowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#rowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/006_IndexedRowMatrix">IndexedRowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#indexedrowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/007_CoordinateMatrix">CoordinateMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#coordinatematrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/008_BlockMatrix">BlockMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix">URL</a></li>
</ul>
</li>
</ul>
<p>MLlib supports local vectors and matrices stored on a single machine, as well as distributed matrices backed by one or more RDDs. Local vectors and local matrices are simple data models that serve as public interfaces. The underlying linear algebra operations are provided by <a href="http://www.scalanlp.org/">Breeze</a> and <a href="http://jblas.org/">jblas</a>. A training example used in supervised learning is called a “labeled point” in MLlib.</p>
</div>
<div class="cell markdown">
<h3 id="rowmatrix-in-scala"><a class="header" href="#rowmatrix-in-scala">RowMatrix in Scala</a></h3>
<p>A <code>RowMatrix</code> is a row-oriented distributed matrix without meaningful row indices, backed by an RDD of its rows, where each row is a local vector. Since each row is represented by a local vector, <strong>the number of columns is limited by the integer range but it should be much smaller in practice</strong>.</p>
<p>A <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix"><code>RowMatrix</code></a> can be created from an <code>RDD[Vector]</code> instance. Then we can compute its column summary statistics and decompositions.</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/QR_decomposition">QR decomposition</a> is of the form A = QR where Q is an orthogonal matrix and R is an upper triangular matrix.</li>
<li>For <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition (SVD)</a> and <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">principal component analysis (PCA)</a>, please refer to <a href="http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html">Dimensionality reduction</a>.</li>
</ul>
<p>Refer to the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix"><code>RowMatrix</code> Scala docs</a> for details on the API.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.mllib.linalg.distributed.RowMatrix
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.mllib.linalg.distributed.RowMatrix
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rows: RDD[Vector] = sc.parallelize(Array(Vectors.dense(12.0, -51.0, 4.0), Vectors.dense(6.0, 167.0, -68.0), Vectors.dense(-4.0, 24.0, -41.0))) // an RDD of local vectors
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rows: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = ParallelCollectionRDD[18] at parallelize at &lt;console&gt;:36
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create a RowMatrix from an RDD[Vector].
val mat: RowMatrix = new RowMatrix(rows)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>mat: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@720029a1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">mat.rows.collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: Array[org.apache.spark.mllib.linalg.Vector] = Array([12.0,-51.0,4.0], [6.0,167.0,-68.0], [-4.0,24.0,-41.0])
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Get its size.
val m = mat.numRows()
val n = mat.numCols()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>m: Long = 3
n: Long = 3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// QR decomposition
val qrResult = mat.tallSkinnyQR(true)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>qrResult: org.apache.spark.mllib.linalg.QRDecomposition[org.apache.spark.mllib.linalg.distributed.RowMatrix,org.apache.spark.mllib.linalg.Matrix] = 
QRDecomposition(org.apache.spark.mllib.linalg.distributed.RowMatrix@299d426,14.0  21.0                 -14.0                
0.0   -174.99999999999997  70.00000000000001    
0.0   0.0                  -35.000000000000014  )
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">qrResult.R
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res1: org.apache.spark.mllib.linalg.Matrix = 
14.0  21.0                 -14.0                
0.0   -174.99999999999997  70.00000000000001    
0.0   0.0                  -35.000000000000014  
</code></pre>
</div>
</div>
<div class="cell markdown">
<hr />
<hr />
<h3 id="rowmatrix-in-python"><a class="header" href="#rowmatrix-in-python">RowMatrix in Python</a></h3>
<p>A <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.RowMatrix"><code>RowMatrix</code></a> can be created from an <code>RDD</code> of vectors.</p>
<p>Refer to the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.RowMatrix"><code>RowMatrix</code> Python docs</a> for more details on the API.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from pyspark.mllib.linalg.distributed import RowMatrix

# Create an RDD of vectors.
rows = sc.parallelize([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])

# Create a RowMatrix from an RDD of vectors.
mat = RowMatrix(rows)

# Get its size.
m = mat.numRows()  # 4
n = mat.numCols()  # 3
print m,'x',n

# Get the rows as an RDD of vectors again.
rowsRDD = mat.rows
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>4 x 3
</code></pre>
</div>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-8"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-8"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/mllib-data-types.html">Apache Spark mllib-progamming-guide on mllib-data-types</a>.</p>
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa-6"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa-6"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/000_MLlibProgGuide">Overview</a></a></h1>
<h2 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea-6"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea-6"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/000_dataTypesProgGuide">Data Types - MLlib Programming Guide</a></a></h2>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/001_LocalVector">Local vector</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-vector">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/002_LabeledPoint">Labeled point</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/003_LocalMatrix">Local matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-matrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/004_DistributedMatrix">Distributed matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#distributed-matrix">URL</a>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/005_RowMatrix">RowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#rowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/006_IndexedRowMatrix">IndexedRowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#indexedrowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/007_CoordinateMatrix">CoordinateMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#coordinatematrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/008_BlockMatrix">BlockMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix">URL</a></li>
</ul>
</li>
</ul>
<p>MLlib supports local vectors and matrices stored on a single machine, as well as distributed matrices backed by one or more RDDs. Local vectors and local matrices are simple data models that serve as public interfaces. The underlying linear algebra operations are provided by <a href="http://www.scalanlp.org/">Breeze</a> and <a href="http://jblas.org/">jblas</a>. A training example used in supervised learning is called a “labeled point” in MLlib.</p>
</div>
<div class="cell markdown">
<h3 id="indexedrowmatrix-in-scala"><a class="header" href="#indexedrowmatrix-in-scala">IndexedRowMatrix in Scala</a></h3>
<p>An <code>IndexedRowMatrix</code> is similar to a <code>RowMatrix</code> but with meaningful row indices. It is backed by an RDD of indexed rows, so that each row is represented by its index (long-typed) and a local vector.</p>
<p>An <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix"><code>IndexedRowMatrix</code></a> can be created from an <code>RDD[IndexedRow]</code> instance, where <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.IndexedRow"><code>IndexedRow</code></a> is a wrapper over <code>(Long, Vector)</code>. An <code>IndexedRowMatrix</code> can be converted to a <code>RowMatrix</code> by dropping its row indices.</p>
<p>Refer to the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix"><code>IndexedRowMatrix</code> Scala docs</a> for details on the API.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix, RowMatrix}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix, RowMatrix}
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">Vector(12.0, -51.0, 4.0) // note Vector is a scala collection
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res8: scala.collection.immutable.Vector[Double] = Vector(12.0, -51.0, 4.0)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">Vectors.dense(12.0, -51.0, 4.0) // while this is a mllib.linalg.Vector
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res9: org.apache.spark.mllib.linalg.Vector = [12.0,-51.0,4.0]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rows: RDD[IndexedRow] = sc.parallelize(Array(IndexedRow(2, Vectors.dense(1,3)), IndexedRow(4, Vectors.dense(4,5)))) // an RDD of indexed rows
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rows: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.distributed.IndexedRow] = ParallelCollectionRDD[252] at parallelize at &lt;console&gt;:41
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create an IndexedRowMatrix from an RDD[IndexedRow].
val mat: IndexedRowMatrix = new IndexedRowMatrix(rows)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>mat: org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix = org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@2a57e8ca
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Get its size.
val m = mat.numRows()
val n = mat.numCols()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>m: Long = 5
n: Long = 2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Drop its row indices.
val rowMat: RowMatrix = mat.toRowMatrix()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rowMat: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@37fba875
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rowMat.rows.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res11: Array[org.apache.spark.mllib.linalg.Vector] = Array([1.0,3.0], [4.0,5.0])
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="indexedrowmatrix-in-python"><a class="header" href="#indexedrowmatrix-in-python">IndexedRowMatrix in Python</a></h3>
<p>An <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRowMatrix"><code>IndexedRowMatrix</code></a> can be created from an <code>RDD</code> of <code>IndexedRow</code>s, where <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRow"><code>IndexedRow</code></a> is a wrapper over <code>(long, vector)</code>. An <code>IndexedRowMatrix</code> can be converted to a <code>RowMatrix</code> by dropping its row indices.</p>
<p>Refer to the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRowMatrix"><code>IndexedRowMatrix</code> Python docs</a> for more details on the API.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix

# Create an RDD of indexed rows.
#   - This can be done explicitly with the IndexedRow class:
indexedRows = sc.parallelize([IndexedRow(0, [1, 2, 3]),
                              IndexedRow(1, [4, 5, 6]),
                              IndexedRow(2, [7, 8, 9]),
                              IndexedRow(3, [10, 11, 12])])

#   - or by using (long, vector) tuples:
indexedRows = sc.parallelize([(0, [1, 2, 3]), (1, [4, 5, 6]),
                              (2, [7, 8, 9]), (3, [10, 11, 12])])

# Create an IndexedRowMatrix from an RDD of IndexedRows.
mat = IndexedRowMatrix(indexedRows)

# Get its size.
m = mat.numRows()  # 4
n = mat.numCols()  # 3
print (m,n)

# Get the rows as an RDD of IndexedRows.
rowsRDD = mat.rows

# Convert to a RowMatrix by dropping the row indices.
rowMat = mat.toRowMatrix()

# Convert to a CoordinateMatrix.
coordinateMat = mat.toCoordinateMatrix()

# Convert to a BlockMatrix.
blockMat = mat.toBlockMatrix()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>(4L, 3L)
</code></pre>
</div>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-9"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-9"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/mllib-data-types.html">Apache Spark mllib-progamming-guide on mllib-data-types</a>.</p>
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa-7"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa-7"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/000_MLlibProgGuide">Overview</a></a></h1>
<h2 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea-7"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea-7"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/000_dataTypesProgGuide">Data Types - MLlib Programming Guide</a></a></h2>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/001_LocalVector">Local vector</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-vector">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/002_LabeledPoint">Labeled point</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/003_LocalMatrix">Local matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-matrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/004_DistributedMatrix">Distributed matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#distributed-matrix">URL</a>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/005_RowMatrix">RowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#rowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/006_IndexedRowMatrix">IndexedRowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#indexedrowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/007_CoordinateMatrix">CoordinateMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#coordinatematrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/008_BlockMatrix">BlockMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix">URL</a></li>
</ul>
</li>
</ul>
<p>MLlib supports local vectors and matrices stored on a single machine, as well as distributed matrices backed by one or more RDDs. Local vectors and local matrices are simple data models that serve as public interfaces. The underlying linear algebra operations are provided by <a href="http://www.scalanlp.org/">Breeze</a> and <a href="http://jblas.org/">jblas</a>. A training example used in supervised learning is called a “labeled point” in MLlib.</p>
</div>
<div class="cell markdown">
<h3 id="coordinatematrix-in-scala"><a class="header" href="#coordinatematrix-in-scala">CoordinateMatrix in Scala</a></h3>
<p>A <code>CoordinateMatrix</code> is a distributed matrix backed by an RDD of its entries. Each entry is a tuple of <code>(i: Long, j: Long, value: Double)</code>, where <code>i</code> is the row index, <code>j</code> is the column index, and <code>value</code> is the entry value. A <code>CoordinateMatrix</code> should be used only when both dimensions of the matrix are huge and the matrix is very sparse.</p>
<p>A <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.CoordinateMatrix"><code>CoordinateMatrix</code></a> can be created from an <code>RDD[MatrixEntry]</code> instance, where <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.MatrixEntry"><code>MatrixEntry</code></a> is a wrapper over <code>(Long, Long, Double)</code>. A <code>CoordinateMatrix</code> can be converted to an <code>IndexedRowMatrix</code> with sparse rows by calling <code>toIndexedRowMatrix</code>. Other computations for <code>CoordinateMatrix</code> are not currently supported.</p>
<p>Refer to the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.CoordinateMatrix"><code>CoordinateMatrix</code> Scala docs</a> for details on the API.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val entries: RDD[MatrixEntry] = sc.parallelize(Array(MatrixEntry(0, 0, 1.2), MatrixEntry(1, 0, 2.1), MatrixEntry(6, 1, 3.7))) // an RDD of matrix entries
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>entries: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.distributed.MatrixEntry] = ParallelCollectionRDD[454] at parallelize at &lt;console&gt;:35
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create a CoordinateMatrix from an RDD[MatrixEntry].
val mat: CoordinateMatrix = new CoordinateMatrix(entries)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>mat: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix = org.apache.spark.mllib.linalg.distributed.CoordinateMatrix@73dc93f3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Get its size.
val m = mat.numRows()
val n = mat.numCols()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>m: Long = 7
n: Long = 2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Convert it to an IndexRowMatrix whose rows are sparse vectors.
val indexedRowMatrix = mat.toIndexedRowMatrix()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>indexedRowMatrix: org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix = org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix@4a8e753a
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">indexedRowMatrix.rows.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res3: Array[org.apache.spark.mllib.linalg.distributed.IndexedRow] = Array(IndexedRow(0,(2,[0],[1.2])), IndexedRow(6,(2,[1],[3.7])), IndexedRow(1,(2,[0],[2.1])))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="coordinatematrix-in-scala-1"><a class="header" href="#coordinatematrix-in-scala-1">CoordinateMatrix in Scala</a></h3>
<p>A <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.CoordinateMatrix"><code>CoordinateMatrix</code></a> can be created from an <code>RDD</code> of <code>MatrixEntry</code> entries, where <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.MatrixEntry"><code>MatrixEntry</code></a> is a wrapper over <code>(long, long, float)</code>. A <code>CoordinateMatrix</code> can be converted to a <code>RowMatrix</code> by calling <code>toRowMatrix</code>, or to an <code>IndexedRowMatrix</code> with sparse rows by calling <code>toIndexedRowMatrix</code>.</p>
<p>Refer to the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.CoordinateMatrix"><code>CoordinateMatrix</code> Python docs</a> for more details on the API.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry

# Create an RDD of coordinate entries.
#   - This can be done explicitly with the MatrixEntry class:
entries = sc.parallelize([MatrixEntry(0, 0, 1.2), MatrixEntry(1, 0, 2.1), MatrixEntry(6, 1, 3.7)])

#   - or using (long, long, float) tuples:
entries = sc.parallelize([(0, 0, 1.2), (1, 0, 2.1), (2, 1, 3.7)])

# Create an CoordinateMatrix from an RDD of MatrixEntries.
mat = CoordinateMatrix(entries)

# Get its size.
m = mat.numRows()  # 3
n = mat.numCols()  # 2
print (m,n)

# Get the entries as an RDD of MatrixEntries.
entriesRDD = mat.entries

# Convert to a RowMatrix.
rowMat = mat.toRowMatrix()

# Convert to an IndexedRowMatrix.
indexedRowMat = mat.toIndexedRowMatrix()

# Convert to a BlockMatrix.
blockMat = mat.toBlockMatrix()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>(3L, 2L)
</code></pre>
</div>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-10"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-10"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/mllib-data-types.html">Apache Spark mllib-progamming-guide on mllib-data-types</a>.</p>
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa-8"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguide000_mllibprogguideoverviewa-8"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/000_MLlibProgGuide">Overview</a></a></h1>
<h2 id="a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea-8"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2mllibprogrammingguidedatatypesworkspacescalable-data-sciencextraresourcesprogguides2_2mllibprogrammingguidedatatypes000_datatypesprogguidedata-types---mllib-programming-guidea-8"><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/000_dataTypesProgGuide">Data Types - MLlib Programming Guide</a></a></h2>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/001_LocalVector">Local vector</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-vector">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/002_LabeledPoint">Labeled point</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/003_LocalMatrix">Local matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#local-matrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/004_DistributedMatrix">Distributed matrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#distributed-matrix">URL</a>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/005_RowMatrix">RowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#rowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/006_IndexedRowMatrix">IndexedRowMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#indexedrowmatrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/007_CoordinateMatrix">CoordinateMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#coordinatematrix">URL</a></li>
<li><a href="contents/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/MLlibProgrammingGuide/dataTypes/008_BlockMatrix">BlockMatrix</a> and <a href="http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix">URL</a></li>
</ul>
</li>
</ul>
<p>MLlib supports local vectors and matrices stored on a single machine, as well as distributed matrices backed by one or more RDDs. Local vectors and local matrices are simple data models that serve as public interfaces. The underlying linear algebra operations are provided by <a href="http://www.scalanlp.org/">Breeze</a> and <a href="http://jblas.org/">jblas</a>. A training example used in supervised learning is called a “labeled point” in MLlib.</p>
</div>
<div class="cell markdown">
<h3 id="blockmatrix-in-scala"><a class="header" href="#blockmatrix-in-scala">BlockMatrix in Scala</a></h3>
<p>A <code>BlockMatrix</code> is a distributed matrix backed by an RDD of <code>MatrixBlock</code>s, where a <code>MatrixBlock</code> is a tuple of <code>((Int, Int), Matrix)</code>, where the <code>(Int, Int)</code> is the index of the block, and <code>Matrix</code> is the sub-matrix at the given index with size <code>rowsPerBlock</code> x <code>colsPerBlock</code>. <code>BlockMatrix</code> supports methods such as <code>add</code> and <code>multiply</code> with another <code>BlockMatrix</code>. <code>BlockMatrix</code> also has a helper function <code>validate</code> which can be used to check whether the <code>BlockMatrix</code> is set up properly.</p>
<p>A <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.BlockMatrix"><code>BlockMatrix</code></a> can be most easily created from an <code>IndexedRowMatrix</code> or <code>CoordinateMatrix</code> by calling <code>toBlockMatrix</code>. <code>toBlockMatrix</code> creates blocks of size 1024 x 1024 by default. Users may change the block size by supplying the values through <code>toBlockMatrix(rowsPerBlock, colsPerBlock)</code>.</p>
<p>Refer to the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.BlockMatrix"><code>BlockMatrix</code> Scala docs</a> for details on the API.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//import org.apache.spark.mllib.linalg.{Matrix, Matrices}
import org.apache.spark.mllib.linalg.distributed.{BlockMatrix, CoordinateMatrix, MatrixEntry}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.mllib.linalg.distributed.{BlockMatrix, CoordinateMatrix, MatrixEntry}
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val entries: RDD[MatrixEntry] = sc.parallelize(Array(MatrixEntry(0, 0, 1.2), MatrixEntry(1, 0, 2.1), MatrixEntry(6, 1, 3.7))) // an RDD of matrix entries
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>entries: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.distributed.MatrixEntry] = ParallelCollectionRDD[692] at parallelize at &lt;console&gt;:35
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create a CoordinateMatrix from an RDD[MatrixEntry].
val coordMat: CoordinateMatrix = new CoordinateMatrix(entries)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>coordMat: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix = org.apache.spark.mllib.linalg.distributed.CoordinateMatrix@68f1d303
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Transform the CoordinateMatrix to a BlockMatrix
val matA: BlockMatrix = coordMat.toBlockMatrix().cache()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>matA: org.apache.spark.mllib.linalg.distributed.BlockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@1de2a311
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Validate whether the BlockMatrix is set up properly. Throws an Exception when it is not valid.
// Nothing happens if it is valid.
matA.validate()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Calculate A^T A.
val ata = matA.transpose.multiply(matA)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>ata: org.apache.spark.mllib.linalg.distributed.BlockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@16a80e13
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">ata.blocks.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res1: Array[((Int, Int), org.apache.spark.mllib.linalg.Matrix)] = 
Array(((0,0),5.85  0.0                 
0.0   13.690000000000001  ))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">ata.toLocalMatrix()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res3: org.apache.spark.mllib.linalg.Matrix = 
5.85  0.0                 
0.0   13.690000000000001  
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="blockmatrix-in-scala-1"><a class="header" href="#blockmatrix-in-scala-1">BlockMatrix in Scala</a></h3>
<p>A <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.BlockMatrix"><code>BlockMatrix</code></a> can be created from an <code>RDD</code> of sub-matrix blocks, where a sub-matrix block is a <code>((blockRowIndex, blockColIndex), sub-matrix)</code> tuple.</p>
<p>Refer to the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.BlockMatrix"><code>BlockMatrix</code> Python docs</a> for more details on the API.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from pyspark.mllib.linalg import Matrices
from pyspark.mllib.linalg.distributed import BlockMatrix

# Create an RDD of sub-matrix blocks.
blocks = sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])),
                         ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))])

# Create a BlockMatrix from an RDD of sub-matrix blocks.
mat = BlockMatrix(blocks, 3, 2)

# Get its size.
m = mat.numRows() # 6
n = mat.numCols() # 2
print (m,n)

# Get the blocks as an RDD of sub-matrix blocks.
blocksRDD = mat.blocks

# Convert to a LocalMatrix.
localMat = mat.toLocalMatrix()

# Convert to an IndexedRowMatrix.
indexedRowMat = mat.toIndexedRowMatrix()

# Convert to a CoordinateMatrix.
coordinateMat = mat.toCoordinateMatrix()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>(6L, 2L)
</code></pre>
</div>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-11"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-11"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
<h1 id="raaz-at-work---unfinished"><a class="header" href="#raaz-at-work---unfinished">RAAZ at WORK - unfinished.</a></h1>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">http://spark.apache.org/docs/latest/sql-programming-guide.html</a> by Ivan Sadikov and Raazesh Sainudiin.</p>
<p>Any contributions in this 'databricksification' of the programming guide are most welcome. Please feel free to send pull-requests or just fork and push yourself at/from <a href="https://github.com/lamastex/scalable-data-science">https://github.com/lamastex/scalable-data-science</a>.</p>
<p><strong>NOTE:</strong> The links that do not have standard URLs for hyper-text transfer protocol, qualified here by (http) or (https), are <em>in general</em> internal links and will/should work if you follow the instructions in the lectures (from the YouTube play list, watched sequential in chronological order that is linked from <a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">https://lamastex.github.io/scalable-data-science/sds/2/2/</a>) on how to download the <code>.dbc</code> archive for the course and upload it into your community edition with the correctly named expected directory structures.</p>
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides2_2sqlprogrammingguide000_sqlprogguidespark-sql-programming-guidea"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides2_2sqlprogrammingguide000_sqlprogguidespark-sql-programming-guidea"><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/sqlProgrammingGuide/000_sqlProgGuide">Spark Sql Programming Guide</a></a></h1>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/sqlProgrammingGuide/001_overview_sqlProgGuide">Overview</a>
<ul>
<li>SQL</li>
<li>DataFrames</li>
<li>Datasets</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/sqlProgrammingGuide/002_gettingStarted_sqlProgGuide">Getting Started</a>
<ul>
<li>Starting Point: SQLContext</li>
<li>Creating DataFrames</li>
<li>DataFrame Operations</li>
<li>Running SQL Queries Programmatically</li>
<li>Creating Datasets</li>
<li>Interoperating with RDDs
<ul>
<li>Inferring the Schema Using Reflection</li>
<li>Programmatically Specifying the Schema</li>
</ul>
</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/sqlProgrammingGuide/003_dataSources_sqlProgGuide">Data Sources</a>
<ul>
<li>Generic Load/Save Functions
<ul>
<li>Manually Specifying Options</li>
<li>Run SQL on files directly</li>
<li>Save Modes</li>
<li>Saving to Persistent Tables</li>
</ul>
</li>
<li>Parquet Files
<ul>
<li>Loading Data Programmatically</li>
<li>Partition Discovery</li>
<li>Schema Merging</li>
<li>Hive metastore Parquet table conversion
<ul>
<li>Hive/Parquet Schema Reconciliation</li>
<li>Metadata Refreshing</li>
</ul>
</li>
<li>Configuration</li>
</ul>
</li>
<li>JSON Datasets</li>
<li>Hive Tables
<ul>
<li>Interacting with Different Versions of Hive Metastore</li>
</ul>
</li>
<li>JDBC To Other Databases</li>
<li>Troubleshooting</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/sqlProgrammingGuide/004_performanceTuning_sqlProgGuide">Performance Tuning</a>
<ul>
<li>Caching Data In Memory</li>
<li>Other Configuration Options</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/sqlProgrammingGuide/005_distributedSqlEngine_sqlProgGuide">Distributed SQL Engine</a>
<ul>
<li>Running the Thrift JDBC/ODBC server</li>
<li>Running the Spark SQL CLI</li>
</ul>
</li>
</ul>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-12"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-12"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">http://spark.apache.org/docs/latest/sql-programming-guide.html</a> by Ivan Sadikov and Raazesh Sainudiin.</p>
<p>Any contributions in this 'databricksification' of the programming guide are most welcome. Please feel free to send pull-requests or just fork and push yourself at/from <a href="https://github.com/lamastex/scalable-data-science">https://github.com/lamastex/scalable-data-science</a>.</p>
<p><strong>NOTE:</strong> The links that do not have standard URLs for hyper-text transfer protocol, qualified here by (http) or (https), are <em>in general</em> internal links and will/should work if you follow the instructions in the lectures (from the YouTube play list, watched sequential in chronological order that is linked from <a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">https://lamastex.github.io/scalable-data-science/sds/2/2/</a>) on how to download the <code>.dbc</code> archive for the course and upload it into your community edition with the correctly named expected directory structures.</p>
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides2_2sqlprogrammingguide001_overview_sqlprogguideoverviewa"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides2_2sqlprogrammingguide001_overview_sqlprogguideoverviewa"><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/sqlProgrammingGuide/001_overview_sqlProgGuide">Overview</a></a></h1>
<h2 id="a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides2_2sqlprogrammingguide000_sqlprogguidespark-sql-programming-guidea-1"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides2_2sqlprogrammingguide000_sqlprogguidespark-sql-programming-guidea-1"><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides2_2/sqlProgrammingGuide/000_sqlProgGuide">Spark Sql Programming Guide</a></a></h2>
<h3 id="todo-fix-internalexternal-links-below"><a class="header" href="#todo-fix-internalexternal-links-below">TODO fix internal/external links below</a></h3>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/001_overview_sqlProgGuide">Overview</a>
<ul>
<li>SQL</li>
<li>DataFrames</li>
<li>Datasets</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/002_gettingStarted_sqlProgGuide">Getting Started</a>
<ul>
<li>Starting Point: SQLContext</li>
<li>Creating DataFrames</li>
<li>DataFrame Operations</li>
<li>Running SQL Queries Programmatically</li>
<li>Creating Datasets</li>
<li>Interoperating with RDDs
<ul>
<li>Inferring the Schema Using Reflection</li>
<li>Programmatically Specifying the Schema</li>
</ul>
</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/003_dataSources_sqlProgGuide">Data Sources</a>
<ul>
<li>Generic Load/Save Functions
<ul>
<li>Manually Specifying Options</li>
<li>Run SQL on files directly</li>
<li>Save Modes</li>
<li>Saving to Persistent Tables</li>
</ul>
</li>
<li>Parquet Files
<ul>
<li>Loading Data Programmatically</li>
<li>Partition Discovery</li>
<li>Schema Merging</li>
<li>Hive metastore Parquet table conversion
<ul>
<li>Hive/Parquet Schema Reconciliation</li>
<li>Metadata Refreshing</li>
</ul>
</li>
<li>Configuration</li>
</ul>
</li>
<li>JSON Datasets</li>
<li>Hive Tables
<ul>
<li>Interacting with Different Versions of Hive Metastore</li>
</ul>
</li>
<li>JDBC To Other Databases</li>
<li>Troubleshooting</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/004_performanceTuning_sqlProgGuide">Performance Tuning</a>
<ul>
<li>Caching Data In Memory</li>
<li>Other Configuration Options</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/005_distributedSqlEngine_sqlProgGuide">Distributed SQL Engine</a>
<ul>
<li>Running the Thrift JDBC/ODBC server</li>
<li>Running the Spark SQL CLI</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide001_overview_sqlprogguideoverviewa"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide001_overview_sqlprogguideoverviewa"><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/001_overview_sqlProgGuide">Overview</a></a></h1>
<p>Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations. There are several ways to interact with Spark SQL including:</p>
<ul>
<li>SQL (SQL 2003 standard compliant)</li>
<li>the DataFrames API (since Spark 1.4, was generalized in Spark 2.0 and is alias for <code>Dataset[Row]</code>)</li>
<li>the Datasets API (offers strongly-typed interface)</li>
</ul>
<p>When computing a result the same execution engine is used, independent of which API/language you are using to express the computation. This unification means that developers can easily switch back and forth between the various APIs based on which provides the most natural way to express a given transformation.</p>
<p>All of the examples on this page use sample data included in the Spark distribution and can be run in the <code>spark-shell</code>, <code>pyspark</code> shell, or <code>sparkR</code> shell.</p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://en.wikipedia.org/wiki/SQL"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/ml-features.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://en.wikipedia.org/wiki/Apache_Hive#HiveQL"
 width="95%" height="175"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/ml-features.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h2 id="sql"><a class="header" href="#sql">SQL</a></h2>
<p>One use of Spark SQL is to execute SQL queries written using either a basic SQL syntax or HiveQL. Spark SQL can also be used to read data from an existing Hive installation. For more on how to configure this feature, please refer to the <a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide/sql-programming-guide.html#hive-tables">Hive Tables</a> section. When running SQL from within another programming language the results will be returned as a <a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide/sql-programming-guide.html#DataFrames">DataFrame</a>. You can also interact with the SQL interface using the <a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide/sql-programming-guide.html#running-the-spark-sql-cli">command-line</a> or over <a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide/sql-programming-guide.html#running-the-thrift-jdbcodbc-server">JDBC/ODBC</a>.</p>
<h2 id="datasets-and-dataframes"><a class="header" href="#datasets-and-dataframes">Datasets and DataFrames</a></h2>
<p>A Dataset is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine, which has been improved in 2.x versions. A Dataset can be <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#creating-datasets">constructed (http)</a> from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). The Dataset API is available in <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset">Scala (http)</a> and <a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html">Java (http)</a>. Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally row.columnName). The case for R is similar.</p>
<p>A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources">sources</a> such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame">Python</a>, and <a href="http://spark.apache.org/docs/latest/api/R/index.html">R</a>. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset">the Scala API</a>, DataFrame is simply a type alias of <code>Dataset[Row]</code>. While, in <a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html">Java API</a>, users need to use <code>Dataset&lt;Row&gt;</code> to represent a DataFrame.</p>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-13"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-13"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Apache Spark 2.2 sql-progamming-guide</a>.</p>
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide002_gettingstarted_sqlprogguidegetting-starteda"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide002_gettingstarted_sqlprogguidegetting-starteda"><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/002_gettingStarted_sqlProgGuide">Getting Started</a></a></h1>
<h2 id="a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide000_sqlprogguidespark-sql-programming-guidea"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide000_sqlprogguidespark-sql-programming-guidea"><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/000_sqlProgGuide">Spark Sql Programming Guide</a></a></h2>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/001_overview_sqlProgGuide">Overview</a>
<ul>
<li>SQL</li>
<li>DataFrames</li>
<li>Datasets</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/002_gettingStarted_sqlProgGuide">Getting Started</a>
<ul>
<li>Starting Point: SQLContext</li>
<li>Creating DataFrames</li>
<li>DataFrame Operations</li>
<li>Running SQL Queries Programmatically</li>
<li>Creating Datasets</li>
<li>Interoperating with RDDs
<ul>
<li>Inferring the Schema Using Reflection</li>
<li>Programmatically Specifying the Schema</li>
</ul>
</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/003_dataSources_sqlProgGuide">Data Sources</a>
<ul>
<li>Generic Load/Save Functions
<ul>
<li>Manually Specifying Options</li>
<li>Run SQL on files directly</li>
<li>Save Modes</li>
<li>Saving to Persistent Tables</li>
</ul>
</li>
<li>Parquet Files
<ul>
<li>Loading Data Programmatically</li>
<li>Partition Discovery</li>
<li>Schema Merging</li>
<li>Hive metastore Parquet table conversion
<ul>
<li>Hive/Parquet Schema Reconciliation</li>
<li>Metadata Refreshing</li>
</ul>
</li>
<li>Configuration</li>
</ul>
</li>
<li>JSON Datasets</li>
<li>Hive Tables
<ul>
<li>Interacting with Different Versions of Hive Metastore</li>
</ul>
</li>
<li>JDBC To Other Databases</li>
<li>Troubleshooting</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/004_performanceTuning_sqlProgGuide">Performance Tuning</a>
<ul>
<li>Caching Data In Memory</li>
<li>Other Configuration Options</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/005_distributedSqlEngine_sqlProgGuide">Distributed SQL Engine</a>
<ul>
<li>Running the Thrift JDBC/ODBC server</li>
<li>Running the Spark SQL CLI</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide002_gettingstarted_sqlprogguidegetting-starteda-1"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide002_gettingstarted_sqlprogguidegetting-starteda-1"><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/002_gettingStarted_sqlProgGuide">Getting Started</a></a></h1>
<h2 id="starting-point-sqlcontext"><a class="header" href="#starting-point-sqlcontext">Starting Point: SQLContext</a></h2>
<p>The entry point into all functionality in Spark SQL is the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession"><code>SparkSession</code></a> class and/or <code>SQLContext</code>/<code>HiveContext</code>. Spark session is created for you as <code>spark</code> when you start <strong>spark-shell</strong> or <strong>pyspark</strong>. You will need to create <code>SparkSession</code> usually when building an application (running on production-like on-premises cluster). n this case follow code below to create Spark session.</p>
<pre><code class="language-scala">import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder().appName(&quot;Spark SQL basic example&quot;).getOrCreate()

// you could get SparkContext and SQLContext from SparkSession
val sc = spark.sparkContext
val sqlContext = spark.sqlContext

// This is used to implicitly convert an RDD or Seq to a DataFrame (see examples below)
import spark.implicits._
</code></pre>
<p>But in Databricks notebook (similar to <code>spark-shell</code>) <code>SparkSession</code> is already created for you and is available as <code>spark</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Evaluation of the cell by Ctrl+Enter will print spark session available in notebook
spark
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2d0c6c9
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>After evaluation you should see something like this:</p>
<pre><code>res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2d0c6c9
</code></pre>
<p>In order to enable Hive support use <code>enableHiveSupport()</code> method on builder when constructing Spark session, which provides richer functionality over standard Spark SQL context, for example, usage of Hive user-defined functions or loading and writing data from/into Hive. Note that most of the SQL functionality is available regardless Hive support.</p>
</div>
<div class="cell markdown">
<h2 id="creating-dataframes"><a class="header" href="#creating-dataframes">Creating DataFrames</a></h2>
<p>With a <code>SparkSessions</code>, applications can create <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset">Dataset</a> or <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame"><code>DataFrame</code></a> from an <a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide/sql-programming-guide.html#interoperating-with-rdds">existing <code>RDD</code></a>, from a Hive table, or from various <a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide/sql-programming-guide.html#data-sources">datasources</a>.</p>
<p>Just to recap, a DataFrame is a distributed collection of data organized into named columns. You can think of it as an organized into table RDD of case class <code>Row</code> (which is not exactly true). DataFrames, in comparison to RDDs, are backed by rich optimizations, including tracking their own schema, adaptive query execution, code generation including whole stage codegen, extensible Catalyst optimizer, and project <a href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html">Tungsten</a>.</p>
<p>Dataset provides type-safety when working with SQL, since <code>Row</code> is mapped to a case class, so that each column can be referenced by property of that class.</p>
<blockquote>
<p>Note that performance for Dataset/DataFrames is the same across languages Scala, Java, Python, and R. This is due to the fact that the planning phase is just language-specific, only logical plan is constructed in Python, and all the physical execution is compiled and executed as JVM bytecode.</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Spark has some of the pre-built methods to create simple Dataset/DataFrame

// 1. Empty Dataset/DataFrame, not really interesting, is it?
println(spark.emptyDataFrame)
println(spark.emptyDataset[Int])
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>[]
[value: int]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// 2. Range of numbers, note that Spark automatically names column as &quot;id&quot;
val range = spark.range(0, 10)

// In order to get a preview of data in DataFrame use &quot;show()&quot;
range.show(3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+---+
| id|
+---+
|  0|
|  1|
|  2|
+---+
only showing top 3 rows

range: org.apache.spark.sql.Dataset[Long] = [id: bigint]
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>You can also use different datasources that will be shown later or load Hive tables directly into Spark.</p>
<p>We have already created a table of social media usage from NYC (you will see later how this table was built from raw data).</p>
<blockquote>
<p>See the very bottom of this worksheet to see how this was done.</p>
</blockquote>
<p>First let's make sure this table is available for us.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Let's find out what tables are already available for loading
spark.catalog.listTables.show()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+--------------------+--------+-----------+---------+-----------+
|                name|database|description|tableType|isTemporary|
+--------------------+--------+-----------+---------+-----------+
|          cities_csv| default|       null| EXTERNAL|      false|
|       cleaned_taxes| default|       null|  MANAGED|      false|
|commdettrumpclint...| default|       null|  MANAGED|      false|
|   donaldtrumptweets| default|       null| EXTERNAL|      false|
|             linkage| default|       null| EXTERNAL|      false|
|             nations| default|       null| EXTERNAL|      false|
|           newmplist| default|       null| EXTERNAL|      false|
|       ny_baby_names| default|       null|  MANAGED|      false|
|       nzmpsandparty| default|       null| EXTERNAL|      false|
|    pos_neg_category| default|       null| EXTERNAL|      false|
|                 rna| default|       null|  MANAGED|      false|
|                samh| default|       null| EXTERNAL|      false|
|  social_media_usage| default|       null| EXTERNAL|      false|
|              table1| default|       null| EXTERNAL|      false|
|          test_table| default|       null| EXTERNAL|      false|
|             uscites| default|       null| EXTERNAL|      false|
+--------------------+--------+-----------+---------+-----------+
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>It looks like the table <code>social_media_usage</code> is available as a permanent table (<code>isTemporary</code> set as <code>false</code>).</p>
<p>Next let us do the following: * load this table as a DataFrame * print its schema and * show the first 20 rows.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val df = spark.table(&quot;social_media_usage&quot;) // Ctrl+Enter
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>df: org.apache.spark.sql.DataFrame = [agency: string, platform: string ... 3 more fields]
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>As you can see the immutable value <code>df</code> is a DataFrame and more specifically it is:</p>
<blockquote>
<p><code>org.apache.spark.sql.DataFrame = [agency: string, platform: string, url: string, visits: int]</code>.</p>
</blockquote>
</div>
<div class="cell markdown">
<p>Now let us print schema of the DataFrame <code>df</code> and have a look at the actual data:</p>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-scala">// Ctrl+Enter
df.printSchema() // prints schema of the DataFrame
df.show() // shows first n (default is 20) rows
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>root
 |-- agency: string (nullable = true)
 |-- platform: string (nullable = true)
 |-- url: string (nullable = true)
 |-- date: string (nullable = true)
 |-- visits: integer (nullable = true)

+----------+----------+--------------------+--------------------+------+
|    agency|  platform|                 url|                date|visits|
+----------+----------+--------------------+--------------------+------+
|       OEM|       SMS|                null|02/17/2012 12:00:...| 61652|
|       OEM|       SMS|                null|11/09/2012 12:00:...| 44547|
|       EDC|    Flickr|http://www.flickr...|05/09/2012 12:00:...|  null|
|     NYCHA|Newsletter|                null|05/09/2012 12:00:...|  null|
|       DHS|   Twitter|www.twitter.com/n...|06/13/2012 12:00:...|   389|
|       DHS|   Twitter|www.twitter.com/n...|08/02/2012 12:00:...|   431|
|       DOH|   Android|       Condom Finder|08/08/2011 12:00:...|  5026|
|       DOT|   Android|         You The Man|08/08/2011 12:00:...|  null|
|      MOME|   Android|      MiNY Venor app|08/08/2011 12:00:...|   313|
|       DOT|Broadcastr|                null|08/08/2011 12:00:...|  null|
|       DPR|Broadcastr|http://beta.broad...|08/08/2011 12:00:...|  null|
|     ENDHT|  Facebook|http://www.facebo...|08/08/2011 12:00:...|     3|
|       VAC|  Facebook|https://www.faceb...|08/08/2011 12:00:...|    36|
|    PlaNYC|  Facebook|http://www.facebo...|08/08/2011 12:00:...|    47|
|      DFTA|  Facebook|http://www.facebo...|08/08/2011 12:00:...|    90|
| energyNYC|  Facebook|http://www.facebo...|08/08/2011 12:00:...|   105|
|      MOIA|  Facebook|http://www.facebo...|08/08/2011 12:00:...|   123|
|City Store|  Facebook|http://www.facebo...|08/08/2011 12:00:...|   119|
|      OCDV|  Facebook|http://www.facebo...|08/08/2011 12:00:...|   148|
|       HIA|  Facebook|http://www.facebo...|08/08/2011 12:00:...|   197|
+----------+----------+--------------------+--------------------+------+
only showing top 20 rows
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p>Note that <code>(nullable = true)</code> simply means if the value is allowed to be <code>null</code>.</p>
</blockquote>
<p>Let us count the number of rows in <code>df</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">df.count() // Ctrl+Enter
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res7: Long = 5899
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>So there are 5899 records or rows in the DataFrame <code>df</code>. Pretty good! You can also select individual columns using so-called DataFrame API, as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val platforms = df.select(&quot;platform&quot;) // Shift+Enter
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>platforms: org.apache.spark.sql.DataFrame = [platform: string]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">platforms.count() // Shift+Enter to count the number of rows
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res8: Long = 5899
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">platforms.show(5) // Ctrl+Enter to show top 5 rows
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+----------+
|  platform|
+----------+
|       SMS|
|       SMS|
|    Flickr|
|Newsletter|
|   Twitter|
+----------+
only showing top 5 rows
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>You can also apply <code>.distinct()</code> to extract only unique entries as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val uniquePlatforms = df.select(&quot;platform&quot;).distinct() // Shift+Enter
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>uniquePlatforms: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [platform: string]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">uniquePlatforms.count() // Ctrl+Enter to count the number of distinct platforms
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res10: Long = 23
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's see all the rows of the DataFrame <code>uniquePlatforms</code>.</p>
<blockquote>
<p>Note that <code>display(uniquePlatforms)</code> unlike <code>uniquePlatforms.show()</code> displays all rows of the DataFrame + gives you ability to select different view, e.g. charts.</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-scala">display(uniquePlatforms) // Ctrl+Enter to show all rows; use the scroll-bar on the right of the display to see all platforms
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>platform</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>nyc.gov</td>
</tr>
<tr class="even">
<td>Flickr</td>
</tr>
<tr class="odd">
<td>Vimeo</td>
</tr>
<tr class="even">
<td>iPhone</td>
</tr>
<tr class="odd">
<td>YouTube</td>
</tr>
<tr class="even">
<td>WordPress</td>
</tr>
<tr class="odd">
<td>SMS</td>
</tr>
<tr class="even">
<td>iPhone App</td>
</tr>
<tr class="odd">
<td>Youtube</td>
</tr>
<tr class="even">
<td>Instagram</td>
</tr>
<tr class="odd">
<td>iPhone app</td>
</tr>
<tr class="even">
<td>Linked-In</td>
</tr>
<tr class="odd">
<td>Twitter</td>
</tr>
<tr class="even">
<td>TOTAL</td>
</tr>
<tr class="odd">
<td>Tumblr</td>
</tr>
<tr class="even">
<td>Newsletter</td>
</tr>
<tr class="odd">
<td>Pinterest</td>
</tr>
<tr class="even">
<td>Broadcastr</td>
</tr>
<tr class="odd">
<td>Android</td>
</tr>
<tr class="even">
<td>Foursquare</td>
</tr>
<tr class="odd">
<td>Google+</td>
</tr>
<tr class="even">
<td>Foursquare (Badge Unlock)</td>
</tr>
<tr class="odd">
<td>Facebook</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell markdown">
<h3 id="spark-sql-and-dataframe-api"><a class="header" href="#spark-sql-and-dataframe-api">Spark SQL and DataFrame API</a></h3>
<p>Spark SQL provides DataFrame API that can perform relational operations on both external data sources and internal collections, which is similar to widely used data frame concept in R, but evaluates operations support lazily (remember RDDs?), so that it can perform relational optimizations. This API is also available in Java, Python and R, but some functionality may not be available, although with every release of Spark people minimize this gap.</p>
<p>So we give some examples how to query data in Python and R, but continue with Scala. You can do all DataFrame operations in this notebook using Python or R.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># Ctrl+Enter to evaluate this python cell, recall '#' is the pre-comment character in python
# Using Python to query our &quot;social_media_usage&quot; table
pythonDF = spark.table(&quot;social_media_usage&quot;).select(&quot;platform&quot;).distinct()
pythonDF.show(3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+--------+
|platform|
+--------+
| nyc.gov|
|  Flickr|
|   Vimeo|
+--------+
only showing top 3 rows
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-sql">-- Ctrl+Enter to achieve the same result using standard SQL syntax!
select distinct platform from social_media_usage
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>platform</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>nyc.gov</td>
</tr>
<tr class="even">
<td>Flickr</td>
</tr>
<tr class="odd">
<td>Vimeo</td>
</tr>
<tr class="even">
<td>iPhone</td>
</tr>
<tr class="odd">
<td>YouTube</td>
</tr>
<tr class="even">
<td>WordPress</td>
</tr>
<tr class="odd">
<td>SMS</td>
</tr>
<tr class="even">
<td>iPhone App</td>
</tr>
<tr class="odd">
<td>Youtube</td>
</tr>
<tr class="even">
<td>Instagram</td>
</tr>
<tr class="odd">
<td>iPhone app</td>
</tr>
<tr class="even">
<td>Linked-In</td>
</tr>
<tr class="odd">
<td>Twitter</td>
</tr>
<tr class="even">
<td>TOTAL</td>
</tr>
<tr class="odd">
<td>Tumblr</td>
</tr>
<tr class="even">
<td>Newsletter</td>
</tr>
<tr class="odd">
<td>Pinterest</td>
</tr>
<tr class="even">
<td>Broadcastr</td>
</tr>
<tr class="odd">
<td>Android</td>
</tr>
<tr class="even">
<td>Foursquare</td>
</tr>
<tr class="odd">
<td>Google+</td>
</tr>
<tr class="even">
<td>Foursquare (Badge Unlock)</td>
</tr>
<tr class="odd">
<td>Facebook</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell markdown">
<p>Now it is time for some tips around how you use <code>select</code> and what the difference is between <code>$&quot;a&quot;</code>, <code>col(&quot;a&quot;)</code>, <code>df(&quot;a&quot;)</code>.</p>
<p>As you probably have noticed by now, you can specify individual columns to select by providing String values in select statement. But sometimes you need to: - distinguish between columns with the same name - use it to filter (actually you can still filter using full String expression) - do some &quot;magic&quot; with joins and user-defined functions (this will be shown later)</p>
<p>So Spark gives you ability to actually specify columns when you select. Now the difference between all those three notations is ... none, those things are just aliases for a <code>Column</code> in Spark SQL, which means following expressions yield the same result:</p>
<pre><code class="language-scala">// Using string expressions
df.select(&quot;agency&quot;, &quot;visits&quot;)

// Using &quot;$&quot; alias for column
df.select($&quot;agency&quot;, $&quot;visits&quot;)

// Using &quot;col&quot; alias for column
df.select(col(&quot;agency&quot;), col(&quot;visits&quot;))

// Using DataFrame name for column
df.select(df(&quot;agency&quot;), df(&quot;visits&quot;))
</code></pre>
<p>This &quot;same-difference&quot; applies to filtering, i.e. you can either use full expression to filter, or column as shown in the following example:</p>
<pre><code class="language-scala">// Using column to filter
df.select(&quot;visits&quot;).filter($&quot;visits&quot; &gt; 100)

// Or you can use full expression as string
df.select(&quot;visits&quot;).filter(&quot;visits &gt; 100&quot;)
</code></pre>
<blockquote>
<p>Note that <code>$&quot;visits&quot; &gt; 100</code> expression looks amazing, but under the hood it is just another column, and it equals to <code>df(&quot;visits&quot;).&gt;(100)</code>, where, thanks to Scala paradigm <code>&gt;</code> is just another function that you can define.</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val sms = df.select($&quot;agency&quot;, $&quot;platform&quot;, $&quot;visits&quot;).filter($&quot;platform&quot; === &quot;SMS&quot;)
sms.show() // Ctrl+Enter
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+------+--------+------+
|agency|platform|visits|
+------+--------+------+
|   OEM|     SMS| 61652|
|   OEM|     SMS| 44547|
|   DOE|     SMS|   382|
| NYCHA|     SMS|  null|
|   OEM|     SMS| 61652|
|   DOE|     SMS|   382|
| NYCHA|     SMS|  null|
|   OEM|     SMS| 61652|
|   OEM|     SMS|  null|
|   DOE|     SMS|  null|
| NYCHA|     SMS|  null|
|   OEM|     SMS|  null|
|   DOE|     SMS|  null|
| NYCHA|     SMS|  null|
|   DOE|     SMS|   382|
| NYCHA|     SMS|  null|
|   OEM|     SMS| 61652|
|   DOE|     SMS|   382|
| NYCHA|     SMS|  null|
|   OEM|     SMS| 61652|
+------+--------+------+
only showing top 20 rows

sms: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [agency: string, platform: string ... 1 more field]
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Again you could have written the query above using any column aliases or String names or even writing the query directly.</p>
<p>For example, we can do it using String names, as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Ctrl+Enter Note that we are using &quot;platform = 'SMS'&quot; since it will be evaluated as actual SQL
val sms = df.select(df(&quot;agency&quot;), df(&quot;platform&quot;), df(&quot;visits&quot;)).filter(&quot;platform = 'SMS'&quot;)
sms.show(5)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+------+--------+------+
|agency|platform|visits|
+------+--------+------+
|   OEM|     SMS| 61652|
|   OEM|     SMS| 44547|
|   DOE|     SMS|   382|
| NYCHA|     SMS|  null|
|   OEM|     SMS| 61652|
+------+--------+------+
only showing top 5 rows

sms: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [agency: string, platform: string ... 1 more field]
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Refer to the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame">DataFrame API</a> for more detailed API. In addition to simple column references and expressions, DataFrames also have a rich library of functions including string manipulation, date arithmetic, common math operations and more. The complete list is available in the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$">DataFrame Function Reference</a>.</p>
</div>
<div class="cell markdown">
<p>Let's next explore some of the functionality that is available by transforming this DataFrame <code>df</code> into a new DataFrame called <code>fixedDF</code>.</p>
<ul>
<li>First, note that some columns are not exactly what we want them to be.
<ul>
<li>For example date column should be standard Date/Timestamp SQL column, and</li>
<li>visits should not contain null values, but <code>0</code>s instead.</li>
</ul>
</li>
<li>Let us fix it using some code that is briefly explained here (don't worry if you don't get it completely now, you will get the hang of it by playing more)
<ul>
<li>The <code>coalesce</code> function is similar to <code>if-else</code> statement, i.e., if first column in expression is <code>null</code>, then the value of the second column is used and so on.</li>
<li><code>lit</code> just means column of constant value (<code>lit</code>erally speaking!).</li>
<li>the &quot;funky&quot; time conversion is essentially conversion from current format -&gt; unix timestamp as a number -&gt; Spark SQL Date format</li>
<li>we also remove <code>TOTAL</code> value from <code>platform</code> column.</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-scala">// Ctrl+Enter to make fixedDF

// import the needed sql functions
import org.apache.spark.sql.functions.{coalesce, from_unixtime, lit, to_date, unix_timestamp}

// make the fixedDF DataFrame
val fixedDF = df.
   select(
     $&quot;agency&quot;, 
     $&quot;platform&quot;, 
     $&quot;url&quot;, 
     to_date(from_unixtime(unix_timestamp($&quot;date&quot;, &quot;MM/dd/yyyy hh:mm:ss aaa&quot;))).as(&quot;date&quot;), 
     coalesce($&quot;visits&quot;, lit(0)).as(&quot;visits&quot;)).
   filter($&quot;platform&quot; !== &quot;TOTAL&quot;)

fixedDF.printSchema() // print its schema 
// and show the top 20 records fully
fixedDF.show(false) // the false argument does not truncate the rows, so you will not see something like this &quot;anot...&quot;
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>root
 |-- agency: string (nullable = true)
 |-- platform: string (nullable = true)
 |-- url: string (nullable = true)
 |-- date: date (nullable = true)
 |-- visits: integer (nullable = false)

+----------+----------+---------------------------------------------------------------------------------------+----------+------+
|agency    |platform  |url                                                                                    |date      |visits|
+----------+----------+---------------------------------------------------------------------------------------+----------+------+
|OEM       |SMS       |null                                                                                   |2012-02-17|61652 |
|OEM       |SMS       |null                                                                                   |2012-11-09|44547 |
|EDC       |Flickr    |http://www.flickr.com/nycedc                                                           |2012-05-09|0     |
|NYCHA     |Newsletter|null                                                                                   |2012-05-09|0     |
|DHS       |Twitter   |www.twitter.com/nycdhs                                                                 |2012-06-13|389   |
|DHS       |Twitter   |www.twitter.com/nycdhs                                                                 |2012-08-02|431   |
|DOH       |Android   |Condom Finder                                                                          |2011-08-08|5026  |
|DOT       |Android   |You The Man                                                                            |2011-08-08|0     |
|MOME      |Android   |MiNY Venor app                                                                         |2011-08-08|313   |
|DOT       |Broadcastr|null                                                                                   |2011-08-08|0     |
|DPR       |Broadcastr|http://beta.broadcastr.com/Echo.html?audioId=670026-4001                               |2011-08-08|0     |
|ENDHT     |Facebook  |http://www.facebook.com/pages/NYC-Lets-End-Human-Trafficking/125730490795659?sk=wall   |2011-08-08|3     |
|VAC       |Facebook  |https://www.facebook.com/pages/NYC-Voter-Assistance-Commission/110226709012110         |2011-08-08|36    |
|PlaNYC    |Facebook  |http://www.facebook.com/pages/New-York-NY/PlaNYC/160454173971169?ref=ts                |2011-08-08|47    |
|DFTA      |Facebook  |http://www.facebook.com/pages/NYC-Department-for-the-Aging/109028655823590             |2011-08-08|90    |
|energyNYC |Facebook  |http://www.facebook.com/EnergyNYC?sk=wall                                              |2011-08-08|105   |
|MOIA      |Facebook  |http://www.facebook.com/ihwnyc                                                         |2011-08-08|123   |
|City Store|Facebook  |http://www.facebook.com/citystorenyc                                                   |2011-08-08|119   |
|OCDV      |Facebook  |http://www.facebook.com/pages/NYC-Healthy-Relationship-Training-Academy/134637829901065|2011-08-08|148   |
|HIA       |Facebook  |http://www.facebook.com/pages/New-York-City-Health-Insurance-Link/145920551598         |2011-08-08|197   |
+----------+----------+---------------------------------------------------------------------------------------+----------+------+
only showing top 20 rows

&lt;console&gt;:47: warning: method !== in class Column is deprecated: !== does not have the same precedence as ===, use =!= instead
          filter($&quot;platform&quot; !== &quot;TOTAL&quot;)
                             ^
import org.apache.spark.sql.functions.{coalesce, from_unixtime, lit, to_date, unix_timestamp}
fixedDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [agency: string, platform: string ... 3 more fields]
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Okay, this is better, but <code>url</code>s are still inconsistent.</p>
<p>Let's fix this by writing our own UDF (user-defined function) to deal with special cases.</p>
<p>Note that if you <strong>CAN USE Spark functions library</strong>, do it. But for the sake of the example, custom UDF is shown below.</p>
<p>We take value of a column as String type and return the same String type, but ignore values that do not start with <code>http</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Ctrl+Enter to evaluate this UDF which takes a input String called &quot;value&quot;
// and converts it into lower-case if it begins with http and otherwise leaves it as null, so we sort of remove non valid web-urls
val cleanUrl = udf((value: String) =&gt; if (value != null &amp;&amp; value.startsWith(&quot;http&quot;)) value.toLowerCase() else null)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>cleanUrl: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType)))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let us apply our UDF on <code>fixedDF</code> to create a new DataFrame called <code>cleanedDF</code> as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Ctrl+Enter
val cleanedDF = fixedDF.select($&quot;agency&quot;, $&quot;platform&quot;, cleanUrl($&quot;url&quot;).as(&quot;url&quot;), $&quot;date&quot;, $&quot;visits&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>cleanedDF: org.apache.spark.sql.DataFrame = [agency: string, platform: string ... 3 more fields]
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Now, let's check that it actually worked by seeing the first 5 rows of the <code>cleanedDF</code> whose <code>url</code> <code>isNull</code> and <code>isNotNull</code>, as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Shift+Enter
cleanedDF.filter($&quot;url&quot;.isNull).show(5)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+------+----------+----+----------+------+
|agency|  platform| url|      date|visits|
+------+----------+----+----------+------+
|   OEM|       SMS|null|2012-02-17| 61652|
|   OEM|       SMS|null|2012-11-09| 44547|
| NYCHA|Newsletter|null|2012-05-09|     0|
|   DHS|   Twitter|null|2012-06-13|   389|
|   DHS|   Twitter|null|2012-08-02|   431|
+------+----------+----+----------+------+
only showing top 5 rows
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Ctrl+Enter
cleanedDF.filter($&quot;url&quot;.isNotNull).show(5, false) // false in .show(5, false) shows rows untruncated
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+------+----------+------------------------------------------------------------------------------------+----------+------+
|agency|platform  |url                                                                                 |date      |visits|
+------+----------+------------------------------------------------------------------------------------+----------+------+
|EDC   |Flickr    |http://www.flickr.com/nycedc                                                        |2012-05-09|0     |
|DPR   |Broadcastr|http://beta.broadcastr.com/echo.html?audioid=670026-4001                            |2011-08-08|0     |
|ENDHT |Facebook  |http://www.facebook.com/pages/nyc-lets-end-human-trafficking/125730490795659?sk=wall|2011-08-08|3     |
|VAC   |Facebook  |https://www.facebook.com/pages/nyc-voter-assistance-commission/110226709012110      |2011-08-08|36    |
|PlaNYC|Facebook  |http://www.facebook.com/pages/new-york-ny/planyc/160454173971169?ref=ts             |2011-08-08|47    |
+------+----------+------------------------------------------------------------------------------------+----------+------+
only showing top 5 rows
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Now there is a suggestion from you manager's manager's manager that due to some perceived privacy concerns we want to replace <code>agency</code> with some unique identifier.</p>
<p>So we need to do the following: * create unique list of agencies with ids and * join them with main DataFrame.</p>
<p>Sounds easy, right? Let's do it.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Crtl+Enter
// Import Spark SQL function that will give us unique id across all the records in this DataFrame
import org.apache.spark.sql.functions.monotonically_increasing_id

// We append column as SQL function that creates unique ids across all records in DataFrames 
val agencies = cleanedDF.select(cleanedDF(&quot;agency&quot;))
                        .distinct()
                        .withColumn(&quot;id&quot;, monotonically_increasing_id())
agencies.show(5)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+--------------------+-----------+
|              agency|         id|
+--------------------+-----------+
|              PlaNYC|34359738368|
|                 HIA|34359738369|
|NYC Digital: exte...|34359738370|
|           NYCGLOBAL|42949672960|
|              nycgov|68719476736|
+--------------------+-----------+
only showing top 5 rows

import org.apache.spark.sql.functions.monotonically_increasing_id
agencies: org.apache.spark.sql.DataFrame = [agency: string, id: bigint]
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Those who want to understand left/right inner/outer joins can see the <a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/edXBigDataSeries2015/CS100-1x/Module%203:%20Lectures">video lectures in Module 3 of Anthony Joseph's Introduction to Big data edX course</a> from the Community Edition of databricks. The course has been added to this databricks shard at <a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/edXBigDataSeries2015/CS100-1x">/#workspace/scalable-data-science/xtraResources/edXBigDataSeries2015/CS100-1x</a> as extra resources for the project-focussed course <a href="http://www.math.canterbury.ac.nz/%7Er.sainudiin/courses/ScalableDataScience/">Scalable Data Science</a>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Ctrl+Enter
// And join with the rest of the data, note how join condition is specified 
val anonym = cleanedDF.join(agencies, cleanedDF(&quot;agency&quot;) === agencies(&quot;agency&quot;), &quot;inner&quot;).select(&quot;id&quot;, &quot;platform&quot;, &quot;url&quot;, &quot;date&quot;, &quot;visits&quot;)

// We also cache DataFrame since it can be quite expensive to recompute join
anonym.cache()

// Display result
anonym.show(5)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+-------------+----------+--------------------+----------+------+
|           id|  platform|                 url|      date|visits|
+-------------+----------+--------------------+----------+------+
|1580547964928|       SMS|                null|2012-02-17| 61652|
|1580547964928|       SMS|                null|2012-11-09| 44547|
| 412316860416|    Flickr|http://www.flickr...|2012-05-09|     0|
|1649267441664|Newsletter|                null|2012-05-09|     0|
|1529008357376|   Twitter|                null|2012-06-13|   389|
+-------------+----------+--------------------+----------+------+
only showing top 5 rows

anonym: org.apache.spark.sql.DataFrame = [id: bigint, platform: string ... 3 more fields]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">spark.catalog.listTables().show() // look at the available tables
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+--------------------+--------+-----------+---------+-----------+
|                name|database|description|tableType|isTemporary|
+--------------------+--------+-----------+---------+-----------+
|          cities_csv| default|       null| EXTERNAL|      false|
|       cleaned_taxes| default|       null|  MANAGED|      false|
|commdettrumpclint...| default|       null|  MANAGED|      false|
|   donaldtrumptweets| default|       null| EXTERNAL|      false|
|             linkage| default|       null| EXTERNAL|      false|
|             nations| default|       null| EXTERNAL|      false|
|           newmplist| default|       null| EXTERNAL|      false|
|       ny_baby_names| default|       null|  MANAGED|      false|
|       nzmpsandparty| default|       null| EXTERNAL|      false|
|    pos_neg_category| default|       null| EXTERNAL|      false|
|                 rna| default|       null|  MANAGED|      false|
|                samh| default|       null| EXTERNAL|      false|
|  social_media_usage| default|       null| EXTERNAL|      false|
|              table1| default|       null| EXTERNAL|      false|
|          test_table| default|       null| EXTERNAL|      false|
|             uscites| default|       null| EXTERNAL|      false|
+--------------------+--------+-----------+---------+-----------+
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">-- to remove a TempTable if it exists already
drop table if exists anonym
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Register table for Spark SQL, we also import &quot;month&quot; function 
import org.apache.spark.sql.functions.month

anonym.createOrReplaceTempView(&quot;anonym&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.sql.functions.month
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-sql">-- Interesting. Now let's do some aggregation. Display platform, month, visits
-- Date column allows us to extract month directly

select platform, month(date) as month, sum(visits) as visits from anonym group by platform, month(date)
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>platform</th>
<th>month</th>
<th>visits</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Instagram</td>
<td>9.0</td>
<td>27891.0</td>
</tr>
<tr class="even">
<td>Linked-In</td>
<td>10.0</td>
<td>60156.0</td>
</tr>
<tr class="odd">
<td>Foursquare (Badge Unlock)</td>
<td>6.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>iPhone</td>
<td>8.0</td>
<td>10336.0</td>
</tr>
<tr class="odd">
<td>Instagram</td>
<td>1.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Twitter</td>
<td>9.0</td>
<td>819290.0</td>
</tr>
<tr class="odd">
<td>Vimeo</td>
<td>11.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Linked-In</td>
<td>1.0</td>
<td>19007.0</td>
</tr>
<tr class="odd">
<td>iPhone app</td>
<td>9.0</td>
<td>33348.0</td>
</tr>
<tr class="even">
<td>SMS</td>
<td>10.0</td>
<td>54100.0</td>
</tr>
<tr class="odd">
<td>YouTube</td>
<td>2.0</td>
<td>4937.0</td>
</tr>
<tr class="even">
<td>Instagram</td>
<td>11.0</td>
<td>58968.0</td>
</tr>
<tr class="odd">
<td>YouTube</td>
<td>3.0</td>
<td>6066.0</td>
</tr>
<tr class="even">
<td>iPhone</td>
<td>2.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Newsletter</td>
<td>11.0</td>
<td>3079091.0</td>
</tr>
<tr class="even">
<td>Google+</td>
<td>2.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Android</td>
<td>4.0</td>
<td>724.0</td>
</tr>
<tr class="even">
<td>Instagram</td>
<td>2.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Android</td>
<td>3.0</td>
<td>343.0</td>
</tr>
<tr class="even">
<td>Youtube</td>
<td>10.0</td>
<td>429.0</td>
</tr>
<tr class="odd">
<td>Android</td>
<td>11.0</td>
<td>11259.0</td>
</tr>
<tr class="even">
<td>Newsletter</td>
<td>12.0</td>
<td>1606654.0</td>
</tr>
<tr class="odd">
<td>iPhone App</td>
<td>4.0</td>
<td>55960.0</td>
</tr>
<tr class="even">
<td>iPhone app</td>
<td>12.0</td>
<td>21352.0</td>
</tr>
<tr class="odd">
<td>Facebook</td>
<td>3.0</td>
<td>291971.0</td>
</tr>
<tr class="even">
<td>Google+</td>
<td>5.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Newsletter</td>
<td>5.0</td>
<td>847813.0</td>
</tr>
<tr class="even">
<td>Instagram</td>
<td>10.0</td>
<td>28145.0</td>
</tr>
<tr class="odd">
<td>Linked-In</td>
<td>7.0</td>
<td>31758.0</td>
</tr>
<tr class="even">
<td>Tumblr</td>
<td>5.0</td>
<td>26932.0</td>
</tr>
<tr class="odd">
<td>SMS</td>
<td>2.0</td>
<td>62034.0</td>
</tr>
<tr class="even">
<td>Linked-In</td>
<td>6.0</td>
<td>30867.0</td>
</tr>
<tr class="odd">
<td>YouTube</td>
<td>11.0</td>
<td>22820.0</td>
</tr>
<tr class="even">
<td>Foursquare</td>
<td>3.0</td>
<td>25786.0</td>
</tr>
<tr class="odd">
<td>SMS</td>
<td>11.0</td>
<td>170234.0</td>
</tr>
<tr class="even">
<td>Foursquare (Badge Unlock)</td>
<td>11.0</td>
<td>22512.0</td>
</tr>
<tr class="odd">
<td>Youtube</td>
<td>11.0</td>
<td>770.0</td>
</tr>
<tr class="even">
<td>iPhone App</td>
<td>2.0</td>
<td>21672.0</td>
</tr>
<tr class="odd">
<td>YouTube</td>
<td>12.0</td>
<td>9505.0</td>
</tr>
<tr class="even">
<td>Foursquare</td>
<td>8.0</td>
<td>42230.0</td>
</tr>
<tr class="odd">
<td>Foursquare (Badge Unlock)</td>
<td>4.0</td>
<td>20152.0</td>
</tr>
<tr class="even">
<td>Facebook</td>
<td>5.0</td>
<td>351601.0</td>
</tr>
<tr class="odd">
<td>Tumblr</td>
<td>3.0</td>
<td>5098.0</td>
</tr>
<tr class="even">
<td>Linked-In</td>
<td>8.0</td>
<td>45346.0</td>
</tr>
<tr class="odd">
<td>Google+</td>
<td>10.0</td>
<td>8.0</td>
</tr>
<tr class="even">
<td>Facebook</td>
<td>6.0</td>
<td>399330.0</td>
</tr>
<tr class="odd">
<td>Foursquare</td>
<td>1.0</td>
<td>10126.0</td>
</tr>
<tr class="even">
<td>YouTube</td>
<td>4.0</td>
<td>12542.0</td>
</tr>
<tr class="odd">
<td>Newsletter</td>
<td>6.0</td>
<td>1137677.0</td>
</tr>
<tr class="even">
<td>YouTube</td>
<td>7.0</td>
<td>6748.0</td>
</tr>
<tr class="odd">
<td>Google+</td>
<td>4.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Youtube</td>
<td>6.0</td>
<td>229.0</td>
</tr>
<tr class="odd">
<td>Linked-In</td>
<td>11.0</td>
<td>75747.0</td>
</tr>
<tr class="even">
<td>Facebook</td>
<td>1.0</td>
<td>259797.0</td>
</tr>
<tr class="odd">
<td>Youtube</td>
<td>5.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Vimeo</td>
<td>10.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>iPhone App</td>
<td>1.0</td>
<td>21672.0</td>
</tr>
<tr class="even">
<td>Android</td>
<td>5.0</td>
<td>381.0</td>
</tr>
<tr class="odd">
<td>Flickr</td>
<td>8.0</td>
<td>493.0</td>
</tr>
<tr class="even">
<td>iPhone app</td>
<td>8.0</td>
<td>41389.0</td>
</tr>
<tr class="odd">
<td>Facebook</td>
<td>10.0</td>
<td>1010914.0</td>
</tr>
<tr class="even">
<td>Foursquare (Badge Unlock)</td>
<td>10.0</td>
<td>11256.0</td>
</tr>
<tr class="odd">
<td>Instagram</td>
<td>8.0</td>
<td>5995.0</td>
</tr>
<tr class="even">
<td>Vimeo</td>
<td>12.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Flickr</td>
<td>1.0</td>
<td>217.0</td>
</tr>
<tr class="even">
<td>WordPress</td>
<td>2.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Android</td>
<td>10.0</td>
<td>5473.0</td>
</tr>
<tr class="even">
<td>Youtube</td>
<td>2.0</td>
<td>155.0</td>
</tr>
<tr class="odd">
<td>Android</td>
<td>6.0</td>
<td>102.0</td>
</tr>
<tr class="even">
<td>Tumblr</td>
<td>7.0</td>
<td>47121.0</td>
</tr>
<tr class="odd">
<td>Pinterest</td>
<td>1.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Linked-In</td>
<td>3.0</td>
<td>20761.0</td>
</tr>
<tr class="odd">
<td>iPhone app</td>
<td>10.0</td>
<td>30713.0</td>
</tr>
<tr class="even">
<td>Youtube</td>
<td>3.0</td>
<td>163.0</td>
</tr>
<tr class="odd">
<td>WordPress</td>
<td>6.0</td>
<td>4641.0</td>
</tr>
<tr class="even">
<td>iPhone</td>
<td>11.0</td>
<td>25848.0</td>
</tr>
<tr class="odd">
<td>Broadcastr</td>
<td>3.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Youtube</td>
<td>7.0</td>
<td>240.0</td>
</tr>
<tr class="odd">
<td>iPhone App</td>
<td>11.0</td>
<td>96342.0</td>
</tr>
<tr class="even">
<td>SMS</td>
<td>12.0</td>
<td>124068.0</td>
</tr>
<tr class="odd">
<td>Facebook</td>
<td>4.0</td>
<td>674303.0</td>
</tr>
<tr class="even">
<td>Youtube</td>
<td>12.0</td>
<td>291.0</td>
</tr>
<tr class="odd">
<td>Twitter</td>
<td>1.0</td>
<td>364376.0</td>
</tr>
<tr class="even">
<td>Newsletter</td>
<td>3.0</td>
<td>803327.0</td>
</tr>
<tr class="odd">
<td>iPhone</td>
<td>5.0</td>
<td>8203.0</td>
</tr>
<tr class="even">
<td>iPhone app</td>
<td>5.0</td>
<td>30598.0</td>
</tr>
<tr class="odd">
<td>Tumblr</td>
<td>8.0</td>
<td>54224.0</td>
</tr>
<tr class="even">
<td>iPhone App</td>
<td>12.0</td>
<td>43344.0</td>
</tr>
<tr class="odd">
<td>WordPress</td>
<td>1.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>iPhone app</td>
<td>3.0</td>
<td>10676.0</td>
</tr>
<tr class="odd">
<td>Flickr</td>
<td>12.0</td>
<td>432.0</td>
</tr>
<tr class="even">
<td>Twitter</td>
<td>10.0</td>
<td>859354.0</td>
</tr>
<tr class="odd">
<td>Android</td>
<td>12.0</td>
<td>686.0</td>
</tr>
<tr class="even">
<td>Vimeo</td>
<td>6.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Vimeo</td>
<td>8.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>iPhone App</td>
<td>6.0</td>
<td>34966.0</td>
</tr>
<tr class="odd">
<td>iPhone</td>
<td>6.0</td>
<td>9643.0</td>
</tr>
<tr class="even">
<td>SMS</td>
<td>5.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Vimeo</td>
<td>7.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Android</td>
<td>1.0</td>
<td>343.0</td>
</tr>
<tr class="odd">
<td>WordPress</td>
<td>12.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>iPhone app</td>
<td>6.0</td>
<td>2713.0</td>
</tr>
<tr class="odd">
<td>Newsletter</td>
<td>10.0</td>
<td>2359712.0</td>
</tr>
<tr class="even">
<td>Pinterest</td>
<td>11.0</td>
<td>312.0</td>
</tr>
<tr class="odd">
<td>Broadcastr</td>
<td>6.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Google+</td>
<td>1.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Android</td>
<td>8.0</td>
<td>5784.0</td>
</tr>
<tr class="even">
<td>Pinterest</td>
<td>6.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Newsletter</td>
<td>4.0</td>
<td>1606654.0</td>
</tr>
<tr class="even">
<td>Newsletter</td>
<td>9.0</td>
<td>1941202.0</td>
</tr>
<tr class="odd">
<td>WordPress</td>
<td>10.0</td>
<td>66299.0</td>
</tr>
<tr class="even">
<td>iPhone</td>
<td>1.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>SMS</td>
<td>8.0</td>
<td>116134.0</td>
</tr>
<tr class="even">
<td>Foursquare (Badge Unlock)</td>
<td>12.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Twitter</td>
<td>7.0</td>
<td>470477.0</td>
</tr>
<tr class="even">
<td>Linked-In</td>
<td>12.0</td>
<td>35231.0</td>
</tr>
<tr class="odd">
<td>Foursquare (Badge Unlock)</td>
<td>1.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>iPhone</td>
<td>4.0</td>
<td>8203.0</td>
</tr>
<tr class="odd">
<td>Flickr</td>
<td>3.0</td>
<td>227.0</td>
</tr>
<tr class="even">
<td>iPhone app</td>
<td>11.0</td>
<td>72102.0</td>
</tr>
<tr class="odd">
<td>YouTube</td>
<td>1.0</td>
<td>4904.0</td>
</tr>
<tr class="even">
<td>Flickr</td>
<td>10.0</td>
<td>153231.0</td>
</tr>
<tr class="odd">
<td>Instagram</td>
<td>4.0</td>
<td>3404.0</td>
</tr>
<tr class="even">
<td>WordPress</td>
<td>8.0</td>
<td>5017.0</td>
</tr>
<tr class="odd">
<td>YouTube</td>
<td>9.0</td>
<td>12107.0</td>
</tr>
<tr class="even">
<td>iPhone app</td>
<td>4.0</td>
<td>41274.0</td>
</tr>
<tr class="odd">
<td>Broadcastr</td>
<td>12.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Linked-In</td>
<td>4.0</td>
<td>43582.0</td>
</tr>
<tr class="odd">
<td>YouTube</td>
<td>8.0</td>
<td>10974.0</td>
</tr>
<tr class="even">
<td>iPhone</td>
<td>12.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Newsletter</td>
<td>1.0</td>
<td>803327.0</td>
</tr>
<tr class="even">
<td>Broadcastr</td>
<td>9.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Broadcastr</td>
<td>2.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Flickr</td>
<td>5.0</td>
<td>287.0</td>
</tr>
<tr class="odd">
<td>Tumblr</td>
<td>10.0</td>
<td>97401.0</td>
</tr>
<tr class="even">
<td>Linked-In</td>
<td>2.0</td>
<td>19920.0</td>
</tr>
<tr class="odd">
<td>Broadcastr</td>
<td>8.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>iPhone App</td>
<td>9.0</td>
<td>42128.0</td>
</tr>
<tr class="odd">
<td>Pinterest</td>
<td>2.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Twitter</td>
<td>4.0</td>
<td>844718.0</td>
</tr>
<tr class="odd">
<td>Broadcastr</td>
<td>10.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Foursquare</td>
<td>12.0</td>
<td>19110.0</td>
</tr>
<tr class="odd">
<td>Tumblr</td>
<td>6.0</td>
<td>41248.0</td>
</tr>
<tr class="even">
<td>Vimeo</td>
<td>2.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>WordPress</td>
<td>4.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Twitter</td>
<td>11.0</td>
<td>1660542.0</td>
</tr>
<tr class="odd">
<td>Linked-In</td>
<td>9.0</td>
<td>49299.0</td>
</tr>
<tr class="even">
<td>Twitter</td>
<td>12.0</td>
<td>690189.0</td>
</tr>
<tr class="odd">
<td>Android</td>
<td>9.0</td>
<td>445.0</td>
</tr>
<tr class="even">
<td>iPhone</td>
<td>9.0</td>
<td>12924.0</td>
</tr>
<tr class="odd">
<td>WordPress</td>
<td>9.0</td>
<td>4897.0</td>
</tr>
<tr class="even">
<td>Tumblr</td>
<td>1.0</td>
<td>2645.0</td>
</tr>
<tr class="odd">
<td>Foursquare</td>
<td>2.0</td>
<td>21181.0</td>
</tr>
<tr class="even">
<td>Foursquare</td>
<td>10.0</td>
<td>69598.0</td>
</tr>
<tr class="odd">
<td>Broadcastr</td>
<td>1.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Pinterest</td>
<td>10.0</td>
<td>141.0</td>
</tr>
<tr class="odd">
<td>Facebook</td>
<td>9.0</td>
<td>754875.0</td>
</tr>
<tr class="even">
<td>YouTube</td>
<td>10.0</td>
<td>9100.0</td>
</tr>
<tr class="odd">
<td>Twitter</td>
<td>3.0</td>
<td>400250.0</td>
</tr>
<tr class="even">
<td>Broadcastr</td>
<td>11.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Pinterest</td>
<td>8.0</td>
<td>38.0</td>
</tr>
<tr class="even">
<td>Twitter</td>
<td>6.0</td>
<td>461261.0</td>
</tr>
<tr class="odd">
<td>Foursquare (Badge Unlock)</td>
<td>2.0</td>
<td>8878.0</td>
</tr>
<tr class="even">
<td>Google+</td>
<td>12.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Google+</td>
<td>3.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Linked-In</td>
<td>5.0</td>
<td>29808.0</td>
</tr>
<tr class="odd">
<td>Vimeo</td>
<td>1.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Foursquare</td>
<td>5.0</td>
<td>29991.0</td>
</tr>
<tr class="odd">
<td>Foursquare</td>
<td>7.0</td>
<td>38590.0</td>
</tr>
<tr class="even">
<td>iPhone app</td>
<td>7.0</td>
<td>30713.0</td>
</tr>
<tr class="odd">
<td>Newsletter</td>
<td>2.0</td>
<td>803327.0</td>
</tr>
<tr class="even">
<td>Twitter</td>
<td>8.0</td>
<td>704438.0</td>
</tr>
<tr class="odd">
<td>Facebook</td>
<td>2.0</td>
<td>107993.0</td>
</tr>
<tr class="even">
<td>Broadcastr</td>
<td>4.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Instagram</td>
<td>5.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Android</td>
<td>7.0</td>
<td>445.0</td>
</tr>
<tr class="odd">
<td>Flickr</td>
<td>9.0</td>
<td>549.0</td>
</tr>
<tr class="even">
<td>WordPress</td>
<td>11.0</td>
<td>9294.0</td>
</tr>
<tr class="odd">
<td>Flickr</td>
<td>11.0</td>
<td>1007.0</td>
</tr>
<tr class="even">
<td>Pinterest</td>
<td>4.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>iPhone</td>
<td>10.0</td>
<td>12924.0</td>
</tr>
<tr class="even">
<td>Pinterest</td>
<td>12.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Flickr</td>
<td>4.0</td>
<td>545.0</td>
</tr>
<tr class="even">
<td>Facebook</td>
<td>7.0</td>
<td>451076.0</td>
</tr>
<tr class="odd">
<td>Google+</td>
<td>8.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Tumblr</td>
<td>9.0</td>
<td>62742.0</td>
</tr>
<tr class="odd">
<td>Android</td>
<td>2.0</td>
<td>343.0</td>
</tr>
<tr class="even">
<td>Foursquare</td>
<td>4.0</td>
<td>57337.0</td>
</tr>
<tr class="odd">
<td>SMS</td>
<td>1.0</td>
<td>62034.0</td>
</tr>
<tr class="even">
<td>WordPress</td>
<td>3.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Instagram</td>
<td>3.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>iPhone App</td>
<td>3.0</td>
<td>21672.0</td>
</tr>
<tr class="odd">
<td>Tumblr</td>
<td>4.0</td>
<td>31247.0</td>
</tr>
<tr class="even">
<td>Pinterest</td>
<td>9.0</td>
<td>74.0</td>
</tr>
<tr class="odd">
<td>Broadcastr</td>
<td>5.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>YouTube</td>
<td>5.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Facebook</td>
<td>8.0</td>
<td>657312.0</td>
</tr>
<tr class="even">
<td>iPhone App</td>
<td>5.0</td>
<td>34288.0</td>
</tr>
<tr class="odd">
<td>Youtube</td>
<td>1.0</td>
<td>150.0</td>
</tr>
<tr class="even">
<td>SMS</td>
<td>9.0</td>
<td>116134.0</td>
</tr>
<tr class="odd">
<td>iPhone</td>
<td>3.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Vimeo</td>
<td>5.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Google+</td>
<td>7.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>SMS</td>
<td>6.0</td>
<td>54100.0</td>
</tr>
<tr class="odd">
<td>Vimeo</td>
<td>9.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Google+</td>
<td>11.0</td>
<td>26.0</td>
</tr>
<tr class="odd">
<td>Pinterest</td>
<td>7.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Foursquare (Badge Unlock)</td>
<td>9.0</td>
<td>11256.0</td>
</tr>
<tr class="odd">
<td>SMS</td>
<td>3.0</td>
<td>62034.0</td>
</tr>
<tr class="even">
<td>Instagram</td>
<td>7.0</td>
<td>5450.0</td>
</tr>
<tr class="odd">
<td>iPhone App</td>
<td>10.0</td>
<td>64589.0</td>
</tr>
<tr class="even">
<td>Instagram</td>
<td>12.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Flickr</td>
<td>2.0</td>
<td>219.0</td>
</tr>
<tr class="even">
<td>Instagram</td>
<td>6.0</td>
<td>4764.0</td>
</tr>
<tr class="odd">
<td>Foursquare (Badge Unlock)</td>
<td>7.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Flickr</td>
<td>6.0</td>
<td>332.0</td>
</tr>
<tr class="odd">
<td>SMS</td>
<td>7.0</td>
<td>54100.0</td>
</tr>
<tr class="even">
<td>Tumblr</td>
<td>12.0</td>
<td>5005.0</td>
</tr>
<tr class="odd">
<td>Twitter</td>
<td>5.0</td>
<td>435148.0</td>
</tr>
<tr class="even">
<td>Youtube</td>
<td>9.0</td>
<td>281.0</td>
</tr>
<tr class="odd">
<td>iPhone App</td>
<td>8.0</td>
<td>57513.0</td>
</tr>
<tr class="even">
<td>Google+</td>
<td>9.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Foursquare (Badge Unlock)</td>
<td>8.0</td>
<td>11256.0</td>
</tr>
<tr class="even">
<td>Pinterest</td>
<td>5.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>Facebook</td>
<td>11.0</td>
<td>1408965.0</td>
</tr>
<tr class="even">
<td>Twitter</td>
<td>2.0</td>
<td>385091.0</td>
</tr>
<tr class="odd">
<td>iPhone App</td>
<td>7.0</td>
<td>35841.0</td>
</tr>
<tr class="even">
<td>iPhone app</td>
<td>1.0</td>
<td>10676.0</td>
</tr>
<tr class="odd">
<td>Vimeo</td>
<td>3.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>SMS</td>
<td>4.0</td>
<td>124068.0</td>
</tr>
<tr class="odd">
<td>WordPress</td>
<td>7.0</td>
<td>4647.0</td>
</tr>
<tr class="even">
<td>Youtube</td>
<td>4.0</td>
<td>367.0</td>
</tr>
<tr class="odd">
<td>Foursquare (Badge Unlock)</td>
<td>5.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Foursquare</td>
<td>6.0</td>
<td>34193.0</td>
</tr>
<tr class="odd">
<td>Google+</td>
<td>6.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Youtube</td>
<td>8.0</td>
<td>258.0</td>
</tr>
<tr class="odd">
<td>Pinterest</td>
<td>3.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Newsletter</td>
<td>7.0</td>
<td>1137868.0</td>
</tr>
<tr class="odd">
<td>Foursquare</td>
<td>9.0</td>
<td>50489.0</td>
</tr>
<tr class="even">
<td>Foursquare</td>
<td>11.0</td>
<td>118323.0</td>
</tr>
<tr class="odd">
<td>iPhone app</td>
<td>2.0</td>
<td>10676.0</td>
</tr>
<tr class="even">
<td>Flickr</td>
<td>7.0</td>
<td>342.0</td>
</tr>
<tr class="odd">
<td>Newsletter</td>
<td>8.0</td>
<td>1941197.0</td>
</tr>
<tr class="even">
<td>Tumblr</td>
<td>11.0</td>
<td>195881.0</td>
</tr>
<tr class="odd">
<td>Foursquare (Badge Unlock)</td>
<td>3.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>Facebook</td>
<td>12.0</td>
<td>502687.0</td>
</tr>
<tr class="odd">
<td>Broadcastr</td>
<td>7.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>WordPress</td>
<td>5.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>YouTube</td>
<td>6.0</td>
<td>6509.0</td>
</tr>
<tr class="even">
<td>Tumblr</td>
<td>2.0</td>
<td>4406.0</td>
</tr>
<tr class="odd">
<td>Vimeo</td>
<td>4.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>iPhone</td>
<td>7.0</td>
<td>10336.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p>Note, that we could have done aggregation using DataFrame API instead of Spark SQL.</p>
</blockquote>
</div>
<div class="cell markdown">
<p>Alright, now let's see some <em>cool</em> operations with window functions.</p>
<p>Our next task is to compute <code>(daily visits / monthly average)</code> for all platforms.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.sql.functions.{dayofmonth, month, row_number, sum}
import org.apache.spark.sql.expressions.Window

val coolDF = anonym.select($&quot;id&quot;, $&quot;platform&quot;, dayofmonth($&quot;date&quot;).as(&quot;day&quot;), month($&quot;date&quot;).as(&quot;month&quot;), $&quot;visits&quot;).
  groupBy($&quot;id&quot;, $&quot;platform&quot;, $&quot;day&quot;, $&quot;month&quot;).agg(sum(&quot;visits&quot;).as(&quot;visits&quot;))

// Run window aggregation on visits per month and platform
val window = coolDF.select($&quot;id&quot;, $&quot;day&quot;, $&quot;visits&quot;, sum($&quot;visits&quot;).over(Window.partitionBy(&quot;platform&quot;, &quot;month&quot;)).as(&quot;monthly_visits&quot;))

// Create and register percent table
val percent = window.select($&quot;id&quot;, $&quot;day&quot;, ($&quot;visits&quot; / $&quot;monthly_visits&quot;).as(&quot;percent&quot;))

percent.createOrReplaceTempView(&quot;percent&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.sql.functions.{dayofmonth, month, row_number, sum}
import org.apache.spark.sql.expressions.Window
coolDF: org.apache.spark.sql.DataFrame = [id: bigint, platform: string ... 3 more fields]
window: org.apache.spark.sql.DataFrame = [id: bigint, day: int ... 2 more fields]
percent: org.apache.spark.sql.DataFrame = [id: bigint, day: int ... 1 more field]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-sql">-- A little bit of visualization as result of our efforts
select id, day, `percent` from percent where `percent` &gt; 0.3 and day = 2
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>id</th>
<th>day</th>
<th>percent</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>6.52835028992e11</td>
<td>2.0</td>
<td>1.0</td>
</tr>
<tr class="even">
<td>5.06806140929e11</td>
<td>2.0</td>
<td>0.4993894993894994</td>
</tr>
<tr class="odd">
<td>6.52835028992e11</td>
<td>2.0</td>
<td>0.446576072475353</td>
</tr>
<tr class="even">
<td>9.01943132161e11</td>
<td>2.0</td>
<td>0.5</td>
</tr>
<tr class="odd">
<td>6.52835028992e11</td>
<td>2.0</td>
<td>0.3181818181818182</td>
</tr>
<tr class="even">
<td>9.01943132161e11</td>
<td>2.0</td>
<td>0.6180914042150131</td>
</tr>
<tr class="odd">
<td>2.147483648e11</td>
<td>2.0</td>
<td>0.3663035756571158</td>
</tr>
<tr class="even">
<td>1.322849927168e12</td>
<td>2.0</td>
<td>0.5265514047545539</td>
</tr>
<tr class="odd">
<td>1.322849927168e12</td>
<td>2.0</td>
<td>0.3109034021149352</td>
</tr>
<tr class="even">
<td>1.408749273089e12</td>
<td>2.0</td>
<td>0.6937119675456389</td>
</tr>
<tr class="odd">
<td>6.52835028992e11</td>
<td>2.0</td>
<td>0.6765082509845611</td>
</tr>
<tr class="even">
<td>5.06806140929e11</td>
<td>2.0</td>
<td>1.0</td>
</tr>
<tr class="odd">
<td>6.52835028992e11</td>
<td>2.0</td>
<td>0.5</td>
</tr>
<tr class="even">
<td>4.12316860416e11</td>
<td>2.0</td>
<td>0.3408084980820301</td>
</tr>
<tr class="odd">
<td>1.580547964928e12</td>
<td>2.0</td>
<td>0.383582757848692</td>
</tr>
<tr class="even">
<td>6.52835028992e11</td>
<td>2.0</td>
<td>0.38833874233724447</td>
</tr>
<tr class="odd">
<td>2.06158430208e11</td>
<td>2.0</td>
<td>0.9262507474586407</td>
</tr>
<tr class="even">
<td>1.666447310848e12</td>
<td>2.0</td>
<td>0.9473684210526315</td>
</tr>
<tr class="odd">
<td>2.06158430208e11</td>
<td>2.0</td>
<td>0.5</td>
</tr>
<tr class="even">
<td>1.408749273089e12</td>
<td>2.0</td>
<td>0.394240317775571</td>
</tr>
<tr class="odd">
<td>6.8719476736e10</td>
<td>2.0</td>
<td>0.38461538461538464</td>
</tr>
<tr class="even">
<td>1.640677507072e12</td>
<td>2.0</td>
<td>0.44748143897901344</td>
</tr>
<tr class="odd">
<td>9.01943132161e11</td>
<td>2.0</td>
<td>1.0</td>
</tr>
<tr class="even">
<td>6.52835028992e11</td>
<td>2.0</td>
<td>0.8449612403100775</td>
</tr>
<tr class="odd">
<td>9.01943132161e11</td>
<td>2.0</td>
<td>0.3060168545490231</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-sql">-- You also could just use plain SQL to write query above, note that you might need to group by id and day as well.
with aggr as (
  select id, dayofmonth(date) as day, visits / sum(visits) over (partition by (platform, month(date))) as percent
  from anonym
)
select * from aggr where day = 2 and percent &gt; 0.3
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>id</th>
<th>day</th>
<th>percent</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>6.52835028992e11</td>
<td>2.0</td>
<td>1.0</td>
</tr>
<tr class="even">
<td>5.06806140929e11</td>
<td>2.0</td>
<td>0.4993894993894994</td>
</tr>
<tr class="odd">
<td>6.52835028992e11</td>
<td>2.0</td>
<td>0.446576072475353</td>
</tr>
<tr class="even">
<td>9.01943132161e11</td>
<td>2.0</td>
<td>0.5</td>
</tr>
<tr class="odd">
<td>6.52835028992e11</td>
<td>2.0</td>
<td>0.3181818181818182</td>
</tr>
<tr class="even">
<td>2.147483648e11</td>
<td>2.0</td>
<td>0.3663035756571158</td>
</tr>
<tr class="odd">
<td>9.01943132161e11</td>
<td>2.0</td>
<td>0.6180914042150131</td>
</tr>
<tr class="even">
<td>1.322849927168e12</td>
<td>2.0</td>
<td>0.4718608035989944</td>
</tr>
<tr class="odd">
<td>1.408749273089e12</td>
<td>2.0</td>
<td>0.6937119675456389</td>
</tr>
<tr class="even">
<td>6.52835028992e11</td>
<td>2.0</td>
<td>0.6765082509845611</td>
</tr>
<tr class="odd">
<td>5.06806140929e11</td>
<td>2.0</td>
<td>1.0</td>
</tr>
<tr class="even">
<td>6.52835028992e11</td>
<td>2.0</td>
<td>0.5</td>
</tr>
<tr class="odd">
<td>4.12316860416e11</td>
<td>2.0</td>
<td>0.3408084980820301</td>
</tr>
<tr class="even">
<td>1.580547964928e12</td>
<td>2.0</td>
<td>0.383582757848692</td>
</tr>
<tr class="odd">
<td>6.52835028992e11</td>
<td>2.0</td>
<td>0.38833874233724447</td>
</tr>
<tr class="even">
<td>2.06158430208e11</td>
<td>2.0</td>
<td>0.9262507474586407</td>
</tr>
<tr class="odd">
<td>1.666447310848e12</td>
<td>2.0</td>
<td>0.9473684210526315</td>
</tr>
<tr class="even">
<td>2.06158430208e11</td>
<td>2.0</td>
<td>0.5</td>
</tr>
<tr class="odd">
<td>1.408749273089e12</td>
<td>2.0</td>
<td>0.394240317775571</td>
</tr>
<tr class="even">
<td>6.8719476736e10</td>
<td>2.0</td>
<td>0.38461538461538464</td>
</tr>
<tr class="odd">
<td>1.640677507072e12</td>
<td>2.0</td>
<td>0.44748143897901344</td>
</tr>
<tr class="even">
<td>9.01943132161e11</td>
<td>2.0</td>
<td>1.0</td>
</tr>
<tr class="odd">
<td>6.52835028992e11</td>
<td>2.0</td>
<td>0.8449612403100775</td>
</tr>
<tr class="even">
<td>9.01943132161e11</td>
<td>2.0</td>
<td>0.3060168545490231</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell markdown">
<h2 id="interoperating-with-rdds"><a class="header" href="#interoperating-with-rdds">Interoperating with RDDs</a></h2>
<p>Spark SQL supports two different methods for converting existing RDDs into DataFrames. The first method uses reflection to infer the schema of an RDD that contains specific types of objects. This reflection based approach leads to more concise code and works well when you already know the schema.</p>
<p>The second method for creating DataFrames is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. While this method is more verbose, it allows you to construct DataFrames when the columns and their types are not known until runtime.</p>
<h3 id="inferring-the-schema-using-reflection"><a class="header" href="#inferring-the-schema-using-reflection">Inferring the Schema Using Reflection</a></h3>
<p>The Scala interface for Spark SQL supports automatically converting an RDD containing case classes to a DataFrame. The case class defines the schema of the table. The names of the arguments to the case class are read using reflection and become the names of the columns. Case classes can also be nested or contain complex types such as Sequences or Arrays. This RDD can be implicitly converted to a DataFrame and then be registered as a table.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Define case class that will be our schema for DataFrame
case class Hubot(name: String, year: Int, manufacturer: String, version: Array[Int], details: Map[String, String])

// You can process a text file, for example, to convert every row to our Hubot, but we will create RDD manually
val rdd = sc.parallelize(
  Array(
    Hubot(&quot;Jerry&quot;, 2015, &quot;LCorp&quot;, Array(1, 2, 3), Map(&quot;eat&quot; -&gt; &quot;yes&quot;, &quot;sleep&quot; -&gt; &quot;yes&quot;, &quot;drink&quot; -&gt; &quot;yes&quot;)),
    Hubot(&quot;Mozart&quot;, 2010, &quot;LCorp&quot;, Array(1, 2), Map(&quot;eat&quot; -&gt; &quot;no&quot;, &quot;sleep&quot; -&gt; &quot;no&quot;, &quot;drink&quot; -&gt; &quot;no&quot;)),
    Hubot(&quot;Einstein&quot;, 2012, &quot;LCorp&quot;, Array(1, 2, 3), Map(&quot;eat&quot; -&gt; &quot;yes&quot;, &quot;sleep&quot; -&gt; &quot;yes&quot;, &quot;drink&quot; -&gt; &quot;no&quot;))
  )
)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class Hubot
rdd: org.apache.spark.rdd.RDD[Hubot] = ParallelCollectionRDD[23107] at parallelize at &lt;console&gt;:45
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// In order to convert RDD into DataFrame you need to do this:
val hubots = rdd.toDF()

// Display DataFrame, note how array and map fields are displayed
hubots.printSchema()
hubots.show()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>root
 |-- name: string (nullable = true)
 |-- year: integer (nullable = false)
 |-- manufacturer: string (nullable = true)
 |-- version: array (nullable = true)
 |    |-- element: integer (containsNull = false)
 |-- details: map (nullable = true)
 |    |-- key: string
 |    |-- value: string (valueContainsNull = true)

+--------+----+------------+---------+--------------------+
|    name|year|manufacturer|  version|             details|
+--------+----+------------+---------+--------------------+
|   Jerry|2015|       LCorp|[1, 2, 3]|Map(eat -&gt; yes, s...|
|  Mozart|2010|       LCorp|   [1, 2]|Map(eat -&gt; no, sl...|
|Einstein|2012|       LCorp|[1, 2, 3]|Map(eat -&gt; yes, s...|
+--------+----+------------+---------+--------------------+

hubots: org.apache.spark.sql.DataFrame = [name: string, year: int ... 3 more fields]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// You can query complex type the same as you query any other column
// By the way you can use `sql` function to invoke Spark SQL to create DataFrame
hubots.createOrReplaceTempView(&quot;hubots&quot;)

val onesThatEat = sqlContext.sql(&quot;select name, details.eat from hubots where details.eat = 'yes'&quot;)

onesThatEat.show()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+--------+---+
|    name|eat|
+--------+---+
|   Jerry|yes|
|Einstein|yes|
+--------+---+

onesThatEat: org.apache.spark.sql.DataFrame = [name: string, eat: string]
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="programmatically-specifying-the-schema"><a class="header" href="#programmatically-specifying-the-schema">Programmatically Specifying the Schema</a></h3>
<p>When case classes cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a <code>DataFrame</code> can be created programmatically with three steps.</p>
<ol>
<li>Create an RDD of <code>Row</code>s from the original RDD</li>
<li>Create the schema represented by a <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.types.StructType">StructType</a> and <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.types.StructField">StructField</a> classes matching the structure of <code>Row</code>s in the RDD created in Step 1.</li>
<li>Apply the schema to the RDD of <code>Row</code>s via <code>createDataFrame</code> method provided by <code>SQLContext</code>.</li>
</ol>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.sql.types._

// Let's say we have an RDD of String and we need to convert it into a DataFrame with schema &quot;name&quot;, &quot;year&quot;, and &quot;manufacturer&quot;
// As you can see every record is space-separated.
val rdd = sc.parallelize(
  Array(
    &quot;Jerry 2015 LCorp&quot;,
    &quot;Mozart 2010 LCorp&quot;,
    &quot;Einstein 2012 LCorp&quot;
  )
)

// Create schema as StructType //
val schema = StructType(
  StructField(&quot;name&quot;, StringType, false) :: 
  StructField(&quot;year&quot;, IntegerType, false) :: 
  StructField(&quot;manufacturer&quot;, StringType, false) :: 
  Nil
)

// Prepare RDD[Row]
val rows = rdd.map { entry =&gt; 
  val arr = entry.split(&quot;\\s+&quot;)
  val name = arr(0)
  val year = arr(1).toInt
  val manufacturer = arr(2)
  
  Row(name, year, manufacturer)
}

// Create DataFrame
val bots = sqlContext.createDataFrame(rows, schema)
bots.printSchema()
bots.show()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>root
 |-- name: string (nullable = false)
 |-- year: integer (nullable = false)
 |-- manufacturer: string (nullable = false)

+--------+----+------------+
|    name|year|manufacturer|
+--------+----+------------+
|   Jerry|2015|       LCorp|
|  Mozart|2010|       LCorp|
|Einstein|2012|       LCorp|
+--------+----+------------+

import org.apache.spark.sql.types._
rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[23118] at parallelize at &lt;console&gt;:47
schema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,false), StructField(year,IntegerType,false), StructField(manufacturer,StringType,false))
rows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[23119] at map at &lt;console&gt;:64
bots: org.apache.spark.sql.DataFrame = [name: string, year: int ... 1 more field]
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="creating-datasets"><a class="header" href="#creating-datasets">Creating Datasets</a></h2>
<p>A <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset">Dataset</a> is a strongly-typed, immutable collection of objects that are mapped to a relational schema. At the core of the Dataset API is a new concept called an <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder">encoder</a>, which is responsible for converting between JVM objects and tabular representation. The tabular representation is stored using Spark’s internal Tungsten binary format, allowing for operations on serialized data and improved memory utilization. Spark 2.2 comes with support for automatically generating encoders for a wide variety of types, including primitive types (e.g. String, Integer, Long), and Scala case classes.</p>
<blockquote>
<p>Simply put, you will get all the benefits of DataFrames with fair amount of flexibility of RDD API.</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// We can start working with Datasets by using our &quot;hubots&quot; table

// To create Dataset from DataFrame do this (assuming that case class Hubot exists):
val ds = hubots.as[Hubot]
ds.show()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+--------+----+------------+---------+--------------------+
|    name|year|manufacturer|  version|             details|
+--------+----+------------+---------+--------------------+
|   Jerry|2015|       LCorp|[1, 2, 3]|Map(eat -&gt; yes, s...|
|  Mozart|2010|       LCorp|   [1, 2]|Map(eat -&gt; no, sl...|
|Einstein|2012|       LCorp|[1, 2, 3]|Map(eat -&gt; yes, s...|
+--------+----+------------+---------+--------------------+

ds: org.apache.spark.sql.Dataset[Hubot] = [name: string, year: int ... 3 more fields]
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p><strong>Side-note:</strong> Dataset API is first-class citizen in Spark, and DataFrame is an alias for Dataset[Row]. Note that Python and R still use DataFrames (since they are dynamically typed), but it is essentially a Dataset.</p>
</blockquote>
</div>
<div class="cell markdown">
<h2 id="finally"><a class="header" href="#finally">Finally</a></h2>
<p>DataFrames and Datasets can simplify and improve most of the applications pipelines by bringing concise syntax and performance optimizations. We would highly recommend you to check out the official API documentation, specifically around * <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame">DataFrame API</a>, * <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$">Spark SQL functions library</a>, * <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.GroupedData">GroupBy clause and aggregated functions</a>.</p>
<p>Unfortunately, this is just <em>a getting started quickly</em> course, and we skip features like custom aggregations, types, pivoting, etc., but if you are keen to know then start from the links above and this notebook and others in this directory.</p>
</div>
<div class="cell markdown">
<h2 id="appendix"><a class="header" href="#appendix">Appendix</a></h2>
<h3 id="how-to-download-data-and-make-a-table"><a class="header" href="#how-to-download-data-and-make-a-table">How to download data and make a table</a></h3>
<p>Okay, so how did we actually make table &quot;social<em>media</em>usage&quot;? Databricks allows us to upload/link external data and make it available as registerd SQL table. It involves several steps: 1. Find interesting set of data - Google can be your friend for most cases here, or you can have your own dataset as CSV file, for example. Good source of data can also be found here: http://www.data.gov/ 2. Download / prepare it to be either on S3, or human-readable format like CSV, or JSON 3. Go to Databricks cloud (where you log in to use Databricks notebooks) and open tab <strong>Tables</strong> 4. On the very top of the left sub-menu you will see button <strong>+ Create table</strong>, click on it 5. You will see page with drop-down menu of the list of sources you can provide, <strong>File</strong> means any file (Parquet, Avro, CSV), but it works the best with CSV format 6. Once you have chosen file and loaded it, you can change column names, or tweak types (mainly for CSV format) 7. That is it. Just click on final button to create table. After that you can refer to the table using <code>sqlContext.table(&quot;YOUR_TABLE_NAME&quot;)</code></p>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-14"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-14"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Apache Spark 2.2 sql-progamming-guide</a>.</p>
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide003_datasources_sqlprogguidedata-sourcesa"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide003_datasources_sqlprogguidedata-sourcesa"><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/003_dataSources_sqlProgGuide">Data Sources</a></a></h1>
<h2 id="a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide000_sqlprogguidespark-sql-programming-guidea-1"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide000_sqlprogguidespark-sql-programming-guidea-1"><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/000_sqlProgGuide">Spark Sql Programming Guide</a></a></h2>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/001_overview_sqlProgGuide">Overview</a>
<ul>
<li>SQL</li>
<li>DataFrames</li>
<li>Datasets</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/002_gettingStarted_sqlProgGuide">Getting Started</a>
<ul>
<li>Starting Point: SQLContext</li>
<li>Creating DataFrames</li>
<li>DataFrame Operations</li>
<li>Running SQL Queries Programmatically</li>
<li>Creating Datasets</li>
<li>Interoperating with RDDs
<ul>
<li>Inferring the Schema Using Reflection</li>
<li>Programmatically Specifying the Schema</li>
</ul>
</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/003_dataSources_sqlProgGuide">Data Sources</a>
<ul>
<li>Generic Load/Save Functions
<ul>
<li>Manually Specifying Options</li>
<li>Run SQL on files directly</li>
<li>Save Modes</li>
<li>Saving to Persistent Tables</li>
</ul>
</li>
<li>Parquet Files
<ul>
<li>Loading Data Programmatically</li>
<li>Partition Discovery</li>
<li>Schema Merging</li>
<li>Hive metastore Parquet table conversion
<ul>
<li>Hive/Parquet Schema Reconciliation</li>
<li>Metadata Refreshing</li>
</ul>
</li>
<li>Configuration</li>
</ul>
</li>
<li>JSON Datasets</li>
<li>Hive Tables
<ul>
<li>Interacting with Different Versions of Hive Metastore</li>
</ul>
</li>
<li>JDBC To Other Databases</li>
<li>Troubleshooting</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/004_performanceTuning_sqlProgGuide">Performance Tuning</a>
<ul>
<li>Caching Data In Memory</li>
<li>Other Configuration Options</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/005_distributedSqlEngine_sqlProgGuide">Distributed SQL Engine</a>
<ul>
<li>Running the Thrift JDBC/ODBC server</li>
<li>Running the Spark SQL CLI</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide003_datasources_sqlprogguidedata-sourcesa-1"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide003_datasources_sqlprogguidedata-sourcesa-1"><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/003_dataSources_sqlProgGuide">Data Sources</a></a></h1>
<p>Spark SQL supports operating on a variety of data sources through the <code>DataFrame</code> or <code>DataFrame</code> interfaces. A Dataset can be operated on as normal RDDs and can also be registered as a temporary table. Registering a Dataset as a table allows you to run SQL queries over its data. But from time to time you would need to either load or save Dataset. Spark SQL provides built-in data sources as well as Data Source API to define your own data source and use it read / write data into Spark.</p>
</div>
<div class="cell markdown">
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>Spark provides some built-in datasources that you can use straight out of the box, such as <a href="https://parquet.apache.org/">Parquet</a>, <a href="http://www.json.org/">JSON</a>, <a href="https://en.wikipedia.org/wiki/Java_Database_Connectivity">JDBC</a>, <a href="https://orc.apache.org/">ORC</a> (available with enabled Hive Support, but this is changing, and ORC will not require Hive support and will work with default Spark session starting from next release), and Text (since Spark 1.6) and CSV (since Spark 2.0, before that it is accessible as a package).</p>
<h2 id="third-party-datasource-packages"><a class="header" href="#third-party-datasource-packages">Third-party datasource packages</a></h2>
<p>Community also have built quite a few datasource packages to provide easy access to the data from other formats. You can find list of those packages on http://spark-packages.org/, e.g. <a href="http://spark-packages.org/package/databricks/spark-avro">Avro</a>, <a href="http://spark-packages.org/package/databricks/spark-csv">CSV</a>, <a href="http://spark-packages.org/package/databricks/spark-redshift">Amazon Redshit</a> (for Spark &lt; 2.0), <a href="http://spark-packages.org/package/HyukjinKwon/spark-xml">XML</a>, <a href="http://spark-packages.org/package/sadikovi/spark-netflow">NetFlow</a> and many others.</p>
</div>
<div class="cell markdown">
<h2 id="generic-loadsave-functions"><a class="header" href="#generic-loadsave-functions">Generic Load/Save functions</a></h2>
<p>In order to load or save DataFrame you have to call either <code>read</code> or <code>write</code>. This will return <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader">DataFrameReader</a> or <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter">DataFrameWriter</a> depending on what you are trying to achieve. Essentially these classes are entry points to the reading / writing actions. They allow you to specify writing mode or provide additional options to read data source.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// This will return DataFrameReader to read data source
println(spark.read)

val df = spark.range(0, 10)

// This will return DataFrameWriter to save DataFrame
println(df.write)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>org.apache.spark.sql.DataFrameReader@7e7dee07
org.apache.spark.sql.DataFrameWriter@1f45f4b
df: org.apache.spark.sql.Dataset[Long] = [id: bigint]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Saving Parquet table in Scala
val df_save = spark.table(&quot;social_media_usage&quot;).select(&quot;platform&quot;, &quot;visits&quot;)
df_save.write.mode(&quot;overwrite&quot;).parquet(&quot;/tmp/platforms.parquet&quot;)

// Loading Parquet table in Scala
val df = spark.read.parquet(&quot;/tmp/platforms.parquet&quot;)
df.show(5)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+----------+------+
|  platform|visits|
+----------+------+
|       SMS| 61652|
|       SMS| 44547|
|    Flickr|  null|
|Newsletter|  null|
|   Twitter|   389|
+----------+------+
only showing top 5 rows

df_save: org.apache.spark.sql.DataFrame = [platform: string, visits: int]
df: org.apache.spark.sql.DataFrame = [platform: string, visits: int]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># Loading Parquet table in Python
dfPy = spark.read.parquet(&quot;/tmp/platforms.parquet&quot;)
dfPy.show(5)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+----------+------+
|  platform|visits|
+----------+------+
|       SMS| 61652|
|       SMS| 44547|
|    Flickr|  null|
|Newsletter|  null|
|   Twitter|   389|
+----------+------+
only showing top 5 rows
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Saving JSON dataset in Scala
val df_save = spark.table(&quot;social_media_usage&quot;).select(&quot;platform&quot;, &quot;visits&quot;)
df_save.write.json(&quot;/tmp/platforms.json&quot;)

// Loading JSON dataset in Scala
val df = spark.read.json(&quot;/tmp/platforms.json&quot;)
df.show(5)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+----------+------+
|  platform|visits|
+----------+------+
|       SMS| 61652|
|       SMS| 44547|
|    Flickr|  null|
|Newsletter|  null|
|   Twitter|   389|
+----------+------+
only showing top 5 rows

df_save: org.apache.spark.sql.DataFrame = [platform: string, visits: int]
df: org.apache.spark.sql.DataFrame = [platform: string, visits: bigint]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># Loading JSON dataset in Python
dfPy = spark.read.json(&quot;/tmp/platforms.json&quot;)
dfPy.show(5)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+----------+------+
|  platform|visits|
+----------+------+
|       SMS| 61652|
|       SMS| 44547|
|    Flickr|  null|
|Newsletter|  null|
|   Twitter|   389|
+----------+------+
only showing top 5 rows
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="manually-specifying-options"><a class="header" href="#manually-specifying-options">Manually Specifying Options</a></h3>
<p>You can also manually specify the data source that will be used along with any extra options that you would like to pass to the data source. Data sources are specified by their fully qualified name (i.e., <code>org.apache.spark.sql.parquet</code>), but for built-in sources you can also use their short names (<code>json</code>, <code>parquet</code>, <code>jdbc</code>). DataFrames of any type can be converted into other types using this syntax.</p>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-scala">val json = sqlContext.read.format(&quot;json&quot;).load(&quot;/tmp/platforms.json&quot;)
json.select(&quot;platform&quot;).show(10)

val parquet = sqlContext.read.format(&quot;parquet&quot;).load(&quot;/tmp/platforms.parquet&quot;)
parquet.select(&quot;platform&quot;).show(10)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+----------+
|  platform|
+----------+
|       SMS|
|       SMS|
|    Flickr|
|Newsletter|
|   Twitter|
|   Twitter|
|   Android|
|   Android|
|   Android|
|Broadcastr|
+----------+
only showing top 10 rows

+----------+
|  platform|
+----------+
|       SMS|
|       SMS|
|    Flickr|
|Newsletter|
|   Twitter|
|   Twitter|
|   Android|
|   Android|
|   Android|
|Broadcastr|
+----------+
only showing top 10 rows

json: org.apache.spark.sql.DataFrame = [platform: string, visits: bigint]
parquet: org.apache.spark.sql.DataFrame = [platform: string, visits: int]
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="run-sql-on-files-directly"><a class="header" href="#run-sql-on-files-directly">Run SQL on files directly</a></h3>
<p>Instead of using read API to load a file into DataFrame and query it, you can also query that file directly with SQL.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val df = sqlContext.sql(&quot;SELECT * FROM parquet.`/tmp/platforms.parquet`&quot;)
df.printSchema()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>root
 |-- platform: string (nullable = true)
 |-- visits: integer (nullable = true)

df: org.apache.spark.sql.DataFrame = [platform: string, visits: int]
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="save-modes"><a class="header" href="#save-modes">Save Modes</a></h3>
<p>Save operations can optionally take a <code>SaveMode</code>, that specifies how to handle existing data if present. It is important to realize that these save modes do not utilize any locking and are not atomic. Additionally, when performing a <code>Overwrite</code>, the data will be deleted before writing out the new data.</p>
<table><thead><tr><th>Scala/Java</th><th>Any language</th><th>Meaning</th></tr></thead><tbody>
<tr><td><code>SaveMode.ErrorIfExists</code> (default)</td><td><code>&quot;error&quot;</code> (default)</td><td>When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.</td></tr>
<tr><td><code>SaveMode.Append</code></td><td><code>&quot;append&quot;</code></td><td>When saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.</td></tr>
<tr><td><code>SaveMode.Overwrite</code></td><td><code>&quot;overwrite&quot;</code></td><td>Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.</td></tr>
<tr><td><code>SaveMode.Ignore</code></td><td><code>&quot;ignore&quot;</code></td><td>Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data. This is similar to a <code>CREATE TABLE IF NOT EXISTS</code> in SQL.</td></tr>
</tbody></table>
</div>
<div class="cell markdown">
<h3 id="saving-to-persistent-tables"><a class="header" href="#saving-to-persistent-tables">Saving to Persistent Tables</a></h3>
<p><code>DataFrame</code> and <code>Dataset</code> can also be saved as persistent tables using the <code>saveAsTable</code> command. Unlike the <code>createOrReplaceTempView</code> command, <code>saveAsTable</code> will materialize the contents of the dataframe and create a pointer to the data in the metastore. Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. A DataFrame for a persistent table can be created by calling the <code>table</code> method on a <code>SparkSession</code> with the name of the table.</p>
<p>By default <code>saveAsTable</code> will create a “managed table”, meaning that the location of the data will be controlled by the metastore. Managed tables will also have their data deleted automatically when a table is dropped.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// First of all list tables to see that table we are about to create does not exist
spark.catalog.listTables.show()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+--------------------+--------+-----------+---------+-----------+
|                name|database|description|tableType|isTemporary|
+--------------------+--------+-----------+---------+-----------+
|          cities_csv| default|       null| EXTERNAL|      false|
|       cleaned_taxes| default|       null|  MANAGED|      false|
|commdettrumpclint...| default|       null|  MANAGED|      false|
|   donaldtrumptweets| default|       null| EXTERNAL|      false|
|             linkage| default|       null| EXTERNAL|      false|
|             nations| default|       null| EXTERNAL|      false|
|           newmplist| default|       null| EXTERNAL|      false|
|       ny_baby_names| default|       null|  MANAGED|      false|
|       nzmpsandparty| default|       null| EXTERNAL|      false|
|    pos_neg_category| default|       null| EXTERNAL|      false|
|                 rna| default|       null|  MANAGED|      false|
|                samh| default|       null| EXTERNAL|      false|
|  social_media_usage| default|       null| EXTERNAL|      false|
|              table1| default|       null| EXTERNAL|      false|
|          test_table| default|       null| EXTERNAL|      false|
|             uscites| default|       null| EXTERNAL|      false|
+--------------------+--------+-----------+---------+-----------+
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">drop table if exists simple_range
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val df = spark.range(0, 100)
df.write.saveAsTable(&quot;simple_range&quot;)

// Verify that table is saved and it is marked as persistent (&quot;isTemporary&quot; value should be &quot;false&quot;)
spark.catalog.listTables.show()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+--------------------+--------+-----------+---------+-----------+
|                name|database|description|tableType|isTemporary|
+--------------------+--------+-----------+---------+-----------+
|          cities_csv| default|       null| EXTERNAL|      false|
|       cleaned_taxes| default|       null|  MANAGED|      false|
|commdettrumpclint...| default|       null|  MANAGED|      false|
|   donaldtrumptweets| default|       null| EXTERNAL|      false|
|             linkage| default|       null| EXTERNAL|      false|
|             nations| default|       null| EXTERNAL|      false|
|           newmplist| default|       null| EXTERNAL|      false|
|       ny_baby_names| default|       null|  MANAGED|      false|
|       nzmpsandparty| default|       null| EXTERNAL|      false|
|    pos_neg_category| default|       null| EXTERNAL|      false|
|                 rna| default|       null|  MANAGED|      false|
|                samh| default|       null| EXTERNAL|      false|
|        simple_range| default|       null|  MANAGED|      false|
|  social_media_usage| default|       null| EXTERNAL|      false|
|              table1| default|       null| EXTERNAL|      false|
|          test_table| default|       null| EXTERNAL|      false|
|             uscites| default|       null| EXTERNAL|      false|
+--------------------+--------+-----------+---------+-----------+

df: org.apache.spark.sql.Dataset[Long] = [id: bigint]
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="parquet-files"><a class="header" href="#parquet-files">Parquet Files</a></h2>
<p><a href="http://parquet.io">Parquet</a> is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. When writing Parquet files, all columns are automatically converted to be nullable for compatibility reasons.</p>
</div>
<div class="cell markdown">
<h3 id="more-on-parquet"><a class="header" href="#more-on-parquet">More on Parquet</a></h3>
<p><a href="https://parquet.apache.org/">Apache Parquet</a> is a <a href="http://en.wikipedia.org/wiki/Column-oriented_DBMS">columnar storage</a> format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language. It is a more efficient way to store data frames.</p>
<ul>
<li>To understand the ideas read <a href="http://research.google.com/pubs/pub36632.html">Dremel: Interactive Analysis of Web-Scale Datasets, Sergey Melnik, Andrey Gubarev, Jing Jing Long, Geoffrey Romer, Shiva Shivakumar, Matt Tolton and Theo Vassilakis,Proc. of the 36th Int'l Conf on Very Large Data Bases (2010), pp. 330-339</a>, whose Abstract is as follows:
<ul>
<li>Dremel is a scalable, interactive ad-hoc query system for analysis of read-only nested data. By combining multi-level execution trees and columnar data layouts it is <strong>capable of running aggregation queries over trillion-row tables in seconds</strong>. The system <strong>scales to thousands of CPUs and petabytes of data, and has thousands of users at Google</strong>. In this paper, we describe the architecture and implementation of Dremel, and explain how it complements MapReduce-based computing. We present a novel columnar storage representation for nested records and discuss experiments on few-thousand node instances of the system.</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://parquet.apache.org/documentation/latest/"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/ml-features.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h3 id="loading-data-programmatically"><a class="header" href="#loading-data-programmatically">Loading Data Programmatically</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Read in the parquet file created above. Parquet files are self-describing so the schema is preserved.
// The result of loading a Parquet file is also a DataFrame.
val parquetFile = sqlContext.read.parquet(&quot;/tmp/platforms.parquet&quot;)

// Parquet files can also be registered as tables and then used in SQL statements.
parquetFile.createOrReplaceTempView(&quot;parquetFile&quot;)
val platforms = sqlContext.sql(&quot;SELECT platform FROM parquetFile WHERE visits &gt; 0&quot;)
platforms.distinct.map(t =&gt; &quot;Name: &quot; + t(0)).collect().foreach(println)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Name: Flickr
Name: iPhone
Name: YouTube
Name: WordPress
Name: SMS
Name: iPhone App
Name: Youtube
Name: Instagram
Name: iPhone app
Name: Linked-In
Name: Twitter
Name: TOTAL
Name: Tumblr
Name: Newsletter
Name: Pinterest
Name: Android
Name: Foursquare
Name: Google+
Name: Foursquare (Badge Unlock)
Name: Facebook
parquetFile: org.apache.spark.sql.DataFrame = [platform: string, visits: int]
platforms: org.apache.spark.sql.DataFrame = [platform: string]
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="partition-discovery"><a class="header" href="#partition-discovery">Partition Discovery</a></h3>
<p>Table partitioning is a common optimization approach used in systems like Hive. In a partitioned table, data are usually stored in different directories, with partitioning column values encoded in the path of each partition directory. The Parquet data source is now able to discover and infer partitioning information automatically. For example, we can store all our previously used population data (from the programming guide example!) into a partitioned table using the following directory structure, with two extra columns, <code>gender</code> and <code>country</code> as partitioning columns:</p>
<pre><code>    path
    └── to
        └── table
            ├── gender=male
            │   ├── ...
            │   │
            │   ├── country=US
            │   │   └── data.parquet
            │   ├── country=CN
            │   │   └── data.parquet
            │   └── ...
            └── gender=female
                ├── ...
                │
                ├── country=US
                │   └── data.parquet
                ├── country=CN
                │   └── data.parquet
                └── ...
</code></pre>
<p>By passing <code>path/to/table</code> to either <code>SparkSession.read.parquet</code> or <code>SparkSession.read.load</code>, Spark SQL will automatically extract the partitioning information from the paths. Now the schema of the returned DataFrame becomes:</p>
<pre><code>    root
    |-- name: string (nullable = true)
    |-- age: long (nullable = true)
    |-- gender: string (nullable = true)
    |-- country: string (nullable = true)
</code></pre>
<p>Notice that the data types of the partitioning columns are automatically inferred. Currently, numeric data types and string type are supported. Sometimes users may not want to automatically infer the data types of the partitioning columns. For these use cases, the automatic type inference can be configured by <code>spark.sql.sources.partitionColumnTypeInference.enabled</code>, which is default to <code>true</code>. When type inference is disabled, string type will be used for the partitioning columns.</p>
<p>Starting from Spark 1.6.0, partition discovery only finds partitions under the given paths by default. For the above example, if users pass <code>path/to/table/gender=male</code> to either <code>SparkSession.read.parquet</code> or <code>SparkSession.read.load</code>, <code>gender</code> will not be considered as a partitioning column. If users need to specify the base path that partition discovery should start with, they can set <code>basePath</code> in the data source options. For example, when <code>path/to/table/gender=male</code> is the path of the data and users set <code>basePath</code> to <code>path/to/table/</code>, <code>gender</code> will be a partitioning column.</p>
</div>
<div class="cell markdown">
<h3 id="schema-merging"><a class="header" href="#schema-merging">Schema Merging</a></h3>
<p>Like ProtocolBuffer, Avro, and Thrift, Parquet also supports schema evolution. Users can start with a simple schema, and gradually add more columns to the schema as needed. In this way, users may end up with multiple Parquet files with different but mutually compatible schemas. The Parquet data source is now able to automatically detect this case and merge schemas of all these files.</p>
<p>Since schema merging is a relatively expensive operation, and is not a necessity in most cases, we turned it off by default starting from 1.5.0. You may enable it by: 1. setting data source option <code>mergeSchema</code> to <code>true</code> when reading Parquet files (as shown in the examples below), or 2. setting the global SQL option <code>spark.sql.parquet.mergeSchema</code> to <code>true</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create a simple DataFrame, stored into a partition directory
val df1 = sc.parallelize(1 to 5).map(i =&gt; (i, i * 2)).toDF(&quot;single&quot;, &quot;double&quot;)
df1.write.mode(&quot;overwrite&quot;).parquet(&quot;/tmp/data/test_table/key=1&quot;)

// Create another DataFrame in a new partition directory, adding a new column and dropping an existing column
val df2 = sc.parallelize(6 to 10).map(i =&gt; (i, i * 3)).toDF(&quot;single&quot;, &quot;triple&quot;)
df2.write.mode(&quot;overwrite&quot;).parquet(&quot;/tmp/data/test_table/key=2&quot;)

// Read the partitioned table
val df3 = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;/tmp/data/test_table&quot;)
df3.printSchema()

// The final schema consists of all 3 columns in the Parquet files together
// with the partitioning column appeared in the partition directory paths.
// root
//  |-- single: integer (nullable = true)
//  |-- double: integer (nullable = true)
//  |-- triple: integer (nullable = true)
//  |-- key: integer (nullable = true))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>root
 |-- single: integer (nullable = true)
 |-- double: integer (nullable = true)
 |-- triple: integer (nullable = true)
 |-- key: integer (nullable = true)

df1: org.apache.spark.sql.DataFrame = [single: int, double: int]
df2: org.apache.spark.sql.DataFrame = [single: int, triple: int]
df3: org.apache.spark.sql.DataFrame = [single: int, double: int ... 2 more fields]
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="hive-metastore-parquet-table-conversion"><a class="header" href="#hive-metastore-parquet-table-conversion">Hive metastore Parquet table conversion</a></h3>
<p>When reading from and writing to Hive metastore Parquet tables, Spark SQL will try to use its own Parquet support instead of Hive SerDe for better performance. This behavior is controlled by the <code>spark.sql.hive.convertMetastoreParquet</code> configuration, and is turned on by default.</p>
<h4 id="hiveparquet-schema-reconciliation"><a class="header" href="#hiveparquet-schema-reconciliation">Hive/Parquet Schema Reconciliation</a></h4>
<p>There are two key differences between Hive and Parquet from the perspective of table schema processing. 1. Hive is case insensitive, while Parquet is not 2. Hive considers all columns nullable, while nullability in Parquet is significant</p>
<p>Due to this reason, we must reconcile Hive metastore schema with Parquet schema when converting a Hive metastore Parquet table to a Spark SQL Parquet table. The reconciliation rules are: 1. Fields that have the same name in both schema must have the same data type regardless of nullability. The reconciled field should have the data type of the Parquet side, so that nullability is respected. 2. The reconciled schema contains exactly those fields defined in Hive metastore schema. - Any fields that only appear in the Parquet schema are dropped in the reconciled schema. - Any fileds that only appear in the Hive metastore schema are added as nullable field in the reconciled schema.</p>
<h4 id="metadata-refreshing"><a class="header" href="#metadata-refreshing">Metadata Refreshing</a></h4>
<p>Spark SQL caches Parquet metadata for better performance. When Hive metastore Parquet table conversion is enabled, metadata of those converted tables are also cached. If these tables are updated by Hive or other external tools, you need to refresh them manually to ensure consistent metadata.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// should refresh table metadata
spark.catalog.refreshTable(&quot;simple_range&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">-- Or you can use SQL to refresh table
REFRESH TABLE simple_range;
</code></pre>
</div>
<div class="cell markdown">
<h3 id="configuration"><a class="header" href="#configuration">Configuration</a></h3>
<p>Configuration of Parquet can be done using the <code>setConf</code> method on <code>SQLContext</code> or by running <code>SET key=value</code> commands using SQL.</p>
<table><thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th><th></th></tr></thead><tbody>
<tr><td><code>spark.sql.parquet.binaryAsString</code></td><td>false</td><td>Some other Parquet-producing systems, in particular Impala, Hive, and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.</td><td></td></tr>
<tr><td><code>spark.sql.parquet.int96AsTimestamp</code></td><td>true</td><td>Some Parquet-producing systems, in particular Impala and Hive, store Timestamp into INT96. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.</td><td></td></tr>
<tr><td><code>spark.sql.parquet.cacheMetadata</code></td><td>true</td><td>Turns on caching of Parquet schema metadata. Can speed up querying of static data.</td><td></td></tr>
<tr><td><code>spark.sql.parquet.compression.codec</code></td><td>gzip</td><td>Sets the compression codec use when writing Parquet files. Acceptable values include: uncompressed, snappy, gzip, lzo.</td><td></td></tr>
<tr><td><code>spark.sql.parquet.filterPushdown</code></td><td>true</td><td>Enables Parquet filter push-down optimization when set to true.</td><td></td></tr>
<tr><td><code>spark.sql.hive.convertMetastoreParquet</code></td><td>true</td><td>When set to false, Spark SQL will use the Hive SerDe for parquet tables instead of the built in support.</td><td></td></tr>
<tr><td><code>spark.sql.parquet.output.committer.class</code></td><td><code>org.apache.parquet.hadoop.ParquetOutputCommitter</code></td><td>The output committer class used by Parquet. The specified class needs to be a subclass of <code>org.apache.hadoop.mapreduce.OutputCommitter</code>. Typically, it's also a subclass of <code>org.apache.parquet.hadoop.ParquetOutputCommitter</code>. Spark SQL comes with a builtin <code>org.apache.spark.sql.parquet.DirectParquetOutputCommitter</code>, which can be more efficient then the default Parquet output committer when writing data to S3.</td><td></td></tr>
<tr><td><code>spark.sql.parquet.mergeSchema</code></td><td><code>false</code></td><td>When true, the Parquet data source merges schemas collected from all data files, otherwise the schema is picked from the summary file or a random data file if no summary file is available.</td><td></td></tr>
</tbody></table>
</div>
<div class="cell markdown">
<h2 id="json-datasets"><a class="header" href="#json-datasets">JSON Datasets</a></h2>
<p>Spark SQL can automatically infer the schema of a JSON dataset and load it as a DataFrame. This conversion can be done using <code>SparkSession.read.json()</code> on either an RDD of String, or a JSON file.</p>
<p>Note that the file that is offered as <em>a json file</em> is not a typical JSON file. Each line must contain a separate, self-contained valid JSON object. As a consequence, a regular multi-line JSON file will most often fail.</p>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-scala">// A JSON dataset is pointed to by path.
// The path can be either a single text file or a directory storing text files.
val path = &quot;/tmp/platforms.json&quot;
val platforms = spark.read.json(path)

// The inferred schema can be visualized using the printSchema() method.
platforms.printSchema()
// root
//  |-- platform: string (nullable = true)
//  |-- visits: long (nullable = true)

// Register this DataFrame as a table.
platforms.createOrReplaceTempView(&quot;platforms&quot;)

// SQL statements can be run by using the sql methods provided by sqlContext.
val facebook = spark.sql(&quot;SELECT platform, visits FROM platforms WHERE platform like 'Face%k'&quot;)
facebook.show()

// Alternatively, a DataFrame can be created for a JSON dataset represented by
// an RDD[String] storing one JSON object per string.
val rdd = sc.parallelize(&quot;&quot;&quot;{&quot;name&quot;:&quot;IWyn&quot;,&quot;address&quot;:{&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;}}&quot;&quot;&quot; :: Nil)
val anotherPlatforms = spark.read.json(rdd)
anotherPlatforms.show()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>root
 |-- platform: string (nullable = true)
 |-- visits: long (nullable = true)

+--------+------+
|platform|visits|
+--------+------+
|Facebook|     3|
|Facebook|    36|
|Facebook|    47|
|Facebook|    90|
|Facebook|   105|
|Facebook|   123|
|Facebook|   119|
|Facebook|   148|
|Facebook|   197|
|Facebook|   179|
|Facebook|   200|
|Facebook|   200|
|Facebook|   183|
|Facebook|   190|
|Facebook|   227|
|Facebook|   194|
|Facebook|   243|
|Facebook|   237|
|Facebook|   234|
|Facebook|   238|
+--------+------+
only showing top 20 rows

+---------------+----+
|        address|name|
+---------------+----+
|[Columbus,Ohio]|IWyn|
+---------------+----+

&lt;console&gt;:63: warning: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
val anotherPlatforms = spark.read.json(rdd)
                                  ^
path: String = /tmp/platforms.json
platforms: org.apache.spark.sql.DataFrame = [platform: string, visits: bigint]
facebook: org.apache.spark.sql.DataFrame = [platform: string, visits: bigint]
rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[23304] at parallelize at &lt;console&gt;:62
anotherPlatforms: org.apache.spark.sql.DataFrame = [address: struct&lt;city: string, state: string&gt;, name: string]
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="hive-tables"><a class="header" href="#hive-tables">Hive Tables</a></h2>
<p>Spark SQL also supports reading and writing data stored in <a href="http://hive.apache.org/">Apache Hive</a>. However, since Hive has a large number of dependencies, it is not included in the default Spark assembly. Hive support is enabled by adding the <code>-Phive</code> and <code>-Phive-thriftserver</code> flags to Spark’s build. This command builds a new assembly jar that includes Hive. Note that this Hive assembly jar must also be present on all of the worker nodes, as they will need access to the Hive serialization and deserialization libraries (SerDes) in order to access data stored in Hive.</p>
<p>Configuration of Hive is done by placing your <code>hive-site.xml</code>, <code>core-site.xml</code> (for security configuration), <code>hdfs-site.xml</code> (for HDFS configuration) file in <code>conf/</code>. Please note when running the query on a YARN cluster (<code>cluster</code> mode), the <code>datanucleus</code> jars under the <code>lib_managed/jars</code> directory and <code>hive-site.xml</code> under <code>conf/</code> directory need to be available on the driver and all executors launched by the YARN cluster. The convenient way to do this is adding them through the <code>--jars</code> option and <code>--file</code> option of the <code>spark-submit</code> command.</p>
<p>When working with Hive one must construct a <code>HiveContext</code>, which inherits from <code>SQLContext</code>, and adds support for finding tables in the MetaStore and writing queries using HiveQL. Users who do not have an existing Hive deployment can still create a <code>HiveContext</code>. When not configured by the hive-site.xml, the context automatically creates <code>metastore_db</code> in the current directory and creates <code>warehouse</code> directory indicated by HiveConf, which defaults to <code>/user/hive/warehouse</code>. Note that you may need to grant write privilege on <code>/user/hive/warehouse</code> to the user who starts the spark application.</p>
<pre><code class="language-scala">val spark = SparkSession.builder.enableHiveSupport().getOrCreate()

spark.sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;)
spark.sql(&quot;LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src&quot;)

// Queries are expressed in HiveQL
spark.sql(&quot;FROM src SELECT key, value&quot;).collect().foreach(println)
</code></pre>
<h3 id="interacting-with-different-versions-of-hive-metastore"><a class="header" href="#interacting-with-different-versions-of-hive-metastore">Interacting with Different Versions of Hive Metastore</a></h3>
<p>One of the most important pieces of Spark SQL’s Hive support is interaction with Hive metastore, which enables Spark SQL to access metadata of Hive tables. Starting from Spark 1.4.0, a single binary build of Spark SQL can be used to query different versions of Hive metastores, using the configuration described below. Note that independent of the version of Hive that is being used to talk to the metastore, internally Spark SQL will compile against Hive 1.2.1 and use those classes for internal execution (serdes, UDFs, UDAFs, etc).</p>
<p>The following options can be used to configure the version of Hive that is used to retrieve metadata:</p>
<table><thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th></tr></thead><tbody>
<tr><td><code>spark.sql.hive.metastore.version</code></td><td><code>1.2.1</code></td><td>Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>1.2.1</code>.</td></tr>
<tr><td><code>spark.sql.hive.metastore.jars</code></td><td><code>builtin</code></td><td>Location of the jars that should be used to instantiate the HiveMetastoreClient. This property can be one of three options: <code>builtin</code>, <code>maven</code>, a classpath in the standard format for the JVM. This classpath must include all of Hive and its dependencies, including the correct version of Hadoop. These jars only need to be present on the driver, but if you are running in yarn cluster mode then you must ensure they are packaged with you application.</td></tr>
<tr><td><code>spark.sql.hive.metastore.sharedPrefixes</code></td><td><code>com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc</code></td><td>A comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log4j.</td></tr>
<tr><td><code>spark.sql.hive.metastore.barrierPrefixes</code></td><td><code>(empty)</code></td><td>A comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e. <code>org.apache.spark.*</code>).</td></tr>
</tbody></table>
</div>
<div class="cell markdown">
<h2 id="jdbc-to-other-databases"><a class="header" href="#jdbc-to-other-databases">JDBC To Other Databases</a></h2>
<p>Spark SQL also includes a data source that can read data from other databases using JDBC. This functionality should be preferred over using <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.JdbcRDD">JdbcRDD</a>. This is because the results are returned as a DataFrame and they can easily be processed in Spark SQL or joined with other data sources. The JDBC data source is also easier to use from Java or Python as it does not require the user to provide a ClassTag. (Note that this is different than the Spark SQL JDBC server, which allows other applications to run queries using Spark SQL).</p>
<p>To get started you will need to include the JDBC driver for you particular database on the spark classpath. For example, to connect to postgres from the Spark Shell you would run the following command:</p>
<pre><code class="language-scala">SPARK_CLASSPATH=postgresql-9.3-1102-jdbc41.jar bin/spark-shell
</code></pre>
<p>Tables from the remote database can be loaded as a DataFrame or Spark SQL Temporary table using the Data Sources API. The following options are supported:</p>
<table><thead><tr><th>Property Name</th><th>Meaning</th><th></th></tr></thead><tbody>
<tr><td><code>url</code></td><td>The JDBC URL to connect to.</td><td></td></tr>
<tr><td><code>dbtable</code></td><td>The JDBC table that should be read. Note that anything that is valid in a <code>FROM</code> clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses.</td><td></td></tr>
<tr><td><code>driver</code></td><td>The class name of the JDBC driver needed to connect to this URL. This class will be loaded on the master and workers before running an JDBC commands to allow the driver to register itself with the JDBC subsystem.</td><td></td></tr>
<tr><td><code>partitionColumn, lowerBound, upperBound, numPartitions</code></td><td>These options must all be specified if any of them is specified. They describe how to partition the table when reading in parallel from multiple workers. <code>partitionColumn</code> must be a numeric column from the table in question. Notice that <code>lowerBound</code> and <code>upperBound</code> are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned.</td><td></td></tr>
<tr><td><code>fetchSize</code></td><td>The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows).</td><td></td></tr>
</tbody></table>
<pre><code class="language-scala">// Example of using JDBC datasource
val jdbcDF = spark.read.format(&quot;jdbc&quot;).options(Map(&quot;url&quot; -&gt; &quot;jdbc:postgresql:dbserver&quot;, &quot;dbtable&quot; -&gt; &quot;schema.tablename&quot;)).load()
</code></pre>
<pre><code class="language-sql">-- Or using JDBC datasource in SQL
CREATE TEMPORARY TABLE jdbcTable
USING org.apache.spark.sql.jdbc
OPTIONS (
  url &quot;jdbc:postgresql:dbserver&quot;,
  dbtable &quot;schema.tablename&quot;
)
</code></pre>
</div>
<div class="cell markdown">
<h3 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h3>
<ul>
<li>The JDBC driver class must be visible to the primordial class loader on the client session and on all executors. This is because Java’s DriverManager class does a security check that results in it ignoring all drivers not visible to the primordial class loader when one goes to open a connection. One convenient way to do this is to modify compute_classpath.sh on all worker nodes to include your driver JARs.</li>
<li>Some databases, such as H2, convert all names to upper case. You’ll need to use upper case to refer to those names in Spark SQL.</li>
</ul>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-15"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-15"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Apache Spark 2.2 sql-progamming-guide</a>.</p>
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide004_performancetuning_sqlprogguideperformance-tuninga"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide004_performancetuning_sqlprogguideperformance-tuninga"><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/004_performanceTuning_sqlProgGuide">Performance Tuning</a></a></h1>
<h2 id="a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide000_sqlprogguidespark-sql-programming-guidea-2"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide000_sqlprogguidespark-sql-programming-guidea-2"><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/000_sqlProgGuide">Spark Sql Programming Guide</a></a></h2>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/001_overview_sqlProgGuide">Overview</a>
<ul>
<li>SQL</li>
<li>DataFrames</li>
<li>Datasets</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/002_gettingStarted_sqlProgGuide">Getting Started</a>
<ul>
<li>Starting Point: SQLContext</li>
<li>Creating DataFrames</li>
<li>DataFrame Operations</li>
<li>Running SQL Queries Programmatically</li>
<li>Creating Datasets</li>
<li>Interoperating with RDDs
<ul>
<li>Inferring the Schema Using Reflection</li>
<li>Programmatically Specifying the Schema</li>
</ul>
</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/003_dataSources_sqlProgGuide">Data Sources</a>
<ul>
<li>Generic Load/Save Functions
<ul>
<li>Manually Specifying Options</li>
<li>Run SQL on files directly</li>
<li>Save Modes</li>
<li>Saving to Persistent Tables</li>
</ul>
</li>
<li>Parquet Files
<ul>
<li>Loading Data Programmatically</li>
<li>Partition Discovery</li>
<li>Schema Merging</li>
<li>Hive metastore Parquet table conversion
<ul>
<li>Hive/Parquet Schema Reconciliation</li>
<li>Metadata Refreshing</li>
</ul>
</li>
<li>Configuration</li>
</ul>
</li>
<li>JSON Datasets</li>
<li>Hive Tables
<ul>
<li>Interacting with Different Versions of Hive Metastore</li>
</ul>
</li>
<li>JDBC To Other Databases</li>
<li>Troubleshooting</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/004_performanceTuning_sqlProgGuide">Performance Tuning</a>
<ul>
<li>Caching Data In Memory</li>
<li>Other Configuration Options</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/005_distributedSqlEngine_sqlProgGuide">Distributed SQL Engine</a>
<ul>
<li>Running the Thrift JDBC/ODBC server</li>
<li>Running the Spark SQL CLI</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide004_performancetuning_sqlprogguideperformance-tuninga-1"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide004_performancetuning_sqlprogguideperformance-tuninga-1"><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/004_performanceTuning_sqlProgGuide">Performance Tuning</a></a></h1>
<p>For some workloads it is possible to improve performance by either caching data in memory, or by turning on some experimental options.</p>
<h2 id="caching-data-in-memory"><a class="header" href="#caching-data-in-memory">Caching Data In Memory</a></h2>
<p>Spark SQL can cache tables using an in-memory columnar format by calling <code>spark.cacheTable(&quot;tableName&quot;)</code> or <code>dataset.cache()</code>. Then Spark SQL will scan only required columns and will automatically tune compression to minimize memory usage and GC pressure. You can call <code>spark.uncacheTable(&quot;tableName&quot;)</code> to remove the table from memory.</p>
<p>Configuration of in-memory caching can be done using the <code>setConf</code> method on <code>SparkSession</code> or by running <code>SET key=value</code> commands using SQL.</p>
<table><thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th></tr></thead><tbody>
<tr><td><code>spark.sql.inMemoryColumnarStorage.compressed</code></td><td>true</td><td>When set to true Spark SQL will automatically select a compression codec for each column based on statistics of the data.</td></tr>
<tr><td><code>spark.sql.inMemoryColumnarStorage.batchSize</code></td><td>10000</td><td>Controls the size of batches for columnar caching. Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data.</td></tr>
</tbody></table>
<h2 id="other-configuration-options"><a class="header" href="#other-configuration-options">Other Configuration Options</a></h2>
<p>The following options can also be used to tune the performance of query execution. It is possible that these options will be deprecated in future release as more optimizations are performed automatically.</p>
<table><thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th></tr></thead><tbody>
<tr><td><code>spark.sql.autoBroadcastJoinThreshold</code></td><td>10485760 (10 MB)</td><td>Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command <code>ANALYZE TABLE &lt;tableName&gt; COMPUTE STATISTICS noscan</code> has been run.</td></tr>
<tr><td><code>spark.sql.tungsten.enabled</code></td><td>true</td><td>When true, use the optimized Tungsten physical execution backend which explicitly manages memory and dynamically generates bytecode for expression evaluation.</td></tr>
<tr><td><code>spark.sql.shuffle.partitions</code></td><td>200</td><td>Configures the number of partitions to use when shuffling data for joins or aggregations.</td></tr>
</tbody></table>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-16"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-16"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Apache Spark 2.2 sql-progamming-guide</a>.</p>
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide005_distributedsqlengine_sqlprogguidedistributed-sql-enginea"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide005_distributedsqlengine_sqlprogguidedistributed-sql-enginea"><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/005_distributedSqlEngine_sqlProgGuide">Distributed SQL Engine</a></a></h1>
<h2 id="a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide000_sqlprogguidespark-sql-programming-guidea-3"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide000_sqlprogguidespark-sql-programming-guidea-3"><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/000_sqlProgGuide">Spark Sql Programming Guide</a></a></h2>
<ul>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/001_overview_sqlProgGuide">Overview</a>
<ul>
<li>SQL</li>
<li>DataFrames</li>
<li>Datasets</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/002_gettingStarted_sqlProgGuide">Getting Started</a>
<ul>
<li>Starting Point: SQLContext</li>
<li>Creating DataFrames</li>
<li>DataFrame Operations</li>
<li>Running SQL Queries Programmatically</li>
<li>Creating Datasets</li>
<li>Interoperating with RDDs
<ul>
<li>Inferring the Schema Using Reflection</li>
<li>Programmatically Specifying the Schema</li>
</ul>
</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/003_dataSources_sqlProgGuide">Data Sources</a>
<ul>
<li>Generic Load/Save Functions
<ul>
<li>Manually Specifying Options</li>
<li>Run SQL on files directly</li>
<li>Save Modes</li>
<li>Saving to Persistent Tables</li>
</ul>
</li>
<li>Parquet Files
<ul>
<li>Loading Data Programmatically</li>
<li>Partition Discovery</li>
<li>Schema Merging</li>
<li>Hive metastore Parquet table conversion
<ul>
<li>Hive/Parquet Schema Reconciliation</li>
<li>Metadata Refreshing</li>
</ul>
</li>
<li>Configuration</li>
</ul>
</li>
<li>JSON Datasets</li>
<li>Hive Tables
<ul>
<li>Interacting with Different Versions of Hive Metastore</li>
</ul>
</li>
<li>JDBC To Other Databases</li>
<li>Troubleshooting</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/004_performanceTuning_sqlProgGuide">Performance Tuning</a>
<ul>
<li>Caching Data In Memory</li>
<li>Other Configuration Options</li>
</ul>
</li>
<li><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/005_distributedSqlEngine_sqlProgGuide">Distributed SQL Engine</a>
<ul>
<li>Running the Thrift JDBC/ODBC server</li>
<li>Running the Spark SQL CLI</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h1 id="a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide005_distributedsqlengine_sqlprogguidedistributed-sql-enginea-1"><a class="header" href="#a-hrefcontentsxtraresourcesprogguides2_2sqlprogrammingguideworkspacescalable-data-sciencextraresourcesprogguides1_6sqlprogrammingguide005_distributedsqlengine_sqlprogguidedistributed-sql-enginea-1"><a href="contents/xtraResources/ProgGuides2_2/sqlProgrammingGuide//#workspace/scalable-data-science/xtraResources/ProgGuides1_6/sqlProgrammingGuide/005_distributedSqlEngine_sqlProgGuide">Distributed SQL Engine</a></a></h1>
<p>Spark SQL can also act as a distributed query engine using its JDBC/ODBC or command-line interface. In this mode, end-users or applications can interact with Spark SQL directly to run SQL queries, without the need to write any code.</p>
<h2 id="running-the-thrift-jdbcodbc-server"><a class="header" href="#running-the-thrift-jdbcodbc-server">Running the Thrift JDBC/ODBC server</a></h2>
<p>The Thrift JDBC/ODBC server implemented here corresponds to the <a href="https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2"><code>HiveServer2</code></a> in Hive 1.2.1 You can test the JDBC server with the beeline script that comes with either Spark or Hive 1.2.1.</p>
<p>To start the JDBC/ODBC server, run the following in the Spark directory:</p>
<pre><code>./sbin/start-thriftserver.sh
</code></pre>
<p>This script accepts all <code>bin/spark-submit</code> command line options, plus a <code>--hiveconf</code> option to specify Hive properties. You may run <code>./sbin/start-thriftserver.sh --help</code> for a complete list of all available options. By default, the server listens on localhost:10000. You may override this behaviour via either environment variables, i.e.:</p>
<pre><code>export HIVE_SERVER2_THRIFT_PORT=&lt;listening-port&gt;
export HIVE_SERVER2_THRIFT_BIND_HOST=&lt;listening-host&gt;
./sbin/start-thriftserver.sh \
  --master &lt;master-uri&gt; \
  ...
</code></pre>
<p>or system properties:</p>
<pre><code>./sbin/start-thriftserver.sh \
  --hiveconf hive.server2.thrift.port=&lt;listening-port&gt; \
  --hiveconf hive.server2.thrift.bind.host=&lt;listening-host&gt; \
  --master &lt;master-uri&gt;
  ...
</code></pre>
<p>Now you can use beeline to test the Thrift JDBC/ODBC server:</p>
<pre><code>./bin/beeline
</code></pre>
<p>Connect to the JDBC/ODBC server in beeline with:</p>
<pre><code>beeline&gt; !connect jdbc:hive2://localhost:10000
</code></pre>
<p>Beeline will ask you for a username and password. In non-secure mode, simply enter the username on your machine and a blank password. For secure mode, please follow the instructions given in the <a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients">beeline documentation</a>.</p>
<p>Configuration of Hive is done by placing your <code>hive-site.xml</code>, <code>core-site.xml</code> and <code>hdfs-site.xml</code> files in <code>conf/</code>.</p>
<p>You may also use the beeline script that comes with Hive.</p>
<p>Thrift JDBC server also supports sending thrift RPC messages over HTTP transport. Use the following setting to enable HTTP mode as system property or in <code>hive-site.xml</code> file in <code>conf/</code>:</p>
<pre><code>hive.server2.transport.mode - Set this to value: http
hive.server2.thrift.http.port - HTTP port number fo listen on; default is 10001
hive.server2.http.endpoint - HTTP endpoint; default is cliservice
</code></pre>
<p>To test, use beeline to connect to the JDBC/ODBC server in http mode with:</p>
<pre><code>beeline&gt; !connect jdbc:hive2://&lt;host&gt;:&lt;port&gt;/&lt;database&gt;?hive.server2.transport.mode=http;hive.server2.thrift.http.path=&lt;http_endpoint&gt;
</code></pre>
<h2 id="running-the-spark-sql-cli"><a class="header" href="#running-the-spark-sql-cli">Running the Spark SQL CLI</a></h2>
<p>The Spark SQL CLI is a convenient tool to run the Hive metastore service in local mode and execute queries input from the command line. Note that the Spark SQL CLI cannot talk to the Thrift JDBC server.</p>
<p>To start the Spark SQL CLI, run the following in the Spark directory:</p>
<pre><code>./bin/spark-sql
</code></pre>
<p>Configuration of Hive is done by placing your <code>hive-site.xml</code>, <code>core-site.xml</code> and <code>hdfs-site.xml</code> files in <code>conf/</code>. You may run <code>./bin/spark-sql --help</code> for a complete list of all available options.</p>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-17"><a class="header" href="#a-hrefhttpslamastexgithubioscalable-data-sciencesds22sds-22-scalable-data-sciencea-17"><a href="https://lamastex.github.io/scalable-data-science/sds/2/2/">SDS-2.2, Scalable Data Science</a></a></h1>
</div>
<div class="cell markdown">
<h1 id="extract-transform-and-load-etl-of-the-sou-addresses"><a class="header" href="#extract-transform-and-load-etl-of-the-sou-addresses">Extract, Transform and Load (ETL) of the SoU Addresses</a></h1>
<h3 id="a-bit-of-bash-and-lynx-to-achieve-the-scraping-of-the-state-of-the-union-addresses-of-the-us-presidents"><a class="header" href="#a-bit-of-bash-and-lynx-to-achieve-the-scraping-of-the-state-of-the-union-addresses-of-the-us-presidents">A bit of bash and lynx to achieve the scraping of the state of the union addresses of the US Presidents</a></h3>
<h4 id="by-paul-brouwers"><a class="header" href="#by-paul-brouwers">by Paul Brouwers</a></h4>
<h3 id="and-some-shell-level-parsed-data-exploration-injection-into-the-distributed-file-system-and-testing"><a class="header" href="#and-some-shell-level-parsed-data-exploration-injection-into-the-distributed-file-system-and-testing">And some Shell-level parsed-data exploration, injection into the distributed file system and testing</a></h3>
<h4 id="by-raazesh-sainudiin"><a class="header" href="#by-raazesh-sainudiin">by Raazesh Sainudiin</a></h4>
<p>This SoU dataset is used in the <code>006_WordCount</code> notebook.</p>
<p>The code below is mainly there to show how the text content of each state of the union address was scraped from the following URL: * <a href="http://stateoftheunion.onetwothree.net/texts/index.html">http://stateoftheunion.onetwothree.net/texts/index.html</a></p>
<p>Such data acquisition task or ETL is usually the first and crucial step in a data scientist's workflow. A data scientist generally does the scraping and parsing of the data by her/himself.
Data ingestion not only allows the scientist to start the analysis but also determines the quality of the analysis by the limits it imposes on the accessible feature space.</p>
<p>We have done this and put the data in the distributed file system for easy loading into our notebooks for further analysis. This keeps us from having to install unix programs like <code>lynx</code>, <code>sed</code>, etc. that are needed in the shell script below.</p>
<pre><code class="language-%sh">for i in $(lynx --dump http://stateoftheunion.onetwothree.net/texts/index.html | grep texts | grep -v index | sed 's/.*http/http/') ; do lynx --dump $i | tail -n+13 | head -n-14 | sed 's/^\s\+//' | sed -e ':a;N;$!ba;s/\(.\)\n/\1 /g' -e 's/\n/\n\n/' &gt; $(echo $i | sed 's/.*\([0-9]\{8\}\).*/\1/').txt ; done
</code></pre>
<p>Or in a more atomic form:</p>
<pre><code class="language-%sh">for i in $(lynx --dump http://stateoftheunion.onetwothree.net/texts/index.html \

        | grep texts \

        | grep -v index \

        | sed 's/.*http/http/')

do 

        lynx --dump $i \

               | tail -n+13 \

               | head -n-14 \

               | sed 's/^\s\+//' \

               | sed -e ':a;N;$!ba;s/\(.\)\n/\1 /g' -e 's/\n/\n\n/' \

               &gt; $(echo $i | sed 's/.*\([0-9]\{8\}\).*/\1/').txt

done
</code></pre>
<p><strong>Don't re-evaluate!</strong></p>
<p>The following BASH (shell) script can be made to work on databricks cloud directly by installing the dependencies such as <code>lynx</code>, etc. Since we have already scraped it and put the data in our distributed file system <strong>let's not evaluate or <code>&lt;Ctrl+Enter&gt;</code> the cell below</strong>. The cell is mainly there to show how it can be done (you may want to modify it to scrape other sites for other text data).</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">#remove the hash character from the line below to evaluate when needed
#for i in $(lynx --dump http://stateoftheunion.onetwothree.net/texts/index.html | grep texts | grep -v index | sed 's/.*http/http/') ; do lynx --dump $i | tail -n+13 | head -n-14 | sed 's/^\s\+//' | sed -e ':a;N;$!ba;s/\(.\)\n/\1 /g' -e 's/\n/\n\n/' &gt; $(echo $i | sed 's/.*\([0-9]\{8\}\).*/\1/').txt ; done
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">pwd &amp;&amp; ls &amp;&amp; du -sh .
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>/databricks/driver
conf
derby.log
eventlogs
ganglia
logs
5.2M	.
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">ls /home/ubuntu &amp;&amp; du -sh /home/ubuntu
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>databricks
40K	/home/ubuntu
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>We can just grab the data as a tarball (gnuZipped tar archive) file <code>sou.tar.gz</code> using wget as follows:</p>
<p>%sh wget http://www.math.canterbury.ac.nz/~r.sainudiin/datasets/public/SOU/sou.tar.gz</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">df -h
pwd
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Filesystem                                                                                                                           Size  Used Avail Use% Mounted on
/var/lib/lxc/base-images/release__7.2.x-snapshot-scala2.12__databricks-universe__head__0652b44__47ac4a2__jenkins__0b22b70__format-2  148G   11G  130G   8% /
none                                                                                                                                 492K     0  492K   0% /dev
udev                                                                                                                                 3.9G     0  3.9G   0% /dev/tty
/dev/xvdb                                                                                                                            148G   11G  130G   8% /mnt/readonly
/dev/mapper/vg-lv                                                                                                                    296G   11G  271G   4% /local_disk0
tmpfs                                                                                                                                3.9G     0  3.9G   0% /dev/shm
tmpfs                                                                                                                                3.9G  232K  3.9G   1% /run
tmpfs                                                                                                                                5.0M     0  5.0M   0% /run/lock
tmpfs                                                                                                                                3.9G     0  3.9G   0% /sys/fs/cgroup
tmpfs                                                                                                                                315M     0  315M   0% /run/user/1000
/:                                                                                                                                   1.0P     0  1.0P   0% /dbfs
/databricks/driver
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">pwd
ls
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>/databricks/driver
conf
derby.log
eventlogs
ganglia
logs
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-sh">wget http://lamastex.org/datasets/public/SOU/README.md
cat README.md
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>--2020-09-16 18:49:05--  http://lamastex.org/datasets/public/SOU/README.md
Resolving lamastex.org (lamastex.org)... 166.62.28.100
Connecting to lamastex.org (lamastex.org)|166.62.28.100|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 8087 (7.9K)
Saving to: ‘README.md’

     0K .......                                               100%  553M=0s

2020-09-16 18:49:06 (553 MB/s) - ‘README.md’ saved [8087/8087]

## A bit of bash and lynx to achieve the scraping of the state of the union addresses of the US Presidents
### by Paul Brouwers

The code below is mainly there to show how the text content of each state of the union address was scraped from the following URL:
* [http://stateoftheunion.onetwothree.net/texts/index.html](http://stateoftheunion.onetwothree.net/texts/index.html)

Such data acquisition tasks is usually the first and cucial step in a data scientist's workflow.

We have done this and put the data in the distributed file system for easy loading into our notebooks for further analysis.  This keeps us from having to install unix programs like ``lynx``, ``sed``, etc. that are needed in the shell script below.

```%sh
for i in $(lynx --dump http://stateoftheunion.onetwothree.net/texts/index.html | grep texts | grep -v index | sed 's/.*http/http/') ; do lynx --dump $i | tail -n+13 | head -n-14 | sed 's/^\s\+//' | sed -e ':a;N;$!ba;s/\(.\)\n/\1 /g' -e 's/\n/\n\n/' &gt; $(echo $i | sed 's/.*\([0-9]\{8\}\).*/\1/').txt ; done
```

Or in a more atomic form:

```%sh
for i in $(lynx --dump http://stateoftheunion.onetwothree.net/texts/index.html \

        | grep texts \

        | grep -v index \

        | sed 's/.*http/http/')

do 

        lynx --dump $i \

               | tail -n+13 \

               | head -n-14 \

               | sed 's/^\s\+//' \

               | sed -e ':a;N;$!ba;s/\(.\)\n/\1 /g' -e 's/\n/\n\n/' \

               &gt; $(echo $i | sed 's/.*\([0-9]\{8\}\).*/\1/').txt

done
```
**Don't re-evaluate!**

The following BASH (shell) script can be made to work on databricks cloud directly by installing the dependencies such as ``lynx``, etc.  Since we have already scraped it and put the data in our distributed file system **let's not evaluate or ``&lt;Ctrl+Enter&gt;`` the cell below**.  The cell is mainly there to show how it can be done (you may want to modify it to scrape other sites for other text data).

```%sh 
#remove the hash character from the line below to evaluate when needed
#for i in $(lynx --dump http://stateoftheunion.onetwothree.net/texts/index.html | grep texts | grep -v index | sed 's/.*http/http/') ; do lynx --dump $i | tail -n+13 | head -n-14 | sed 's/^\s\+//' | sed -e ':a;N;$!ba;s/\(.\)\n/\1 /g' -e 's/\n/\n\n/' &gt; $(echo $i | sed 's/.*\([0-9]\{8\}\).*/\1/').txt ; done
```
We can just grab the data as a tarball (gnuZipped tar archive) file ``sou.tar.gz`` using wget as follows:

```%sh
wget http://www.math.canterbury.ac.nz/~r.sainudiin/datasets/public/SOU/sou.tar.gz
```

Raazesh Sainudiin

Fri Feb 19 11:08:00 NZDT 2016

# Atlantis Redo

Redone to include President Trump's SOU address in 2017.
```%sh
zesh@raazesh-Inspiron-15-7579:~$ sudo apt-get -y install lynx
[sudo] password for raazesh: 
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  lynx-common
The following NEW packages will be installed:
  lynx lynx-common
0 upgraded, 2 newly installed, 0 to remove and 0 not upgraded.
Need to get 1,035 kB of archives.
After this operation, 2,761 kB of additional disk space will be used.
Get:1 http://ubuntu.mirror.su.se/ubuntu xenial/universe amd64 lynx-common all 2.8.9dev8-4ubuntu1 [411 kB]
Get:2 http://ubuntu.mirror.su.se/ubuntu xenial/universe amd64 lynx amd64 2.8.9dev8-4ubuntu1 [624 kB]
Fetched 1,035 kB in 0s (9,643 kB/s)
Selecting previously unselected package lynx-common.
(Reading database ... 459514 files and directories currently installed.)
Preparing to unpack .../lynx-common_2.8.9dev8-4ubuntu1_all.deb ...
Unpacking lynx-common (2.8.9dev8-4ubuntu1) ...
Selecting previously unselected package lynx.
Preparing to unpack .../lynx_2.8.9dev8-4ubuntu1_amd64.deb ...
Unpacking lynx (2.8.9dev8-4ubuntu1) ...
Processing triggers for mime-support (3.59ubuntu1) ...
Processing triggers for doc-base (0.10.7) ...
Processing 1 added doc-base file...
Processing triggers for man-db (2.7.5-1) ...
Setting up lynx-common (2.8.9dev8-4ubuntu1) ...
Setting up lynx (2.8.9dev8-4ubuntu1) ...
update-alternatives: using /usr/bin/lynx to provide /usr/bin/www-browser (www-browser) in auto mode
raazesh@raazesh-Inspiron-15-7579:~$ mkdir sou
raazesh@raazesh-Inspiron-15-7579:~$ cd sou/
raazesh@raazesh-Inspiron-15-7579:~/sou$ for i in $(lynx --dump http://stateoftheunion.onetwothree.net/texts/index.html | grep texts | grep -v index | sed 's/.*http/http/') ; do lynx --dump $i | tail -n+13 | head -n-14 | sed 's/^\s\+//' | sed -e ':a;N;$!ba;s/\(.\)\n/\1 /g' -e 's/\n/\n\n/' &gt; $(echo $i | sed 's/.*\([0-9]\{8\}\).*/\1/').txt ; done
raazesh@raazesh-Inspiron-15-7579:~/sou$ ls
17900108.txt  18101205.txt  18311206.txt  18521206.txt  18731201.txt  18941202.txt  19151207.txt  19370106.txt  19570110.txt  19770112.txt  19980127.txt
17901208.txt  18111105.txt  18321204.txt  18531205.txt  18741207.txt  18951207.txt  19161205.txt  19380103.txt  19580109.txt  19780119.txt  19990119.txt
17911025.txt  18121104.txt  18331203.txt  18541204.txt  18751207.txt  18961204.txt  19171204.txt  19390104.txt  19590109.txt  19790125.txt  20000127.txt
17921106.txt  18131207.txt  18341201.txt  18551231.txt  18761205.txt  18971206.txt  19181202.txt  19400103.txt  19600107.txt  19800121.txt  20010227.txt
17931203.txt  18140920.txt  18351207.txt  18561202.txt  18771203.txt  18981205.txt  19191202.txt  19410106.txt  19610112.txt  19810116.txt  20010920.txt
17941119.txt  18151205.txt  18361205.txt  18571208.txt  18781202.txt  18991205.txt  19201207.txt  19420106.txt  19610130.txt  19820126.txt  20020129.txt
17951208.txt  18161203.txt  18371205.txt  18581206.txt  18791201.txt  19001203.txt  19211206.txt  19430107.txt  19620111.txt  19830125.txt  20030128.txt
17961207.txt  18171212.txt  18381203.txt  18591219.txt  18801206.txt  19011203.txt  19221208.txt  19440111.txt  19630114.txt  19840125.txt  20040120.txt
17971122.txt  18181116.txt  18391202.txt  18601203.txt  18811206.txt  19021202.txt  19231206.txt  19450106.txt  19640108.txt  19850206.txt  20050202.txt
17981208.txt  18191207.txt  18401205.txt  18611203.txt  18821204.txt  19031207.txt  19241203.txt  19460121.txt  19650104.txt  19860204.txt  20060131.txt
17991203.txt  18201114.txt  18411207.txt  18621201.txt  18831204.txt  19041206.txt  19251208.txt  19470106.txt  19660112.txt  19870127.txt  20070123.txt
18001111.txt  18211203.txt  18421206.txt  18631208.txt  18841201.txt  19051205.txt  19261207.txt  19480107.txt  19670110.txt  19880125.txt  20080128.txt
18011208.txt  18221203.txt  18431206.txt  18641206.txt  18851208.txt  19061203.txt  19271206.txt  19490105.txt  19680117.txt  19890209.txt  20090224.txt
18021215.txt  18231202.txt  18441203.txt  18651204.txt  18861206.txt  19071203.txt  19281204.txt  19500104.txt  19690114.txt  19900131.txt  20100127.txt
18031017.txt  18241207.txt  18451202.txt  18661203.txt  18871206.txt  19081208.txt  19291203.txt  19510108.txt  19700122.txt  19910129.txt  20110125.txt
18041108.txt  18251206.txt  18461208.txt  18671203.txt  18881203.txt  19091207.txt  19301202.txt  19520109.txt  19710122.txt  19920128.txt  20120124.txt
18051203.txt  18261205.txt  18471207.txt  18681209.txt  18891203.txt  19101206.txt  19311208.txt  19530107.txt  19720120.txt  19930217.txt  20130212.txt
18061202.txt  18271204.txt  18481205.txt  18691206.txt  18901201.txt  19111205.txt  19321206.txt  19530202.txt  19730202.txt  19940125.txt  20140128.txt
18071027.txt  18281202.txt  18491204.txt  18701205.txt  18911209.txt  19121203.txt  19340103.txt  19540107.txt  19740130.txt  19950124.txt  20150120.txt
18081108.txt  18291208.txt  18501202.txt  18711204.txt  18921206.txt  19131202.txt  19350104.txt  19550106.txt  19750115.txt  19960123.txt  20160112.txt
18091129.txt  18301206.txt  18511202.txt  18721202.txt  18931203.txt  19141208.txt  19360103.txt  19560105.txt  19760119.txt  19970204.txt  20170228.txt
raazesh@raazesh-Inspiron-15-7579:~/sou$ vim 20170228.txt 
raazesh@raazesh-Inspiron-15-7579:~/sou$ cd ..
raazesh@raazesh-Inspiron-15-7579:~$ tar zcvf sou.tgz sou
$ ls sou.tgz 
sou.tgz

``` 

Raazesh Sainudiin

Thu Aug 31 08:33:38 CEST 2017
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-sh"># wget http://lamastex.org/datasets/public/SOU/sou.tar.gz
wget http://lamastex.org/datasets/public/SOU/sou.tgz 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>--2020-09-17 14:09:47--  http://lamastex.org/datasets/public/SOU/sou.tgz
Resolving lamastex.org (lamastex.org)... 166.62.28.100
Connecting to lamastex.org (lamastex.org)|166.62.28.100|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 3666765 (3.5M) [application/x-tar]
Saving to: ‘sou.tgz’

     0K .......... .......... .......... .......... ..........  1%  132K 27s
    50K .......... .......... .......... .......... ..........  2%  266K 20s
   100K .......... .......... .......... .......... ..........  4%  269K 17s
   150K .......... .......... .......... .......... ..........  5% 2.10M 13s
   200K .......... .......... .......... .......... ..........  6%  303K 13s
   250K .......... .......... .......... .......... ..........  8% 16.9M 10s
   300K .......... .......... .......... .......... ..........  9% 2.34M 9s
   350K .......... .......... .......... .......... .......... 11%  302K 9s
   400K .......... .......... .......... .......... .......... 12% 2.22M 8s
   450K .......... .......... .......... .......... .......... 13% 44.6M 7s
   500K .......... .......... .......... .......... .......... 15%  299K 7s
   550K .......... .......... .......... .......... .......... 16%  197M 7s
   600K .......... .......... .......... .......... .......... 18% 2.48M 6s
   650K .......... .......... .......... .......... .......... 19% 50.2M 6s
   700K .......... .......... .......... .......... .......... 20% 1.35M 5s
   750K .......... .......... .......... .......... .......... 22%  379K 5s
   800K .......... .......... .......... .......... .......... 23% 47.7M 5s
   850K .......... .......... .......... .......... .......... 25% 2.62M 5s
   900K .......... .......... .......... .......... .......... 26% 34.0M 4s
   950K .......... .......... .......... .......... .......... 27% 76.1M 4s
  1000K .......... .......... .......... .......... .......... 29%  301K 4s
  1050K .......... .......... .......... .......... .......... 30% 13.0M 4s
  1100K .......... .......... .......... .......... .......... 32%  240M 4s
  1150K .......... .......... .......... .......... .......... 33% 3.05M 3s
  1200K .......... .......... .......... .......... .......... 34% 29.8M 3s
  1250K .......... .......... .......... .......... .......... 36% 44.9M 3s
  1300K .......... .......... .......... .......... .......... 37% 49.1M 3s
  1350K .......... .......... .......... .......... .......... 39%  302K 3s
  1400K .......... .......... .......... .......... .......... 40% 15.4M 3s
  1450K .......... .......... .......... .......... .......... 41% 46.0M 3s
  1500K .......... .......... .......... .......... .......... 43% 3.36M 2s
  1550K .......... .......... .......... .......... .......... 44% 35.9M 2s
  1600K .......... .......... .......... .......... .......... 46% 34.6M 2s
  1650K .......... .......... .......... .......... .......... 47% 40.9M 2s
  1700K .......... .......... .......... .......... .......... 48% 22.9M 2s
  1750K .......... .......... .......... .......... .......... 50% 1.55M 2s
  1800K .......... .......... .......... .......... .......... 51%  378K 2s
  1850K .......... .......... .......... .......... .......... 53% 17.9M 2s
  1900K .......... .......... .......... .......... .......... 54% 22.8M 2s
  1950K .......... .......... .......... .......... .......... 55% 40.8M 2s
  2000K .......... .......... .......... .......... .......... 57% 3.86M 2s
  2050K .......... .......... .......... .......... .......... 58% 43.2M 1s
  2100K .......... .......... .......... .......... .......... 60% 49.5M 1s
  2150K .......... .......... .......... .......... .......... 61% 32.4M 1s
  2200K .......... .......... .......... .......... .......... 62% 31.4M 1s
  2250K .......... .......... .......... .......... .......... 64% 36.0M 1s
  2300K .......... .......... .......... .......... .......... 65% 1.61M 1s
  2350K .......... .......... .......... .......... .......... 67% 15.5M 1s
  2400K .......... .......... .......... .......... .......... 68%  386K 1s
  2450K .......... .......... .......... .......... .......... 69% 15.2M 1s
  2500K .......... .......... .......... .......... .......... 71% 46.6M 1s
  2550K .......... .......... .......... .......... .......... 72% 47.0M 1s
  2600K .......... .......... .......... .......... .......... 74% 6.25M 1s
  2650K .......... .......... .......... .......... .......... 75% 7.25M 1s
  2700K .......... .......... .......... .......... .......... 76% 53.9M 1s
  2750K .......... .......... .......... .......... .......... 78% 15.9M 1s
  2800K .......... .......... .......... .......... .......... 79% 22.1M 1s
  2850K .......... .......... .......... .......... .......... 80%  144M 1s
  2900K .......... .......... .......... .......... .......... 82%  172M 0s
  2950K .......... .......... .......... .......... .......... 83%  115M 0s
  3000K .......... .......... .......... .......... .......... 85% 53.4M 0s
  3050K .......... .......... .......... .......... .......... 86% 1.72M 0s
  3100K .......... .......... .......... .......... .......... 87% 21.2M 0s
  3150K .......... .......... .......... .......... .......... 89%  377K 0s
  3200K .......... .......... .......... .......... .......... 90% 42.5M 0s
  3250K .......... .......... .......... .......... .......... 92% 6.89M 0s
  3300K .......... .......... .......... .......... .......... 93% 46.7M 0s
  3350K .......... .......... .......... .......... .......... 94% 8.75M 0s
  3400K .......... .......... .......... .......... .......... 96% 15.4M 0s
  3450K .......... .......... .......... .......... .......... 97%  197M 0s
  3500K .......... .......... .......... .......... .......... 99% 27.2M 0s
  3550K .......... .......... ..........                      100% 51.8M=2.5s

2020-09-17 14:09:50 (1.41 MB/s) - ‘sou.tgz’ saved [3666765/3666765]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">ls
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>conf
derby.log
eventlogs
ganglia
logs
sou.tgz
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-sh">env
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>MASTER=spark://10.149.224.176:7077
_=/usr/bin/env
LANG=en_US.UTF-8
SUDO_GID=0
OLDPWD=/databricks/chauffeur
PTY_LIB_FOLDER=/usr/lib/libpty
HIVE_HOME=/home/ubuntu/hive-0.9.0-bin
JAVA_OPTS= -Djava.io.tmpdir=/local_disk0/tmp -XX:MaxPermSize=512m -XX:-OmitStackTraceInFastThrow -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true  -Ddatabricks.serviceName=driver-1 -Xms1492m -Xmx1492m -Dspark.ui.port=47839 -Dspark.executor.extraJavaOptions=&quot;-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true  -Ddatabricks.serviceName=spark-executor-1&quot; -Dspark.executor.memory=2516m -Dspark.executor.extraClassPath=/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/api-base--api-base_java-spark_3.0_2.12_deploy.jar:/databricks/jars/api-base--api-base-spark_3.0_2.12_deploy.jar:/databricks/jars/api--rpc--rpc_parser-spark_3.0_2.12_deploy.jar:/databricks/jars/chauffeur-api--api--endpoints--endpoints-spark_3.0_2.12_deploy.jar:/databricks/jars/chauffeur-api--chauffeur-api-spark_3.0_2.12_deploy.jar:/databricks/jars/common--client--client-spark_3.0_2.12_deploy.jar:/databricks/jars/common--common-spark_3.0_2.12_deploy.jar:/databricks/jars/common--credentials--credentials-spark_3.0_2.12_deploy.jar:/databricks/jars/common--hadoop--hadoop-spark_3.0_2.12_deploy.jar:/databricks/jars/common--java-flight-recorder--java-flight-recorder-spark_3.0_2.12_deploy.jar:/databricks/jars/common--jetty--client--client-spark_3.0_2.12_deploy.jar:/databricks/jars/common--lazy--lazy-spark_3.0_2.12_deploy.jar:/databricks/jars/common--libcommon_resources.jar:/databricks/jars/common--path--path-spark_3.0_2.12_deploy.jar:/databricks/jars/common--rate-limiter--rate-limiter-spark_3.0_2.12_deploy.jar:/databricks/jars/common--storage--storage-spark_3.0_2.12_deploy.jar:/databricks/jars/common--tracing--tracing-spark_3.0_2.12_deploy.jar:/databricks/jars/common--util--locks-spark_3.0_2.12_deploy.jar:/databricks/jars/daemon--data--client--client-spark_3.0_2.12_deploy.jar:/databricks/jars/daemon--data--client--conf--conf-spark_3.0_2.12_deploy.jar:/databricks/jars/daemon--data--client--utils-spark_3.0_2.12_deploy.jar:/databricks/jars/daemon--data--data-common--data-common-spark_3.0_2.12_deploy.jar:/databricks/jars/dbfs--exceptions--exceptions-spark_3.0_2.12_deploy.jar:/databricks/jars/dbfs--utils--dbfs-utils-spark_3.0_2.12_deploy.jar:/databricks/jars/extern--acl--auth--auth-spark_3.0_2.12_deploy.jar:/databricks/jars/extern--extern-spark_3.0_2.12_deploy.jar:/databricks/jars/extern--libaws-regions.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/jsonutil--jsonutil-spark_3.0_2.12_deploy.jar:/databricks/jars/libraries--api--managedLibraries--managedLibraries-spark_3.0_2.12_deploy.jar:/databricks/jars/libraries--api--typemappers--typemappers-spark_3.0_2.12_deploy.jar:/databricks/jars/libraries--libraries-spark_3.0_2.12_deploy.jar:/databricks/jars/logging--log4j-mod--log4j-mod-spark_3.0_2.12_deploy.jar:/databricks/jars/logging--utils--logging-utils-spark_3.0_2.12_deploy.jar:/databricks/jars/s3commit--client--client-spark_3.0_2.12_deploy.jar:/databricks/jars/s3commit--common--common-spark_3.0_2.12_deploy.jar:/databricks/jars/s3--s3-spark_3.0_2.12_deploy.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/secret-manager--api--api-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--common--spark-common-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--common-utils--utils-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--conf-reader--conf-reader_lib-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--dbutils--dbutils-api-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--antlr--parser-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--common--driver-common-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--display--display-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--driver-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--events-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--ml--ml-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--secret-redaction-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--driver--spark--resources-resources.jar:/databricks/jars/spark--sql-extension--sql-extension-spark_3.0_2.12_deploy.jar:/databricks/jars/spark--versions--3.0--shim_2.12_deploy.jar:/databricks/jars/spark--versions--3.0--spark_2.12_deploy.jar:/databricks/jars/sqlgateway--common--endpoint_id-spark_3.0_2.12_deploy.jar:/databricks/jars/sqlgateway--history--api--api-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-annotations_com.fasterxml.jackson.core__jackson-annotations__2.11.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-core_com.fasterxml.jackson.core__jackson-core__2.11.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-databind_com.fasterxml.jackson.core__jackson-databind__2.11.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.android_annotations_com.google.android__annotations__4.1.1.4_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.api.grpc_proto-google-common-protos_com.google.api.grpc__proto-google-common-protos__1.17.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.code.findbugs_jsr305_com.google.code.findbugs__jsr305__3.0.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.code.gson_gson_com.google.code.gson__gson__2.8.6_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.errorprone_error_prone_annotations_com.google.errorprone__error_prone_annotations__2.3.4_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_failureaccess_com.google.guava__failureaccess__1.0.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_guava_com.google.guava__guava__28.2-android_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.guava_listenablefuture_com.google.guava__listenablefuture__9999.0-empty-to-avoid-conflict-with-guava_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.j2objc_j2objc-annotations_com.google.j2objc__j2objc-annotations__1.3_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java_com.google.protobuf__protobuf-java__3.12.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java-util_com.google.protobuf__protobuf-java-util__3.12.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-brave_com.linecorp.armeria__armeria-brave__0.99.8_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria_com.linecorp.armeria__armeria__0.99.8_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc_com.linecorp.armeria__armeria-grpc__0.99.8_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc-protocol_com.linecorp.armeria__armeria-grpc-protocol__0.99.8_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-api_io.grpc__grpc-api__1.30.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-context_io.grpc__grpc-context__1.30.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-core_io.grpc__grpc-core__1.30.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf_io.grpc__grpc-protobuf__1.30.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf-lite_io.grpc__grpc-protobuf-lite__1.30.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-services_io.grpc__grpc-services__1.30.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.grpc_grpc-stub_io.grpc__grpc-stub__1.30.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.micrometer_micrometer-core_io.micrometer__micrometer-core__1.5.2_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-buffer_io.netty__netty-buffer__4.1.50.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-dns_io.netty__netty-codec-dns__4.1.50.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-haproxy_io.netty__netty-codec-haproxy__4.1.50.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http2_io.netty__netty-codec-http2__4.1.50.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http_io.netty__netty-codec-http__4.1.50.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec_io.netty__netty-codec__4.1.50.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-socks_io.netty__netty-codec-socks__4.1.50.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-common_io.netty__netty-common__4.1.50.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-handler_io.netty__netty-handler__4.1.50.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-handler-proxy_io.netty__netty-handler-proxy__4.1.50.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver-dns_io.netty__netty-resolver-dns__4.1.50.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver_io.netty__netty-resolver__4.1.50.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-tcnative-boringssl-static_io.netty__netty-tcnative-boringssl-static__2.0.30.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport_io.netty__netty-transport__4.1.50.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-epoll-linux-x86_64_io.netty__netty-transport-native-epoll-linux-x86_64__4.1.50.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-unix-common_io.netty__netty-transport-native-unix-common__4.1.50.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-unix-common-linux-x86_64_io.netty__netty-transport-native-unix-common-linux-x86_64__4.1.50.Final_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.perfmark_perfmark-api_io.perfmark__perfmark-api__0.19.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.zipkin.brave_brave-instrumentation-http_io.zipkin.brave__brave-instrumentation-http__5.12.3_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.zipkin.brave_brave_io.zipkin.brave__brave__5.12.3_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.zipkin.reporter2_zipkin-reporter-brave_io.zipkin.reporter2__zipkin-reporter-brave__2.15.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.zipkin.reporter2_zipkin-reporter_io.zipkin.reporter2__zipkin-reporter__2.15.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_io.zipkin.zipkin2_zipkin_io.zipkin.zipkin2__zipkin__2.21.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_jakarta.annotation_jakarta.annotation-api_jakarta.annotation__jakarta.annotation-api__1.3.5_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_liball_deps_2.12_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_net.bytebuddy_byte-buddy_net.bytebuddy__byte-buddy__1.10.9_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.checkerframework_checker-compat-qual_org.checkerframework__checker-compat-qual__2.5.5_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.codehaus.mojo_animal-sniffer-annotations_org.codehaus.mojo__animal-sniffer-annotations__1.18_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.curioswitch.curiostack_protobuf-jackson_org.curioswitch.curiostack__protobuf-jackson__1.1.0_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.hdrhistogram_HdrHistogram_org.hdrhistogram__HdrHistogram__2.1.12_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.joda_joda-convert_org.joda__joda-convert__2.2.1_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.latencyutils_LatencyUtils_org.latencyutils__LatencyUtils__2.0.3_shaded.jar:/databricks/jars/third_party--armeria--maven-trees_armeria_org.reactivestreams_reactive-streams_org.reactivestreams__reactive-streams__1.0.3_shaded.jar:/databricks/jars/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-client-runtime__1.7.0_container_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.azure__azure-storage__8.6.4_shaded.jar:/databricks/jars/third_party--azure--com.microsoft.rest__client-runtime__1.7.0_container_shaded.jar:/databricks/jars/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:/databricks/jars/third_party--datalake--datalake-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--dropwizard-metrics-log4j-v3.2.6--metrics-log4j-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--gcs--animal-sniffer-annotations_shaded.jar:/databricks/jars/third_party--gcs--annotations_shaded.jar:/databricks/jars/third_party--gcs--api-common_shaded.jar:/databricks/jars/third_party--gcs--auto-value-annotations_shaded.jar:/databricks/jars/third_party--gcs--checker-compat-qual_shaded.jar:/databricks/jars/third_party--gcs--checker-qual_shaded.jar:/databricks/jars/third_party--gcs--commons-codec_shaded.jar:/databricks/jars/third_party--gcs--commons-lang3_shaded.jar:/databricks/jars/third_party--gcs--commons-logging_shaded.jar:/databricks/jars/third_party--gcs--conscrypt-openjdk-uber_shaded.jar:/databricks/jars/third_party--gcs--error_prone_annotations_shaded.jar:/databricks/jars/third_party--gcs--failureaccess_shaded.jar:/databricks/jars/third_party--gcs--flogger_shaded.jar:/databricks/jars/third_party--gcs--flogger-slf4j-backend_shaded.jar:/databricks/jars/third_party--gcs--flogger-system-backend_shaded.jar:/databricks/jars/third_party--gcs--gcs-connector_shaded.jar:/databricks/jars/third_party--gcs--gcsio_shaded.jar:/databricks/jars/third_party--gcs--gcs-shaded-jetty9-hadoop1_2.12_deploy.jar:/databricks/jars/third_party--gcs--google-api-client-jackson2_shaded.jar:/databricks/jars/third_party--gcs--google-api-client-java6_shaded.jar:/databricks/jars/third_party--gcs--google-api-client_shaded.jar:/databricks/jars/third_party--gcs--google-api-services-storage_shaded.jar:/databricks/jars/third_party--gcs--google-auth-library-credentials_shaded.jar:/databricks/jars/third_party--gcs--google-auth-library-oauth2-http_shaded.jar:/databricks/jars/third_party--gcs--google-extensions_shaded.jar:/databricks/jars/third_party--gcs--google-http-client-jackson2_shaded.jar:/databricks/jars/third_party--gcs--google-http-client_shaded.jar:/databricks/jars/third_party--gcs--google-oauth-client-java6_shaded.jar:/databricks/jars/third_party--gcs--google-oauth-client_shaded.jar:/databricks/jars/third_party--gcs--grpc-alts_shaded.jar:/databricks/jars/third_party--gcs--grpc-api_shaded.jar:/databricks/jars/third_party--gcs--grpc-auth_shaded.jar:/databricks/jars/third_party--gcs--grpc-context_shaded.jar:/databricks/jars/third_party--gcs--grpc-core_shaded.jar:/databricks/jars/third_party--gcs--grpc-grpclb_shaded.jar:/databricks/jars/third_party--gcs--grpc-netty-shaded_shaded.jar:/databricks/jars/third_party--gcs--grpc-protobuf-lite_shaded.jar:/databricks/jars/third_party--gcs--grpc-protobuf_shaded.jar:/databricks/jars/third_party--gcs--grpc-stub_shaded.jar:/databricks/jars/third_party--gcs--gson_shaded.jar:/databricks/jars/third_party--gcs--guava_shaded.jar:/databricks/jars/third_party--gcs--httpclient_shaded.jar:/databricks/jars/third_party--gcs--httpcore_shaded.jar:/databricks/jars/third_party--gcs--j2objc-annotations_shaded.jar:/databricks/jars/third_party--gcs--jackson-core_shaded.jar:/databricks/jars/third_party--gcs--javax.annotation-api_shaded.jar:/databricks/jars/third_party--gcs--jsr305_shaded.jar:/databricks/jars/third_party--gcs--listenablefuture_shaded.jar:/databricks/jars/third_party--gcs--opencensus-api_shaded.jar:/databricks/jars/third_party--gcs--opencensus-contrib-http-util_shaded.jar:/databricks/jars/third_party--gcs--perfmark-api_shaded.jar:/databricks/jars/third_party--gcs--protobuf-java_shaded.jar:/databricks/jars/third_party--gcs--protobuf-java-util_shaded.jar:/databricks/jars/third_party--gcs--proto-google-common-protos_shaded.jar:/databricks/jars/third_party--gcs--proto-google-iam-v1_shaded.jar:/databricks/jars/third_party--gcs--util-hadoop_shaded.jar:/databricks/jars/third_party--gcs--util_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__8.6.4_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_3.0_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_3.0_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180625_3682417_spark_3.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180920_b33d810_spark_3.0.jar:/databricks/jars/third_party--hadoop--hadoop-tools--hadoop-aws--lib-spark_3.0_2.12_deploy_shaded.jar:/databricks/jars/third_party--jackson--guava_only_shaded.jar:/databricks/jars/third_party--jackson--jackson-module-scala-shaded_2.12_deploy.jar:/databricks/jars/third_party--jackson--jsr305_only_shaded.jar:/databricks/jars/third_party--jackson--paranamer_only_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--databricks-patched-jetty-client-jar_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--databricks-patched-jetty-http-jar_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--jetty-io_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--jetty-jmx_shaded.jar:/databricks/jars/third_party--jetty8-shaded-client--jetty-util_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-client_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-http_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-io_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-util_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar:/databricks/jars/third_party--prometheus-client--jmx_collector-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--prometheus-client--simpleclient_common-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--prometheus-client--simpleclient_dropwizard-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--prometheus-client--simpleclient_servlet-spark_3.0_2.12_deploy.jar:/databricks/jars/third_party--prometheus-client--simpleclient-spark_3.0_2.12_deploy.jar:/databricks/jars/utils--process_utils-spark_3.0_2.12_deploy.jar:/databricks/jars/workflow--workflow-spark_3.0_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--kvstore--kvstore-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--network-common--network-common-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--network-shuffle--network-shuffle-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--sketch--sketch-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--tags--tags-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--common--unsafe--unsafe-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--core--core-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_0--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_0--graphx--graphx-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--launcher--launcher-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.655.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.chuusai--shapeless_2.12--com.chuusai__shapeless_2.12__2.3.3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.9.6.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.10.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.10.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.10.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.10.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.10.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.10.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-scala_2.12--com.fasterxml.jackson.module__jackson-module-scala_2.12__2.10.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.ben-manes.caffeine--caffeine--com.github.ben-manes.caffeine__caffeine__2.3.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.joshelser--dropwizard-metrics-hadoop-metrics2-reporter--com.github.joshelser__dropwizard-metrics-hadoop-metrics2-reporter__0.1.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.4.4-3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.wendykierp--JTransforms--com.github.wendykierp__JTransforms__3.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__3.0.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.google.flatbuffers--flatbuffers-java--com.google.flatbuffers__flatbuffers-java__1.9.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.h2database--h2--com.h2database__h2__1.4.195.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.helger--profiler--com.helger__profiler__1.1.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.lihaoyi--sourcecode_2.12--com.lihaoyi__sourcecode_2.12__0.1.9.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.2.8.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.microsoft.sqlserver--mssql-jdbc--com.microsoft.sqlserver__mssql-jdbc__8.2.1.jre8.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-io--commons-io--commons-io__commons-io__2.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-net--commons-net--commons-net__commons-net__3.1.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.tdunning--json--com.tdunning__json__1.8.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.trueaccord.lenses--lenses_2.12--com.trueaccord.lenses__lenses_2.12__0.4.12.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill_2.12--com.twitter__chill_2.12__0.9.5.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill-java--com.twitter__chill-java__0.9.5.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-app_2.12--com.twitter__util-app_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-core_2.12--com.twitter__util-core_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-function_2.12--com.twitter__util-function_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-jvm_2.12--com.twitter__util-jvm_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_0--ma
*** WARNING: skipped 122159 bytes of output ***

container=lxc
USER=root
PYTHONHASHSEED=0
MLFLOW_PYTHON_EXECUTABLE=/databricks/python/bin/python
PWD=/databricks/driver
R_LIBS=/databricks/spark/R/lib
HOME=/root
DB_HOME=/databricks
SPARK_LOCAL_DIRS=/local_disk0
SUDO_USER=root
PYSPARK_GATEWAY_PORT=40242
DATABRICKS_ROOT_VIRTUALENV_ENV=/databricks/python3
ENABLE_IPTABLES=false
SPARK_LOCAL_IP=10.149.224.176
SPARK_SCALA_VERSION=2.12
SPARK_HOME=/databricks/spark
CLUSTER_DB_HOME=/databricks
SUDO_UID=0
SPARK_WORKER_MEMORY=3146m
MAIL=/var/mail/root
TERM=unknown
SHELL=/bin/bash
MPLBACKEND=AGG
DATABRICKS_RUNTIME_VERSION=7.2
SPARK_ENV_LOADED=1
SHLVL=4
PYTHONPATH=/databricks/spark/python:/databricks/spark/python/lib/py4j-0.10.9-src.zip:/databricks/jars/spark--driver--driver-spark_3.0_2.12_deploy.jar:/databricks/spark/python
SCALA_VERSION=2.10
PYSPARK_GATEWAY_SECRET=cbfa4e3656b7584c09eb35b9ceeaf9b9369fac897eb3569e0ccfc719dd762523
LOGNAME=root
MLFLOW_TRACKING_URI=databricks
SPARK_CONF_DIR=/databricks/spark/conf
PYSPARK_PYTHON=/databricks/python/bin/python
KOALAS_USAGE_LOGGER=pyspark.databricks.koalas.usage_logger
PATH=/databricks/python3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin
DRIVER_PID_FILE=/tmp/driver-daemon.pid
PIP_NO_INPUT=1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-sh">tar zxvf sou.tgz
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>sou/
sou/18101205.txt
sou/19390104.txt
sou/18361205.txt
sou/18701205.txt
sou/17911025.txt
sou/18511202.txt
sou/18661203.txt
sou/18751207.txt
sou/18731201.txt
sou/18861206.txt
sou/19470106.txt
sou/18721202.txt
sou/19690114.txt
sou/19980127.txt
sou/18241207.txt
sou/19440111.txt
sou/18261205.txt
sou/18501202.txt
sou/18081108.txt
sou/18421206.txt
sou/19870127.txt
sou/19490105.txt
sou/19520109.txt
sou/19131202.txt
sou/18811206.txt
sou/19460121.txt
sou/18201114.txt
sou/18021215.txt
sou/18371205.txt
sou/18491204.txt
sou/19350104.txt
sou/19610112.txt
sou/19360103.txt
sou/18881203.txt
sou/18671203.txt
sou/19171204.txt
sou/20090224.txt
sou/18921206.txt
sou/19271206.txt
sou/18691206.txt
sou/19720120.txt
sou/20100127.txt
sou/17951208.txt
sou/19071203.txt
sou/18651204.txt
sou/18971206.txt
sou/19800121.txt
sou/19251208.txt
sou/19700122.txt
sou/18231202.txt
sou/20010227.txt
sou/17931203.txt
sou/18441203.txt
sou/19920128.txt
sou/18471207.txt
sou/18951207.txt
sou/19081208.txt
sou/18251206.txt
sou/19830125.txt
sou/18411207.txt
sou/18771203.txt
sou/19590109.txt
sou/18351207.txt
sou/18601203.txt
sou/18291208.txt
sou/17991203.txt
sou/19610130.txt
sou/20060131.txt
sou/19580109.txt
sou/18061202.txt
sou/18831204.txt
sou/19780119.txt
sou/19061203.txt
sou/17981208.txt
sou/19650104.txt
sou/19161205.txt
sou/19740130.txt
sou/19940125.txt
sou/17901208.txt
sou/19001203.txt
sou/18181116.txt
sou/18871206.txt
sou/19051205.txt
sou/19450106.txt
sou/19231206.txt
sou/19970204.txt
sou/19640108.txt
sou/19261207.txt
sou/20140128.txt
sou/19770112.txt
sou/20030128.txt
sou/19840125.txt
sou/18401205.txt
sou/19430107.txt
sou/17971122.txt
sou/19311208.txt
sou/18091129.txt
sou/18781202.txt
sou/18521206.txt
sou/18851208.txt
sou/19570110.txt
sou/19750115.txt
sou/19370106.txt
sou/19281204.txt
sou/17921106.txt
sou/19620111.txt
sou/18121104.txt
sou/19221208.txt
sou/18531205.txt
sou/19550106.txt
sou/18581206.txt
sou/19540107.txt
sou/19880125.txt
sou/19211206.txt
sou/18281202.txt
sou/19730202.txt
sou/18681209.txt
sou/20080128.txt
sou/18841201.txt
sou/18551231.txt
sou/19041206.txt
sou/18011208.txt
sou/20040120.txt
sou/18131207.txt
sou/18991205.txt
sou/18031017.txt
sou/19530107.txt
sou/19201207.txt
sou/19710122.txt
sou/18791201.txt
sou/19480107.txt
sou/18271204.txt
sou/17900108.txt
sou/19930217.txt
sou/18911209.txt
sou/18641206.txt
sou/19021202.txt
sou/18891203.txt
sou/18961204.txt
sou/19530202.txt
sou/20120124.txt
sou/18481205.txt
sou/19860204.txt
sou/19400103.txt
sou/19340103.txt
sou/19630114.txt
sou/19850206.txt
sou/18051203.txt
sou/19510108.txt
sou/19890209.txt
sou/19560105.txt
sou/19810116.txt
sou/19380103.txt
sou/20010920.txt
sou/20000127.txt
sou/19241203.txt
sou/19091207.txt
sou/19011203.txt
sou/18001111.txt
sou/18111105.txt
sou/19820126.txt
sou/19500104.txt
sou/18171212.txt
sou/19101206.txt
sou/18461208.txt
sou/19600107.txt
sou/18451202.txt
sou/20160112.txt
sou/18311206.txt
sou/19960123.txt
sou/19680117.txt
sou/18161203.txt
sou/19151207.txt
sou/18151205.txt
sou/18621201.txt
sou/19301202.txt
sou/19191202.txt
sou/19790125.txt
sou/19670110.txt
sou/18901201.txt
sou/18071027.txt
sou/18140920.txt
sou/20170228.txt
sou/19990119.txt
sou/20020129.txt
sou/19760119.txt
sou/18801206.txt
sou/18741207.txt
sou/19121203.txt
sou/18941202.txt
sou/20110125.txt
sou/18631208.txt
sou/18041108.txt
sou/18561202.txt
sou/19410106.txt
sou/20070123.txt
sou/18211203.txt
sou/19181202.txt
sou/20150120.txt
sou/18541204.txt
sou/18931203.txt
sou/18591219.txt
sou/18571208.txt
sou/19141208.txt
sou/19291203.txt
sou/17961207.txt
sou/18391202.txt
sou/18761205.txt
sou/18341201.txt
sou/18821204.txt
sou/18221203.txt
sou/18981205.txt
sou/19900131.txt
sou/19031207.txt
sou/19111205.txt
sou/18301206.txt
sou/19950124.txt
sou/18431206.txt
sou/20130212.txt
sou/17941119.txt
sou/19660112.txt
sou/18321204.txt
sou/19420106.txt
sou/18381203.txt
sou/19910129.txt
sou/18711204.txt
sou/18331203.txt
sou/19321206.txt
sou/20050202.txt
sou/18611203.txt
sou/18191207.txt
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-sh">cd sou &amp;&amp; ls
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>17900108.txt
17901208.txt
17911025.txt
17921106.txt
17931203.txt
17941119.txt
17951208.txt
17961207.txt
17971122.txt
17981208.txt
17991203.txt
18001111.txt
18011208.txt
18021215.txt
18031017.txt
18041108.txt
18051203.txt
18061202.txt
18071027.txt
18081108.txt
18091129.txt
18101205.txt
18111105.txt
18121104.txt
18131207.txt
18140920.txt
18151205.txt
18161203.txt
18171212.txt
18181116.txt
18191207.txt
18201114.txt
18211203.txt
18221203.txt
18231202.txt
18241207.txt
18251206.txt
18261205.txt
18271204.txt
18281202.txt
18291208.txt
18301206.txt
18311206.txt
18321204.txt
18331203.txt
18341201.txt
18351207.txt
18361205.txt
18371205.txt
18381203.txt
18391202.txt
18401205.txt
18411207.txt
18421206.txt
18431206.txt
18441203.txt
18451202.txt
18461208.txt
18471207.txt
18481205.txt
18491204.txt
18501202.txt
18511202.txt
18521206.txt
18531205.txt
18541204.txt
18551231.txt
18561202.txt
18571208.txt
18581206.txt
18591219.txt
18601203.txt
18611203.txt
18621201.txt
18631208.txt
18641206.txt
18651204.txt
18661203.txt
18671203.txt
18681209.txt
18691206.txt
18701205.txt
18711204.txt
18721202.txt
18731201.txt
18741207.txt
18751207.txt
18761205.txt
18771203.txt
18781202.txt
18791201.txt
18801206.txt
18811206.txt
18821204.txt
18831204.txt
18841201.txt
18851208.txt
18861206.txt
18871206.txt
18881203.txt
18891203.txt
18901201.txt
18911209.txt
18921206.txt
18931203.txt
18941202.txt
18951207.txt
18961204.txt
18971206.txt
18981205.txt
18991205.txt
19001203.txt
19011203.txt
19021202.txt
19031207.txt
19041206.txt
19051205.txt
19061203.txt
19071203.txt
19081208.txt
19091207.txt
19101206.txt
19111205.txt
19121203.txt
19131202.txt
19141208.txt
19151207.txt
19161205.txt
19171204.txt
19181202.txt
19191202.txt
19201207.txt
19211206.txt
19221208.txt
19231206.txt
19241203.txt
19251208.txt
19261207.txt
19271206.txt
19281204.txt
19291203.txt
19301202.txt
19311208.txt
19321206.txt
19340103.txt
19350104.txt
19360103.txt
19370106.txt
19380103.txt
19390104.txt
19400103.txt
19410106.txt
19420106.txt
19430107.txt
19440111.txt
19450106.txt
19460121.txt
19470106.txt
19480107.txt
19490105.txt
19500104.txt
19510108.txt
19520109.txt
19530107.txt
19530202.txt
19540107.txt
19550106.txt
19560105.txt
19570110.txt
19580109.txt
19590109.txt
19600107.txt
19610112.txt
19610130.txt
19620111.txt
19630114.txt
19640108.txt
19650104.txt
19660112.txt
19670110.txt
19680117.txt
19690114.txt
19700122.txt
19710122.txt
19720120.txt
19730202.txt
19740130.txt
19750115.txt
19760119.txt
19770112.txt
19780119.txt
19790125.txt
19800121.txt
19810116.txt
19820126.txt
19830125.txt
19840125.txt
19850206.txt
19860204.txt
19870127.txt
19880125.txt
19890209.txt
19900131.txt
19910129.txt
19920128.txt
19930217.txt
19940125.txt
19950124.txt
19960123.txt
19970204.txt
19980127.txt
19990119.txt
20000127.txt
20010227.txt
20010920.txt
20020129.txt
20030128.txt
20040120.txt
20050202.txt
20060131.txt
20070123.txt
20080128.txt
20090224.txt
20100127.txt
20110125.txt
20120124.txt
20130212.txt
20140128.txt
20150120.txt
20160112.txt
20170228.txt
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">head sou/17900108.txt
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>George Washington 

January 8, 1790 
Fellow-Citizens of the Senate and House of Representatives: 
I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity. 
In resuming your consultations for the general good you can not but derive encouragement from the reflection that the measures of the last session have been as satisfactory to your constituents as the novelty and difficulty of the work allowed you to hope. Still further to realize their expectations and to secure the blessings which a gracious Providence has placed within our reach will in the course of the present important session call for the cool and deliberate exertion of your patriotism, firmness, and wisdom. 
Among the many interesting objects which will engage your attention that of providing for the common defense will merit particular regard. To be prepared for war is one of the most effectual means of preserving peace. 
A free people ought not only to be armed, but disciplined; to which end a uniform and well-digested plan is requisite; and their safety and interest require that they should promote such manufactories as tend to render them independent of others for essential, particularly military, supplies. 
The proper establishment of the troops which may be deemed indispensable will be entitled to mature consideration. In the arrangements which may be made respecting it it will be of importance to conciliate the comfortable support of the officers and soldiers with a due regard to economy. 
There was reason to hope that the pacific measures adopted with regard to certain hostile tribes of Indians would have relieved the inhabitants of our southern and western frontiers from their depredations, but you will perceive from the information contained in the papers which I shall direct to be laid before you (comprehending a communication from the Commonwealth of Virginia) that we ought to be prepared to afford protection to those parts of the Union, and, if necessary, to punish aggressors. 
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">tail sou/17900108.txt
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>The advancement of agriculture, commerce, and manufactures by all proper means will not, I trust, need recommendation; but I can not forbear intimating to you the expediency of giving effectual encouragement as well to the introduction of new and useful inventions from abroad as to the exertions of skill and genius in producing them at home, and of facilitating the intercourse between the distant parts of our country by a due attention to the post-office and post-roads. 
Nor am I less persuaded that you will agree with me in opinion that there is nothing which can better deserve your patronage than the promotion of science and literature. Knowledge is in every country the surest basis of public happiness. In one in which the measures of government receive their impressions so immediately from the sense of the community as in ours it is proportionably essential. 
To the security of a free constitution it contributes in various ways--by convincing those who are intrusted with the public administration that every valuable end of government is best answered by the enlightened confidence of the people, and by teaching the people themselves to know and to value their own rights; to discern and provide against invasions of them; to distinguish between oppression and the necessary exercise of lawful authority; between burthens proceeding from a disregard to their convenience and those resulting from the inevitable exigencies of society; to discriminate the spirit of liberty from that of licentiousness-- cherishing the first, avoiding the last--and uniting a speedy but temperate vigilance against encroachments, with an inviolable respect to the laws. 
Whether this desirable object will be best promoted by affording aids to seminaries of learning already established, by the institution of a national university, or by any other expedients will be well worthy of a place in the deliberations of the legislature. 
Gentlemen of the House of Representatives: 
I saw with peculiar pleasure at the close of the last session the resolution entered into by you expressive of your opinion that an adequate provision for the support of the public credit is a matter of high importance to the national honor and prosperity. In this sentiment I entirely concur; and to a perfect confidence in your best endeavors to devise such a provision as will be truly with the end I add an equal reliance on the cheerful cooperation of the other branch of the legislature. 
It would be superfluous to specify inducements to a measure in which the character and interests of the United States are so obviously so deeply concerned, and which has received so explicit a sanction from your declaration. 
Gentlemen of the Senate and House of Representatives: 
I have directed the proper officers to lay before you, respectively, such papers and estimates as regard the affairs particularly recommended to your consideration, and necessary to convey to you that information of the state of the Union which it is my duty to afford. 
The welfare of our country is the great object to which our cares and efforts ought to be directed, and I shall derive great satisfaction from a cooperation with you in the pleasing though arduous task of insuring to our fellow citizens the blessings which they have a right to expect from a free, efficient, and equal government.
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">head sou/20150120.txt
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Barack Obama 

January 20, 2015 
Mr. Speaker, Mr. Vice President, Members of Congress, my fellow Americans: We are 15 years into this new century. Fifteen years that dawned with terror touching our shores, that unfolded with a new generation fighting two long and costly wars, that saw a vicious recession spread across our Nation and the world. It has been and still is a hard time for many. 
But tonight we turn the page. Tonight, after a breakthrough year for America, our economy is growing and creating jobs at the fastest pace since 1999. Our unemployment rate is now lower than it was before the financial crisis. More of our kids are graduating than ever before. More of our people are insured than ever before. And we are as free from the grip of foreign oil as we've been in almost 30 years. 
Tonight, for the first time since 9/11, our combat mission in Afghanistan is over. Six years ago, nearly 180,000 American troops served in Iraq and Afghanistan. Today, fewer than 15,000 remain. And we salute the courage and sacrifice of every man and woman in this 9/11 generation who has served to keep us safe. We are humbled and grateful for your service. 
America, for all that we have endured, for all the grit and hard work required to come back, for all the tasks that lie ahead, know this: The shadow of crisis has passed, and the State of the Union is strong. 
At this momentwith a growing economy, shrinking deficits, bustling industry, booming energy productionwe have risen from recession freer to write our own future than any other nation on Earth. It's now up to us to choose who we want to be over the next 15 years and for decades to come. 
Will we accept an economy where only a few of us do spectacularly well? Or will we commit ourselves to an economy that generates rising incomes and chances for everyone who makes the effort? 
Will we approach the world fearful and reactive, dragged into costly conflicts that strain our military and set back our standing? Or will we lead wisely, using all elements of our power to defeat new threats and protect our planet? 
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">tail sou/20150120.txt
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>We may have different takes on the events of Ferguson and New York. But surely we can understand a father who fears his son can't walk home without being harassed. And surely we can understand the wife who won't rest until the police officer she married walks through the front door at the end of his shift. And surely we can agree that it's a good thing that for the first time in 40 years, the crime rate and the incarceration rate have come down together, and use that as a starting point for Democrats and Republicans, community leaders and law enforcement, to reform America's criminal justice system so that it protects and serves all of us. 
That's a better politics. That's how we start rebuilding trust. That's how we move this country forward. That's what the American people want. And that's what they deserve. 
I have no more campaigns to run. 
My only agenda 
I know because I won both of them. My only agenda for the next 2 years is the same as the one I've had since the day I swore an oath on the steps of this Capitol: to do what I believe is best for America. If you share the broad vision I outlined tonight, I ask you to join me in the work at hand. If you disagree with parts of it, I hope you'll at least work with me where you do agree. And I commit to every Republican here tonight that I will not only seek out your ideas, I will seek to work with you to make this country stronger. 
Because I want this Chamber, I want this city to reflect the truth: that for all our blind spots and shortcomings, we are a people with the strength and generosity of spirit to bridge divides, to unite in common effort, to help our neighbors, whether down the street or on the other side of the world. 
I want our actions to tell every child in every neighborhood, your life matters, and we are committed to improving your life chances, as committed as we are to working on behalf of our own kids. I want future generations to know that we are a people who see our differences as a great gift, that we're a people who value the dignity and worth of every citizen: man and woman, young and old, Black and White, Latino, Asian, immigrant, Native American, gay, straight, Americans with mental illness or physical disability. Everybody matters. I want them to grow up in a country that shows the world what we still know to be true: that we are still more than a collection of red States and blue States, that we are the United States of America. 
I want them to grow up in a country where a young mom can sit down and write a letter to her President with a story that sums up these past 6 years: &quot;It's amazing what you can bounce back from when you have to. . . . We are a strong, tight-knit family who's made it through some very, very hard times.&quot; 
My fellow Americans, we too are a strong, tight-knit family. We too have made it through some hard times. Fifteen years into this new century, we have picked ourselves up, dusted ourselves off, and begun again the work of remaking America. We have laid a new foundation. A brighter future is ours to write. Let's begin this new chapter together, and let's start the work right now. 
Thank you. God bless you. God bless this country we love. Thank you.
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">head sou/20170228.txt
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Donald J. Trump 

February 28, 2017 
Thank you very much. Mr. Speaker, Mr. Vice President, members of Congress, the first lady of the United States ... 
... and citizens of America, tonight, as we mark the conclusion of our celebration of Black History Month, we are reminded of our nation's path toward civil rights and the work that still remains to be done. 
Recent threats ... 
Recent threats targeting Jewish community centers and vandalism of Jewish cemeteries, as well as last week's shooting in Kansas City, remind us that while we may be a nation divided on policies, we are a country that stands united in condemning hate and evil in all of its very ugly forms. 
Each American generation passes the torch of truth, liberty and justice, in an unbroken chain all the way down to the present. That torch is now in our hands. And we will use it to light up the world. 
I am here tonight to deliver a message of unity and strength, and it is a message deeply delivered from my heart. A new chapter ... 
... of American greatness is now beginning. A new national pride is sweeping across our nation. And a new surge of optimism is placing impossible dreams firmly within our grasp. What we are witnessing today is the renewal of the American spirit. Our allies will find that America is once again ready to lead. 
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">tail sou/20170228.txt
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>America is willing to find new friends, and to forge new partnerships, where shared interests align. We want harmony and stability, not war and conflict. We want peace, wherever peace can be found. America is friends today with former enemies. Some of our closest allies, decades ago, fought on the opposite side of these terrible, terrible wars. This history should give us all faith in the possibilities for a better world. 
Hopefully, the 250th year for America will see a world that is more peaceful, more just, and more free. 
On our 100th anniversary in 1876, citizens from across our nation came to Philadelphia to celebrate America's centennial. At that celebration, the country's builders and artists and inventors showed off their wonderful creations. Alexander Graham Bell displayed his telephone for the first time. Remington unveiled the first typewriter. An early attempt was made at electric light. Thomas Edison showed an automatic telegraph and an electric pen. Imagine the wonders our country could know in America's 250th year. 
Think of the marvels we could achieve if we simply set free the dreams of our people. Cures to the illnesses that have always plagued us are not too much to hope. American footprints on distant worlds are not too big a dream. Millions lifted from welfare to work is not too much to expect. And streets where mothers are safe from fear schools where children learn in peace, and jobs where Americans prosper and grow are not too much to ask. 
When we have all of this, we will have made America greater than ever before, for all Americans. This is our vision. This is our mission. But we can only get there together. We are one people, with one destiny. 
We all bleed the same blood. We all salute the same great American flag. And we are all made by the same God. 
When we fulfill this vision, when we celebrate our 250 years of glorious freedom, we will look back on tonight as when this new chapter of American greatness began. The time for small thinking is over. The time for trivial fights is behind us. We just need the courage to share the dreams that fill our hearts, the bravery to express the hopes that stir our souls, and the confidence to turn those hopes and those dreams into action. 
From now on, America will be empowered by our aspirations, not burdened by our fears, inspired by the future, not bound by failures of the past, and guided by a vision, not blinded by our doubts. 
I am asking all citizens to embrace this renewal of the American spirit. I am asking all members of Congress to join me in dreaming big and bold and daring things for our country. I am asking everyone watching tonight to seize this moment. Believe in yourselves. Believe in your future. And believe, once more, in America. 
Thank you, God bless you, and God bless the United States.
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/&quot;))
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/FileStore/</td>
<td>FileStore/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/_checkpoint/</td>
<td>_checkpoint/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/databricks/</td>
<td>databricks/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/databricks-datasets/</td>
<td>databricks-datasets/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/databricks-results/</td>
<td>databricks-results/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/</td>
<td>datasets/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/digsum-dataframe.csv/</td>
<td>digsum-dataframe.csv/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/ml/</td>
<td>ml/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/mnt/</td>
<td>mnt/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/mytmpdir-forUserTimeLine/</td>
<td>mytmpdir-forUserTimeLine/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/results/</td>
<td>results/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/test/</td>
<td>test/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/tmp/</td>
<td>tmp/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/tmpdir/</td>
<td>tmpdir/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/user/</td>
<td>user/</td>
<td>0.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/datasets&quot;))
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/datasets/MEP/</td>
<td>MEP/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/beijing/</td>
<td>beijing/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/graphhopper/</td>
<td>graphhopper/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/magellan/</td>
<td>magellan/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/maps/</td>
<td>maps/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/osm/</td>
<td>osm/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/</td>
<td>sou/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/twitterAccountsOfInterest.csv</td>
<td>twitterAccountsOfInterest.csv</td>
<td>32981.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.rm(&quot;dbfs:/datasets/sou/&quot;, recurse=true) //need to be done only if you want to delete such an existing directory, if any!
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.mkdirs(&quot;dbfs:/datasets/sou&quot;) //need not be done again if it already exists as desired!
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res3: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/datasets&quot;))
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/datasets/MEP/</td>
<td>MEP/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/beijing/</td>
<td>beijing/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/graphhopper/</td>
<td>graphhopper/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/magellan/</td>
<td>magellan/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/maps/</td>
<td>maps/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/osm/</td>
<td>osm/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/</td>
<td>sou/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/twitterAccountsOfInterest.csv</td>
<td>twitterAccountsOfInterest.csv</td>
<td>32981.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">pwd &amp;&amp; ls
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>/databricks/driver
conf
derby.log
eventlogs
ganglia
logs
sou
sou.tgz
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">dbutils.fs.help
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<div class = "ansiout"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in
this package can take either a DBFS path (e.g., "/foo" or "dbfs:/foo"), or
another FileSystem URI.
<p>For more info about a method, use <b>dbutils.fs.help(&quot;methodName&quot;)</b>.</p>
<p>In notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps
straightforwardly onto dbutils calls. For example, &quot;%fs head --maxBytes=10000 /file/path&quot;
translates into &quot;dbutils.fs.head(&quot;/file/path&quot;, maxBytes = 10000)&quot;.
<h3 id="fsutils"><a class="header" href="#fsutils">fsutils</a></h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -&gt; Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -&gt; Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -&gt; Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -&gt; Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -&gt; Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -&gt; Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -&gt; Removes a file or directory<br /><br /><h3 id="mount"><a class="header" href="#mount">mount</a></h3><b>mount(source: String, mountPoint: String, encryptionType: String = &quot;&quot;, owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -&gt; Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -&gt; Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -&gt; Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -&gt; Deletes a DBFS mount point<br /><br /></div></p>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.cp(&quot;file:/databricks/driver/sou&quot;, &quot;dbfs:/datasets/sou/&quot;,recurse=true)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res6: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/datasets/sou&quot;))
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/datasets/sou/17900108.txt</td>
<td>17900108.txt</td>
<td>6725.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/17901208.txt</td>
<td>17901208.txt</td>
<td>8427.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/17911025.txt</td>
<td>17911025.txt</td>
<td>14175.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/17921106.txt</td>
<td>17921106.txt</td>
<td>12736.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/17931203.txt</td>
<td>17931203.txt</td>
<td>11668.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/17941119.txt</td>
<td>17941119.txt</td>
<td>17615.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/17951208.txt</td>
<td>17951208.txt</td>
<td>12296.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/17961207.txt</td>
<td>17961207.txt</td>
<td>17340.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/17971122.txt</td>
<td>17971122.txt</td>
<td>12473.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/17981208.txt</td>
<td>17981208.txt</td>
<td>13394.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/17991203.txt</td>
<td>17991203.txt</td>
<td>9236.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18001111.txt</td>
<td>18001111.txt</td>
<td>8382.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18011208.txt</td>
<td>18011208.txt</td>
<td>19342.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18021215.txt</td>
<td>18021215.txt</td>
<td>13003.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18031017.txt</td>
<td>18031017.txt</td>
<td>14022.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18041108.txt</td>
<td>18041108.txt</td>
<td>12652.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18051203.txt</td>
<td>18051203.txt</td>
<td>17190.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18061202.txt</td>
<td>18061202.txt</td>
<td>17135.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18071027.txt</td>
<td>18071027.txt</td>
<td>14334.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18081108.txt</td>
<td>18081108.txt</td>
<td>16225.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18091129.txt</td>
<td>18091129.txt</td>
<td>11050.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18101205.txt</td>
<td>18101205.txt</td>
<td>15028.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18111105.txt</td>
<td>18111105.txt</td>
<td>13941.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18121104.txt</td>
<td>18121104.txt</td>
<td>19615.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18131207.txt</td>
<td>18131207.txt</td>
<td>19532.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18140920.txt</td>
<td>18140920.txt</td>
<td>12632.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18151205.txt</td>
<td>18151205.txt</td>
<td>19398.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18161203.txt</td>
<td>18161203.txt</td>
<td>20331.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18171212.txt</td>
<td>18171212.txt</td>
<td>26236.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18181116.txt</td>
<td>18181116.txt</td>
<td>26445.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18191207.txt</td>
<td>18191207.txt</td>
<td>27880.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18201114.txt</td>
<td>18201114.txt</td>
<td>20503.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18211203.txt</td>
<td>18211203.txt</td>
<td>34364.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18221203.txt</td>
<td>18221203.txt</td>
<td>28154.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18231202.txt</td>
<td>18231202.txt</td>
<td>38329.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18241207.txt</td>
<td>18241207.txt</td>
<td>49869.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18251206.txt</td>
<td>18251206.txt</td>
<td>53992.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18261205.txt</td>
<td>18261205.txt</td>
<td>46482.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18271204.txt</td>
<td>18271204.txt</td>
<td>42481.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18281202.txt</td>
<td>18281202.txt</td>
<td>44202.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18291208.txt</td>
<td>18291208.txt</td>
<td>62923.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18301206.txt</td>
<td>18301206.txt</td>
<td>90641.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18311206.txt</td>
<td>18311206.txt</td>
<td>42902.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18321204.txt</td>
<td>18321204.txt</td>
<td>46879.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18331203.txt</td>
<td>18331203.txt</td>
<td>46991.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18341201.txt</td>
<td>18341201.txt</td>
<td>80364.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18351207.txt</td>
<td>18351207.txt</td>
<td>64395.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18361205.txt</td>
<td>18361205.txt</td>
<td>73306.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18371205.txt</td>
<td>18371205.txt</td>
<td>68927.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18381203.txt</td>
<td>18381203.txt</td>
<td>69880.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18391202.txt</td>
<td>18391202.txt</td>
<td>80147.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18401205.txt</td>
<td>18401205.txt</td>
<td>55025.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18411207.txt</td>
<td>18411207.txt</td>
<td>48792.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18421206.txt</td>
<td>18421206.txt</td>
<td>49788.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18431206.txt</td>
<td>18431206.txt</td>
<td>47670.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18441203.txt</td>
<td>18441203.txt</td>
<td>55494.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18451202.txt</td>
<td>18451202.txt</td>
<td>95894.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18461208.txt</td>
<td>18461208.txt</td>
<td>107852.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18471207.txt</td>
<td>18471207.txt</td>
<td>96912.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18481205.txt</td>
<td>18481205.txt</td>
<td>127557.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18491204.txt</td>
<td>18491204.txt</td>
<td>46003.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18501202.txt</td>
<td>18501202.txt</td>
<td>49823.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18511202.txt</td>
<td>18511202.txt</td>
<td>79335.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18521206.txt</td>
<td>18521206.txt</td>
<td>59438.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18531205.txt</td>
<td>18531205.txt</td>
<td>58031.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18541204.txt</td>
<td>18541204.txt</td>
<td>61917.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18551231.txt</td>
<td>18551231.txt</td>
<td>70459.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18561202.txt</td>
<td>18561202.txt</td>
<td>63906.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18571208.txt</td>
<td>18571208.txt</td>
<td>82051.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18581206.txt</td>
<td>18581206.txt</td>
<td>98523.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18591219.txt</td>
<td>18591219.txt</td>
<td>74089.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18601203.txt</td>
<td>18601203.txt</td>
<td>84283.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18611203.txt</td>
<td>18611203.txt</td>
<td>41587.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18621201.txt</td>
<td>18621201.txt</td>
<td>50008.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18631208.txt</td>
<td>18631208.txt</td>
<td>37109.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18641206.txt</td>
<td>18641206.txt</td>
<td>36201.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18651204.txt</td>
<td>18651204.txt</td>
<td>54781.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18661203.txt</td>
<td>18661203.txt</td>
<td>44152.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18671203.txt</td>
<td>18671203.txt</td>
<td>71650.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18681209.txt</td>
<td>18681209.txt</td>
<td>60650.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18691206.txt</td>
<td>18691206.txt</td>
<td>46099.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18701205.txt</td>
<td>18701205.txt</td>
<td>52113.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18711204.txt</td>
<td>18711204.txt</td>
<td>38805.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18721202.txt</td>
<td>18721202.txt</td>
<td>23984.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18731201.txt</td>
<td>18731201.txt</td>
<td>60406.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18741207.txt</td>
<td>18741207.txt</td>
<td>55136.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18751207.txt</td>
<td>18751207.txt</td>
<td>73272.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18761205.txt</td>
<td>18761205.txt</td>
<td>40873.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18771203.txt</td>
<td>18771203.txt</td>
<td>48620.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18781202.txt</td>
<td>18781202.txt</td>
<td>48552.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18791201.txt</td>
<td>18791201.txt</td>
<td>71149.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18801206.txt</td>
<td>18801206.txt</td>
<td>41294.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18811206.txt</td>
<td>18811206.txt</td>
<td>24189.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18821204.txt</td>
<td>18821204.txt</td>
<td>19065.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18831204.txt</td>
<td>18831204.txt</td>
<td>23860.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18841201.txt</td>
<td>18841201.txt</td>
<td>55230.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18851208.txt</td>
<td>18851208.txt</td>
<td>121030.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18861206.txt</td>
<td>18861206.txt</td>
<td>92873.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18871206.txt</td>
<td>18871206.txt</td>
<td>31685.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18881203.txt</td>
<td>18881203.txt</td>
<td>55460.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18891203.txt</td>
<td>18891203.txt</td>
<td>77944.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18901201.txt</td>
<td>18901201.txt</td>
<td>69588.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18911209.txt</td>
<td>18911209.txt</td>
<td>96894.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18921206.txt</td>
<td>18921206.txt</td>
<td>81825.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18931203.txt</td>
<td>18931203.txt</td>
<td>76786.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18941202.txt</td>
<td>18941202.txt</td>
<td>97793.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18951207.txt</td>
<td>18951207.txt</td>
<td>89791.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18961204.txt</td>
<td>18961204.txt</td>
<td>94943.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18971206.txt</td>
<td>18971206.txt</td>
<td>72748.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/18981205.txt</td>
<td>18981205.txt</td>
<td>123819.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/18991205.txt</td>
<td>18991205.txt</td>
<td>93175.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19001203.txt</td>
<td>19001203.txt</td>
<td>118487.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19011203.txt</td>
<td>19011203.txt</td>
<td>115838.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19021202.txt</td>
<td>19021202.txt</td>
<td>57671.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19031207.txt</td>
<td>19031207.txt</td>
<td>90262.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19041206.txt</td>
<td>19041206.txt</td>
<td>104031.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19051205.txt</td>
<td>19051205.txt</td>
<td>147449.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19061203.txt</td>
<td>19061203.txt</td>
<td>138165.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19071203.txt</td>
<td>19071203.txt</td>
<td>161983.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19081208.txt</td>
<td>19081208.txt</td>
<td>115609.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19091207.txt</td>
<td>19091207.txt</td>
<td>84749.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19101206.txt</td>
<td>19101206.txt</td>
<td>42598.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19111205.txt</td>
<td>19111205.txt</td>
<td>143491.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19121203.txt</td>
<td>19121203.txt</td>
<td>153124.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19131202.txt</td>
<td>19131202.txt</td>
<td>20536.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19141208.txt</td>
<td>19141208.txt</td>
<td>25441.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19151207.txt</td>
<td>19151207.txt</td>
<td>44773.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19161205.txt</td>
<td>19161205.txt</td>
<td>12773.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19171204.txt</td>
<td>19171204.txt</td>
<td>22077.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19181202.txt</td>
<td>19181202.txt</td>
<td>31400.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19191202.txt</td>
<td>19191202.txt</td>
<td>28511.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19201207.txt</td>
<td>19201207.txt</td>
<td>16119.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19211206.txt</td>
<td>19211206.txt</td>
<td>34334.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19221208.txt</td>
<td>19221208.txt</td>
<td>35419.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19231206.txt</td>
<td>19231206.txt</td>
<td>41144.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19241203.txt</td>
<td>19241203.txt</td>
<td>42503.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19251208.txt</td>
<td>19251208.txt</td>
<td>66289.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19261207.txt</td>
<td>19261207.txt</td>
<td>62608.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19271206.txt</td>
<td>19271206.txt</td>
<td>54125.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19281204.txt</td>
<td>19281204.txt</td>
<td>50110.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19291203.txt</td>
<td>19291203.txt</td>
<td>68959.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19301202.txt</td>
<td>19301202.txt</td>
<td>29041.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19311208.txt</td>
<td>19311208.txt</td>
<td>36649.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19321206.txt</td>
<td>19321206.txt</td>
<td>26421.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19340103.txt</td>
<td>19340103.txt</td>
<td>13545.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19350104.txt</td>
<td>19350104.txt</td>
<td>21221.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19360103.txt</td>
<td>19360103.txt</td>
<td>22300.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19370106.txt</td>
<td>19370106.txt</td>
<td>16738.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19380103.txt</td>
<td>19380103.txt</td>
<td>28069.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19390104.txt</td>
<td>19390104.txt</td>
<td>22563.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19400103.txt</td>
<td>19400103.txt</td>
<td>18722.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19410106.txt</td>
<td>19410106.txt</td>
<td>19386.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19420106.txt</td>
<td>19420106.txt</td>
<td>19911.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19430107.txt</td>
<td>19430107.txt</td>
<td>26314.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19440111.txt</td>
<td>19440111.txt</td>
<td>22151.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19450106.txt</td>
<td>19450106.txt</td>
<td>48891.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19460121.txt</td>
<td>19460121.txt</td>
<td>174651.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19470106.txt</td>
<td>19470106.txt</td>
<td>37406.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19480107.txt</td>
<td>19480107.txt</td>
<td>30550.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19490105.txt</td>
<td>19490105.txt</td>
<td>20792.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19500104.txt</td>
<td>19500104.txt</td>
<td>30423.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19510108.txt</td>
<td>19510108.txt</td>
<td>22924.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19520109.txt</td>
<td>19520109.txt</td>
<td>30228.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19530107.txt</td>
<td>19530107.txt</td>
<td>56767.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19530202.txt</td>
<td>19530202.txt</td>
<td>43620.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19540107.txt</td>
<td>19540107.txt</td>
<td>37843.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19550106.txt</td>
<td>19550106.txt</td>
<td>46532.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19560105.txt</td>
<td>19560105.txt</td>
<td>52138.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19570110.txt</td>
<td>19570110.txt</td>
<td>25846.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19580109.txt</td>
<td>19580109.txt</td>
<td>30344.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19590109.txt</td>
<td>19590109.txt</td>
<td>30145.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19600107.txt</td>
<td>19600107.txt</td>
<td>35099.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19610112.txt</td>
<td>19610112.txt</td>
<td>40396.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19610130.txt</td>
<td>19610130.txt</td>
<td>31641.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19620111.txt</td>
<td>19620111.txt</td>
<td>39488.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19630114.txt</td>
<td>19630114.txt</td>
<td>31666.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19640108.txt</td>
<td>19640108.txt</td>
<td>18659.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19650104.txt</td>
<td>19650104.txt</td>
<td>25389.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19660112.txt</td>
<td>19660112.txt</td>
<td>30570.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19670110.txt</td>
<td>19670110.txt</td>
<td>41668.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19680117.txt</td>
<td>19680117.txt</td>
<td>28834.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19690114.txt</td>
<td>19690114.txt</td>
<td>23634.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19700122.txt</td>
<td>19700122.txt</td>
<td>25408.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19710122.txt</td>
<td>19710122.txt</td>
<td>25793.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19720120.txt</td>
<td>19720120.txt</td>
<td>23099.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19730202.txt</td>
<td>19730202.txt</td>
<td>9844.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19740130.txt</td>
<td>19740130.txt</td>
<td>29231.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19750115.txt</td>
<td>19750115.txt</td>
<td>24801.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19760119.txt</td>
<td>19760119.txt</td>
<td>29731.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19770112.txt</td>
<td>19770112.txt</td>
<td>27923.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19780119.txt</td>
<td>19780119.txt</td>
<td>26564.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19790125.txt</td>
<td>19790125.txt</td>
<td>19544.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19800121.txt</td>
<td>19800121.txt</td>
<td>20124.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19810116.txt</td>
<td>19810116.txt</td>
<td>217980.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19820126.txt</td>
<td>19820126.txt</td>
<td>31166.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19830125.txt</td>
<td>19830125.txt</td>
<td>33255.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19840125.txt</td>
<td>19840125.txt</td>
<td>29705.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19850206.txt</td>
<td>19850206.txt</td>
<td>25364.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19860204.txt</td>
<td>19860204.txt</td>
<td>20449.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19870127.txt</td>
<td>19870127.txt</td>
<td>22334.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19880125.txt</td>
<td>19880125.txt</td>
<td>28565.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19890209.txt</td>
<td>19890209.txt</td>
<td>27855.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19900131.txt</td>
<td>19900131.txt</td>
<td>21434.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19910129.txt</td>
<td>19910129.txt</td>
<td>22433.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19920128.txt</td>
<td>19920128.txt</td>
<td>26644.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19930217.txt</td>
<td>19930217.txt</td>
<td>39255.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19940125.txt</td>
<td>19940125.txt</td>
<td>42320.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19950124.txt</td>
<td>19950124.txt</td>
<td>51325.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19960123.txt</td>
<td>19960123.txt</td>
<td>36386.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19970204.txt</td>
<td>19970204.txt</td>
<td>39038.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/19980127.txt</td>
<td>19980127.txt</td>
<td>42255.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/19990119.txt</td>
<td>19990119.txt</td>
<td>43592.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20000127.txt</td>
<td>20000127.txt</td>
<td>44244.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20010227.txt</td>
<td>20010227.txt</td>
<td>25330.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20010920.txt</td>
<td>20010920.txt</td>
<td>17383.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20020129.txt</td>
<td>20020129.txt</td>
<td>22653.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20030128.txt</td>
<td>20030128.txt</td>
<td>31878.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20040120.txt</td>
<td>20040120.txt</td>
<td>30611.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20050202.txt</td>
<td>20050202.txt</td>
<td>29875.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20060131.txt</td>
<td>20060131.txt</td>
<td>31449.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20070123.txt</td>
<td>20070123.txt</td>
<td>31998.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20080128.txt</td>
<td>20080128.txt</td>
<td>33830.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20090224.txt</td>
<td>20090224.txt</td>
<td>33640.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20100127.txt</td>
<td>20100127.txt</td>
<td>40980.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20110125.txt</td>
<td>20110125.txt</td>
<td>39582.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20120124.txt</td>
<td>20120124.txt</td>
<td>40338.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20130212.txt</td>
<td>20130212.txt</td>
<td>37815.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20140128.txt</td>
<td>20140128.txt</td>
<td>39625.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20150120.txt</td>
<td>20150120.txt</td>
<td>38528.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/sou/20160112.txt</td>
<td>20160112.txt</td>
<td>31083.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/20170228.txt</td>
<td>20170228.txt</td>
<td>29323.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/datasets/&quot;))
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/datasets/MEP/</td>
<td>MEP/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/beijing/</td>
<td>beijing/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/graphhopper/</td>
<td>graphhopper/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/magellan/</td>
<td>magellan/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/maps/</td>
<td>maps/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/osm/</td>
<td>osm/</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td>dbfs:/datasets/sou/</td>
<td>sou/</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>dbfs:/datasets/twitterAccountsOfInterest.csv</td>
<td>twitterAccountsOfInterest.csv</td>
<td>32981.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val sou17900108 = sc.textFile(&quot;dbfs:/datasets/sou/17900108.txt&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>sou17900108: org.apache.spark.rdd.RDD[String] = dbfs:/datasets/sou/17900108.txt MapPartitionsRDD[51] at textFile at command-3718738704207872:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108.take(5)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res11: Array[String] = Array(&quot;George Washington &quot;, &quot;&quot;, &quot;January 8, 1790 &quot;, &quot;Fellow-Citizens of the Senate and House of Representatives: &quot;, &quot;I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity. &quot;)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108.collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res12: Array[String] = Array(&quot;George Washington &quot;, &quot;&quot;, &quot;January 8, 1790 &quot;, &quot;Fellow-Citizens of the Senate and House of Representatives: &quot;, &quot;I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity. &quot;, &quot;In resuming your consultations for the general good you can not but derive encouragement from the reflection that the measures of the last session have been as satisfactory to your constituents as the novelty and difficulty of the work allowed you to hope. Still further to realize their expectations and to secure the blessings which a gracious Providence has placed within our reach will in the course of the present important session call for the cool and deliberate exertion of your patriotism, firmness, and wisdom. &quot;, &quot;Among the many interesting objects which will engage your attention that of providing for the common defense will merit particular regard. To be prepared for war is one of the most effectual means of preserving peace. &quot;, &quot;A free people ought not only to be armed, but disciplined; to which end a uniform and well-digested plan is requisite; and their safety and interest require that they should promote such manufactories as tend to render them independent of others for essential, particularly military, supplies. &quot;, &quot;The proper establishment of the troops which may be deemed indispensable will be entitled to mature consideration. In the arrangements which may be made respecting it it will be of importance to conciliate the comfortable support of the officers and soldiers with a due regard to economy. &quot;, &quot;There was reason to hope that the pacific measures adopted with regard to certain hostile tribes of Indians would have relieved the inhabitants of our southern and western frontiers from their depredations, but you will perceive from the information contained in the papers which I shall direct to be laid before you (comprehending a communication from the Commonwealth of Virginia) that we ought to be prepared to afford protection to those parts of the Union, and, if necessary, to punish aggressors. &quot;, &quot;The interests of the United States require that our intercourse with other nations should be facilitated by such provisions as will enable me to fulfill my duty in that respect in the manner which circumstances may render most conducive to the public good, and to this end that the compensation to be made to the persons who may be employed should, according to the nature of their appointments, be defined by law, and a competent fund designated for defraying the expenses incident to the conduct of foreign affairs. &quot;, &quot;Various considerations also render it expedient that the terms on which foreigners may be admitted to the rights of citizens should be speedily ascertained by a uniform rule of naturalization. &quot;, &quot;Uniformity in the currency, weights, and measures of the United States is an object of great importance, and will, I am persuaded, be duly attended to. &quot;, &quot;The advancement of agriculture, commerce, and manufactures by all proper means will not, I trust, need recommendation; but I can not forbear intimating to you the expediency of giving effectual encouragement as well to the introduction of new and useful inventions from abroad as to the exertions of skill and genius in producing them at home, and of facilitating the intercourse between the distant parts of our country by a due attention to the post-office and post-roads. &quot;, &quot;Nor am I less persuaded that you will agree with me in opinion that there is nothing which can better deserve your patronage than the promotion of science and literature. Knowledge is in every country the surest basis of public happiness. In one in which the measures of government receive their impressions so immediately from the sense of the community as in ours it is proportionably essential. &quot;, &quot;To the security of a free constitution it contributes in various ways--by convincing those who are intrusted with the public administration that every valuable end of government is best answered by the enlightened confidence of the people, and by teaching the people themselves to know and to value their own rights; to discern and provide against invasions of them; to distinguish between oppression and the necessary exercise of lawful authority; between burthens proceeding from a disregard to their convenience and those resulting from the inevitable exigencies of society; to discriminate the spirit of liberty from that of licentiousness-- cherishing the first, avoiding the last--and uniting a speedy but temperate vigilance against encroachments, with an inviolable respect to the laws. &quot;, &quot;Whether this desirable object will be best promoted by affording aids to seminaries of learning already established, by the institution of a national university, or by any other expedients will be well worthy of a place in the deliberations of the legislature. &quot;, &quot;Gentlemen of the House of Representatives: &quot;, &quot;I saw with peculiar pleasure at the close of the last session the resolution entered into by you expressive of your opinion that an adequate provision for the support of the public credit is a matter of high importance to the national honor and prosperity. In this sentiment I entirely concur; and to a perfect confidence in your best endeavors to devise such a provision as will be truly with the end I add an equal reliance on the cheerful cooperation of the other branch of the legislature. &quot;, &quot;It would be superfluous to specify inducements to a measure in which the character and interests of the United States are so obviously so deeply concerned, and which has received so explicit a sanction from your declaration. &quot;, &quot;Gentlemen of the Senate and House of Representatives: &quot;, &quot;I have directed the proper officers to lay before you, respectively, such papers and estimates as regard the affairs particularly recommended to your consideration, and necessary to convey to you that information of the state of the Union which it is my duty to afford. &quot;, The welfare of our country is the great object to which our cares and efforts ought to be directed, and I shall derive great satisfaction from a cooperation with you in the pleasing though arduous task of insuring to our fellow citizens the blessings which they have a right to expect from a free, efficient, and equal government.)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108.takeOrdered(5)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res13: Array[String] = Array(&quot;&quot;, &quot;A free people ought not only to be armed, but disciplined; to which end a uniform and well-digested plan is requisite; and their safety and interest require that they should promote such manufactories as tend to render them independent of others for essential, particularly military, supplies. &quot;, &quot;Among the many interesting objects which will engage your attention that of providing for the common defense will merit particular regard. To be prepared for war is one of the most effectual means of preserving peace. &quot;, &quot;Fellow-Citizens of the Senate and House of Representatives: &quot;, &quot;Gentlemen of the House of Representatives: &quot;)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val souAll = sc.wholeTextFiles(&quot;dbfs:/datasets/sou/*.txt&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>souAll: org.apache.spark.rdd.RDD[(String, String)] = dbfs:/datasets/sou/*.txt MapPartitionsRDD[53] at wholeTextFiles at command-3718738704207876:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">souAll.count // should be 231
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res14: Long = 231
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="true">
<pre><code class="language-scala">souAll.take(2)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res15: Array[(String, String)] =
Array((dbfs:/datasets/sou/17900108.txt,&quot;George Washington

January 8, 1790
Fellow-Citizens of the Senate and House of Representatives:
I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity.
In resuming your consultations for the general good you can not but derive encouragement from the reflection that the measures of the last session have been as satisfactory to your constituents as the novelty and difficulty of the work allowed you to hope. Still further to realize their expectations and to secure the blessings which a gracious Providence has placed within our reach will in the course of the present important session call for the cool and deliberate exertion of your patriotism, firmness, and wisdom.
Among the many interesting objects which will engage your attention that of providing for the common defense will merit particular regard. To be prepared for war is one of the most effectual means of preserving peace.
A free people ought not only to be armed, but disciplined; to which end a uniform and well-digested plan is requisite; and their safety and interest require that they should promote such manufactories as tend to render them independent of others for essential, particularly military, supplies.
The proper establishment of the troops which may be deemed indispensable will be entitled to mature consideration. In the arrangements which may be made respecting it it will be of importance to conciliate the comfortable support of the officers and soldiers with a due regard to economy.
There was reason to hope that the pacific measures adopted with regard to certain hostile tribes of Indians would have relieved the inhabitants of our southern and western frontiers from their depredations, but you will perceive from the information contained in the papers which I shall direct to be laid before you (comprehending a communication from the Commonwealth of Virginia) that we ought to be prepared to afford protection to those parts of the Union, and, if necessary, to punish aggressors.
The interests of the United States require that our intercourse with other nations should be facilitated by such provisions as will enable me to fulfill my duty in that respect in the manner which circumstances may render most conducive to the public good, and to this end that the compensation to be made to the persons who may be employed should, according to the nature of their appointments, be defined by law, and a competent fund designated for defraying the expenses incident to the conduct of foreign affairs.
Various considerations also render it expedient that the terms on which foreigners may be admitted to the rights of citizens should be speedily ascertained by a uniform rule of naturalization.
Uniformity in the currency, weights, and measures of the United States is an object of great importance, and will, I am persuaded, be duly attended to.
The advancement of agriculture, commerce, and manufactures by all proper means will not, I trust, need recommendation; but I can not forbear intimating to you the expediency of giving effectual encouragement as well to the introduction of new and useful inventions from abroad as to the exertions of skill and genius in producing them at home, and of facilitating the intercourse between the distant parts of our country by a due attention to the post-office and post-roads.
Nor am I less persuaded that you will agree with me in opinion that there is nothing which can better deserve your patronage than the promotion of science and literature. Knowledge is in every country the surest basis of public happiness. In one in which the measures of government receive their impressions so immediately from the sense of the community as in ours it is proportionably essential.
To the security of a free constitution it contributes in various ways--by convincing those who are intrusted with the public administration that every valuable end of government is best answered by the enlightened confidence of the people, and by teaching the people themselves to know and to value their own rights; to discern and provide against invasions of them; to distinguish between oppression and the necessary exercise of lawful authority; between burthens proceeding from a disregard to their convenience and those resulting from the inevitable exigencies of society; to discriminate the spirit of liberty from that of licentiousness-- cherishing the first, avoiding the last--and uniting a speedy but temperate vigilance against encroachments, with an inviolable respect to the laws.
Whether this desirable object will be best promoted by affording aids to seminaries of learning already established, by the institution of a national university, or by any other expedients will be well worthy of a place in the deliberations of the legislature.
Gentlemen of the House of Representatives:
I saw with peculiar pleasure at the close of the last session the resolution entered into by you expressive of your opinion that an adequate provision for the support of the public credit is a matter of high importance to the national honor and prosperity. In this sentiment I entirely concur; and to a perfect confidence in your best endeavors to devise such a provision as will be truly with the end I add an equal reliance on the cheerful cooperation of the other branch of the legislature.
It would be superfluous to specify inducements to a measure in which the character and interests of the United States are so obviously so deeply concerned, and which has received so explicit a sanction from your declaration.
Gentlemen of the Senate and House of Representatives:
I have directed the proper officers to lay before you, respectively, such papers and estimates as regard the affairs particularly recommended to your consideration, and necessary to convey to you that information of the state of the Union which it is my duty to afford.
The welfare of our country is the great object to which our cares and efforts ought to be directed, and I shall derive great satisfaction from a cooperation with you in the pleasing though arduous task of insuring to our fellow citizens the blessings which they have a right to expect from a free, efficient, and equal government.
&quot;), (dbfs:/datasets/sou/17901208.txt,&quot;George Washington

December 8, 1790
Fellow-Citizens of the Senate and House of Representatives:
In meeting you again I feel much satisfaction in being able to repeat my congratulations on the favorable prospects which continue to distinguish our public affairs. The abundant fruits of another year have blessed our country with plenty and with the means of a flourishing commerce.
The progress of public credit is witnessed by a considerable rise of American stock abroad as well as at home, and the revenues allotted for this and other national purposes have been productive beyond the calculations by which they were regulated. This latter circumstance is the more pleasing, as it is not only a proof of the fertility of our resources, but as it assures us of a further increase of the national respectability and credit, and, let me add, as it bears an honorable testimony to the patriotism and integrity of the mercantile and marine part of our citizens. The punctuality of the former in discharging their engagements has been exemplary.
In conformity to the powers vested in me by acts of the last session, a loan of 3,000,000 florins, toward which some provisional measures had previously taken place, has been completed in Holland. As well the celerity with which it has been filled as the nature of the terms (considering the more than ordinary demand for borrowing created by the situation of Europe) give a reasonable hope that the further execution of those powers may proceed with advantage and success. The Secretary of the Treasury has my directions to communicate such further particulars as may be requisite for more precise information.
Since your last sessions I have received communications by which it appears that the district of Kentucky, at present a part of Virginia, has concurred in certain propositions contained in a law of that State, in consequence of which the district is to become a distinct member of the Union, in case the requisite sanction of Congress be added. For this sanction application is now made. I shall cause the papers on this very transaction to be laid before you.
The liberality and harmony with which it has been conducted will be found to do great honor to both the parties, and the sentiments of warm attachment to the Union and its present Government expressed by our fellow citizens of Kentucky can not fail to add an affectionate concern for their particular welfare to the great national impressions under which you will decide on the case submitted to you.
It has been heretofore known to Congress that frequent incursions have been made on our frontier settlements by certain banditti of Indians from the northwest side of the Ohio. These, with some of the tribes dwelling on and near the Wabash, have of late been particularly active in their depredations, and being emboldened by the impunity of their crimes and aided by such parts of the neighboring tribes as could be seduced to join in their hostilities or afford them a retreat for their prisoners and plunder, they have, instead of listening to the humane invitations and overtures made on the part of the United States, renewed their violences with fresh alacrity and greater effect. The lives of a number of valuable citizens have thus been sacrificed, and some of them under circumstances peculiarly shocking, whilst others have been carried into a deplorable captivity.
These aggravated provocations rendered it essential to the safety of the Western settlements that the aggressors should be made sensible that the Government of the Union is not less capable of punishing their crimes than it is disposed to respect their rights and reward their attachments. As this object could not be effected by defensive measures, it became necessary to put in force the act which empowers the President to call out the militia for the protection of the frontiers, and I have accordingly authorized an expedition in which the regular troops in that quarter are combined with such drafts of militia as were deemed sufficient. The event of the measure is yet unknown to me. The Secretary of War is directed to lay before you a statement of the information on which it is founded, as well as an estimate of the expense with which it will be attended.
The disturbed situation of Europe, and particularly the critical posture of the great maritime powers, whilst it ought to make us the more thankful for the general peace and security enjoyed by the United States, reminds us at the same time of the circumspection with which it becomes us to preserve these blessings. It requires also that we should not overlook the tendency of a war, and even of preparations for a war, among the nations most concerned in active commerce with this country to abridge the means, and thereby at least enhance the price, of transporting its valuable productions to their markets. I recommend it to your serious reflections how far and in what mode it may be expedient to guard against embarrassments from these contingencies by such encouragements to our own navigation as will render our commerce and agriculture less dependent on foreign bottoms, which may fail us in the very moments most interesting to both of these great objects. Our fisheries and the transportation of our own produce offer us abundant means for guarding ourselves against this evil.
Your attention seems to be not less due to that particular branch of our trade which belongs to the Mediterranean. So many circumstances unite in rendering the present state of it distressful to us that you will not think any deliberations misemployed which may lead to its relief and protection.
The laws you have already passed for the establishment of a judiciary system have opened the doors of justice to all descriptions of persons. You will consider in your wisdom whether improvements in that system may yet be made, and particularly whether an uniform process of execution on sentences issuing from the Federal courts be not desirable through all the States.
The patronage of our commerce, of our merchants and sea men, has called for the appointment of consuls in foreign countries. It seems expedient to regulate by law the exercise of that jurisdiction and those functions which are permitted them, either by express convention or by a friendly indulgence, in the places of their residence. The consular convention, too, with His Most Christian Majesty has stipulated in certain cases the aid of the national authority to his consuls established here. Some legislative provision is requisite to carry these stipulations into full effect.
The establishment of the militia, of a mint, of standards of weights and measures, of the post office and post roads are subjects which I presume you will resume of course, and which are abundantly urged by their own importance.
Gentlemen of the House of Representatives:
The sufficiency of the revenues you have established for the objects to which they are appropriated leaves no doubt that the residuary provisions will be commensurate to the other objects for which the public faith stands now pledged. Allow me, moreover, to hope that it will be a favorite policy with you, not merely to secure a payment of the interest of the debt funded, but as far and as fast as the growing resources of the country will permit to exonerate it of the principal itself. The appropriation you have made of the Western land explains your dispositions on this subject, and I am persuaded that the sooner that valuable fund can be made to contribute, along with the other means, to the actual reduction of the public debt the more salutary will the measure be to every public interest, as well as the more satisfactory to our constituents.
Gentlemen of the Senate and House of Representatives:
In pursuing the various and weighty business of the present session I indulge the fullest persuasion that your consultation will be equally marked with wisdom and animated by the love of your country. In whatever belongs to my duty you shall have all the cooperation which an undiminished zeal for its welfare can inspire. It will be happy for us both, and our best reward, if, by a successful administration of our respective trusts, we can make the established Government more and more instrumental in promoting the good of our fellow citizens, and more and more the object of their attachment and confidence.
GO. WASHINGTON
&quot;))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// This is a sequence of functions that can be accessed by other notebooks using the following %run command:
// %run &quot;/scalable-data-science/xtraResources/support/sdsFunctions&quot;
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//This allows easy embedding of publicly available information into any other notebook
//when viewing in git-book just ignore this block - you may have to manually chase the URL in frameIt(&quot;URL&quot;).
//Example usage:
// displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;,250))
def frameIt( u:String, h:Int ) : String = {
      &quot;&quot;&quot;&lt;iframe 
 src=&quot;&quot;&quot;&quot;+ u+&quot;&quot;&quot;&quot;
 width=&quot;95%&quot; height=&quot;&quot;&quot;&quot; + h + &quot;&quot;&quot;&quot;
 sandbox&gt;
  &lt;p&gt;
    &lt;a href=&quot;http://spark.apache.org/docs/latest/index.html&quot;&gt;
      Fallback link for browsers that, unlikely, don't support frames
    &lt;/a&gt;
  &lt;/p&gt;
&lt;/iframe&gt;&quot;&quot;&quot;
   }
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>frameIt: (u: String, h: Int)String
</code></pre>
</div>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="a-visual-guide-to-the-transformations-and-actions-on-rdds"><a class="header" href="#a-visual-guide-to-the-transformations-and-actions-on-rdds">A visual guide to the transformations and actions on RDDs</a></h1>
<h1 id="transformations"><a class="header" href="#transformations">Transformations</a></h1>
<ol>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/map">map</a></li>
</ol>
<ul>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/filter">filter</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/flatMap">flatMap</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/groupBy">groupBy</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/groupByKey">groupByKey</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/reduceByKey">reduceByKey</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/mapPartitions">mapPartitions</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/mapPartitionsWithIndex">mapPartitionsWithIndex</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/sample">sample</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/union">union</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/join">join</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/distinct">distinct</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/coalesce">coalesce</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/keyBy">keyBy</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/partitionBy">partitionBy</a></li>
</ul>
<h1 id="actions"><a class="header" href="#actions">Actions</a></h1>
<ol>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/getNumPartitions">getNumPartitions</a></li>
</ol>
<ul>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/collect">collect</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/reduce">reduce</a></li>
<li>...</li>
</ul>
<p><strong>Volunteers please...</strong> make PRs with the programing guide completed for ALL functions for which PNG images exist in: * <a href="https://github.com/lamastex/scalable-data-science/tree/master/db/visualapi/med">https://github.com/lamastex/scalable-data-science/tree/master/db/visualapi/med</a></p>
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-0.png" alt="" /></p>
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-1.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-3.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-4.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-5.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-6.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-7.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-9.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-8.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-10.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-11.png" alt="" /></p>
</div>
<div class="cell markdown">
<h1 id="transformations-1"><a class="header" href="#transformations-1">Transformations</a></h1>
<ol>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/map">map</a></li>
</ol>
<ul>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/filter">filter</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/flatMap">flatMap</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/groupBy">groupBy</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/groupByKey">groupByKey</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/reduceByKey">reduceByKey</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/mapPartitions">mapPartitions</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/mapPartitionsWithIndex">mapPartitionsWithIndex</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/sample">sample</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/union">union</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/join">join</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/distinct">distinct</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/coalesce">coalesce</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/keyBy">keyBy</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/partitionBy">partitionBy</a></li>
</ul>
</div>
<div class="cell markdown">
<p>Continue from:</p>
</div>
<div class="cell markdown">
<hr />
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-85.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-86.png" alt="" /></p>
</div>
<div class="cell markdown">
<h1 id="actions-1"><a class="header" href="#actions-1">Actions</a></h1>
<ol>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/getNumPartitions">getNumPartitions</a></li>
</ol>
<ul>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/collect">collect</a></li>
<li><a href="contents/xtraResources/visualRDDApi//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/reduce">reduce</a></li>
</ul>
</div>
<div class="cell markdown">
<p>Continue from:</p>
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-95.png" alt="" /></p>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="collect"><a class="header" href="#collect">collect</a></h1>
<p>Return all items in the RDD to the driver in a single list.</p>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/actions//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-89.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-90.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(1,2,3), 2) // make a RDD with two partitions
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[89108] at parallelize at &lt;console&gt;:34
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// simply returns all elements in RDD x to the driver as an Array
val y = x.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: Array[Int] = Array(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//glom() flattens elements on the same partition
val xOut = x.glom().collect() 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>xOut: Array[Array[Int]] = Array(Array(1), Array(2, 3))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="getnumpartitions"><a class="header" href="#getnumpartitions">getNumPartitions</a></h1>
<p>Return the number of partitions in RDD.</p>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/actions//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-87.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-88.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(1,2,3), 2) // RDD with 2 partitions
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[655] at parallelize at &lt;console&gt;:34
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val y = x.partitions.size
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: Int = 2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//glom() flattens elements on the same partition
val xOut = x.glom().collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>xOut: Array[Array[Int]] = Array(Array(1), Array(2, 3))
</code></pre>
</div>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="reduce"><a class="header" href="#reduce">reduce</a></h1>
<p>Aggregate all the elements of the RDD by applying a user function pairwise to elements and partial results, and returns a result to the driver.</p>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/actions//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-91.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-92.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-93.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-94.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(1,2,3,4))
val y = x.reduce((a,b) =&gt; a+b)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[944] at parallelize at &lt;console&gt;:34
y: Int = 10
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(x.collect.mkString(&quot;, &quot;))
println(y)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1, 2, 3, 4
10
</code></pre>
</div>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="coalesce"><a class="header" href="#coalesce">coalesce</a></h1>
<p>Return a new RDD which is reduced to a smaller number of partitions.</p>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/transformations//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-64.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-65.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-66.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-67.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(1,2,3,4,5), 3) // make RDD with 3 partitions
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1419] at parallelize at &lt;console&gt;:34
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: Array[Int] = Array(1, 2, 3, 4, 5)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val y = x.coalesce(2) // coalesce RDD x from 3 to 2 partitions
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[1428] at coalesce at &lt;console&gt;:36
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">y.collect() // x and y will yield the same list to driver upon collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res1: Array[Int] = Array(1, 2, 3, 4, 5)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//glom() flattens elements on the same partition
val xOut = x.glom().collect() // x has three partitions
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>xOut: Array[Array[Int]] = Array(Array(1), Array(2, 3), Array(4, 5))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val yOut = y.glom().collect() // However y has only 2 partitions
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>yOut: Array[Array[Int]] = Array(Array(1), Array(2, 3, 4, 5))
</code></pre>
</div>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="distinct"><a class="header" href="#distinct">distinct</a></h1>
<p>Return a new RDD containing distinct items from the original RDD (omitting all duplicates).</p>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/transformations//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-60.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-61.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-62.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-63.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(1,2,3,3,4))
val y = x.distinct()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1320] at parallelize at &lt;console&gt;:36
y: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1323] at distinct at &lt;console&gt;:37
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">y.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Array[Int] = Array(4, 2, 1, 3)
</code></pre>
</div>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="filter"><a class="header" href="#filter">filter</a></h1>
<p>Return a new RDD containing only elements of this RDD that satisfy a predicate.</p>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/transformations//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-19.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-20.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-21.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-22.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-23.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-24.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(1,2,3))
val y = x.filter(n =&gt; n%2 == 1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[374] at parallelize at &lt;console&gt;:34
y: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[375] at filter at &lt;console&gt;:35
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(x.collect().mkString(&quot;, &quot;))
println(y.collect().mkString(&quot;, &quot;))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1, 2, 3
1, 3
</code></pre>
</div>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="flatmap"><a class="header" href="#flatmap">flatMap</a></h1>
<p>Return a new RDD by first applying a function to each element of this RDD and then flattening the results.</p>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/transformations//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-25.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-26.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-27.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-28.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-29.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-30.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-31.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(1,2,3))
val y = x.flatMap(n =&gt; Array(n, n*100, 42))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[289] at parallelize at &lt;console&gt;:34
y: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[290] at flatMap at &lt;console&gt;:35
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(x.collect().mkString(&quot;, &quot;))
println(y.collect().mkString(&quot;, &quot;))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1, 2, 3
1, 100, 42, 2, 200, 42, 3, 300, 42
</code></pre>
</div>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="groupbykey"><a class="header" href="#groupbykey">groupByKey</a></h1>
<p>Group the values for each key in the original RDD. Create a new pair where the original key corresponds to this collected group of values.</p>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/transformations//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-38.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-39.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-40.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-41.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(('B',5),('B',4),('A',3),('A',2),('A',1)))
val y = x.groupByKey()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[(Char, Int)] = ParallelCollectionRDD[476] at parallelize at &lt;console&gt;:34
y: org.apache.spark.rdd.RDD[(Char, Iterable[Int])] = ShuffledRDD[477] at groupByKey at &lt;console&gt;:35
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(x.collect().mkString(&quot;, &quot;))
println(y.collect().mkString(&quot;, &quot;))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>(B,5), (B,4), (A,3), (A,2), (A,1)
(B,CompactBuffer(5, 4)), (A,CompactBuffer(3, 2, 1))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="groupby"><a class="header" href="#groupby">groupBy</a></h1>
<p>Group the data in the original RDD. Create pairs where the key is the output of a user function, and the value is all items for which the function yield this key.</p>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/transformations//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-32.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-33.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-34.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-35.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-36.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-37.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(&quot;John&quot;, &quot;Fred&quot;, &quot;Anna&quot;, &quot;James&quot;))
val y = x.groupBy(w =&gt; w.charAt(0))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[366] at parallelize at &lt;console&gt;:34
y: org.apache.spark.rdd.RDD[(Char, Iterable[String])] = ShuffledRDD[368] at groupBy at &lt;console&gt;:35
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(x.collect().mkString(&quot;, &quot;))
println(y.collect().mkString(&quot;, &quot;))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>John, Fred, Anna, James
(J,CompactBuffer(John, James)), (F,CompactBuffer(Fred)), (A,CompactBuffer(Anna))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="join"><a class="header" href="#join">join</a></h1>
<p>Return a new RDD containing all pairs of elements having the same key in the original RDDs.</p>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/transformations//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-55.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-56.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-57.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-58.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-59.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array((&quot;a&quot;, 1), (&quot;b&quot;, 2)))
val y = sc.parallelize(Array((&quot;a&quot;, 3), (&quot;a&quot;, 4), (&quot;b&quot;, 5)))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[1159] at parallelize at &lt;console&gt;:34
y: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[1160] at parallelize at &lt;console&gt;:35
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val z = x.join(y)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>z: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[1178] at join at &lt;console&gt;:38
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">z.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Array[(String, (Int, Int))] = Array((b,(2,5)), (a,(1,3)), (a,(1,4)))
</code></pre>
</div>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="keyby"><a class="header" href="#keyby">keyBy</a></h1>
<p>Create a Pair RDD, forming one pair for each item in the original RDD. The pair's key is calculated from the value via a user-supplied function.</p>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/transformations//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-68.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-69.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-70.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-71.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-72.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(&quot;John&quot;, &quot;Fred&quot;, &quot;Anna&quot;, &quot;James&quot;))
val y = x.keyBy(w =&gt; w.charAt(0))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(y.collect().mkString(&quot;, &quot;))
</code></pre>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="map"><a class="header" href="#map">map</a></h1>
<p>Return a new RDD by applying a function to each element of this RDD.</p>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/transformations//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-12.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-13.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-14.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-15.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-16.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-17.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-18.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(&quot;b&quot;, &quot;a&quot;, &quot;c&quot;))
val y = x.map(z =&gt; (z,1))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[176] at parallelize at &lt;console&gt;:36
y: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[177] at map at &lt;console&gt;:37
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(x.collect().mkString(&quot;,&quot;))
println(y.collect().mkString(&quot;,&quot;))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>b,a,c
(b,1),(a,1),(c,1)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="mappartitions"><a class="header" href="#mappartitions">mapPartitions</a></h1>
<p>Return a new RDD by applying a function to each partition of the RDD.</p>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/transformations//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-42.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-47.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(1,2,3), 2) // RDD with 2 partitions
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:34
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def f(i:Iterator[Int])={ (i.sum, 42).productIterator }
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>f: (i: Iterator[Int])Iterator[Any]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val y = x.mapPartitions(f)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: org.apache.spark.rdd.RDD[Any] = MapPartitionsRDD[12] at mapPartitions at &lt;console&gt;:38
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// glom() flattens elements on the same partition
val xOut = x.glom().collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>xOut: Array[Array[Int]] = Array(Array(1), Array(2, 3))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val yOut = y.glom().collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>yOut: Array[Array[Any]] = Array(Array(1, 42), Array(5, 42))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="mappartitionswithindex"><a class="header" href="#mappartitionswithindex">mapPartitionsWithIndex</a></h1>
<p>Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition.</p>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/transformations//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-48.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-50.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc. parallelize(Array(1,2,3), 2) // make an RDD with 2 partitions
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[289] at parallelize at &lt;console&gt;:34
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def f(partitionIndex:Int, i:Iterator[Int]) = {
  (partitionIndex, i.sum).productIterator
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>f: (partitionIndex: Int, i: Iterator[Int])Iterator[Any]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val y = x.mapPartitionsWithIndex(f)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: org.apache.spark.rdd.RDD[Any] = MapPartitionsRDD[304] at mapPartitionsWithIndex at &lt;console&gt;:38
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//glom() flattens elements on the same partition
val xout = x.glom().collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>xout: Array[Array[Int]] = Array(Array(1), Array(2, 3))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val yout = y.glom().collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>yout: Array[Array[Any]] = Array(Array(0, 1), Array(1, 5))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="partitionby"><a class="header" href="#partitionby">partitionBy</a></h1>
<p>Return a new RDD with the specified number of partitions, placing original items into the partition returned by a user supplied function.</p>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/transformations//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-73.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-74.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-75.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-76.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-77.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-78.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-79.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.Partitioner
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.Partitioner
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(('J',&quot;James&quot;),('F',&quot;Fred&quot;), ('A',&quot;Anna&quot;),('J',&quot;John&quot;)), 3)
val y = x.partitionBy(new Partitioner() {
  val numPartitions = 2
  def getPartition(k:Any) = {
    if (k.asInstanceOf[Char] &lt; 'H') 0 else 1
  }
})
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[(Char, String)] = ParallelCollectionRDD[328] at parallelize at &lt;console&gt;:37
y: org.apache.spark.rdd.RDD[(Char, String)] = ShuffledRDD[329] at partitionBy at &lt;console&gt;:38
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val yOut = y.glom().collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>yOut: Array[Array[(Char, String)]] = Array(Array((F,Fred), (A,Anna)), Array((J,James), (J,John)))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="reducebykey"><a class="header" href="#reducebykey">reduceByKey</a></h1>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/transformations//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
<p>Also see <a href="contents/xtraResources/visualRDDApi/recall/transformations//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/groupByKey">groupByKey</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-42.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-43.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val words = Array(&quot;one&quot;, &quot;two&quot;, &quot;two&quot;, &quot;three&quot;, &quot;three&quot;, &quot;three&quot;)
val wordPairsRDD = sc.parallelize(words).map(word =&gt; (word, 1))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>words: Array[String] = Array(one, two, two, three, three, three)
wordPairsRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[650] at map at &lt;console&gt;:35
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wordCountsWithReduce = wordPairsRDD
                              .reduceByKey(_ + _)
                              .collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wordCountsWithReduce: Array[(String, Int)] = Array((two,2), (one,1), (three,3))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wordCountsWithGroup = wordPairsRDD
                              .groupByKey()
                              .map(t =&gt; (t._1, t._2.sum))
                              .collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wordCountsWithGroup: Array[(String, Int)] = Array((two,2), (one,1), (three,3))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-44.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-45.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="sample"><a class="header" href="#sample">sample</a></h1>
<p>Return a new RDD containing a statistical sample of the original RDD.</p>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/transformations//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-51.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-52.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(1, 2, 3, 4, 5))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[789] at parallelize at &lt;console&gt;:34
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//omitting seed will return different output
val y = x.sample(false, 0.4)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[805] at sample at &lt;console&gt;:37
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">y.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res1: Array[Int] = Array(2, 5)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//omitting seed will return different output
x.sample(false, 0.4).collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Array[Int] = Array(4, 5)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//including seed will preserve output
sc.parallelize(Seq.range(0, 1000)).sample(false, 0.1, 124L).collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res14: Array[Int] = Array(9, 18, 27, 36, 42, 48, 49, 68, 79, 93, 94, 100, 105, 110, 114, 123, 127, 133, 135, 161, 177, 180, 181, 193, 198, 201, 222, 227, 236, 254, 263, 264, 272, 277, 287, 297, 325, 328, 336, 338, 350, 352, 353, 359, 372, 385, 409, 416, 428, 439, 443, 450, 461, 486, 489, 528, 533, 542, 543, 560, 561, 562, 563, 575, 576, 577, 578, 586, 590, 604, 614, 636, 682, 685, 723, 729, 730, 742, 749, 750, 785, 790, 809, 815, 822, 832, 865, 871, 888, 893, 906, 920, 936, 958, 982, 991, 994)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//including seed will preserve output
sc.parallelize(Seq.range(0, 1000)).sample(false, 0.1, 124L).collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res15: Array[Int] = Array(9, 18, 27, 36, 42, 48, 49, 68, 79, 93, 94, 100, 105, 110, 114, 123, 127, 133, 135, 161, 177, 180, 181, 193, 198, 201, 222, 227, 236, 254, 263, 264, 272, 277, 287, 297, 325, 328, 336, 338, 350, 352, 353, 359, 372, 385, 409, 416, 428, 439, 443, 450, 461, 486, 489, 528, 533, 542, 543, 560, 561, 562, 563, 575, 576, 577, 578, 586, 590, 604, 614, 636, 682, 685, 723, 729, 730, 742, 749, 750, 785, 790, 809, 815, 822, 832, 865, 871, 888, 893, 906, 920, 936, 958, 982, 991, 994)
</code></pre>
</div>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<h1 id="union"><a class="header" href="#union">union</a></h1>
<p>Return a new RDD containing all items from two original RDDs. Duplicates are not culled (this is concatenation not set-theoretic union).</p>
<p>Let us look at the <a href="contents/xtraResources/visualRDDApi/recall/transformations//#workspace/scalable-data-science/xtraResources/visualRDDApi/guide">legend and overview of the visual RDD Api</a>.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-53.png" alt="" /></p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-54.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(1, 2, 3), 2) // an RDD with 2 partitions
val y = sc.parallelize(Array(3, 4), 1) // another RDD with 1 partition
val z = x.union(y)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[965] at parallelize at &lt;console&gt;:36
y: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[966] at parallelize at &lt;console&gt;:37
z: org.apache.spark.rdd.RDD[Int] = UnionRDD[967] at union at &lt;console&gt;:38
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//glom() flattens elements on the same partition
val zOut = z.glom().collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>zOut: Array[Array[Int]] = Array(Array(1), Array(2, 3), Array(3, 4))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
        
        

    </body>
</html>
