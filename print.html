<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>ScaDaMaLe/sds-3.x</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
        <link rel="stylesheet" href="scroll-mdbook-outputs.css">
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/000_ScaDaMaLe.html">000_ScaDaMaLe</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/001_whySpark.html">001_whySpark</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/002_00_loginToDatabricks.html">002_00_loginToDatabricks</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/002_01_multiLingualNotebooks.html">002_01_multiLingualNotebooks</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/003_00_scalaCrashCourse.html">003_00_scalaCrashCourse</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/003_01_scalaCrashCourse.html">003_01_scalaCrashCourse</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/004_RDDsTransformationsActions.html">004_RDDsTransformationsActions</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/005_RDDsTransformationsActionsHOMEWORK.html">005_RDDsTransformationsActionsHOMEWORK</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/006a_PipedRDD.html">006a_PipedRDD</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/006_WordCount.html">006_WordCount</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/007a_SparkSQLProgGuide_HW.html">007a_SparkSQLProgGuide_HW</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/007b_SparkSQLProgGuide_HW.html">007b_SparkSQLProgGuide_HW</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/007c_SparkSQLProgGuide_HW.html">007c_SparkSQLProgGuide_HW</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/007d_SparkSQLProgGuide_HW.html">007d_SparkSQLProgGuide_HW</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/007e_SparkSQLProgGuide_HW.html">007e_SparkSQLProgGuide_HW</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/007f_SparkSQLProgGuide_HW.html">007f_SparkSQLProgGuide_HW</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/007g_PivotInSQL.html">007g_PivotInSQL</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/007_SparkSQLIntroBasics.html">007_SparkSQLIntroBasics</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/008_DiamondsPipeline_01ETLEDA.html">008_DiamondsPipeline_01ETLEDA</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/009_PowerPlantPipeline_01ETLEDA.html">009_PowerPlantPipeline_01ETLEDA</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/010_wikipediaClickStream_01ETLEDA.html">010_wikipediaClickStream_01ETLEDA</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/033_OBO_LoadExtract.html">033_OBO_LoadExtract</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/033_OBO_PipedRDD_RigorousBayesianABTesting.html">033_OBO_PipedRDD_RigorousBayesianABTesting</a></li><li class="chapter-item expanded affix "><a href="contents/000_1-sds-3-x/035_LDA_CornellMovieDialogs.html">035_LDA_CornellMovieDialogs</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">ScaDaMaLe/sds-3.x</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        

                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<ul>
<li><strong>Course Name:</strong> <em>Scalable Data Science and Distributed Machine Learning</em></li>
<li><strong>Course Acronym:</strong> <em>ScaDaMaLe</em> or <em>sds-3.x</em>.</li>
</ul>
<p>The course is the fifth and final mandatory course in the <a href="https://wasp-sweden.org/graduate-school/ai-graduate-school-courses/">AI-Track of the WASP Graduate School</a>. It is given in three modules. In addition to academic lectures there is invited guest speakers from industry.</p>
<p>This site provides course contents for modules 1 and 3 with some background materials for module 2. This content is referred to as <strong>sds-3.x</strong> here.</p>
<p><strong>Module 1</strong> – Introduction to Data Science: Introduction to fault-tolerant distributed file systems and computing.</p>
<p>The whole data science process illustrated with industrial case-studies. Practical introduction to scalable data processing to ingest, extract, load, transform, and explore (un)structured datasets. Scalable machine learning pipelines to model, train/fit, validate, select, tune, test and predict or estimate in an unsupervised and a supervised setting using nonparametric and partitioning methods such as random forests. Introduction to distributed vertex-programming.</p>
<p><strong>Module 2</strong> – Distributed Deep Learning: Introduction to the theory and implementation of distributed deep learning.</p>
<p>Classification and regression using generalised linear models, including different learning, regularization, and hyperparameters tuning techniques. The feedforward deep network as a fundamental network, and the advanced techniques to overcome its main challenges, such as overfitting, vanishing/exploding gradient, and training speed. Various deep neural networks for various kinds of data. For example, the CNN for scaling up neural networks to process large images, RNN to scale up deep neural models to long temporal sequences, and autoencoder and GANs.</p>
<p><strong>Module 3</strong> – Decision-making with Scalable Algorithms</p>
<p>Theoretical foundations of distributed systems and analysis of their scalable algorithms for sorting, joining, streaming, sketching, optimising and computing in numerical linear algebra with applications in scalable machine learning pipelines for typical decision problems (eg. prediction, A/B testing, anomaly detection) with various types of data (eg. time-indexed, space-time-indexed and network-indexed). Privacy-aware decisions with sanitized (cleaned, imputed, anonymised) datasets and datastreams. Practical applications of these algorithms on real-world examples (eg. mobility, social media, machine sensors and logs). Illustration via industrial use-cases.</p>
<h2 id="expected-reference-readings"><a class="header" href="#expected-reference-readings">Expected Reference Readings</a></h2>
<p>Note that you need to be logged into your library with access to these publishers:</p>
<ul>
<li><a href="https://learning.oreilly.com/library/view/high-performance-spark/9781491943199/">https://learning.oreilly.com/library/view/high-performance-spark/9781491943199/</a></li>
<li><a href="https://learning.oreilly.com/library/view/spark-the-definitive/9781491912201/">https://learning.oreilly.com/library/view/spark-the-definitive/9781491912201/</a></li>
<li><a href="https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/">https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/</a></li>
<li>Introduction to Algorithms, Third Edition, Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein from
<ul>
<li><a href="https://ebookcentral.proquest.com/lib/uu/reader.action?docID=3339142">https://ebookcentral.proquest.com/lib/uu/reader.action?docID=3339142</a></li>
</ul>
</li>
<li><a href="https://github.com/lamastex/scalable-data-science/tree/master/read">Reading Materials Provided</a></li>
</ul>
<h2 id="course-contents"><a class="header" href="#course-contents">Course Contents</a></h2>
<p>The databricks notebooks will be made available as the course progresses in the : - course site at: - <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">[site](https://lamastex.github.io/scalable-data-science/sds/3/x/) and [book](https://lamastex.github.io/ScaDaMaLe/index.html)</a> - and course book at: - <a href="https://lamastex.github.io/ScaDaMaLe/index.html">https://lamastex.github.io/ScaDaMaLe/index.html</a></p>
<ul>
<li>You may upload Course Content into Databricks Community Edition from:
<ul>
<li><a href="https://github.com/lamastex/scalable-data-science/tree/master/dbcArchives/2020">2020 dbc ARCHIVES</a></li>
<li><a href="https://github.com/lamastex/scalable-data-science/blob/master/dbcArchives/2017/parts/xtraResources.dbc">Extra Resources</a></li>
</ul>
</li>
</ul>
<h2 id="course-assessment"><a class="header" href="#course-assessment">Course Assessment</a></h2>
<p>There will be minimal reading and coding exercises that will not be graded. The main assessment will be based on a peer-reviewed group project. The group project will include notebooks/codes along with a video of the project presentation. Each group cannot have more than four members and should be seen as an opportunity to do something you are passionate about or interested in, as opposed to completing and auto-gradeable programming assessment in the shortest amount of time.</p>
<p>Detailed instructions will be given in the sequel especially over 12-16 Open Office Hours after the lab/lectures finish after 6 full days of interactions.</p>
<h2 id="course-sponsors"><a class="header" href="#course-sponsors">Course Sponsors</a></h2>
<p>The course builds on contents developed since 2016 with support from New Zealand's Data Industry. The 2017-2019 versions were academically sponsored by Uppsala University's Inter-Faculty Course grant, Department of Mathematics and The Centre for Interdisciplinary Mathematics and industrially sponsored by <a href="https://databricks.com">databricks</a>, <a href="https://aws.amazon.com/">AWS</a> and Swedish data industry via <a href="https://combient.com">Combient AB</a>, <a href="https://seb.se/">SEB</a> and <a href="https://combient.com/mix">Combient Mix AB</a>. This 2021 version is academically sponsored by <a href="https://wasp-sweden.org/graduate-school/ai-graduate-school-courses/">AI-Track of the WASP Graduate School</a> and <a href="https://www.math.uu.se/research/cim/">Centre for Interdisciplinary Mathematics</a> and industrially sponsored by <a href="https://databricks.com">databricks</a> and <a href="https://aws.amazon.com/">AWS</a> via <em>databricks University Alliance</em> and <a href="https://combient.com/mix">Combient Mix AB</a> via industrial mentorships.</p>
<h2 id="course-instructor"><a class="header" href="#course-instructor">Course Instructor</a></h2>
<p>I, Raazesh Sainudiin or <strong>Raaz</strong>, will be an instructor for the course.</p>
<p>I have</p>
<ul>
<li>more than 15 years of academic research experience in applied mathematics and statistics and</li>
<li>over 3 and 5 years of full-time and part-time experience in the data industry.</li>
</ul>
<p>I currently (2020) have an effective joint appointment as:</p>
<ul>
<li><a href="http://katalog.uu.se/profile/?id=N17-214">Associate Professor of Mathematics with specialisation in Data Science</a> at <a href="http://www.math.uu.se/">Department of Mathematics</a>, <a href="http://www.uu.se/">Uppsala University</a>, Uppsala, Sweden and</li>
<li>Director, Technical Strategy and Research at <a href="https://combient.com/mix">Combient Mix AB</a>, Stockholm, Sweden</li>
</ul>
<p>Quick links on Raaz's background:</p>
<ul>
<li><a href="https://www.linkedin.com/in/raazesh-sainudiin-45955845/">https://www.linkedin.com/in/raazesh-sainudiin-45955845/</a></li>
<li><a href="https://lamastex.github.io/cv/">Raaz's academic CV</a></li>
<li><a href="https://lamastex.github.io/publications/">Raaz's publications list</a></li>
</ul>
<h2 id="industrial-case-study"><a class="header" href="#industrial-case-study">Industrial Case Study</a></h2>
<p>We will see an industrial case-study that will illustrate a concrete <strong>data science process</strong> in action in the sequel.</p>
</div>
<div class="cell markdown">
<h1 id="what-is-the-a-hrefhttpsenwikipediaorgwikidata_sciencedata-science-processa"><a class="header" href="#what-is-the-a-hrefhttpsenwikipediaorgwikidata_sciencedata-science-processa">What is the <a href="https://en.wikipedia.org/wiki/Data_science">Data Science Process</a></a></h1>
<p><strong>The Data Science Process in one picture</strong></p>
<p><img src="https://github.com/lamastex/scalable-data-science/raw/master/assets/images/sds.png" alt="what is sds?" title="sds" /></p>
<hr />
<h2 id="what-is-scalable-data-science-and-distributed-machine-learning"><a class="header" href="#what-is-scalable-data-science-and-distributed-machine-learning">What is scalable data science and distributed machine learning?</a></h2>
<p>Scalability merely refers to the ability of the data science process to scale to massive datasets (popularly known as <em>big data</em>).</p>
<p>For this we need <em>distributed fault-tolerant computing</em> typically over large clusters of commodity computers -- the core infrastructure in a public cloud today.</p>
<p><em>Distributed Machine Learning</em> allows the models in the data science process to be scalably trained and extract value from big data.</p>
</div>
<div class="cell markdown">
<h2 id="what-is-data-science"><a class="header" href="#what-is-data-science">What is Data Science?</a></h2>
<p>It is increasingly accepted that <a href="https://en.wikipedia.org/wiki/Data_science">Data Science</a></p>
<blockquote>
<p>is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data.</p>
</blockquote>
<blockquote>
<p>Data science is a &quot;concept to unify statistics, data analysis and their related methods&quot; in order to &quot;understand and analyze actual phenomena&quot; with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, domain knowledge and information science. Turing award winner Jim Gray imagined data science as a &quot;fourth paradigm&quot; of science (empirical, theoretical, computational and now data-driven) and asserted that &quot;everything about science is changing because of the impact of information technology&quot; and the data deluge.</p>
</blockquote>
<p>Now, let us look at two industrially-informed academic papers that influence the above quote on what is Data Science, but with a view towards the contents and syllabus of this course.</p>
<p>Source: <a href="http://dl.acm.org/citation.cfm?id=2500499">Vasant Dhar, Data Science and Prediction, Communications of the ACM, Vol. 56 (1). p. 64, DOI:10.1145/2500499</a></p>
<p><strong>key insights in the above paper</strong></p>
<ul>
<li>Data Science is the study of <em>the generalizabile extraction of knowledge from data</em>.</li>
<li>A common epistemic requirement in assessing whether new knowledge is actionable for decision making is its predictive power, not just its ability to explain the past.</li>
<li>A <em>data scientist requires an integrated skill set spanning</em>
<ul>
<li>mathematics,</li>
<li>machine learning,</li>
<li>artificial intelligence,</li>
<li>statistics,</li>
<li>databases, and</li>
<li>optimization,</li>
<li>along with a deep understanding of the craft of problem formulation to engineer effective solutions.</li>
</ul>
</li>
</ul>
<p>Source: <a href="http://science.sciencemag.org/content/349/6245/255.full-text.pdf+html">Machine learning: Trends, perspectives, and prospects, M. I. Jordan, T. M. Mitchell, Science 17 Jul 2015: Vol. 349, Issue 6245, pp. 255-260, DOI: 10.1126/science.aaa8415</a></p>
<p><strong>key insights in the above paper</strong></p>
<ul>
<li>ML is concerned with the building of computers that improve automatically through experience</li>
<li>ML lies at the intersection of computer science and statistics and at the core of artificial intelligence and data science</li>
<li>Recent progress in ML is due to:
<ul>
<li>development of new algorithms and theory</li>
<li>ongoing explosion in the availability of online data</li>
<li>availability of low-cost computation (*through clusters of commodity hardware in the *cloud* )</li>
</ul>
</li>
<li>The adoption of data science and ML methods is leading to more evidence-based decision-making across:
<ul>
<li>health sciences (neuroscience research, )</li>
<li>manufacturing</li>
<li>robotics (autonomous vehicle)</li>
<li>vision, speech processing, natural language processing</li>
<li>education</li>
<li>financial modeling</li>
<li>policing</li>
<li>marketing</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://en.wikipedia.org/wiki/Data_science"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<p>But what is Data Engineering (including Machine Learning Engineering and Operations) and how does it relate to Data Science?</p>
</div>
<div class="cell markdown">
<h2 id="data-engineering"><a class="header" href="#data-engineering">Data Engineering</a></h2>
<p>There are several views on what a data engineer is supposed to do:</p>
<p>Some views are rather narrow and emphasise division of labour between data engineers and data scientists:</p>
<ul>
<li>https://www.oreilly.com/ideas/data-engineering-a-quick-and-simple-definition
<ul>
<li>Let's check out what skills a data engineer is expected to have according to the link above.</li>
</ul>
</li>
</ul>
<blockquote>
<p>&quot;Ian Buss, principal solutions architect at Cloudera, notes that data scientists focus on finding new insights from a data set, while data engineers are concerned with the production readiness of that data and all that comes with it: formats, scaling, resilience, security, and more.&quot;</p>
</blockquote>
<blockquote>
<p>What skills do data engineers need? Those “10-30 different big data technologies” Anderson references in “Data engineers vs. data scientists” can fall under numerous areas, such as file formats, &gt; ingestion engines, stream processing, batch processing, batch SQL, data storage, cluster management, transaction databases, web frameworks, data visualizations, and machine learning. And that’s just the tip of the iceberg.</p>
</blockquote>
<blockquote>
<p>Buss says data engineers should have the following skills and knowledge:</p>
</blockquote>
<blockquote>
<ul>
<li>They need to know Linux and they should be comfortable using the command line.</li>
<li>They should have experience programming in at least Python or Scala/Java.</li>
<li>They need to know SQL.</li>
<li>They need some understanding of distributed systems in general and how they are different from traditional storage and processing systems.</li>
<li>They need a deep understanding of the ecosystem, including ingestion (e.g. Kafka, Kinesis), processing frameworks (e.g. Spark, Flink) and storage engines (e.g. S3, HDFS, HBase, Kudu). They should know the strengths and weaknesses of each tool and what it's best used for.</li>
<li>They need to know how to access and process data.</li>
</ul>
</blockquote>
<p>Let's dive deeper into such highly compartmentalised views of data engineers and data scientists and the so-called &quot;machine learning engineers&quot; according the following view:</p>
<ul>
<li>https://www.oreilly.com/ideas/data-engineers-vs-data-scientists</li>
</ul>
<p>embedded below.</p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://www.oreilly.com/ideas/data-engineers-vs-data-scientists"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h2 id="the-data-engineering-scientist-as-the-middle-way"><a class="header" href="#the-data-engineering-scientist-as-the-middle-way">The Data Engineering Scientist as &quot;The Middle Way&quot;</a></h2>
<p>Here are some basic axioms that should be self-evident.</p>
<ul>
<li>Yes, there are differences in skillsets across humans
<ul>
<li>some humans will be better and have inclinations for engineering and others for pure mathematics by nature and nurture</li>
<li>one human cannot easily be a master of everything needed for innovating a new data-based product or service (very very rarely though this happens)</li>
</ul>
</li>
<li>Skills can be gained by any human who wants to learn to the extent s/he is able to expend time, energy, etc.</li>
</ul>
<p>For the <strong>Scalable Data Engineering Science Process:</strong> <em>towards Production-Ready and Productisable Prototyping for the Data-based Factory</em> we need to allow each data engineer to be more of a data scientist and each data scientist to be more of a data engineer, up to each individual's <em>comfort zones</em> in technical and mathematical/conceptual and time-availability planes, but with some <strong>minimal expectations</strong> of mutual appreciation.</p>
<p>This course is designed to help you take the first minimal steps towards such a <strong>data engineering science</strong>.</p>
<p>In the sequel it will become apparent <strong>why a team of data engineering scientists</strong> with skills across the conventional (2021) spectrum of data engineer versus data scientist <strong>is crucial</strong> for <strong>Production-Ready and Productisable Prototyping for the Data-based Factory</strong>, whose outputs include standard AI products today.</p>
</div>
<div class="cell markdown">
<h2 id="standing-on-shoulders-of-giants"><a class="header" href="#standing-on-shoulders-of-giants">Standing on shoulders of giants!</a></h2>
<p>This course will build on content owned by the instructors in two other edX courses from 2015 where needed.</p>
<ul>
<li><a href="https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x">BerkeleyX/CS100-1x, Introduction to Big Data Using Apache Spark by Anthony A Joseph, Chancellor's Professor, UC Berkeley</a></li>
<li><a href="https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x">BerkeleyX/CS190-1x, Scalable Machine Learning by Ameet Talwalkar, Ass. Prof., UC Los Angeles</a></li>
</ul>
<p>This course will be an <em>expanded and up-to-date scala version</em> with an emphasis on <em>individualized course project</em> as opposed to completing labs that test sytactic skills that are auto-gradeable.</p>
<p>We will also be borrowing more theoretical aspects from the following course:</p>
<ul>
<li><a href="http://stanford.edu/%7Erezab/dao/">Stanford/CME323, Distributed Algorithms and Optimization by Reza Zadeh, Ass. Prof., Institute for Computational and Mathematical Engineering, Stanford Univ.</a></li>
</ul>
<p>Note the <strong>Expected Reference Readings</strong> above for this course.</p>
</div>
<div class="cell markdown">
<h1 id="a-brief-tour-of-data-science"><a class="header" href="#a-brief-tour-of-data-science">A Brief Tour of Data Science</a></h1>
<h2 id="history-of-data-analysis-and-where-does-big-data-come-from"><a class="header" href="#history-of-data-analysis-and-where-does-big-data-come-from">History of Data Analysis and Where Does &quot;Big Data&quot; Come From?</a></h2>
<p>The following content was created by Anthony Joseph and used in BerkeleyX/CS100.1x from 2015.</p>
<ul>
<li><strong>(watch now 1:53):</strong> A Brief History of Data Analysis
<ul>
<li><a href="https://www.youtube.com/watch?v=5fSSvYlDkag"><img src="http://img.youtube.com/vi/5fSSvYlDkag/0.jpg" alt="A Brief History of Data Analysis by Anthony Joseph in BerkeleyX/CS100.1x" /></a></li>
</ul>
</li>
<li><strong>(watch now 5:05)</strong>: Where does Data Come From?
<ul>
<li><a href="https://www.youtube.com/watch?v=eEJFlHE7Gt4?rel=0&amp;autoplay=1&amp;modestbranding=1"><img src="http://img.youtube.com/vi/eEJFlHE7Gt4/0.jpg" alt="Where Does Data Come From by Anthony Joseph in BerkeleyX/CS100.1x" /></a></li>
<li>SUMMARY of Some of the sources of big data.
<ul>
<li>online click-streams (a lot of it is recorded but a tiny amount is analyzed):
<ul>
<li>record every click</li>
<li>every ad you view</li>
<li>every billing event,</li>
<li>every transaction, every network message, and every fault.</li>
</ul>
</li>
<li>User-generated content (on web and mobile devices):
<ul>
<li>every post that you make on Facebook</li>
<li>every picture sent on Instagram</li>
<li>every review you write for Yelp or TripAdvisor</li>
<li>every tweet you send on Twitter</li>
<li>every video that you post to YouTube.</li>
</ul>
</li>
<li>Science (for scientific computing):
<ul>
<li>data from various repositories for natural language processing:
<ul>
<li>Wikipedia,</li>
<li>the Library of Congress,</li>
<li>twitter firehose and google ngrams and digital archives,</li>
</ul>
</li>
<li>data from scientific instruments/sensors/computers:
<ul>
<li>the Large Hadron Collider (more data in a year than all the other data sources combined!)</li>
<li>genome sequencing data (sequencing cost is dropping much faster than Moore's Law!)</li>
<li>output of high-performance computers (super-computers) for data fusion, estimation/prediction and exploratory data analysis</li>
</ul>
</li>
</ul>
</li>
<li>Graphs are also an interesting source of big data (<em>network science</em>).
<ul>
<li>social networks (collaborations, followers, fb-friends or other relationships),</li>
<li>telecommunication networks,</li>
<li>computer networks,</li>
<li>road networks</li>
</ul>
</li>
<li>machine logs:
<ul>
<li>by servers around the internet (hundreds of millions of machines out there!)</li>
<li>internet of things.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="data-science-defined-cloud-computing-and-whats-hard-about-data-science"><a class="header" href="#data-science-defined-cloud-computing-and-whats-hard-about-data-science">Data Science Defined, Cloud Computing and What's Hard About Data Science?</a></h2>
<p>The following content was created by Anthony Joseph and used in BerkeleyX/CS100.1x from 2015.</p>
<ul>
<li><strong>(watch now 2:03)</strong>: Data Science Defined
<ul>
<li><a href="https://www.youtube.com/watch?v=g4ujW1m2QNc?rel=0&amp;modestbranding=1"><img src="http://img.youtube.com/vi/g4ujW1m2QNc/0.jpg" alt="Data Science Defined by Anthony Joseph in BerkeleyX/CS100.1x" /></a></li>
</ul>
</li>
<li><strong>(watch now 1:11)</strong>: Cloud Computing</li>
<li><a href="https://www.youtube.com/watch?v=TAZvh0WmOHM?rel=0&amp;modestbranding=1"><img src="http://img.youtube.com/vi/TAZvh0WmOHM/0.jpg" alt="Cloud Computing by Anthony Joseph in BerkeleyX/CS100.1x" /></a></li>
<li>In fact, if you are logged into <code>https://*.databricks.com/*</code> you are computing in the cloud!</li>
<li>The Scalable Data Science course is supported by Databricks Academic Partners Program and the AWS Educate Grant to University of Canterbury (applied by Raaz Sainudiin in 2015).</li>
<li><strong>(watch now 3:31)</strong>: What's hard about data science
<ul>
<li><a href="https://www.youtube.com/watch?v=MIqbwJ6AbIY?rel=0&amp;modestbranding=1"><img src="http://img.youtube.com/vi/MIqbwJ6AbIY/0.jpg" alt="What's hard about data science by Anthony Joseph in BerkeleyX/CS100.1x" /></a></li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<p>Here is a recommended light reading on <strong>What is &quot;Big Data&quot; -- Understanding th History</strong> (18 minutes): - <a href="https://towardsdatascience.com/what-is-big-data-understanding-the-history-32078f3b53ce">https://towardsdatascience.com/what-is-big-data-understanding-the-history-32078f3b53ce</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://towardsdatascience.com/what-is-big-data-understanding-the-history-32078f3b53ce&quot;,800))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://towardsdatascience.com/what-is-big-data-understanding-the-history-32078f3b53ce"
 width="95%" height="800"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<hr />
<hr />
</div>
<div class="cell markdown">
<h2 id="background-materials-on-data-science"><a class="header" href="#background-materials-on-data-science">Background Materials on Data Science</a></h2>
<p>The following content was created by Anthony Joseph and used in BerkeleyX/CS100.1x from 2015.</p>
<ul>
<li>
<p><strong>(watch later 2:31)</strong>: Why all the excitement about <em>Big Data Analytics</em>? (using google search to now-cast google flu-trends)</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=16wqonWTAsI"><img src="http://img.youtube.com/vi/16wqonWTAsI/0.jpg" alt="A Brief History of Data Analysis by Anthony Joseph in BerkeleyX/CS100.1x" /></a></li>
</ul>
</li>
<li>
<p>other interesting big data examples - recommender systems and netflix prize?</p>
</li>
<li>
<p><strong>(watch later 10:41)</strong>: Contrasting data science with traditional databases, ML, Scientific computing</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=c7KG0c3ADk0"><img src="http://img.youtube.com/vi/c7KG0c3ADk0/0.jpg" alt="Data Science Database Contrast by Anthony Joseph in BerkeleyX/CS100.1x" /></a></li>
<li>SUMMARY:</li>
<li>traditional databases versus data science
<ul>
<li>preciousness versus cheapness of the data</li>
<li>ACID and eventual consistency, CAP theorem, ...</li>
<li>interactive querying: SQL versus noSQL</li>
<li>querying the past versus querying/predicting the future</li>
</ul>
</li>
<li>traditional scientific computing versus data science
<ul>
<li>science-based or mechanistic models versus data-driven black-box (deep-learning) statistical models (of course both schools co-exist)</li>
<li>super-computers in traditional science-based models versus cluster of commodity computers</li>
</ul>
</li>
<li>traditional ML versus data science
<ul>
<li>smaller amounts of clean data in traditional ML versus massive amounts of dirty data in data science</li>
<li>traditional ML researchers try to publish academic papers versus data scientists try to produce actionable intelligent systems</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>(watch later 1:49)</strong>: Three Approaches to Data Science</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=yAOEyeDVn8s"><img src="http://img.youtube.com/vi/yAOEyeDVn8s/0.jpg" alt="Approaches to Data Science by Anthony Joseph in BerkeleyX/CS100.1x" /></a></li>
</ul>
</li>
<li>
<p><strong>(watch later 4:29)</strong>: Performing Data Science and Preparing Data, Data Acquisition and Preparation, ETL, ...</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=3V6ws_VEzaE"><img src="http://img.youtube.com/vi/3V6ws_VEzaE/0.jpg" alt="Data Science Database Contrast by Anthony Joseph in BerkeleyX/CS100.1x" /></a></li>
</ul>
</li>
<li>
<p><strong>(watch later 2:01)</strong>: Four Examples of Data Science Roles</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=gB-9rdM6W1A"><img src="http://img.youtube.com/vi/gB-9rdM6W1A/0.jpg" alt="Data Science Roles by Anthony Joseph in BerkeleyX/CS100.1x" /></a></li>
<li>SUMMARY of Data Science Roles.</li>
<li>individual roles:
<ol>
<li>business person</li>
<li>programmer</li>
</ol>
</li>
<li>organizational roles:
<ol>
<li>enterprise</li>
<li>web company</li>
</ol>
</li>
<li>Each role has it own unique set of:
<ul>
<li>data sources</li>
<li>Extract-Transform-Load (ETL) process</li>
<li>business intelligence and analytics tools</li>
</ul>
</li>
<li>Most Maths/Stats/Computing programs cater to the <em>programmer</em> role
<ul>
<li>Numpy and Matplotlib, R, Matlab, and Octave.</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h1 id="what-should-you-be-able-to-do-at-the-end-of-this-course"><a class="header" href="#what-should-you-be-able-to-do-at-the-end-of-this-course">What should <em>you</em> be able to do at the end of this course?</a></h1>
<p>By following these online interactions in the form of lab/lectures, asking questions, engaging in discussions, doing HOMEWORK assignments and completing the group project, you should be able to:</p>
<ul>
<li>Understand the principles of fault-tolerant scalable computing in Spark
<ul>
<li>in-memory and generic DAG extensions of Map-reduce</li>
<li>resilient distributed datasets for fault-tolerance</li>
<li>skills to process today's big data using state-of-the art techniques in Apache Spark 3.0, in terms of:
<ul>
<li>hands-on coding with realistic datasets</li>
<li>an intuitive understanding of the ideas behind the technology and methods</li>
<li>pointers to academic papers in the literature, technical blogs and video streams for <em>you to futher your theoretical understanding</em>.</li>
</ul>
</li>
</ul>
</li>
<li>More concretely, you will be able to:
<ul>
<li>Extract, Transform, Load, Interact, Explore and Analyze Data</li>
<li>Build Scalable Machine Learning Pipelines (or help build them) using Distributed Algorithms and Optimization</li>
</ul>
</li>
<li>How to keep up?
<ul>
<li>This is a fast-changing world.</li>
<li>Recent videos around Apache Spark are archived here (these videos are a great way to learn the latest happenings in industrial R&amp;D today!):
<ul>
<li>https://databricks.com/sparkaisummit/north-america/sessions</li>
</ul>
</li>
</ul>
</li>
<li>What is mathematically stable in the world of 'big data'?
<ul>
<li>There is a growing body of work on the analysis of parallel and distributed algorithms, the work-horse of big data and AI.</li>
<li>We will see some of this in a theoretical module later, but the focus here is on how to write programs and analyze data.</li>
</ul>
</li>
</ul>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="why-apache-spark"><a class="header" href="#why-apache-spark">Why Apache Spark?</a></h1>
<ul>
<li><a href="https://cacm.acm.org/magazines/2016/11/209116-apache-spark/fulltext">Apache Spark: A Unified Engine for Big Data Processing</a> By Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J. Franklin, Ali Ghodsi, Joseph Gonzalez, Scott Shenker, Ion Stoica Communications of the ACM, Vol. 59 No. 11, Pages 56-65 10.1145/2934664</li>
</ul>
<p><a href="https://player.vimeo.com/video/185645796"><img src="https://i.vimeocdn.com/video/597494216_640.jpg" alt="Apache Spark ACM Video" /></a></p>
<p>Right-click the above image-link, open in a new tab and watch the video (4 minutes) or read about it in the Communications of the ACM in the frame below or from the link above.</p>
<p>**Key Insights from <a href="https://cacm.acm.org/magazines/2016/11/209116-apache-spark/fulltext">Apache Spark: A Unified Engine for Big Data Processing</a> **</p>
<ul>
<li>A simple programming model can capture streaming, batch, and interactive workloads and enable new applications that combine them.</li>
<li>Apache Spark applications range from finance to scientific data processing and combine libraries for SQL, machine learning, and graphs.</li>
<li>In six years, Apache Spark has grown to 1,000 contributors and thousands of deployments.</li>
</ul>
<p><img src="https://dl.acm.org/cms/attachment/6f54b222-fe96-497a-8bfc-0e6ea250b05d/ins01.gif" alt="Key Insights" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://cacm.acm.org/magazines/2016/11/209116-apache-spark/fulltext"
 width="95%" height="600">
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<p>Spark 3.0 is the latest version now (20200918) and it should be seen as the latest step in the evolution of tools in the big data ecosystem as summarized in <a href="https://towardsdatascience.com/what-is-big-data-understanding-the-history-32078f3b53ce">https://towardsdatascience.com/what-is-big-data-understanding-the-history-32078f3b53ce</a>:</p>
<p><img src="https://miro.medium.com/max/1200/1*0bWwqlOfjRqoUDqrH62GbQ.png" alt="Spark in context" /></p>
</div>
<div class="cell markdown">
<h2 id="the-big-data-problem"><a class="header" href="#the-big-data-problem">The big data problem</a></h2>
<p><strong>Hardware, distributing work, handling failed and slow machines</strong></p>
<p>The following content was created by Anthony Joseph and used in BerkeleyX/CS100.1x from 2015.</p>
<ul>
<li><strong>(watch now 1:48)</strong>: The Big Data Problem
<ul>
<li><a href="https://www.youtube.com/watch?v=0JdJe5iehhw&amp;modestbranding=1&amp;start=1"><img src="http://img.youtube.com/vi/0JdJe5iehhw/0.jpg" alt="The Big Data Problem by Anthony Joseph in BerkeleyX/CS100.1x" /></a></li>
</ul>
</li>
<li><strong>(watch now 1:43)</strong>: Hardware for Big Data</li>
<li><a href="https://www.youtube.com/watch?v=KmIIMdsXGzc&amp;rel=0&amp;autoplay=1&amp;modestbranding=1&amp;start=1"><img src="http://img.youtube.com/vi/KmIIMdsXGzc/0.jpg" alt="Hardware for Big Data by Anthony Joseph in BerkeleyX/CS100.1x" /></a></li>
<li><strong>(watch now 1:17)</strong>: How to distribute work across a cluster of commodity machines?
<ul>
<li><a href="https://www.youtube.com/watch?v=Euk1v3VtNcM&amp;rel=0&amp;autoplay=1&amp;modestbranding=1&amp;start=1"><img src="http://img.youtube.com/vi/Euk1v3VtNcM/0.jpg" alt="How to distribute work across a cluster of commodity machines? by Anthony Joseph in BerkeleyX/CS100.1x" /></a></li>
</ul>
</li>
<li><strong>(watch now 0:36)</strong>: How to deal with failures or slow machines?
<ul>
<li><a href="https://www.youtube.com/watch?v=NaHNsPEK3KA&amp;rel=0&amp;autoplay=1&amp;modestbranding=1&amp;start=1"><img src="http://img.youtube.com/vi/NaHNsPEK3KA/0.jpg" alt="How to deal with failures or slow machines? by Anthony Joseph in BerkeleyX/CS100.1x" /></a></li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="mapreduce-and-apache-spark"><a class="header" href="#mapreduce-and-apache-spark">MapReduce and Apache Spark.</a></h2>
<p>The following content was created by Anthony Joseph and used in BerkeleyX/CS100.1x from 2015.</p>
<ul>
<li><strong>(watch now 1:48)</strong>: Map Reduce (is bounded by Disk I/O)
<ul>
<li><a href="https://www.youtube.com/watch?v=NqG_hYAKjYk&amp;rel=0&amp;autoplay=1&amp;modestbranding=1&amp;start=1"><img src="http://img.youtube.com/vi/NqG_hYAKjYk/0.jpg" alt="The Big Data Problem by Anthony Joseph in BerkeleyX/CS100.1x" /></a></li>
</ul>
</li>
<li><strong>(watch now 2:49)</strong>: Apache Spark (uses Memory instead of Disk)</li>
<li><a href="https://www.youtube.com/watch?v=vat5Jki1lbI&amp;rel=0&amp;autoplay=1&amp;modestbranding=1&amp;start=1"><img src="http://img.youtube.com/vi/vat5Jki1lbI/0.jpg" alt="Apache Spark by Anthony Joseph in BerkeleyX/CS100.1x" /></a></li>
<li><strong>(watch now 3:00)</strong>: Spark Versus MapReduce
<ul>
<li><a href="https://www.youtube.com/watch?v=Ddq3Gua2QFg&amp;rel=0&amp;autoplay=1&amp;modestbranding=1&amp;start=1"><img src="http://img.youtube.com/vi/Ddq3Gua2QFg/0.jpg" alt="Spark Versus MapReduce by Anthony Joseph in BerkeleyX/CS100.1x" /></a></li>
</ul>
</li>
<li>SUMMARY
<ul>
<li>uses memory instead of disk alone and is thus fater than Hadoop MapReduce</li>
<li>resilience abstraction is by RDD (resilient distributed dataset)</li>
<li>RDDs can be recovered upon failures from their <em>lineage graphs</em>, the recipes to make them starting from raw data</li>
<li>Spark supports a lot more than MapReduce, including streaming, interactive in-memory querying, etc.</li>
<li>Spark demonstrated an unprecedented sort of 1 petabyte (1,000 terabytes) worth of data in 234 minutes running on 190 Amazon EC2 instances (in 2015).</li>
<li>Spark expertise corresponds to the highest Median Salary in the US (~ 150K)</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="key-papers"><a class="header" href="#key-papers">Key Papers</a></h2>
<ul>
<li>
<p>Key Historical Milestones</p>
<ul>
<li>1956-1979: <a href="http://www-formal.stanford.edu/jmc/history/lisp/lisp.html">Stanford, MIT, CMU, and other universities develop set/list operations in LISP, Prolog, and other languages for parallel processing</a></li>
<li>2004: <strong>READ</strong>: <a href="http://research.google.com/archive/mapreduce.html">Google's MapReduce: Simplified Data Processing on Large Clusters, by Jeffrey Dean and Sanjay Ghemawat</a></li>
<li>2006: <a href="https://en.wikipedia.org/wiki/Apache_Hadoop">Yahoo!'s Apache Hadoop, originating from the Yahoo!’s Nutch Project, Doug Cutting - wikipedia</a></li>
<li>2009: <a href="http://aws.amazon.com/elasticmapreduce/">Cloud computing with Amazon Web Services Elastic MapReduce</a>, a Hadoop version modified for Amazon Elastic Cloud Computing (EC2) and Amazon Simple Storage System (S3), including support for Apache Hive and Pig.</li>
<li>2010: <strong>READ</strong>: <a href="http://dx.doi.org/10.1109/MSST.2010.5496972">The Hadoop Distributed File System, by Konstantin Shvachko, Hairong Kuang, Sanjay Radia, and Robert Chansler. IEEE MSST</a></li>
</ul>
</li>
<li>
<p>Apache Spark Core Papers</p>
<ul>
<li>2010: <a href="http://people.csail.mit.edu/matei/papers/2010/hotcloud_spark.pdf">Spark: Cluster Computing with Working Sets, Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, Ion Stoica. USENIX HotCloud</a>.</li>
<li>2012: <strong>READ</strong>: <a href="http://usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing, Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauley, Michael J. Franklin, Scott Shenker and Ion Stoica. NSDI</a></li>
<li>2016: <a href="https://cacm.acm.org/magazines/2016/11/209116-apache-spark/fulltext">Apache Spark: A Unified Engine for Big Data Processing</a> By Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J. Franklin, Ali Ghodsi, Joseph Gonzalez, Scott Shenker, Ion Stoica , Communications of the ACM, Vol. 59 No. 11, Pages 56-65, 10.1145/2934664</li>
</ul>
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/week1/dbTrImg_BriefHistoryFuncProgBigData700x.png" alt="brief history of functional programming and big data by SparkCamp" /></p>
</li>
<li>
<p>Here are some directions the creators of Apache Spark at Berkeley and Stanford are currently (2018) taking:</p>
<ul>
<li><a href="http://dawn.cs.stanford.edu/">Stanford's Dawn Lab</a></li>
<li><a href="https://rise.cs.berkeley.edu/">Berkeley's RISE lab</a></li>
</ul>
</li>
<li>
<p>More research papers on Spark are available from here:</p>
<ul>
<li><a href="https://databricks.com/resources?_sft_resource_type=research-papers">https://databricks.com/resources?<em>sft</em>resource_type=research-papers</a></li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<hr />
<hr />
<p><strong>Next let us get everyone to login to databricks</strong> (or another Spark platform) to get our hands dirty with some Spark code!</p>
<hr />
<hr />
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="login-to-databricks"><a class="header" href="#login-to-databricks">Login to databricks</a></h1>
<p>We will use databricks community edition and later on the databricks project shard granted for this course under the <strong>databricks university alliance</strong> with cloud computing grants from databricks for waived DBU units and AWS.</p>
<p>Please go here for a relaxed and detailed-enough tour (later):</p>
<ul>
<li><a href="https://docs.databricks.com/index.html">https://docs.databricks.com/index.html</a></li>
</ul>
</div>
<div class="cell markdown">
<h2 id="databricks-community-edition"><a class="header" href="#databricks-community-edition">databricks community edition</a></h2>
<ol>
<li>First obtain a free Obtain a databricks community edition account at:</li>
</ol>
<ul>
<li><a href="https://community.cloud.databricks.com">https://community.cloud.databricks.com</a></li>
</ul>
<ol>
<li>Let's get an overview of the databricks managed cloud for processing big data with Apache Spark</li>
</ol>
</div>
<div class="cell markdown">
<h2 id="dbc-essentials-what-is-databricks-cloud"><a class="header" href="#dbc-essentials-what-is-databricks-cloud">DBC Essentials: What is Databricks Cloud?</a></h2>
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/week1/dbTrImg_WorkspaceSparkPlatform700x.png" alt="DB workspace, spark, platform" /></p>
</div>
<div class="cell markdown">
<h2 id="dbc-essentials-shard-cluster-notebook-and-dashboard"><a class="header" href="#dbc-essentials-shard-cluster-notebook-and-dashboard">DBC Essentials: Shard, Cluster, Notebook and Dashboard</a></h2>
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/week1/dbTrImg_ShardClusterNotebookDashboard700x.png" alt="DB workspace, spark, platform" /></p>
</div>
<div class="cell markdown">
<p><strong>DBC Essentials: Team, State, Collaboration, Elastic Resources in one picture</strong></p>
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/week1/dbTrImg_TeamStateCollaborationElasticResources700x.png" alt="DB workspace, spark, platform" /></p>
</div>
<div class="cell markdown">
<p><strong>You Should All Have databricks community edition account by now!</strong> and have successfully logged in to it.</p>
</div>
<div class="cell markdown">
<h1 id="import-course-content-now"><a class="header" href="#import-course-content-now">Import Course Content Now!</a></h1>
<p>Two Steps:</p>
<ol>
<li>Create a folder named <code>scalable-data-science</code> in your <code>Workspace</code> (NO Typos due to hard-coding of paths in the sequel!)</li>
</ol>
<ul>
<li>Import the following <code>.dbc</code> archives from the following URL into <code>Workspace/scalable-data-science</code> folder you just created:
<ul>
<li><a href="https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/2021/">https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/2021/</a></li>
<li>start with the first file for now and import more as needed:
<ul>
<li><a href="https://github.com/lamastex/scalable-data-science/tree/master/dbcArchives/2021/000_1-sds-3-x">https://github.com/lamastex/scalable-data-science/tree/master/dbcArchives/2021/000_1-sds-3-x</a></li>
<li>...</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h1 id="cloud-free-computing-environment"><a class="header" href="#cloud-free-computing-environment">Cloud-free Computing Environment</a></h1>
<p><strong>(Optional but strongly recommended)</strong></p>
<p>Before we dive into Scala crash course in a notebook, let's take a look at TASK 2 of the first step in the <a href="https://lamastex.github.io/scalable-data-science/sds/basics/instructions/">instructions</a> to set up a local and &quot;cloud-free&quot; computing environment, say on your laptop computer here:</p>
<ul>
<li>TASK 2 at <a href="https://lamastex.github.io/scalable-data-science/sds/basics/instructions/prep/">https://lamastex.github.io/scalable-data-science/sds/basics/instructions/prep/</a>.</li>
</ul>
<p>This can be handy for prototyping quickly and may even be necessary due to sensitivity of data in certain projects that mandate the data to be confined to some on-premise cluster, etc.</p>
<p><strong>NOTE:</strong> This can be done as an optional exercise as it heavily depends on your local computing environment and your software skills or willingness to acquire them.</p>
<p><strong>CAVEAT:</strong> The docker-compose prepared for your local environment uses Spark 2.x instead of 3.x, but most of the contents here would run in either version of Spark. - Feel free to make PR with latest versions of Spark :)</p>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<p>Please go here for a relaxed and detailed-enough tour (later):</p>
<ul>
<li><a href="https://docs.databricks.com/index.html">https://docs.databricks.com/index.html</a></li>
</ul>
</div>
<div class="cell markdown">
<h1 id="multi-lingual-notebooks"><a class="header" href="#multi-lingual-notebooks">Multi-lingual Notebooks</a></h1>
<p>Write Spark code for processing your data in notebooks.</p>
<p>Note that there are several open-sourced notebook servers including:</p>
<ul>
<li><a href="https://zeppelin.apache.org/">zeppelin</a></li>
<li><a href="https://jupyter.org/">jupyter</a> and its variants</li>
<li>etc</li>
</ul>
<p>Here, we are mainly focused on using databricks notebooks due to its effeciently managed engineering layers over AWS (or Azure public clouds).</p>
<p><strong>NOTE</strong>: You should have already cloned this notebook and attached it to a cluster that you started in the Community Edition of databricks by now.</p>
<h2 id="databricks-notebook"><a class="header" href="#databricks-notebook">Databricks Notebook</a></h2>
<p>Next we delve into the mechanics of working with databricks notebooks. But many of the details also apply to other notebook environments with minor differences.</p>
</div>
<div class="cell markdown">
<h3 id="notebooks-can-be-written-in-python-scala-r-or-sql"><a class="header" href="#notebooks-can-be-written-in-python-scala-r-or-sql">Notebooks can be written in <strong>Python</strong>, <strong>Scala</strong>, <strong>R</strong>, or <strong>SQL</strong>.</a></h3>
<ul>
<li>This is a Scala notebook - which is indicated next to the title above by <code>(Scala)</code>.</li>
<li>One can choose the default language of the notebook when it is created.</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="creating-a-new-notebook"><a class="header" href="#creating-a-new-notebook"><strong>Creating a new Notebook</strong></a></h3>
<p><img src="http://training.databricks.com/databricks_guide/Notebook/createNotebook.png" alt="Change Name" /></p>
<ul>
<li>Click the tiangle on the right side of a folder to open the folder menu.</li>
<li>Select <strong>Create &gt; Notebook</strong>.</li>
<li>Enter the name of the notebook, the language (Python, Scala, R or SQL) for the notebook, and a cluster to run it on.</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="cloning-a-notebook"><a class="header" href="#cloning-a-notebook">Cloning a Notebook</a></h3>
<ul>
<li>You can clone a notebook to create a copy of it, for example if you want to edit or run an Example notebook like this one.</li>
<li>Click <strong>File &gt; Clone</strong> in the notebook context bar above.</li>
<li>Enter a new name and location for your notebook. If Access Control is enabled, you can only clone to folders that you have Manage permissions on.</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="clone-or-import-this-notebook"><a class="header" href="#clone-or-import-this-notebook">Clone Or Import This Notebook</a></h3>
<ul>
<li>From the <strong>File</strong> menu at the top left of this notebook, choose <strong>Clone</strong> or click <strong>Import Notebook</strong> on the top right. This will allow you to interactively execute code cells as you proceed through the notebook.</li>
</ul>
<p><img src="http://training.databricks.com/databricks_guide/2.8/clone.png" alt="Menu Bar Clone Notebook" /> * Enter a name and a desired location for your cloned notebook (i.e. Perhaps clone to your own user directory or the &quot;Shared&quot; directory.) * Navigate to the location you selected (e.g. click Menu &gt; Workspace &gt; <code>Your cloned location</code>)</p>
</div>
<div class="cell markdown">
<h3 id="attach-the-notebook-to-a-cluster"><a class="header" href="#attach-the-notebook-to-a-cluster"><strong>Attach</strong> the Notebook to a <strong>cluster</strong></a></h3>
<ul>
<li>A <strong>Cluster</strong> is a group of machines which can run commands in cells.</li>
<li>Check the upper left corner of your notebook to see if it is <strong>Attached</strong> or <strong>Detached</strong>.</li>
<li>If <strong>Detached</strong>, click on the right arrow and select a cluster to attach your notebook to.
<ul>
<li>If there is no running cluster, create one as described in the <a href="contents/000_1-sds-3-x//#workspace/databricks_guide/00%20Welcome%20to%20Databricks">Welcome to Databricks</a> guide.</li>
</ul>
</li>
</ul>
<p><img src="http://training.databricks.com/databricks_guide/2.8/detached.png" alt="Attach Notebook" /></p>
<h3 id="deep-dive-into-databricks-notebooks"><a class="header" href="#deep-dive-into-databricks-notebooks">Deep-dive into databricks notebooks</a></h3>
<p>Let's take a deeper dive into a databricks notebook next.</p>
</div>
<div class="cell markdown">
<hr />
<h4 id="img-srchttptrainingdatabrickscomdatabricks_guideicon_note3_spng-altquick-note--cells-are-units-that-make-up-notebooks"><a class="header" href="#img-srchttptrainingdatabrickscomdatabricks_guideicon_note3_spng-altquick-note--cells-are-units-that-make-up-notebooks"><img src="http://training.databricks.com/databricks_guide/icon_note3_s.png" alt="Quick Note" /> <strong>Cells</strong> are units that make up notebooks</a></h4>
<p><img src="http://training.databricks.com/databricks_guide/cell.png" alt="A Cell" /></p>
<p>Cells each have a type - including <strong>scala</strong>, <strong>python</strong>, <strong>sql</strong>, <strong>R</strong>, <strong>markdown</strong>, <strong>filesystem</strong>, and <strong>shell</strong>.</p>
<ul>
<li>While cells default to the type of the Notebook, other cell types are supported as well.</li>
<li>This cell is in <strong>markdown</strong> and is used for documentation. <a href="http://en.wikipedia.org/wiki/Markdown">Markdown</a> is a simple text formatting syntax.</li>
</ul>
<hr />
</div>
<div class="cell markdown">
<hr />
<h4 id="create-and-edit-a-new-markdown-cell-in-this-notebook"><a class="header" href="#create-and-edit-a-new-markdown-cell-in-this-notebook"><strong>Create</strong> and <strong>Edit</strong> a New Markdown Cell in this Notebook</a></h4>
<ul>
<li>When you mouse between cells, a + sign will pop up in the center that you can click on to create a new cell.</li>
</ul>
<p><img src="http://training.databricks.com/databricks_guide/create_new_cell.png" alt="New Cell" /> * Type <strong><code>%md Hello, world!</code></strong> into your new cell (<strong><code>%md</code></strong> indicates the cell is markdown).</p>
<ul>
<li>
<p>Click out of the cell to see the cell contents update.</p>
<p><img src="http://training.databricks.com/databricks_guide/run_cell.png" alt="Run cell" /></p>
</li>
</ul>
<hr />
</div>
<div class="cell markdown">
<p>Hello, world!</p>
</div>
<div class="cell markdown">
<h4 id="running-a-cell-in-your-notebook"><a class="header" href="#running-a-cell-in-your-notebook"><strong>Running a cell in your notebook.</strong></a></h4>
<ul>
<li>
<h4 id="press-shiftenter-when-in-the-cell-to-run-it-and-proceed-to-the-next-cell"><a class="header" href="#press-shiftenter-when-in-the-cell-to-run-it-and-proceed-to-the-next-cell">Press <strong>Shift+Enter</strong> when in the cell to <strong>run</strong> it and proceed to the next cell.</a></h4>
<ul>
<li>The cells contents should update. <img src="http://training.databricks.com/databricks_guide/run_cell.png" alt="Run cell" /></li>
</ul>
</li>
<li>
<p><strong>NOTE:</strong> Cells are not automatically run each time you open it.</p>
<ul>
<li>Instead, Previous results from running a cell are saved and displayed.</li>
</ul>
</li>
<li>
<h4 id="alternately-press-ctrlenter-when-in-a-cell-to-run-it-but-not-proceed-to-the-next-cell"><a class="header" href="#alternately-press-ctrlenter-when-in-a-cell-to-run-it-but-not-proceed-to-the-next-cell">Alternately, press <strong>Ctrl+Enter</strong> when in a cell to <strong>run</strong> it, but not proceed to the next cell.</a></h4>
</li>
</ul>
</div>
<div class="cell markdown">
<p><strong>You Try Now!</strong> Just double-click the cell below, modify the text following <code>%md</code> and press <strong>Ctrl+Enter</strong> to evaluate it and see it's mark-down'd output. <code>&gt; %md Hello, world!</code></p>
</div>
<div class="cell markdown">
<p>Hello, world!</p>
</div>
<div class="cell markdown">
<hr />
<h4 id="img-srchttptrainingdatabrickscomdatabricks_guideicon_note3_spng-altquick-note--markdown-cell-tips"><a class="header" href="#img-srchttptrainingdatabrickscomdatabricks_guideicon_note3_spng-altquick-note--markdown-cell-tips"><img src="http://training.databricks.com/databricks_guide/icon_note3_s.png" alt="Quick Note" /> <strong>Markdown Cell Tips</strong></a></h4>
<ul>
<li>To change a non-markdown cell to markdown, add <strong>%md</strong> to very start of the cell.</li>
<li>After updating the contents of a markdown cell, click out of the cell to update the formatted contents of a markdown cell.</li>
<li>To edit an existing markdown cell, <strong>doubleclick</strong> the cell.</li>
</ul>
<p>Learn more about markdown:</p>
<ul>
<li><a href="https://guides.github.com/features/mastering-markdown/">https://guides.github.com/features/mastering-markdown/</a></li>
</ul>
<p>Note that there are flavours or minor variants and enhancements of markdown, including those specific to databricks, github, <a href="https://pandoc.org/MANUAL.html">pandoc</a>, etc.</p>
<p>It will be future-proof to remain in the syntactic zone of <em>pure markdown</em> (at the intersection of various flavours) as much as possible and go with <a href="https://pandoc.org/MANUAL.html">pandoc</a>-compatible style if choices are necessary. ***</p>
</div>
<div class="cell markdown">
<hr />
<h4 id="run-a-scala-cell"><a class="header" href="#run-a-scala-cell">Run a <strong>Scala Cell</strong></a></h4>
<ul>
<li>Run the following scala cell.</li>
<li>Note: There is no need for any special indicator (such as <code>%md</code>) necessary to create a Scala cell in a Scala notebook.</li>
<li>You know it is a scala notebook because of the <code>(Scala)</code> appended to the name of this notebook.</li>
<li>Make sure the cell contents updates before moving on.</li>
<li>Press <strong>Shift+Enter</strong> when in the cell to run it and proceed to the next cell.
<ul>
<li>The cells contents should update.</li>
<li>Alternately, press <strong>Ctrl+Enter</strong> when in a cell to <strong>run</strong> it, but not proceed to the next cell.</li>
</ul>
</li>
<li>characters following <code>//</code> are comments in scala. ***</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">1+1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: Int = 2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(System.currentTimeMillis) // press Ctrl+Enter to evaluate println that prints its argument as a line
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1610582284328
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">1+1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Int = 2
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="spark-is-written-in-scala-but-"><a class="header" href="#spark-is-written-in-scala-but-">Spark is written in Scala, but ...</a></h3>
<p>For this reason Scala will be the primary language for this course is Scala.</p>
<p><strong>However, let us use the best language for the job!</strong> as each cell can be written in a specific language in the same notebook. Such multi-lingual notebooks are the norm in any realistic data science process today!</p>
<p>The beginning of each cells has a language type if it is not the default language of the notebook. Such cell-specific language types include the following with the prefix <code>%</code>:</p>
<ul>
<li>
<p><code>%scala</code> for <strong>Scala</strong>,</p>
</li>
<li>
<p><code>%py</code> for <strong>Python</strong>,</p>
</li>
<li>
<p><code>%r</code> for <strong>R</strong>,</p>
</li>
<li>
<p><code>%sql</code> for <strong>SQL</strong>,</p>
</li>
<li>
<p><code>%fs</code> for databricks' <strong>filesystem</strong>,</p>
</li>
<li>
<p><code>%sh</code> for <strong>BASH SHELL</strong> and</p>
</li>
<li>
<p><code>%md</code> for <strong>markdown</strong>.</p>
</li>
<li>
<p>While cells default to the language type of the Notebook (scala, python, r or sql), other cell types are supported as well in a cell-specific manner.</p>
</li>
<li>
<p>For example, Python Notebooks can contain python, sql, markdown, and even scala cells. This lets you write notebooks that do use multiple languages.</p>
</li>
<li>
<p>This cell is in <strong>markdown</strong> as it begins with <code>%md</code>and is used for documentation purposes.</p>
</li>
</ul>
</div>
<div class="cell markdown">
<p>Thus, <strong>all language-typed cells can be created in any notebook</strong>, regardless of the the default language of the notebook itself.</p>
</div>
<div class="cell markdown">
<p>Cross-language cells can be used to mix commands from other languages.</p>
<p><strong>Examples:</strong></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">print(&quot;For example, this is a scala notebook, but we can use %py to run python commands inline.&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>For example, this is a scala notebook, but we can use %py to run python commands inline.
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-r">print(&quot;We can also access other languages such as R.&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// you can be explicit about the language even if the notebook's default language is the same
println(&quot;We can access Scala like this.&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>We can access Scala like this.
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Command line cells can be used to work with local files on the Spark driver node. * Start a cell with <code>%sh</code> to run a command line command</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh"># This is a command line cell. Commands you write here will be executed as if they were run on the command line.
# For example, in this cell we access the help pages for the bash shell.
ls
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>conf
derby.log
eventlogs
ganglia
logs
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">whoami
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>root
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Filesystem cells allow access to the Databricks File System (DBFS).</p>
<ul>
<li>Start a cell with <code>%fs</code> to run DBFS commands</li>
<li>Type <code>%fs help</code> for a list of commands</li>
</ul>
</div>
<div class="cell markdown">
<h4 id="notebooks-can-be-run-from-other-notebooks-using-run"><a class="header" href="#notebooks-can-be-run-from-other-notebooks-using-run">Notebooks can be run from other notebooks using <strong>%run</strong></a></h4>
<ul>
<li>Syntax: <code>%run /full/path/to/notebook</code></li>
<li>This is commonly used to import functions you defined in other notebooks.</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="further-pointers"><a class="header" href="#further-pointers">Further Pointers</a></h2>
<p>Here are some useful links to bookmark as you will need to use them for Reference.</p>
<p>These links provide a relaxed and detailed-enough tour (that you are strongly encouraged to take later):</p>
<ul>
<li>databricks
<ul>
<li><a href="https://docs.databricks.com/index.html">https://docs.databricks.com/index.html</a></li>
</ul>
</li>
<li>scala
<ul>
<li><a href="http://docs.scala-lang.org/">http://docs.scala-lang.org/</a></li>
</ul>
</li>
</ul>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="scala-crash-course"><a class="header" href="#scala-crash-course">Scala Crash Course</a></h1>
<p>Here we take a minimalist approach to learning just enough Scala, the language that Apache Spark is written in, to be able to use Spark effectively.</p>
<p>In the sequel we can learn more Scala concepts as they arise. This learning can be done by chasing the pointers in this crash course for a detailed deeper dive on your own time.</p>
<p>There are two basic ways in which we can learn Scala:</p>
<p><strong>1. Learn Scala in a notebook environment</strong></p>
<p>For convenience we use databricks Scala notebooks like this one here.</p>
<p>You can learn Scala locally on your own computer using Scala REPL (and Spark using Spark-Shell).</p>
<p><strong>2. Learn Scala in your own computer</strong></p>
<p>The most easy way to get Scala locally is through sbt, the Scala Build Tool. You can also use an IDE that integrates sbt.</p>
<p>See: <a href="https://docs.scala-lang.org/getting-started/index.html">https://docs.scala-lang.org/getting-started/index.html</a> to set up Scala in your own computer.</p>
<p><strong>Software Engineering NOTE:</strong> If you completed TASK 2 for <strong>Cloud-free Computing Environment</strong> in the notebook prefixed <code>002_00</code> using dockerCompose (optional exercise) then you will have Scala 2.11 with sbt and Spark 2.4 inside the docker services you can start and stop locally. Using docker volume binds you can also connect the docker container and its services (including local zeppelin or jupyter notebook servers as well as hadoop file system) to IDEs on your machine, etc.</p>
<h2 id="scala-resources"><a class="header" href="#scala-resources">Scala Resources</a></h2>
<p>You will not be learning scala systematically and thoroughly in this course. You will learn <em>to use</em> Scala by doing various Spark jobs.</p>
<p>If you are interested in learning scala properly, then there are various resources, including:</p>
<ul>
<li><a href="http://www.scala-lang.org/">scala-lang.org</a> is the <strong>core Scala resource</strong>. Bookmark the following three links:
<ul>
<li><a href="https://docs.scala-lang.org/tour/tour-of-scala.html">tour-of-scala</a> - Bite-sized introductions to core language features.
<ul>
<li>we will go through the tour in a hurry now as some Scala familiarity is needed immediately.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/overviews/scala-book/introduction.html">scala-book</a> - An online book introducing the main language features
<ul>
<li>you are expected to use this resource to figure out Scala as needed.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/cheatsheets/index.html">scala-cheatsheet</a> - A handy cheatsheet covering the basics of Scala syntax.</li>
<li><a href="https://superruzafa.github.io/visual-scala-reference/">visual-scala-reference</a> - This guide collects some of the most common functions of the Scala Programming Language and explain them conceptual and graphically in a simple way.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/learn.html">Online Resources</a>, including:
<ul>
<li><a href="https://www.coursera.org/course/progfun">courseera: Functional Programming Principles in Scala</a></li>
</ul>
</li>
<li><a href="http://www.scala-lang.org/documentation/books.html">Books</a>
<ul>
<li><a href="http://www.artima.com/pins1ed/">Programming in Scala, 1st Edition, Free Online Reading</a></li>
</ul>
</li>
</ul>
<p>The main sources for the following content are (you are encouraged to read them for more background):</p>
<ul>
<li><a href="https://www.scala-lang.org/old/sites/default/files/linuxsoft_archives/docu/files/ScalaByExample.pdf">Martin Oderski's Scala by example</a></li>
<li><a href="http://lintool.github.io/SparkTutorial/slides/day1_Scala_crash_course.pdf">Scala crash course by Holden Karau</a></li>
<li><a href="https://darrenjw.wordpress.com/2013/12/30/brief-introduction-to-scala-and-breeze-for-statistical-computing/">Darren's brief introduction to scala and breeze for statistical computing</a></li>
</ul>
<h2 id="what-is-scala"><a class="header" href="#what-is-scala">What is Scala?</a></h2>
<p>&quot;Scala smoothly integrates object-oriented and functional programming. It is designed to express common programming patterns in a concise, elegant, and type-safe way.&quot; by Matrin Odersky.</p>
<ul>
<li>High-level language for the Java Virtual Machine (JVM)</li>
<li>Object oriented + functional programming</li>
<li>Statically typed</li>
<li>Comparable in speed to Java</li>
<li>Type inference saves us from having to write explicit types most of the time Interoperates with Java</li>
<li>Can use any Java class (inherit from, etc.)</li>
<li>Can be called from Java code</li>
</ul>
<p>See a quick tour here:</p>
<ul>
<li><a href="https://docs.scala-lang.org/tour/tour-of-scala.html">https://docs.scala-lang.org/tour/tour-of-scala.html</a></li>
</ul>
<h2 id="why-scala"><a class="header" href="#why-scala">Why Scala?</a></h2>
<ul>
<li>Spark was originally written in Scala, which allows concise function syntax and interactive use</li>
<li>Spark APIs for other languages include:
<ul>
<li>Java API for standalone use</li>
<li>Python API added to reach a wider user community of programmes</li>
<li>R API added more recently to reach a wider community of data analyststs</li>
<li>Unfortunately, Python and R APIs are generally behind Spark's native Scala (for eg. GraphX is only available in Scala currently and datasets are only available in Scala as of 20200918).</li>
</ul>
</li>
<li>See Darren Wilkinson's 11 reasons for <a href="https://darrenjw.wordpress.com/2013/12/23/scala-as-a-platform-for-statistical-computing-and-data-science/">scala as a platform for statistical computing and data science</a>. It is embedded in-place below for your convenience.</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="learn-scala-in-notebook-environment"><a class="header" href="#learn-scala-in-notebook-environment">Learn Scala in Notebook Environment</a></h2>
<hr />
<h3 id="run-a-scala-cell-1"><a class="header" href="#run-a-scala-cell-1">Run a <strong>Scala Cell</strong></a></h3>
<ul>
<li>Run the following scala cell.</li>
<li>Note: There is no need for any special indicator (such as <code>%md</code>) necessary to create a Scala cell in a Scala notebook.</li>
<li>You know it is a scala notebook because of the <code>(Scala)</code> appended to the name of this notebook.</li>
<li>Make sure the cell contents updates before moving on.</li>
<li>Press <strong>Shift+Enter</strong> when in the cell to run it and proceed to the next cell.
<ul>
<li>The cells contents should update.</li>
<li>Alternately, press <strong>Ctrl+Enter</strong> when in a cell to <strong>run</strong> it, but not proceed to the next cell.</li>
</ul>
</li>
<li>characters following <code>//</code> are comments in scala. ***</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">1+1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res0: Int = 2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(System.currentTimeMillis) // press Ctrl+Enter to evaluate println that prints its argument as a line
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1610582084465
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//%run &quot;/scalable-data-science/xtraResources/support/sdsFunctions&quot;
//This allows easy embedding of publicly available information into any other notebook
//when viewing in git-book just ignore this block - you may have to manually chase the URL in frameIt(&quot;URL&quot;).
//Example usage:
// displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;,250))
def frameIt( u:String, h:Int ) : String = {
      &quot;&quot;&quot;&lt;iframe 
 src=&quot;&quot;&quot;&quot;+ u+&quot;&quot;&quot;&quot;
 width=&quot;95%&quot; height=&quot;&quot;&quot;&quot; + h + &quot;&quot;&quot;&quot;
 sandbox&gt;
  &lt;p&gt;
    &lt;a href=&quot;http://spark.apache.org/docs/latest/index.html&quot;&gt;
      Fallback link for browsers that, unlikely, don't support frames
    &lt;/a&gt;
  &lt;/p&gt;
&lt;/iframe&gt;&quot;&quot;&quot;
   }
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>frameIt: (u: String, h: Int)String
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>See <a href="https://darrenjw.wordpress.com/2013/12/23/scala-as-a-platform-for-statistical-computing-and-data-science/">Scala as a platform for statistical computing and data science</a>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://darrenjw.wordpress.com/2013/12/23/scala-as-a-platform-for-statistical-computing-and-data-science/&quot;,500))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://darrenjw.wordpress.com/2013/12/23/scala-as-a-platform-for-statistical-computing-and-data-science/"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h2 id="lets-get-our-hands-dirty-in-scala"><a class="header" href="#lets-get-our-hands-dirty-in-scala">Let's get our hands dirty in Scala</a></h2>
<p>We will go through the <strong>following</strong> programming concepts and tasks by building on <a href="https://docs.scala-lang.org/tour/basics.html">https://docs.scala-lang.org/tour/basics.html</a>.</p>
<ul>
<li><strong>Scala Types</strong></li>
<li><strong>Expressions and Printing</strong></li>
<li><strong>Naming and Assignments</strong></li>
<li><strong>Functions and Methods in Scala</strong></li>
<li><strong>Classes and Case Classes</strong></li>
<li><strong>Methods and Tab-completion</strong></li>
<li><strong>Objects and Traits</strong></li>
<li>Collections in Scala and Type Hierarchy</li>
<li>Functional Programming and MapReduce</li>
<li>Lazy Evaluations and Recursions</li>
</ul>
<p><strong>Remark</strong>: You need to take a computer science course (from CourseEra, for example) to properly learn Scala. Here, we will learn to use Scala by example to accomplish our data science tasks at hand. You can learn more Scala as needed from various sources pointed out above in <strong>Scala Resources</strong>.</p>
</div>
<div class="cell markdown">
<h3 id="scala-types"><a class="header" href="#scala-types">Scala Types</a></h3>
<p>In Scala, all values have a type, including numerical values and functions. The diagram below illustrates a subset of the type hierarchy.</p>
<p><img src="https://docs.scala-lang.org/resources/images/tour/unified-types-diagram.svg" alt="" /></p>
<p>For now, notice some common types we will be usinf including <code>Int</code>, <code>String</code>, <code>Double</code>, <code>Unit</code>, <code>Boolean</code>, <code>List</code>, etc. For more details see <a href="https://docs.scala-lang.org/tour/unified-types.html">https://docs.scala-lang.org/tour/unified-types.html</a>. We will return to this at the end of the notebook after seeing a brief tour of Scala now.</p>
</div>
<div class="cell markdown">
<h3 id="expressions"><a class="header" href="#expressions">Expressions</a></h3>
<p>Expressions are computable statements such as the <code>1+1</code> we have seen before.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">1+1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res3: Int = 2
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>We can print the output of a computed or evaluated expressions as a line using <code>println</code>:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(1+1) // printing 2
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(&quot;hej hej!&quot;) // printing a string
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>hej hej!
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="naming-and-assignments"><a class="header" href="#naming-and-assignments">Naming and Assignments</a></h3>
<p><strong>value and variable as <code>val</code> and <code>var</code></strong></p>
<p>You can name the results of expressions using keywords <code>val</code> and <code>var</code>.</p>
<p>Let us assign the integer value <code>5</code> to <code>x</code> as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x : Int = 5 // &lt;Ctrl+Enter&gt; to declare a value x to be integer 5. 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Int = 5
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><code>x</code> is a named result and it is a value since we used the keyword <code>val</code> when naming it.</p>
</div>
<div class="cell markdown">
<p>Scala is statically typed, but it uses built-in type inference machinery to automatically figure out that <code>x</code> is an integer or <code>Int</code> type as follows. Let's declare a value <code>x</code> to be <code>Int</code> 5 next without explictly using <code>Int</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = 5    // &lt;Ctrl+Enter&gt; to declare a value x as Int 5 (type automatically inferred)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Int = 5
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's declare <code>x</code> as a <code>Double</code> or double-precision floating-point type using decimal such as <code>5.0</code> (a digit has to follow the decimal point!)</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = 5.0   // &lt;Ctrl+Enter&gt; to declare a value x as Double 5
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Double = 5.0
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Alternatively, we can assign <code>x</code> as a <code>Double</code> explicitly. Note that the decimal point is not needed in this case due to explicit typing as <code>Double</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x :  Double = 5    // &lt;Ctrl+Enter&gt; to declare a value x as Double 5 (type automatically inferred)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Double = 5.0
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Next note that labels need to be declared on first use. We have declared <code>x</code> to be a <code>val</code> which is short for <em>value</em>. This makes <code>x</code> immutable (cannot be changed).</p>
<p>Thus, <code>x</code> cannot be just re-assigned, as the following code illustrates in the resulting error: <code>... error: reassignment to val</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//x = 10    //  uncomment and &lt;Ctrl+Enter&gt; to try to reassign val x to 10
</code></pre>
</div>
<div class="cell markdown">
<p>Scala allows declaration of mutable variables as well using <code>var</code>, as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">var y = 2    // &lt;Shift+Enter&gt; to declare a variable y to be integer 2 and go to next cell
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: Int = 2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">y = 3    // &lt;Shift+Enter&gt; to change the value of y to 3
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: Int = 3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">y = y+1 // adds 1 to y
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: Int = 4
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">y += 2 // adds 2 to y
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(y) // the var y is 6 now
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>6
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="blocks"><a class="header" href="#blocks">Blocks</a></h3>
<p>Just combine expressions by surrounding them with <code>{</code> and <code>}</code> called a block.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println({
  val x = 1+1
  x+2 // expression in last line is returned for the block
})// prints 4
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>4
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println({ val x=22; x+2})
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>24
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="functions"><a class="header" href="#functions">Functions</a></h3>
<p>Functions are expressions that have parameters. A function takes arguments as input and returns expressions as output.</p>
<p>A function can be nameless or <em>anonymous</em> and simply return an output from a given input. For example, the following annymous function returns the square of the input integer.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">(x: Int) =&gt; x*x
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res11: Int =&gt; Int = line186c28489fff404184da2d59bd09a90463.$read$$Lambda$5065/1820207503@597d20b
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>On the left of <code>=&gt;</code> is a list of parameters with name and type. On the right is an expression involving the parameters.</p>
</div>
<div class="cell markdown">
<p>You can also name functions:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val multiplyByItself = (x: Int) =&gt; x*x
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>multiplyByItself: Int =&gt; Int = line186c28489fff404184da2d59bd09a90465.$read$$Lambda$5067/2036039718@12f273c8
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(multiplyByItself(10))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>100
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>A function can have no parameters:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val howManyAmI = () =&gt; 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>howManyAmI: () =&gt; Int = line186c28489fff404184da2d59bd09a90469.$read$$Lambda$5070/1826556511@56f9e3f2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(howManyAmI()) // 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>A function can have more than one parameter:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val multiplyTheseTwoIntegers = (a: Int, b: Int) =&gt; a*b
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>multiplyTheseTwoIntegers: (Int, Int) =&gt; Int = line186c28489fff404184da2d59bd09a90473.$read$$Lambda$5071/161461748@62178cca
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(multiplyTheseTwoIntegers(2,4)) // 8
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>8
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="methods"><a class="header" href="#methods">Methods</a></h3>
<p>Methods are very similar to functions, but a few key differences exist.</p>
<p>Methods use the <code>def</code> keyword followed by a name, parameter list(s), a return type, and a body.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def square(x: Int): Int = x*x    // &lt;Shitf+Enter&gt; to define a function named square
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>square: (x: Int)Int
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Note that the return type <code>Int</code> is specified after the parameter list and a <code>:</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">square(5)    // &lt;Shitf+Enter&gt; to call this function on argument 5
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res15: Int = 25
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val y = 3    // &lt;Shitf+Enter&gt; make val y as Int 3
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: Int = 3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">square(y) // &lt;Shitf+Enter&gt; to call the function on val y of the right argument type Int
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res16: Int = 9
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = 5.0     // let x be Double 5.0
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Double = 5.0
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//square(x) // &lt;Shift+Enter&gt; to call the function on val x of type Double will give type mismatch error
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def square(x: Int): Int = { // &lt;Shitf+Enter&gt; to declare function in a block
  val answer = x*x
  answer // the last line of the function block is returned
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>square: (x: Int)Int
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">square(5000)    // &lt;Shift+Enter&gt; to call the function
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res18: Int = 25000000
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; to define function with input and output type as String
def announceAndEmit(text: String): String = 
{
  println(text)
  text // the last line of the function block is returned
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>announceAndEmit: (text: String)String
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Scala has a <code>return</code> keyword but it is rarely used as the expression in the last line of the multi-line block is the method's return value.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; to call function which prints as line and returns as String
announceAndEmit(&quot;roger  roger&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>roger  roger
res19: String = roger  roger
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>A method can have output expressions involving multiple parameter lists:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def multiplyAndTranslate(x: Int, y: Int)(translateBy: Int): Int = (x * y) + translateBy
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>multiplyAndTranslate: (x: Int, y: Int)(translateBy: Int)Int
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(multiplyAndTranslate(2, 3)(4))  // (2*3)+4 = 10
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>10
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>A method can have no parameter lists at all:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def time: Long = System.currentTimeMillis
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>time: Long
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(&quot;Current time in milliseconds is &quot; + time)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Current time in milliseconds is 1610582096790
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(&quot;Current time in milliseconds is &quot; + time)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Current time in milliseconds is 1610582097046
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="classes"><a class="header" href="#classes">Classes</a></h3>
<p>The <code>class</code> keyword followed by the name and constructor parameters is used to define a class.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">class Box(h: Int, w: Int, d: Int) {
  def printVolume(): Unit = println(h*w*d)
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class Box
</code></pre>
</div>
</div>
<div class="cell markdown">
<ul>
<li>The return type of the method <code>printVolume</code> is <code>Unit</code>.</li>
<li>When the return type is <code>Unit</code> it indicates that there is nothing meaningful to return, similar to <code>void</code> in Java and C, but with a difference.</li>
<li>Because every Scala expression must have some value, there is actually a singleton value of type <code>Unit</code>, written <code>()</code> and carrying no information.</li>
</ul>
</div>
<div class="cell markdown">
<p>We can make an instance of the class with the <code>new</code> keyword.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val my1Cube = new Box(1,1,1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>my1Cube: Box = line186c28489fff404184da2d59bd09a904107.$read$Box@6c4cbb75
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>And call the method on the instance.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">my1Cube.printVolume() // 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Our named instance <code>my1Cube</code> of the <code>Box</code> class is immutable due to <code>val</code>.</p>
<p>You can have mutable instances of the class using <code>var</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">var myVaryingCuboid = new Box(1,3,2)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>myVaryingCuboid: Box = line186c28489fff404184da2d59bd09a904107.$read$Box@77404a48
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myVaryingCuboid.printVolume()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>6
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myVaryingCuboid = new Box(1,1,1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>myVaryingCuboid: Box = line186c28489fff404184da2d59bd09a904107.$read$Box@748cdfd1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myVaryingCuboid.printVolume()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>See <a href="https://docs.scala-lang.org/tour/classes.html">https://docs.scala-lang.org/tour/classes.html</a> for more details as needed.</p>
</div>
<div class="cell markdown">
<h3 id="case-classes"><a class="header" href="#case-classes">Case Classes</a></h3>
<p>Scala has a special type of class called a <em>case class</em> that can be defined with the <code>case class</code> keyword.</p>
<p>Unlike classes, whose instances are compared by reference, instances of case classes are immutable by default and compared by value. This makes them useful for defining rows of typed values in Spark.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">case class Point(x: Int, y: Int, z: Int)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class Point
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Case classes can be instantiated without the <code>new</code> keyword.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val point = Point(1, 2, 3)
val anotherPoint = Point(1, 2, 3)
val yetAnotherPoint = Point(2, 2, 2)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>point: Point = Point(1,2,3)
anotherPoint: Point = Point(1,2,3)
yetAnotherPoint: Point = Point(2,2,2)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Instances of case classes are compared by value and not by reference.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">if (point == anotherPoint) {
  println(point + &quot; and &quot; + anotherPoint + &quot; are the same.&quot;)
} else {
  println(point + &quot; and &quot; + anotherPoint + &quot; are different.&quot;)
} // Point(1,2,3) and Point(1,2,3) are the same.

if (point == yetAnotherPoint) {
  println(point + &quot; and &quot; + yetAnotherPoint + &quot; are the same.&quot;)
} else {
  println(point + &quot; and &quot; + yetAnotherPoint + &quot; are different.&quot;)
} // Point(1,2,3) and Point(2,2,2) are different.
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Point(1,2,3) and Point(1,2,3) are the same.
Point(1,2,3) and Point(2,2,2) are different.
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>By contrast, instances of classes are compared by reference.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myVaryingCuboid.printVolume() // should be 1 x 1 x 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">my1Cube.printVolume()  // should be 1 x 1 x 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">if (myVaryingCuboid == my1Cube) {
  println(&quot;myVaryingCuboid and my1Cube are the same.&quot;)
} else {
  println(&quot;myVaryingCuboid and my1Cube are different.&quot;)
} // they are compared by reference and are not the same.
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>myVaryingCuboid and my1Cube are different.
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>More about case classes here: <a href="https://docs.scala-lang.org/tour/case-classes.html">https://docs.scala-lang.org/tour/case-classes.html</a>.</p>
</div>
<div class="cell markdown">
<h3 id="methods-and-tab-completion"><a class="header" href="#methods-and-tab-completion">Methods and Tab-completion</a></h3>
<p>Many methods of a class can be accessed by <code>.</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val s  = &quot;hi&quot;    // &lt;Ctrl+Enter&gt; to declare val s to String &quot;hi&quot;
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>s: String = hi
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>You can place the cursor after <code>.</code> following a declared object and find out the methods available for it as shown in the image below.</p>
<p><img src="https://github.com/raazesh-sainudiin/scalable-data-science/raw/master/images/week1/tabCompletionAfterSDot.png" alt="tabCompletionAfterSDot PNG image" /></p>
<p><strong>You Try</strong> doing this next.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//s.  // place cursor after the '.' and press Tab to see all available methods for s 
</code></pre>
</div>
<div class="cell markdown">
<p>For example,</p>
<ul>
<li>scroll down to <code>contains</code> and double-click on it.</li>
<li>This should lead to <code>s.contains</code> in your cell.</li>
<li>Now add an argument String to see if <code>s</code> contains the argument, for example, try:
<ul>
<li><code>s.contains(&quot;f&quot;)</code></li>
<li><code>s.contains(&quot;&quot;)</code> and</li>
<li><code>s.contains(&quot;i&quot;)</code></li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//s    // &lt;Shift-Enter&gt; recall the value of String s
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">s.contains(&quot;f&quot;)     // &lt;Shift-Enter&gt; returns Boolean false since s does not contain the string &quot;f&quot;
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res32: Boolean = false
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">s.contains(&quot;&quot;)    // &lt;Shift-Enter&gt; returns Boolean true since s contains the empty string &quot;&quot;
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res33: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">s.contains(&quot;i&quot;)    // &lt;Ctrl+Enter&gt; returns Boolean true since s contains the string &quot;i&quot;
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res34: Boolean = true
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="objects"><a class="header" href="#objects">Objects</a></h3>
<p>Objects are single instances of their own definitions using the <code>object</code> keyword. You can think of them as singletons of their own classes.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">object IdGenerator {
  private var currentId = 0
  def make(): Int = {
    currentId += 1
    currentId
  }
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined object IdGenerator
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>You can access an object through its name:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val newId: Int = IdGenerator.make()
val newerId: Int = IdGenerator.make()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>newId: Int = 1
newerId: Int = 2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(newId) // 1
println(newerId) // 2
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1
2
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>For details see <a href="https://docs.scala-lang.org/tour/singleton-objects.html">https://docs.scala-lang.org/tour/singleton-objects.html</a></p>
</div>
<div class="cell markdown">
<h3 id="traits"><a class="header" href="#traits">Traits</a></h3>
<p>Traits are abstract data types containing certain fields and methods. They can be defined using the <code>trait</code> keyword.</p>
<p>In Scala inheritance, a class can only extend one other class, but it can extend multiple traits.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">trait Greeter {
  def greet(name: String): Unit
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined trait Greeter
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Traits can have default implementations also.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">trait Greeter {
  def greet(name: String): Unit =
    println(&quot;Hello, &quot; + name + &quot;!&quot;)
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined trait Greeter
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>You can extend traits with the <code>extends</code> keyword and override an implementation with the <code>override</code> keyword:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">class DefaultGreeter extends Greeter

class SwedishGreeter extends Greeter {
  override def greet(name: String): Unit = {
    println(&quot;Hej hej, &quot; + name + &quot;!&quot;)
  }
}

class CustomizableGreeter(prefix: String, postfix: String) extends Greeter {
  override def greet(name: String): Unit = {
    println(prefix + name + postfix)
  }
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined class DefaultGreeter
defined class SwedishGreeter
defined class CustomizableGreeter
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Instantiate the classes.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val greeter = new DefaultGreeter()
val swedishGreeter = new SwedishGreeter()
val customGreeter = new CustomizableGreeter(&quot;How are you, &quot;, &quot;?&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>greeter: DefaultGreeter = line186c28489fff404184da2d59bd09a904155.$read$DefaultGreeter@5d7c7786
swedishGreeter: SwedishGreeter = line186c28489fff404184da2d59bd09a904155.$read$SwedishGreeter@a1c1128
customGreeter: CustomizableGreeter = line186c28489fff404184da2d59bd09a904155.$read$CustomizableGreeter@7c2dc867
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Call the <code>greet</code> method in each case.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">greeter.greet(&quot;Scala developer&quot;) // Hello, Scala developer!
swedishGreeter.greet(&quot;Scala developer&quot;) // Hej hej, Scala developer!
customGreeter.greet(&quot;Scala developer&quot;) // How are you, Scala developer?
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Hello, Scala developer!
Hej hej, Scala developer!
How are you, Scala developer?
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>A class can also be made to extend multiple traits.</p>
<p>For more details see: <a href="https://docs.scala-lang.org/tour/traits.html">https://docs.scala-lang.org/tour/traits.html</a>.</p>
</div>
<div class="cell markdown">
<h3 id="main-method"><a class="header" href="#main-method">Main Method</a></h3>
<p>The main method is the entry point of a Scala program.</p>
<p>The Java Virtual Machine requires a main method, named <code>main</code>, that takes an array of strings as its only argument.</p>
<p>Using an object, you can define the main method as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">object Main {
  def main(args: Array[String]): Unit =
    println(&quot;Hello, Scala developer!&quot;)
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>defined object Main
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><strong>What I try not do while learning a new language?</strong></p>
<ol>
<li>I don't immediately try to ask questions like:</li>
</ol>
<ul>
<li>&quot;how can I do this particular variation of some small thing I just learned so I can use patterns I am used to from another language I am hooked-on right now?&quot; immediately</li>
<li>first go through the detailed Scala Tour on your own and then through the 50 odd lessons in the Scala Book</li>
<li>then return to 1. and ask detailed cross-language comparison questions by diving deep as needed with the source and scala docs as needed (google or duck-duck-go search!).</li>
</ul>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="scala-crash-course-continued"><a class="header" href="#scala-crash-course-continued">Scala Crash Course Continued</a></h1>
<p>Recall!</p>
<h2 id="scala-resources-1"><a class="header" href="#scala-resources-1">Scala Resources</a></h2>
<p>You will not be learning scala systematically and thoroughly in this course. You will learn <em>to use</em> Scala by doing various Spark jobs.</p>
<p>If you are interested in learning scala properly, then there are various resources, including:</p>
<ul>
<li><a href="http://www.scala-lang.org/">scala-lang.org</a> is the <strong>core Scala resource</strong>. Bookmark the following three links:
<ul>
<li><a href="https://docs.scala-lang.org/tour/tour-of-scala.html">tour-of-scala</a> - Bite-sized introductions to core language features.
<ul>
<li>we will go through the tour in a hurry now as some Scala familiarity is needed immediately.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/overviews/scala-book/introduction.html">scala-book</a> - An online book introducing the main language features
<ul>
<li>you are expected to use this resource to figure out Scala as needed.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/cheatsheets/index.html">scala-cheatsheet</a> - A handy cheatsheet covering the basics of Scala syntax.</li>
<li><a href="https://superruzafa.github.io/visual-scala-reference/">visual-scala-reference</a> - This guide collects some of the most common functions of the Scala Programming Language and explain them conceptual and graphically in a simple way.</li>
</ul>
</li>
<li><a href="https://docs.scala-lang.org/learn.html">Online Resources</a>, including:
<ul>
<li><a href="https://www.coursera.org/course/progfun">courseera: Functional Programming Principles in Scala</a></li>
</ul>
</li>
<li><a href="http://www.scala-lang.org/documentation/books.html">Books</a>
<ul>
<li><a href="http://www.artima.com/pins1ed/">Programming in Scala, 1st Edition, Free Online Reading</a></li>
</ul>
</li>
</ul>
<p>The main sources for the following content are (you are encouraged to read them for more background):</p>
<ul>
<li><a href="https://www.scala-lang.org/old/sites/default/files/linuxsoft_archives/docu/files/ScalaByExample.pdf">Martin Oderski's Scala by example</a></li>
<li><a href="http://lintool.github.io/SparkTutorial/slides/day1_Scala_crash_course.pdf">Scala crash course by Holden Karau</a></li>
<li><a href="https://darrenjw.wordpress.com/2013/12/30/brief-introduction-to-scala-and-breeze-for-statistical-computing/">Darren's brief introduction to scala and breeze for statistical computing</a></li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//%run &quot;/scalable-data-science/xtraResources/support/sdsFunctions&quot;
//This allows easy embedding of publicly available information into any other notebook
//when viewing in git-book just ignore this block - you may have to manually chase the URL in frameIt(&quot;URL&quot;).
//Example usage:
// displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;,250))
def frameIt( u:String, h:Int ) : String = {
      &quot;&quot;&quot;&lt;iframe 
 src=&quot;&quot;&quot;&quot;+ u+&quot;&quot;&quot;&quot;
 width=&quot;95%&quot; height=&quot;&quot;&quot;&quot; + h + &quot;&quot;&quot;&quot;
 sandbox&gt;
  &lt;p&gt;
    &lt;a href=&quot;http://spark.apache.org/docs/latest/index.html&quot;&gt;
      Fallback link for browsers that, unlikely, don't support frames
    &lt;/a&gt;
  &lt;/p&gt;
&lt;/iframe&gt;&quot;&quot;&quot;
   }
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>frameIt: (u: String, h: Int)String
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="lets-continue-getting-our-hands-dirty-in-scala"><a class="header" href="#lets-continue-getting-our-hands-dirty-in-scala">Let's continue getting our hands dirty in Scala</a></h2>
<p>We will go through the <strong>remaining</strong> programming concepts and tasks by building on <a href="https://docs.scala-lang.org/tour/basics.html">https://docs.scala-lang.org/tour/basics.html</a>.</p>
<ul>
<li>Scala Types</li>
<li>Expressions and Printing</li>
<li>Naming and Assignments</li>
<li>Functions and Methods in Scala</li>
<li>Classes and Case Classes</li>
<li>Methods and Tab-completion</li>
<li>Objects and Traits</li>
<li><strong>Collections in Scala and Type Hierarchy</strong></li>
<li><strong>Functional Programming and MapReduce</strong></li>
<li><strong>Lazy Evaluations and Recursions</strong></li>
</ul>
<p><strong>Remark</strong>: You need to take a computer science course (from CourseEra, for example) to properly learn Scala. Here, we will learn to use Scala by example to accomplish our data science tasks at hand. You can learn more Scala as needed from various sources pointed out above in <strong>Scala Resources</strong>.</p>
</div>
<div class="cell markdown">
<h3 id="scala-type-hierarchy"><a class="header" href="#scala-type-hierarchy">Scala Type Hierarchy</a></h3>
<p>In Scala, all values have a type, including numerical values and functions. The diagram below illustrates a subset of the type hierarchy.</p>
<p><img src="https://docs.scala-lang.org/resources/images/tour/unified-types-diagram.svg" alt="" /></p>
<p>For now, notice some common types we will be usinf including <code>Int</code>, <code>String</code>, <code>Double</code>, <code>Unit</code>, <code>Boolean</code>, <code>List</code>, etc. For more details see <a href="https://docs.scala-lang.org/tour/unified-types.html">https://docs.scala-lang.org/tour/unified-types.html</a>.</p>
<p>Let us take a closer look at Scala Type Hierarchy now.</p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://docs.scala-lang.org/tour/unified-types.html&quot;,550))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://docs.scala-lang.org/tour/unified-types.html"
 width="95%" height="550"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h3 id="scala-collections"><a class="header" href="#scala-collections">Scala Collections</a></h3>
<p>Familiarize yourself with the main Scala collections classes here:</p>
<ul>
<li><a href="https://docs.scala-lang.org/overviews/scala-book/collections-101.html">https://docs.scala-lang.org/overviews/scala-book/collections-101.html</a></li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://docs.scala-lang.org/overviews/scala-book/collections-101.html&quot;,550))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://docs.scala-lang.org/overviews/scala-book/collections-101.html"
 width="95%" height="550"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h4 id="list"><a class="header" href="#list">List</a></h4>
<p>Lists are one of the most basic data structures.</p>
<p>There are several other Scala collections and we will introduce them as needed. The other most common ones are <code>Vector</code>, <code>Array</code> and <code>Seq</code> and the <code>ArrayBuffer</code>.</p>
<p>For details on list see: - <a href="https://docs.scala-lang.org/overviews/scala-book/list-class.html">https://docs.scala-lang.org/overviews/scala-book/list-class.html</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; to declare (an immutable) val lst as List of Int's 1,2,3
val lst = List(1, 2, 3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>lst: List[Int] = List(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell markdown">
<h4 id="vectors"><a class="header" href="#vectors">Vectors</a></h4>
<blockquote>
<p>The Vector class is an indexed, immutable sequence. The “indexed” part of the description means that you can access Vector elements very rapidly by their index value, such as accessing listOfPeople(999999).</p>
</blockquote>
<p>In general, except for the difference that Vector is indexed and List is not, the two classes work the same, so we’ll run through these examples quickly.</p>
<p>For details see: - <a href="https://docs.scala-lang.org/overviews/scala-book/vector-class.html">https://docs.scala-lang.org/overviews/scala-book/vector-class.html</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val vec = Vector(1,2,3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>vec: scala.collection.immutable.Vector[Int] = Vector(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="arrays-sequences-and-tuples"><a class="header" href="#arrays-sequences-and-tuples">Arrays, Sequences and Tuples</a></h4>
<p>See <a href="https://www.scala-lang.org/api/current/scala/collection/index.html">https://www.scala-lang.org/api/current/scala/collection/index.html</a> for docs.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val arr = Array(1,2,3) // &lt;Shift-Enter&gt; to declare an Array
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>arr: Array[Int] = Array(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val seq = Seq(1,2,3)    // &lt;Shift-Enter&gt; to declare a Seq
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>seq: Seq[Int] = List(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p>A tuple is a neat class that gives you a simple way to store heterogeneous (different) items in the same container. We will use tuples for key-value pairs in Spark.</p>
</blockquote>
<p>See <a href="https://docs.scala-lang.org/overviews/scala-book/tuples.html">https://docs.scala-lang.org/overviews/scala-book/tuples.html</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val myTuple = ('a',1) // a 2-tuple
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>myTuple: (Char, Int) = (a,1)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myTuple._1 // accessing the first element of the tuple. NOTE index starts at 1 not 0 for tuples
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: Char = a
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">myTuple._2 // accessing the second element of the tuple
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res3: Int = 1
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="functional-programming-and-mapreduce"><a class="header" href="#functional-programming-and-mapreduce">Functional Programming and MapReduce</a></h3>
<p><em>&quot;Functional programming is a style of programming that emphasizes writing applications using only pure functions and immutable values. As Alvin Alexander wrote in Functional Programming, Simplified, rather than using that description, it can be helpful to say that functional programmers have an extremely strong desire to see their code as math — to see the combination of their functions as a series of algebraic equations. In that regard, you could say that functional programmers like to think of themselves as mathematicians. That’s the driving desire that leads them to use only pure functions and immutable values, because that’s what you use in algebra and other forms of math.&quot;</em></p>
<p>See <a href="https://docs.scala-lang.org/overviews/scala-book/functional-programming.html">https://docs.scala-lang.org/overviews/scala-book/functional-programming.html</a> for short lessons in functional programming.</p>
<p>We will apply functions for processing elements of a scala collection to quickly demonstrate functional programming.</p>
<h4 id="five-ways-of-adding-1"><a class="header" href="#five-ways-of-adding-1">Five ways of adding 1</a></h4>
<p>The first four use anonymous functions and the last one uses a named method.</p>
<ol>
<li>
<p>explicit version: <code>%scala (x: Int) =&gt; x + 1</code></p>
</li>
<li>
<p>type-inferred more intuitive version: <code>%scala x =&gt; x + 1</code></p>
</li>
<li>
<p>placeholder syntax (each argument must be used exactly once): <code>%scala _ + 1</code></p>
</li>
<li>
<p>type-inferred more intuitive version with code-block for larger function body: <code>%scala x =&gt; {        // body is a block of code       val integerToAdd = 1       x + integerToAdd }</code></p>
</li>
<li>
<p>as methods using <code>def</code>: <code>%scala def addOne(x: Int): Int = x + 1</code></p>
</li>
</ol>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://superruzafa.github.io/visual-scala-reference/map/&quot;,500))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://superruzafa.github.io/visual-scala-reference/map/"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<p>Now, let's do some functional programming over scala collection (<code>List</code>) using some of their methods: <code>map</code>, <code>filter</code> and <code>reduce</code>. In the end we will write our first mapReduce program!</p>
<p>For more details see:</p>
<ul>
<li><a href="https://docs.scala-lang.org/overviews/scala-book/collections-methods.html">https://docs.scala-lang.org/overviews/scala-book/collections-methods.html</a></li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://superruzafa.github.io/visual-scala-reference/map/&quot;,500))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://superruzafa.github.io/visual-scala-reference/map/"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; to map each value x of lst with x+10 to return a new List(11, 12, 13)
lst.map(x =&gt; x + 10)  
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res6: List[Int] = List(11, 12, 13)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; for the same as above using place-holder syntax
lst.map( _ + 10)  
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res7: List[Int] = List(11, 12, 13)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://superruzafa.github.io/visual-scala-reference/filter/&quot;,600))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://superruzafa.github.io/visual-scala-reference/filter/"
 width="95%" height="600"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; to return a new List(1, 3) after filtering x's from lst if (x % 2 == 1) is true
lst.filter(x =&gt; (x % 2 == 1) )
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res9: List[Int] = List(1, 3)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; for the same as above using place-holder syntax
lst.filter( _ % 2 == 1 )
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res10: List[Int] = List(1, 3)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://superruzafa.github.io/visual-scala-reference/reduce/&quot;,600))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://superruzafa.github.io/visual-scala-reference/reduce/"
 width="95%" height="600"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; to use reduce to add elements of lst two at a time to return Int 6
lst.reduce( (x, y) =&gt; x + y )
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res12: Int = 6
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; for the same as above but using place-holder syntax
lst.reduce( _ + _ )
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res13: Int = 6
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's combine <code>map</code> and <code>reduce</code> programs above to find the sum of after 10 has been added to every element of the original List <code>lst</code> as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">lst.map(x =&gt; x+10)
   .reduce((x,y) =&gt; x+y) // &lt;Ctrl-Enter&gt; to get Int 36 = sum(1+10,2+10,3+10)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res14: Int = 36
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="exercise-in-functional-programming"><a class="header" href="#exercise-in-functional-programming">Exercise in Functional Programming</a></h4>
<p>You should spend an hour or so going through the Functional Programming Section of the Scala Book:</p>
<ul>
<li><a href="https://docs.scala-lang.org/overviews/scala-book/functional-programming.html">https://docs.scala-lang.org/overviews/scala-book/functional-programming.html</a></li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://docs.scala-lang.org/overviews/scala-book/functional-programming.html&quot;,700))
</code></pre>
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://docs.scala-lang.org/overviews/scala-book/functional-programming.html"
 width="95%" height="700"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<p>There are lots of methods in Scala Collections. And much more in this <em>scalable language</em>. See for example <a href="http://docs.scala-lang.org/cheatsheets/index.html">http://docs.scala-lang.org/cheatsheets/index.html</a>.</p>
</div>
<div class="cell markdown">
<h3 id="lazy-evaluation"><a class="header" href="#lazy-evaluation">Lazy Evaluation</a></h3>
<p>Another powerful programming concept we will need is <em>lazy evaluation</em> -- a form of delayed evaluation. So the value of an expression that is lazily evaluated is only available when it is actually needed.</p>
<p>This is to be contrasted with <em>eager evaluation</em> that we have seen so far -- an expression is immediately evaluated.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val eagerImmutableInt = 1 // eagerly evaluated as 1
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>eagerImmutableInt: Int = 1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">var eagerMutableInt = 2 // eagerly evaluated as 2
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>eagerMutableInt: Int = 2
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's demonstrate lazy evaluation using a <code>getTime</code> method and the keyword <code>lazy</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import java.util.Calendar
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import java.util.Calendar
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">lazy val lazyImmutableTime = Calendar.getInstance.getTime // lazily defined and not evaluated immediately
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>lazyImmutableTime: java.util.Date = &lt;lazy&gt;
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val eagerImmutableTime = Calendar.getInstance.getTime // egaerly evaluated immediately
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>eagerImmutableTime: java.util.Date = Wed Jan 13 23:54:53 UTC 2021
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(lazyImmutableTime) // evaluated when actully needed by println
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Wed Jan 13 23:54:53 UTC 2021
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(eagerImmutableTime) // prints what was already evaluated eagerly
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Wed Jan 13 23:54:53 UTC 2021
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def lazyDefinedInt = 5 // you can also use method to lazily define 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>lazyDefinedInt: Int
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">lazyDefinedInt // only evaluated now
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res18: Int = 5
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>See <a href="https://www.scala-exercises.org/scala_tutorial/lazy_evaluation">https://www.scala-exercises.org/scala<em>tutorial/lazy</em>evaluation</a> for more details including the following example with <code>StringBuilder</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val builder = new StringBuilder
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>builder: StringBuilder =
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = { builder += 'x'; 1 } // eagerly evaluates x as 1 after appending 'x' to builder. NOTE: ';' is used to separate multiple expressions on the same line
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: Int = 1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">builder.result()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res19: String = x
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res20: Int = 1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">builder.result() // calling x again should not append x again to builder
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res21: String = x
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">lazy val y = { builder += 'y'; 2 } // lazily evaluate y later when it is called
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>y: Int = &lt;lazy&gt;
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">builder.result() // builder should remain unchanged
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res22: String = x
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def z = { builder += 'z'; 3 } // lazily evaluate z later when the method is called
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>z: Int
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">builder.result() // builder should remain unchanged
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res23: String = x
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>What should <code>builder.result()</code> be after the following arithmetic expression involving <code>x</code>,<code>y</code> and <code>z</code> is evaluated?</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">z + y + x + z + y + x
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res24: Int = 12
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="lazy-evaluation-exercise---you-try-now"><a class="header" href="#lazy-evaluation-exercise---you-try-now">Lazy Evaluation Exercise - You try Now!</a></h4>
<p>Understand why the output above is what it is!</p>
<ul>
<li>Why is <code>z</code> different in its appearance in the final <code>builder</code> string when compared to <code>x</code> and <code>y</code> as we evaluated?</li>
</ul>
<p><code>z + y + x + z + y + x</code></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">builder.result() 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res25: String = xzyz
</code></pre>
</div>
</div>
<div class="cell markdown">
<h3 id="why-lazy"><a class="header" href="#why-lazy">Why Lazy?</a></h3>
<p>Imagine a more complex expression involving the evaluation of millions of values. Lazy evaluation will allow us to actually compute with big data when it may become impossible to hold all the values in memory. This is exactly what Apache Spark does as we will see.</p>
</div>
<div class="cell markdown">
<h3 id="recursions"><a class="header" href="#recursions">Recursions</a></h3>
<p>Recursion is a powerful framework when a function calls another function, including itself, until some terminal condition is reached.</p>
<p>Here we want to distinguish between two ways of implementing a recursion using a simple example of factorial.</p>
<p>Recall that for any natural number \(n\), its factorial is denoted and defined as follows:</p>
<p>\[
n!  :=  n \times (n-1) \times (n-2) \times \cdots \times 2 \times 1 
\]</p>
<p>which has the following recursive expression:</p>
<p>\[
n! = n*(n-1)! , , \qquad  0! = 1
\]</p>
<p>Let us implement it using two approaches: a naive approach that can run out of memory and another tail-recursive approach that uses constant memory. Read <a href="https://www.scala-exercises.org/scala_tutorial/tail_recursion">https://www.scala-exercises.org/scala<em>tutorial/tail</em>recursion</a> for details.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def factorialNaive(n: Int): Int =
  if (n == 0) 1 else n * factorialNaive(n - 1)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>factorialNaive: (n: Int)Int
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">factorialNaive(4)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res26: Int = 24
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>When <code>factorialNaive(4)</code> was evaluated above the following steps were actually done:</p>
<p><code>factorial(4) if (4 == 0) 1 else 4 * factorial(4 - 1) 4 * factorial(3) 4 * (3 * factorial(2)) 4 * (3 * (2 * factorial(1))) 4 * (3 * (2 * (1 * factorial(0))) 4 * (3 * (2 * (1 * 1))) 24</code></p>
</div>
<div class="cell markdown">
<p>Notice how we add one more element to our expression at each recursive call. Our expressions becomes bigger and bigger until we end by reducing it to the final value. So the final expression given by a directed acyclic graph (DAG) of the pairwise multiplications given by the right-branching binary tree, whose leaves are input integers and internal nodes are the bianry <code>*</code> operator, can get very large when the input <code>n</code> is large.</p>
<p><em>Tail recursion</em> is a sophisticated way of implementing certain recursions so that memory requirements can be kept constant, as opposed to naive recursions.</p>
<blockquote>
<p><a href="https://www.scala-exercises.org/scala_tutorial/tail_recursion">Tail Recursion</a></p>
</blockquote>
<blockquote>
<p>That difference in the rewriting rules actually translates directly to a difference in the actual execution on a computer. In fact, it turns out that <strong>if you have a recursive function that calls itself as its last action, then you can reuse the stack frame of that function. This is called tail recursion.</strong></p>
</blockquote>
<blockquote>
<p>And by applying that trick, a tail recursive function can execute in constant stack space, so it's really just another formulation of an iterative process. We could say a tail recursive function is the functional form of a loop, and it executes just as efficiently as a loop.</p>
</blockquote>
<p>Implementation of tail recursion in the Exercise below uses Scala <a href="https://docs.scala-lang.org/tour/annotations.html">annotation</a>, which is a way to associate meta-information with definitions. In our case, the annotation <code>@tailrec</code> ensures that a method is indeed <a href="https://en.wikipedia.org/wiki/Tail_call">tail-recursive</a>. See the last link to understand how memory requirements can be kept constant in tail recursions.</p>
<p>We mainly want you to know that tail recursions are an important functional programming concept.</p>
</div>
<div class="cell markdown">
<h4 id="tail-recursion-exercise---you-try-now"><a class="header" href="#tail-recursion-exercise---you-try-now">Tail Recursion Exercise - You Try Now</a></h4>
<p>Replace <code>???</code> with the correct values to make this a tail recursion for factorial.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import scala.annotation.tailrec

// replace ??? with the right values to make this a tail recursion for factorial
def factorialTail(n: Int): Int = {
  @tailrec
  def iter(x: Int, result: Int): Int =
    if ( x == ??? ) result
    else iter(x - 1, result * x)

  iter( n, ??? )
}
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">factorialTail(3) //shouldBe 6
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">factorialTail(4) //shouldBe 24
</code></pre>
</div>
<div class="cell markdown">
<p>Functional Programming is a vast subject and we are merely covering the fewest core ideas to get started with Apache Spark asap.</p>
<p>We will return to more concepts as we need them in the sequel.</p>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="introduction-to-spark"><a class="header" href="#introduction-to-spark">Introduction to Spark</a></h1>
<h2 id="spark-essentials-rdds-transformations-and-actions"><a class="header" href="#spark-essentials-rdds-transformations-and-actions">Spark Essentials: RDDs, Transformations and Actions</a></h2>
<ul>
<li>This introductory notebook describes how to get started running Spark (Scala) code in Notebooks.</li>
<li>Working with Spark's Resilient Distributed Datasets (RDDs)
<ul>
<li>creating RDDs</li>
<li>performing basic transformations on RDDs</li>
<li>performing basic actions on RDDs</li>
</ul>
</li>
</ul>
<p><strong>RECOLLECT</strong> from <code>001_WhySpark</code> notebook and AJ's videos that <em>Spark does fault-tolerant, distributed, in-memory computing</em></p>
<p><strong>THEORY CAVEAT</strong> This module is focused on getting you to quickly write Spark programs with a high-level appreciation of the underlying concepts.</p>
<p>In the last module, we will spend more time on analyzing the core algorithms in parallel and distributed setting of a typical Spark cluster today -- where several multi-core parallel computers (Spark workers) are networked together to provide a fault-tolerant distributed computing platform.</p>
</div>
<div class="cell markdown">
<h2 id="spark-cluster-overview"><a class="header" href="#spark-cluster-overview">Spark Cluster Overview:</a></h2>
<p><strong>Driver Program, Cluster Manager and Worker Nodes</strong></p>
<p>The <em>driver</em> does the following:</p>
<ol>
<li>connects to a <em>cluster manager</em> to allocate resources across applications</li>
</ol>
<ul>
<li>acquire <em>executors</em> on cluster nodes
<ul>
<li>executor processs run compute tasks and cache data in memory or disk on a <em>worker node</em></li>
</ul>
</li>
<li>sends <em>application</em> (user program built on Spark) to the executors</li>
<li>sends <em>tasks</em> for the executors to run
<ul>
<li>task is a unit of work that will be sent to one executor</li>
</ul>
</li>
</ul>
<p><img src="http://spark.apache.org/docs/latest/img/cluster-overview.png" alt="" /></p>
<p>See <a href="http://spark.apache.org/docs/latest/cluster-overview.html">http://spark.apache.org/docs/latest/cluster-overview.html</a> for an overview of the spark cluster.</p>
</div>
<div class="cell markdown">
<h2 id="the-abstraction-of-resilient-distributed-dataset-rdd"><a class="header" href="#the-abstraction-of-resilient-distributed-dataset-rdd">The Abstraction of Resilient Distributed Dataset (RDD)</a></h2>
<p><strong>RDD is a fault-tolerant collection of elements that can be operated on in parallel.</strong></p>
<p><strong>Two types of Operations are possible on an RDD:</strong></p>
<ul>
<li>Transformations</li>
<li>Actions</li>
</ul>
<p><strong>(watch now 2:26)</strong>:</p>
<p><a href="https://www.youtube.com/watch?v=3nreQ1N7Jvk?rel=0&amp;autoplay=1&amp;modestbranding=1&amp;start=1&amp;end=146"><img src="http://img.youtube.com/vi/3nreQ1N7Jvk/0.jpg" alt="RDD in Spark by Anthony Joseph in BerkeleyX/CS100.1x" /></a></p>
<hr />
<h3 id="transformations"><a class="header" href="#transformations">Transformations</a></h3>
<p><strong>(watch now 1:18)</strong>:</p>
<p><a href="https://www.youtube.com/watch?v=360UHWy052k?rel=0&amp;autoplay=1&amp;modestbranding=1"><img src="http://img.youtube.com/vi/360UHWy052k/0.jpg" alt="Spark Transformations by Anthony Joseph in BerkeleyX/CS100.1x" /></a></p>
<hr />
<h3 id="actions"><a class="header" href="#actions">Actions</a></h3>
<p><strong>(watch now 0:48)</strong>:</p>
<p><a href="https://www.youtube.com/watch?v=F2G4Wbc5ZWQ?rel=0&amp;autoplay=1&amp;modestbranding=1&amp;start=1&amp;end=48"><img src="http://img.youtube.com/vi/F2G4Wbc5ZWQ/0.jpg" alt="Spark Actions by Anthony Joseph in BerkeleyX/CS100.1x" /></a></p>
<hr />
<h3 id="key-points"><a class="header" href="#key-points">Key Points</a></h3>
<ul>
<li>Resilient distributed datasets (RDDs) are the primary abstraction in Spark.</li>
<li>RDDs are immutable once created:
<ul>
<li>can transform it.</li>
<li>can perform actions on it.</li>
<li>but cannot change an RDD once you construct it.</li>
</ul>
</li>
<li>Spark tracks each RDD's lineage information or recipe to enable its efficient recomputation if a machine fails.</li>
<li>RDDs enable operations on collections of elements in parallel.</li>
<li>We can construct RDDs by:
<ul>
<li>parallelizing Scala collections such as lists or arrays</li>
<li>by transforming an existing RDD,</li>
<li>from files in distributed file systems such as (HDFS, S3, etc.).</li>
</ul>
</li>
<li>We can specify the number of partitions for an RDD</li>
<li>The more partitions in an RDD, the more opportunities for parallelism</li>
<li>There are <strong>two types of operations</strong> you can perform on an RDD:
<ul>
<li><strong>transformations</strong> (are lazily evaluated)
<ul>
<li>map</li>
<li>flatMap</li>
<li>filter</li>
<li>distinct</li>
<li>...</li>
</ul>
</li>
<li><strong>actions</strong> (actual evaluation happens)
<ul>
<li>count</li>
<li>reduce</li>
<li>take</li>
<li>collect</li>
<li>takeOrdered</li>
<li>...</li>
</ul>
</li>
</ul>
</li>
<li>Spark transformations enable us to create new RDDs from an existing RDD.</li>
<li>RDD transformations are lazy evaluations (results are not computed right away)</li>
<li>Spark remembers the set of transformations that are applied to a base data set (this is the lineage graph of RDD)</li>
<li>The allows Spark to automatically recover RDDs from failures and slow workers.</li>
<li>The lineage graph is a recipe for creating a result and it can be optimized before execution.</li>
<li>A transformed RDD is executed only when an action runs on it.</li>
<li>You can also persist, or cache, RDDs in memory or on disk (this speeds up iterative ML algorithms that transforms the initial RDD iteratively).</li>
<li>Here is a great reference URL for programming guides for Spark that one should try to cover first
<ul>
<li><a href="http://spark.apache.org/docs/latest/programming-guide.html">http://spark.apache.org/docs/latest/programming-guide.html</a>.</li>
<li>and specifically for RDDs: <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">https://spark.apache.org/docs/latest/rdd-programming-guide.html</a></li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://spark.apache.org/docs/latest/rdd-programming-guide.html"
 width="95%" height="700"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h2 id="lets-get-our-hands-dirty-in-spark"><a class="header" href="#lets-get-our-hands-dirty-in-spark">Let's get our hands dirty in Spark!</a></h2>
<p><strong>DO NOW!</strong></p>
<p>In your databricks community edition:</p>
<ol>
<li>In your <code>WorkSpace</code> create a Folder named <code>scalable-data-science</code></li>
<li><em>Import</em> the databricks archive file at the following URL:
<ul>
<li><a href="https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/2017/parts/xtraResources.dbc">https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/2017/parts/xtraResources.dbc</a></li>
</ul>
</li>
<li>This should open a structure of directories in with path: <code>/Workspace/scalable-data-science/xtraResources/</code></li>
</ol>
</div>
<div class="cell markdown">
<p><strong>Let us look at the legend and overview of the visual RDD Api by doing the following first:</strong></p>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-1.png" alt="" /></p>
</div>
<div class="cell markdown">
<h3 id="running-spark"><a class="header" href="#running-spark">Running <strong>Spark</strong></a></h3>
<p>The variable <strong>sc</strong> allows you to access a Spark Context to run your Spark programs. Recall <code>SparkContext</code> is in the Driver Program.</p>
<p><img src="http://spark.apache.org/docs/latest/img/cluster-overview.png" alt="" /></p>
<p>**NOTE: Do not create the <em>sc</em> variable - it is already initialized for you in spark-shell REPL, that includes notebook environments like databricks, Jupyter, zeppelin, etc. **</p>
</div>
<div class="cell markdown">
<h3 id="we-will-do-the-following-next"><a class="header" href="#we-will-do-the-following-next">We will do the following next:</a></h3>
<ol>
<li>Create an RDD using <code>sc.parallelize</code></li>
</ol>
<ul>
<li>Perform the <code>collect</code> action on the RDD and find the number of partitions it is made of using <code>getNumPartitions</code> action</li>
<li>Perform the <code>take</code> action on the RDD</li>
<li>Transform the RDD by <code>map</code> to make another RDD</li>
<li>Transform the RDD by <code>filter</code> to make another RDD</li>
<li>Perform the <code>reduce</code> action on the RDD</li>
<li>Transform the RDD by <code>flatMap</code> to make another RDD</li>
<li>Create a Pair RDD</li>
<li>Perform some transformations on a Pair RDD</li>
<li>Where in the cluster is your computation running?</li>
<li>Shipping Closures, Broadcast Variables and Accumulator Variables</li>
<li>Spark Essentials: Summary</li>
<li>HOMEWORK</li>
<li>Importing Standard Scala and Java libraries</li>
</ul>
</div>
<div class="cell markdown">
<h4 id="entry-point"><a class="header" href="#entry-point">Entry Point</a></h4>
<p>Now we are ready to start programming in Spark!</p>
<p>Our entry point for Spark 2.x applications is the class <code>SparkSession</code>. An instance of this object is already instantiated for us which can be easily demonstrated by running the next cell</p>
<p>We will need these docs!</p>
<ul>
<li><a href="https://spark.apache.org/docs/3.0.1/api/scala/org/apache/spark/rdd/RDD.html">RDD Scala Docs</a></li>
<li><a href="https://spark.apache.org/docs/3.0.1/api/scala/org/apache/spark/sql/Dataset.html">Dataset Scala Docs</a></li>
<li><a href="https://spark.apache.org/docs/3.0.1/api/scala/index.html">https://spark.apache.org/docs/3.0.1/api/scala/index.html</a> you can simply search for other Spark classes, methods, etc here</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(spark)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>org.apache.spark.sql.SparkSession@69141846
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>NOTE that since Spark 2.0 <code>SparkSession</code> is a replacement for the other entry points: * <code>SparkContext</code>, available in our notebook as <strong>sc</strong>. * <code>SQLContext</code>, or more specifically its subclass <code>HiveContext</code>, available in our notebook as <strong>sqlContext</strong>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(sc)
println(sqlContext)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>org.apache.spark.SparkContext@517c9049
org.apache.spark.sql.hive.HiveContext@6c5b5052
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>We will be using the pre-made SparkContext <code>sc</code> when learning about RDDs.</p>
</div>
<div class="cell markdown">
<h4 id="1-create-an-rdd-using-scparallelize"><a class="header" href="#1-create-an-rdd-using-scparallelize">1. Create an RDD using <code>sc.parallelize</code></a></h4>
<p>First, let us create an RDD of three elements (of integer type <code>Int</code>) from a Scala <code>Seq</code> (or <code>List</code> or <code>Array</code>) with two partitions by using the <code>parallelize</code> method of the available Spark Context <code>sc</code> as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Array(1, 2, 3), 2)    // &lt;Ctrl+Enter&gt; to evaluate this cell (using 2 partitions)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[33] at parallelize at command-685894176422457:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//x.  // place the cursor after 'x.' and hit Tab to see the methods available for the RDD x we created
</code></pre>
</div>
<div class="cell markdown">
<h4 id="2-perform-the-collect-action-on-the-rdd-and-find-the-number-of-partitions-in-it-using-getnumpartitions-action"><a class="header" href="#2-perform-the-collect-action-on-the-rdd-and-find-the-number-of-partitions-in-it-using-getnumpartitions-action">2. Perform the <code>collect</code> action on the RDD and find the number of partitions in it using <code>getNumPartitions</code> action</a></h4>
<p>No action has been taken by <code>sc.parallelize</code> above. To see what is &quot;cooked&quot; by the recipe for RDD <code>x</code> we need to take an action.</p>
<p>The simplest is the <code>collect</code> action which returns all of the elements of the RDD as an <code>Array</code> to the driver program and displays it.</p>
<p><em>So you have to make sure that all of that data will fit in the driver program if you call <code>collect</code> action!</em></p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-a-hrefcontents000_1-sds-3-xworkspacescalable-data-sciencextraresourcesvisualrddapirecallactionscollectcollect-action-in-detaila-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-a-hrefcontents000_1-sds-3-xworkspacescalable-data-sciencextraresourcesvisualrddapirecallactionscollectcollect-action-in-detaila-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/collect">collect action in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-90.png" alt="" /></p>
</div>
<div class="cell markdown">
<p>Let us perform a <code>collect</code> action on RDD <code>x</code> as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x.collect()    // &lt;Ctrl+Enter&gt; to collect (action) elements of rdd; should be (1, 2, 3)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res44: Array[Int] = Array(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p><em>CAUTION:</em> <code>collect</code> can crash the driver when called upon an RDD with massively many elements.
So, it is better to use other diplaying actions like <code>take</code> or <code>takeOrdered</code> as follows:</p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-a-hrefcontents000_1-sds-3-xworkspacescalable-data-sciencextraresourcesvisualrddapirecallactionsgetnumpartitionsgetnumpartitions-action-in-detaila-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-a-hrefcontents000_1-sds-3-xworkspacescalable-data-sciencextraresourcesvisualrddapirecallactionsgetnumpartitionsgetnumpartitions-action-in-detaila-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/getNumPartitions">getNumPartitions action in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-88.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; to evaluate this cell and find the number of partitions in RDD x
x.getNumPartitions 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res45: Int = 2
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>We can see which elements of the RDD are in which parition by calling <code>glom()</code> before <code>collect()</code>.</p>
<p><code>glom()</code> flattens elements of the same partition into an <code>Array</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x.glom().collect() // glom() flattens elements on the same partition
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res46: Array[Array[Int]] = Array(Array(1), Array(2, 3))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val a = x.glom().collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>a: Array[Array[Int]] = Array(Array(1), Array(2, 3))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Thus from the output above, <code>Array[Array[Int]] = Array(Array(1), Array(2, 3))</code>, we know that <code>1</code> is in one partition while <code>2</code> and <code>3</code> are in another partition.</p>
</div>
<div class="cell markdown">
<h5 id="you-try"><a class="header" href="#you-try">You Try!</a></h5>
<p>Crate an RDD <code>x</code> with three elements, 1,2,3, and this time do not specifiy the number of partitions. Then the default number of partitions will be used. Find out what this is for the cluster you are attached to.</p>
<p>The default number of partitions for an RDD depends on the cluster this notebook is attached to among others - see <a href="http://spark.apache.org/docs/latest/programming-guide.html">programming-guide</a>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val x = sc.parallelize(Seq(1, 2, 3))    // &lt;Shift+Enter&gt; to evaluate this cell (using default number of partitions)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[36] at parallelize at command-685894176422471:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x.getNumPartitions // &lt;Shift+Enter&gt; to evaluate this cell
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res47: Int = 8
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x.glom().collect() // &lt;Ctrl+Enter&gt; to evaluate this cell
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res48: Array[Array[Int]] = Array(Array(), Array(), Array(1), Array(), Array(), Array(2), Array(), Array(3))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="3-perform-the-take-action-on-the-rdd"><a class="header" href="#3-perform-the-take-action-on-the-rdd">3. Perform the <code>take</code> action on the RDD</a></h4>
<p>The <code>.take(n)</code> action returns an array with the first <code>n</code> elements of the RDD.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">x.take(2) // Ctrl+Enter to take two elements from the RDD x
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res49: Array[Int] = Array(1, 2)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h5 id="you-try-1"><a class="header" href="#you-try-1">You Try!</a></h5>
<p>Fill in the parenthes <code>( )</code> below in order to <code>take</code> just one element from RDD <code>x</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//x.take(1) // uncomment by removing '//' before x in the cell and fill in the parenthesis to take just one element from RDD x and Cntrl+Enter
</code></pre>
</div>
<div class="cell markdown">
<hr />
<h4 id="4-transform-the-rdd-by-map-to-make-another-rdd"><a class="header" href="#4-transform-the-rdd-by-map-to-make-another-rdd">4. Transform the RDD by <code>map</code> to make another RDD</a></h4>
<p>The <code>map</code> transformation returns a new RDD that's formed by passing each element of the source RDD through a function (closure). The closure is automatically passed on to the workers for evaluation (when an action is called later).</p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-a-hrefcontents000_1-sds-3-xworkspacescalable-data-sciencextraresourcesvisualrddapirecalltransformationsmapmap-transformation-in-detaila-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-a-hrefcontents000_1-sds-3-xworkspacescalable-data-sciencextraresourcesvisualrddapirecalltransformationsmapmap-transformation-in-detaila-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/map">map transformation in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-18.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Shift+Enter to make RDD x and RDD y that is mapped from x
val x = sc.parallelize(Array(&quot;b&quot;, &quot;a&quot;, &quot;c&quot;)) // make RDD x: [b, a, c]
val y = x.map(z =&gt; (z,1))                    // map x into RDD y: [(b, 1), (a, 1), (c, 1)]
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[38] at parallelize at command-685894176422480:2
y: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[39] at map at command-685894176422480:3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Cntrl+Enter to collect and print the two RDDs
println(x.collect().mkString(&quot;, &quot;))
println(y.collect().mkString(&quot;, &quot;))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>b, a, c
(b,1), (a,1), (c,1)
</code></pre>
</div>
</div>
<div class="cell markdown">
<hr />
<h4 id="5-transform-the-rdd-by-filter-to-make-another-rdd"><a class="header" href="#5-transform-the-rdd-by-filter-to-make-another-rdd">5. Transform the RDD by <code>filter</code> to make another RDD</a></h4>
<p>The <code>filter</code> transformation returns a new RDD that's formed by selecting those elements of the source RDD on which the function returns <code>true</code>.</p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-a-hrefcontents000_1-sds-3-xworkspacescalable-data-sciencextraresourcesvisualrddapirecalltransformationsfilterfilter-transformation-in-detaila-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-a-hrefcontents000_1-sds-3-xworkspacescalable-data-sciencextraresourcesvisualrddapirecalltransformationsfilterfilter-transformation-in-detaila-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/filter">filter transformation in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-24.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Shift+Enter to make RDD x and filter it by (n =&gt; n%2 == 1) to make RDD y
val x = sc.parallelize(Array(1,2,3))
// the closure (n =&gt; n%2 == 1) in the filter will 
// return True if element n in RDD x has remainder 1 when divided by 2 (i.e., if n is odd)
val y = x.filter(n =&gt; n%2 == 1) 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[40] at parallelize at command-685894176422484:2
y: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[41] at filter at command-685894176422484:5
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Cntrl+Enter to collect and print the two RDDs
println(x.collect().mkString(&quot;, &quot;))
println(y.collect().mkString(&quot;, &quot;))
//y.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1, 2, 3
1, 3
</code></pre>
</div>
</div>
<div class="cell markdown">
<hr />
<h4 id="6-perform-the-reduce-action-on-the-rdd"><a class="header" href="#6-perform-the-reduce-action-on-the-rdd">6. Perform the <code>reduce</code> action on the RDD</a></h4>
<p>Reduce aggregates a data set element using a function (closure). This function takes two arguments and returns one and can often be seen as a binary operator. This operator has to be commutative and associative so that it can be computed correctly in parallel (where we have little control over the order of the operations!).</p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-a-hrefcontents000_1-sds-3-xworkspacescalable-data-sciencextraresourcesvisualrddapirecallactionsreducereduce-action-in-detaila-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-a-hrefcontents000_1-sds-3-xworkspacescalable-data-sciencextraresourcesvisualrddapirecallactionsreducereduce-action-in-detaila-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/actions/reduce">reduce action in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-94.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Shift+Enter to make RDD x of inteegrs 1,2,3,4 and reduce it to sum
val x = sc.parallelize(Array(1,2,3,4))
val y = x.reduce((a,b) =&gt; a+b)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[42] at parallelize at command-685894176422488:2
y: Int = 10
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Cntrl+Enter to collect and print RDD x and the Int y, sum of x
println(x.collect.mkString(&quot;, &quot;))
println(y)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1, 2, 3, 4
10
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="7-transform-an-rdd-by-flatmap-to-make-another-rdd"><a class="header" href="#7-transform-an-rdd-by-flatmap-to-make-another-rdd">7. Transform an RDD by <code>flatMap</code> to make another RDD</a></h4>
<p><code>flatMap</code> is similar to <code>map</code> but each element from input RDD can be mapped to zero or more output elements. Therefore your function should return a sequential collection such as an <code>Array</code> rather than a single element as shown below.</p>
</div>
<div class="cell markdown">
<h5 id="let-us-look-at-the-a-hrefcontents000_1-sds-3-xworkspacescalable-data-sciencextraresourcesvisualrddapirecalltransformationsflatmapflatmap-transformation-in-detaila-and-return-here-to-try-out-the-example-codes"><a class="header" href="#let-us-look-at-the-a-hrefcontents000_1-sds-3-xworkspacescalable-data-sciencextraresourcesvisualrddapirecalltransformationsflatmapflatmap-transformation-in-detaila-and-return-here-to-try-out-the-example-codes">Let us look at the <a href="contents/000_1-sds-3-x//#workspace/scalable-data-science/xtraResources/visualRDDApi/recall/transformations/flatMap">flatMap transformation in detail</a> and return here to try out the example codes.</a></h5>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-31.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Shift+Enter to make RDD x and flatMap it into RDD by closure (n =&gt; Array(n, n*100, 42))
val x = sc.parallelize(Array(1,2,3))
val y = x.flatMap(n =&gt; Array(n, n*100, 42))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[43] at parallelize at command-685894176422492:2
y: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[44] at flatMap at command-685894176422492:3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Cntrl+Enter to collect and print RDDs x and y
println(x.collect().mkString(&quot;, &quot;))
println(y.collect().mkString(&quot;, &quot;))
sc.parallelize(Array(1,2,3)).map(n =&gt; Array(n,n*100,42)).collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>1, 2, 3
1, 100, 42, 2, 200, 42, 3, 300, 42
res54: Array[Array[Int]] = Array(Array(1, 100, 42), Array(2, 200, 42), Array(3, 300, 42))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="8-create-a-pair-rdd"><a class="header" href="#8-create-a-pair-rdd">8. Create a Pair RDD</a></h4>
<p>Let's next work with RDD of <code>(key,value)</code> pairs called a <em>Pair RDD</em> or <em>Key-Value RDD</em>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Cntrl+Enter to make RDD words and display it by collect
val words = sc.parallelize(Array(&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;))
words.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[47] at parallelize at command-685894176422495:2
res55: Array[String] = Array(a, b, a, a, b, b, a, a, a, b, b)
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's make a Pair RDD called <code>wordCountPairRDD</code> that is made of (key,value) pairs with key=word and value=1 in order to encode each occurrence of each word in the RDD <code>words</code>, as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Cntrl+Enter to make and collect Pair RDD wordCountPairRDD
val wordCountPairRDD = words.map(s =&gt; (s, 1))
wordCountPairRDD.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wordCountPairRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[48] at map at command-685894176422497:2
res56: Array[(String, Int)] = Array((a,1), (b,1), (a,1), (a,1), (b,1), (b,1), (a,1), (a,1), (a,1), (b,1), (b,1))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="wide-transformations-and-shuffles"><a class="header" href="#wide-transformations-and-shuffles">Wide Transformations and Shuffles</a></h4>
<p>So far we have seen transformations that are <strong>narrow</strong> -- with no data transfer between partitions. Think of <code>map</code>.</p>
<p><code>ReduceByKey</code> and <code>GroupByKey</code> are <strong>wide</strong> transformations as data has to be shuffled across the partitions in different executors -- this is generally very expensive operation.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/visualapi/med/visualapi-40.png" alt="" /></p>
</div>
<div class="cell markdown">
<p>READ the <strong>Background</strong> about Shuffles in the programming guide below.</p>
<blockquote>
<p>In Spark, data is generally not distributed across partitions to be in the necessary place for a specific operation. During computations, a single task will operate on a single partition - thus, to organize all the data for a single reduceByKey reduce task to execute, Spark needs to perform an all-to-all operation. It must read from all partitions to find all the values for all keys, and then bring together values across partitions to compute the final result for each key - this is called the shuffle</p>
</blockquote>
<p>READ the <strong>Performance Impact</strong> about Shuffles in the programming guide below.</p>
<blockquote>
<p>The Shuffle is an expensive operation since it involves disk I/O, data serialization, and network I/O. To organize data for the shuffle, Spark generates sets of tasks - map tasks to organize the data, and a set of reduce tasks to aggregate it. This nomenclature comes from MapReduce and does not directly relate to Spark’s map and reduce operations.</p>
</blockquote>
<blockquote>
<p>Internally, results from individual map tasks are kept in memory until they can’t fit. Then, these are sorted based on the target partition and written to a single file. On the reduce side, tasks read the relevant sorted blocks.</p>
</blockquote>
<p><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations">https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h4 id="9-perform-some-transformations-on-a-pair-rdd"><a class="header" href="#9-perform-some-transformations-on-a-pair-rdd">9. Perform some transformations on a Pair RDD</a></h4>
<p>Let's next work with RDD of <code>(key,value)</code> pairs called a <em>Pair RDD</em> or <em>Key-Value RDD</em>.</p>
<p>Now some of the Key-Value transformations that we could perform include the following.</p>
<ul>
<li><strong><code>reduceByKey</code> transformation</strong>
<ul>
<li>which takes an RDD and returns a new RDD of key-value pairs, such that:
<ul>
<li>the values for each key are aggregated using the given reduced function</li>
<li>and the reduce function has to be of the type that takes two values and returns one value.</li>
</ul>
</li>
</ul>
</li>
<li><strong><code>sortByKey</code> transformation</strong>
<ul>
<li>this returns a new RDD of key-value pairs that's sorted by keys in ascending order</li>
</ul>
</li>
<li><strong><code>groupByKey</code> transformation</strong>
<ul>
<li>this returns a new RDD consisting of key and iterable-valued pairs.</li>
</ul>
</li>
</ul>
<p>Let's see some concrete examples next.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-44.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Cntrl+Enter to reduceByKey and collect wordcounts RDD
//val wordcounts = wordCountPairRDD.reduceByKey( _ + _ )
val wordcounts = wordCountPairRDD.reduceByKey( (value1, value2) =&gt; value1 + value2 )
wordcounts.collect()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wordcounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[49] at reduceByKey at command-685894176422504:3
res58: Array[(String, Int)] = Array((a,6), (b,5))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Now, let us do just the crucial steps and avoid collecting intermediate RDDs (something we should avoid for large datasets anyways, as they may not fit in the driver program).</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Cntrl+Enter to make words RDD and do the word count in two lines
val words = sc.parallelize(Array(&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;))
val wordcounts = words
                    .map(s =&gt; (s, 1))
                    .reduceByKey(_ + _)
                    .collect() 
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[50] at parallelize at command-685894176422506:2
wordcounts: Array[(String, Int)] = Array((a,6), (b,5))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h5 id="you-try-2"><a class="header" href="#you-try-2">You Try!</a></h5>
<p>You try evaluating <code>sortByKey()</code> which will make a new RDD that consists of the elements of the original pair RDD that are sorted by Keys.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Shift+Enter and comprehend code
val words = sc.parallelize(Array(&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;))
val wordCountPairRDD = words.map(s =&gt; (s, 1))
val wordCountPairRDDSortedByKey = wordCountPairRDD.sortByKey()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[53] at parallelize at command-685894176422508:2
wordCountPairRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[54] at map at command-685894176422508:3
wordCountPairRDDSortedByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[57] at sortByKey at command-685894176422508:4
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">wordCountPairRDD.collect() // Shift+Enter and comprehend code
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res59: Array[(String, Int)] = Array((a,1), (b,1), (a,1), (a,1), (b,1), (b,1), (a,1), (a,1), (a,1), (b,1), (b,1))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">wordCountPairRDDSortedByKey.collect() // Cntrl+Enter and comprehend code
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res60: Array[(String, Int)] = Array((a,1), (a,1), (a,1), (a,1), (a,1), (a,1), (b,1), (b,1), (b,1), (b,1), (b,1))
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>The next key value transformation we will see is <code>groupByKey</code></p>
<p>When we apply the <code>groupByKey</code> transformation to <code>wordCountPairRDD</code> we end up with a new RDD that contains two elements. The first element is the tuple <code>b</code> and an iterable <code>CompactBuffer(1,1,1,1,1)</code> obtained by grouping the value <code>1</code> for each of the five key value pairs <code>(b,1)</code>. Similarly the second element is the key <code>a</code> and an iterable <code>CompactBuffer(1,1,1,1,1,1)</code> obtained by grouping the value <code>1</code> for each of the six key value pairs <code>(a,1)</code>.</p>
<p><em>CAUTION</em>: <code>groupByKey</code> can cause a large amount of data movement across the network. It also can create very large iterables at a worker. Imagine you have an RDD where you have 1 billion pairs that have the key <code>a</code>. All of the values will have to fit in a single worker if you use group by key. So instead of a group by key, consider using reduced by key.</p>
</div>
<div class="cell markdown">
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/db/visualapi/med/visualapi-45.png" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wordCountPairRDDGroupByKey = wordCountPairRDD.groupByKey() // &lt;Shift+Enter&gt; CAUTION: this transformation can be very wide!
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>wordCountPairRDDGroupByKey: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[58] at groupByKey at command-685894176422513:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">wordCountPairRDDGroupByKey.collect()  // Cntrl+Enter
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res61: Array[(String, Iterable[Int])] = Array((a,CompactBuffer(1, 1, 1, 1, 1, 1)), (b,CompactBuffer(1, 1, 1, 1, 1)))
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="10-understanding-closures---where-in-the-cluster-is-your-computation-running"><a class="header" href="#10-understanding-closures---where-in-the-cluster-is-your-computation-running">10. Understanding Closures - Where in the cluster is your computation running?</a></h4>
<blockquote>
<p>One of the harder things about Spark is understanding the scope and life cycle of variables and methods when executing code across a cluster. RDD operations that modify variables outside of their scope can be a frequent source of confusion. In the example below we’ll look at code that uses <code>foreach()</code> to increment a counter, but similar issues can occur for other operations as well.</p>
</blockquote>
<p><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-">https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val data = Array(1, 2, 3, 4, 5)
var counter = 0
var rdd = sc.parallelize(data)

// Wrong: Don't do this!!
rdd.foreach(x =&gt; counter += x)

println(&quot;Counter value: &quot; + counter)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Counter value: 0
data: Array[Int] = Array(1, 2, 3, 4, 5)
counter: Int = 0
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[59] at parallelize at command-685894176422517:3
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>From RDD programming guide:</p>
<blockquote>
<p>The behavior of the above code is undefined, and may not work as intended. To execute jobs, Spark breaks up the processing of RDD operations into tasks, each of which is executed by an executor. Prior to execution, Spark computes the task’s closure. The closure is those variables and methods which must be visible for the executor to perform its computations on the RDD (in this case foreach()). This closure is serialized and sent to each executor.</p>
</blockquote>
<blockquote>
<p>The variables within the closure sent to each executor are now copies and thus, when counter is referenced within the foreach function, it’s no longer the counter on the driver node. There is still a counter in the memory of the driver node but this is no longer visible to the executors! The executors only see the copy from the serialized closure. Thus, the final value of counter will still be zero since all operations on counter were referencing the value within the serialized closure.</p>
</blockquote>
</div>
<div class="cell markdown">
<h4 id="11-shipping-closures-broadcast-variables-and-accumulator-variables"><a class="header" href="#11-shipping-closures-broadcast-variables-and-accumulator-variables">11. Shipping Closures, Broadcast Variables and Accumulator Variables</a></h4>
<h5 id="closures-broadcast-and-accumulator-variables"><a class="header" href="#closures-broadcast-and-accumulator-variables">Closures, Broadcast and Accumulator Variables</a></h5>
<p><strong>(watch now 2:06)</strong>:</p>
<p><a href="https://www.youtube.com/watch?v=I9Zcr4R35Ao?rel=0&amp;autoplay=1&amp;modestbranding=1"><img src="http://img.youtube.com/vi/I9Zcr4R35Ao/0.jpg" alt="Closures, Broadcast and Accumulators by Anthony Joseph in BerkeleyX/CS100.1x" /></a></p>
<p>We will use these variables in the sequel.</p>
<h5 id="summary"><a class="header" href="#summary">SUMMARY</a></h5>
<p>Spark automatically creates closures</p>
<ul>
<li>for functions that run on RDDs at workers,</li>
<li>and for any global variables that are used by those workers</li>
<li>one closure per worker is sent with every task</li>
<li>and there's no communication between workers</li>
<li>closures are one way from the driver to the worker</li>
<li>any changes that you make to the global variables at the workers
<ul>
<li>are not sent to the driver or</li>
<li>are not sent to other workers.</li>
</ul>
</li>
</ul>
<p>The problem we have is that these closures</p>
<ul>
<li>are automatically created are sent or re-sent with every job</li>
<li>with a large global variable it gets inefficient to send/resend lots of data to each worker</li>
<li>we cannot communicate that back to the driver</li>
</ul>
<p>To do this, Spark provides shared variables in two different types.</p>
<ul>
<li><strong>broadcast variables</strong>
<ul>
<li>lets us to efficiently send large read-only values to all of the workers</li>
<li>these are saved at the workers for use in one or more Spark operations.</li>
</ul>
</li>
<li><strong>accumulator variables</strong>
<ul>
<li>These allow us to aggregate values from workers back to the driver.</li>
<li>only the driver can access the value of the accumulator</li>
<li>for the tasks, the accumulators are basically write-only</li>
</ul>
</li>
</ul>
<hr />
</div>
<div class="cell markdown">
<h5 id="accumulators"><a class="header" href="#accumulators">Accumulators</a></h5>
<blockquote>
<p>Accumulators are variables that are only “added” to through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types.</p>
</blockquote>
<p>Read: <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators">https://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators</a>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p>A numeric accumulator can be created by calling SparkContext.longAccumulator() or SparkContext.doubleAccumulator() to accumulate values of type Long or Double, respectively. Tasks running on a cluster can then add to it using the add method. However, they cannot read its value. Only the driver program can read the accumulator’s value, using its value method.</p>
</blockquote>
<blockquote>
<p>The code below shows an accumulator being used to add up the elements of an array:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val accum = sc.longAccumulator(&quot;My Accumulator&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 1891, name: Some(My Accumulator), value: 0)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum.add(x))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">accum.value
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res66: Long = 10
</code></pre>
</div>
</div>
<div class="cell markdown">
<h5 id="broadcast-variables"><a class="header" href="#broadcast-variables">Broadcast Variables</a></h5>
<p>From <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables">https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables</a>:</p>
<blockquote>
<p>Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.</p>
</blockquote>
<blockquote>
<p>Spark actions are executed through a set of stages, separated by distributed “shuffle” operations. Spark automatically broadcasts the common data needed by tasks within each stage. The data broadcasted this way is cached in serialized form and deserialized before running each task. This means that explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important.</p>
</blockquote>
<blockquote>
<p>Broadcast variables are created from a variable v by calling SparkContext.broadcast(v). The broadcast variable is a wrapper around v, and its value can be accessed by calling the value method. The code below shows this in action.</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val broadcastVar = sc.broadcast(Array(1, 2, 3))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(67)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">broadcastVar.value
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res68: Array[Int] = Array(1, 2, 3)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">broadcastVar.value(0)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res69: Int = 1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd = sc.parallelize(1 to 10)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[61] at parallelize at command-685894176422531:1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res70: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.map(x =&gt; x%3).collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res71: Array[Int] = Array(1, 2, 0, 1, 2, 0, 1, 2, 0, 1)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.map(x =&gt; x+broadcastVar.value(x%3)).collect
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res72: Array[Int] = Array(3, 5, 4, 6, 8, 7, 9, 11, 10, 12)
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p>After the broadcast variable is created, it should be used instead of the value v in any functions run on the cluster so that v is not shipped to the nodes more than once. In addition, the object v should not be modified after it is broadcast in order to ensure that all nodes get the same value of the broadcast variable (e.g. if the variable is shipped to a new node later).</p>
</blockquote>
<blockquote>
<p>To release the resources that the broadcast variable copied onto executors, call .unpersist(). If the broadcast is used again afterwards, it will be re-broadcast. To permanently release all resources used by the broadcast variable, call .destroy(). The broadcast variable can’t be used after that. Note that these methods do not block by default. To block until resources are freed, specify blocking=true when calling them.</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">broadcastVar.unpersist()
</code></pre>
</div>
<div class="cell markdown">
<h5 id="a-more-interesting-example-of-broadcast-variable"><a class="header" href="#a-more-interesting-example-of-broadcast-variable">A more interesting example of broadcast variable</a></h5>
<p>Let us broadcast maps and use them to lookup the values at each executor. This example is taken from: - <a href="https://sparkbyexamples.com/spark/spark-broadcast-variables/">https://sparkbyexamples.com/spark/spark-broadcast-variables/</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val states = Map((&quot;NY&quot;,&quot;New York&quot;),(&quot;CA&quot;,&quot;California&quot;),(&quot;FL&quot;,&quot;Florida&quot;))
val countries = Map((&quot;USA&quot;,&quot;United States of America&quot;),(&quot;IN&quot;,&quot;India&quot;))

val broadcastStates = spark.sparkContext.broadcast(states)
val broadcastCountries = spark.sparkContext.broadcast(countries)

val data = Seq((&quot;James&quot;,&quot;Smith&quot;,&quot;USA&quot;,&quot;CA&quot;),
    (&quot;Michael&quot;,&quot;Rose&quot;,&quot;USA&quot;,&quot;NY&quot;),
    (&quot;Robert&quot;,&quot;Williams&quot;,&quot;USA&quot;,&quot;CA&quot;),
    (&quot;Maria&quot;,&quot;Jones&quot;,&quot;USA&quot;,&quot;FL&quot;))

val rdd = spark.sparkContext.parallelize(data) // spark.sparkContext is the same as sc.parallelize in spark-shell/notebook

  val rdd2 = rdd.map(f=&gt;{
    val country = f._3
    val state = f._4
    val fullCountry = broadcastCountries.value.get(country).get
    val fullState = broadcastStates.value.get(state).get
    (f._1,f._2,fullCountry,fullState)
  })
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>states: scala.collection.immutable.Map[String,String] = Map(NY -&gt; New York, CA -&gt; California, FL -&gt; Florida)
countries: scala.collection.immutable.Map[String,String] = Map(USA -&gt; United States of America, IN -&gt; India)
broadcastStates: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[String,String]] = Broadcast(71)
broadcastCountries: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[String,String]] = Broadcast(72)
data: Seq[(String, String, String, String)] = List((James,Smith,USA,CA), (Michael,Rose,USA,NY), (Robert,Williams,USA,CA), (Maria,Jones,USA,FL))
rdd: org.apache.spark.rdd.RDD[(String, String, String, String)] = ParallelCollectionRDD[64] at parallelize at command-685894176422538:12
rdd2: org.apache.spark.rdd.RDD[(String, String, String, String)] = MapPartitionsRDD[65] at map at command-685894176422538:14
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">println(rdd2.collect().mkString(&quot;\n&quot;))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>(James,Smith,United States of America,California)
(Michael,Rose,United States of America,New York)
(Robert,Williams,United States of America,California)
(Maria,Jones,United States of America,Florida)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h4 id="12-spark-essentials-summary"><a class="header" href="#12-spark-essentials-summary">12. Spark Essentials: Summary</a></h4>
<p><strong>(watch now: 0:29)</strong></p>
<p><a href="https://www.youtube.com/watch?v=F50Vty9Ia8Y?rel=0&amp;autoplay=1&amp;modestbranding=1"><img src="http://img.youtube.com/vi/F50Vty9Ia8Y/0.jpg" alt="Spark Essentials Summary by Anthony Joseph in BerkeleyX/CS100.1x" /></a></p>
<p><em>NOTE:</em> In databricks cluster, we (the course coordinator/administrators) set the number of workers for you.</p>
</div>
<div class="cell markdown">
<h4 id="13-homework"><a class="header" href="#13-homework">13. HOMEWORK</a></h4>
<p>See the notebook in this folder named <code>005_RDDsTransformationsActionsHOMEWORK</code>. This notebook will give you more examples of the operations above as well as others we will be using later, including:</p>
<ul>
<li>Perform the <code>takeOrdered</code> action on the RDD</li>
<li>Transform the RDD by <code>distinct</code> to make another RDD and</li>
<li>Doing a bunch of transformations to our RDD and performing an action in a single cell.</li>
</ul>
</div>
<div class="cell markdown">
<hr />
<hr />
<h4 id="14-importing-standard-scala-and-java-libraries"><a class="header" href="#14-importing-standard-scala-and-java-libraries">14. Importing Standard Scala and Java libraries</a></h4>
<ul>
<li>For other libraries that are not available by default, you can upload other libraries to the Workspace.</li>
<li>Refer to the <strong><a href="https://docs.databricks.com/user-guide/libraries.html">Libraries</a></strong> guide for more details.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import scala.math._
val x = min(1, 10)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import scala.math._
x: Int = 1
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import java.util.HashMap
val map = new HashMap[String, Int]()
map.put(&quot;a&quot;, 1)
map.put(&quot;b&quot;, 2)
map.put(&quot;c&quot;, 3)
map.put(&quot;d&quot;, 4)
map.put(&quot;e&quot;, 5)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import java.util.HashMap
map: java.util.HashMap[String,Int] = {a=1, b=2, c=3, d=4, e=5}
res75: Int = 0
</code></pre>
</div>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="homework-on-rdd-transformations-and-actions"><a class="header" href="#homework-on-rdd-transformations-and-actions">HOMEWORK on RDD Transformations and Actions</a></h1>
<p>Just go through the notebook and familiarize yourself with these transformations and actions.</p>
</div>
<div class="cell markdown">
<ol>
<li>Perform the <code>takeOrdered</code> action on the RDD**</li>
</ol>
<hr />
<p>To illustrate <code>take</code> and <code>takeOrdered</code> actions, let's create a bigger RDD named <code>rdd0_1000000</code> that is made up of a million integers from 0 to 1000000.
We will <code>sc.parallelize</code> the <code>Seq</code> Scala collection by using its <code>.range(startInteger,stopInteger)</code> method.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd0_1000000 = sc.parallelize(Seq.range(0, 1000000)) // &lt;Shift+Enter&gt; to create an RDD of million integers: 0,1,2,...,10^6
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd0_1000000.take(5) // &lt;Ctrl+Enter&gt; gives the first 5 elements of the RDD, (0, 1, 2, 3, 4)
</code></pre>
</div>
<div class="cell markdown">
<p><code>takeordered(n)</code> returns <code>n</code> elements ordered in ascending order (by default) or as specified by the optional key function, as shown below.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd0_1000000.takeOrdered(5) // &lt;Shift+Enter&gt; is same as rdd0_1000000.take(5) 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd0_1000000.takeOrdered(5)(Ordering[Int].reverse) // &lt;Ctrl+Enter&gt; to get the last 5 elements of the RDD 999999, 999998, ..., 999995
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// HOMEWORK: edit the numbers below to get the last 20 elements of an RDD made of a sequence of integers from 669966 to 969696
sc.parallelize(Seq.range(0, 10)).takeOrdered(5)(Ordering[Int].reverse) // &lt;Ctrl+Enter&gt; evaluate this cell after editing it for the right answer
</code></pre>
</div>
<div class="cell markdown">
<ol start="2">
<li>More examples of <code>map</code></li>
</ol>
<hr />
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd = sc.parallelize(Seq(1, 2, 3, 4))    // &lt;Shift+Enter&gt; to evaluate this cell (using default number of partitions)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.map( x =&gt; x*2) // &lt;Ctrl+Enter&gt; to transform rdd by map that doubles each element
</code></pre>
</div>
<div class="cell markdown">
<p>To see what's in the transformed RDD, let's perform the actions of <code>count</code> and <code>collect</code> on the <code>rdd.map( x =&gt; x*2)</code>, the transformation of <code>rdd</code> by the <code>map</code> given by the closure <code>x =&gt; x*2</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.map( x =&gt; x*2).count()    // &lt;Shift+Enter&gt; to perform count (action) the element of the RDD = 4
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.map( x =&gt; x*2).collect()    // &lt;Shift+Enter&gt; to perform collect (action) to show 2, 4, 6, 8
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// HOMEWORK: uncomment the last line in this cell and modify the '&lt;Fill-In-Here&gt;' in the code below to collect and display the square (x*x) of each element of the RDD
// the answer should be Array[Int] = Array(1, 4, 9, 16) Press &lt;Cntrl+Enter&gt; to evaluate the cell after modifying '???'

//sc.parallelize(Seq(1, 2, 3, 4)).map( x =&gt; &lt;Fill-In-Here&gt; ).collect()
</code></pre>
</div>
<div class="cell markdown">
<ol start="3">
<li>More examples of <code>filter</code></li>
</ol>
<hr />
<p>Let's declare another <code>val</code> RDD named <code>rddFiltered</code> by transforming our first RDD named <code>rdd</code> via the <code>filter</code> transformation <code>x%2==0</code> (of being even).</p>
<p>This filter transformation based on the closure <code>x =&gt; x%2==0</code> will return <code>true</code> if the element, modulo two, equals zero. The closure is automatically passed on to the workers for evaluation (when an action is called later). So this will take our RDD of (1,2,3,4) and return RDD of (2, 4).</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rddFiltered = rdd.filter( x =&gt; x%2==0 )    // &lt;Ctrl+Enter&gt; to declare rddFiltered from transforming rdd
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rddFiltered.collect()    // &lt;Ctrl+Enter&gt; to collect (action) elements of rddFiltered; should be (2, 4)
</code></pre>
</div>
<div class="cell markdown">
<ol start="4">
<li>More examples of <code>reduce</code></li>
</ol>
<hr />
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd = sc.parallelize(Array(1,2,3,4,5))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.reduce( (x,y)=&gt;x+y ) // &lt;Shift+Enter&gt; to do reduce (action) to sum and return Int = 15
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.reduce( _ + _ )    // &lt;Shift+Enter&gt; to do same sum as above and return Int = 15 (undescore syntax)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd.reduce( (x,y)=&gt;x*y ) // &lt;Shift+Enter&gt; to do reduce (action) to multiply and return Int = 120
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd0_1000000 = sc.parallelize(Seq.range(0, 1000000)) // &lt;Shift+Enter&gt; to create an RDD of million integers: 0,1,2,...,10^6
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd0_1000000.reduce( (x,y)=&gt;x+y ) // &lt;Ctrl+Enter&gt; to do reduce (action) to sum and return Int 1783293664
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// the following correctly returns Int = 0 although for wrong reason 
// we have flowed out of Int's numeric limits!!! (but got lucky with 0*x=0 for any Int x)
// &lt;Shift+Enter&gt; to do reduce (action) to multiply and return Int = 0
rdd0_1000000.reduce( (x,y)=&gt;x*y ) 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; to do reduce (action) to multiply 1*2*...*9*10 and return correct answer Int = 3628800
sc.parallelize(Seq.range(1, 11)).reduce( (x,y)=&gt;x*y ) 
</code></pre>
</div>
<div class="cell markdown">
<p><strong>CAUTION: Know the limits of your numeric types!</strong></p>
</div>
<div class="cell markdown">
<p>The minimum and maximum value of <code>Int</code> and <code>Long</code> types are as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">(Int.MinValue , Int.MaxValue)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">(Long.MinValue, Long.MaxValue)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Ctrl+Enter&gt; to do reduce (action) to multiply 1*2*...*20 and return wrong answer as Int = -2102132736
//  we have overflowed out of Int's in a circle back to negative Ints!!! (rigorous distributed numerics, anyone?)
sc.parallelize(Seq.range(1, 21)).reduce( (x,y)=&gt;x*y ) 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//&lt;Ctrl+Enter&gt; we can accomplish the multiplication using Long Integer types 
// by adding 'L' ro integer values, Scala infers that it is type Long
sc.parallelize(Seq.range(1L, 21L)).reduce( (x,y)=&gt;x*y ) 
</code></pre>
</div>
<div class="cell markdown">
<p>As the following products over Long Integers indicate, they are limited too!</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala"> // &lt;Shift+Enter&gt; for wrong answer Long = -8718968878589280256 (due to Long's numeric limits)
sc.parallelize(Seq.range(1L, 61L)).reduce( (x,y)=&gt;x*y )
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Cntrl+Enter&gt; for wrong answer Long = 0 (due to Long's numeric limits)
sc.parallelize(Seq.range(1L, 100L)).reduce( (x,y)=&gt;x*y ) 
</code></pre>
</div>
<div class="cell markdown">
<hr />
<ol start="5">
<li>Let us do a bunch of transformations to our RDD and perform an action</li>
</ol>
<hr />
<ul>
<li>start from a Scala <code>Seq</code>,</li>
<li><code>sc.parallelize</code> the list to create an RDD,</li>
<li><code>filter</code> that RDD, creating a new filtered RDD,</li>
<li>do a <code>map</code> transformation that maps that RDD to a new mapped RDD,</li>
<li>and finally, perform a <code>reduce</code> action to sum the elements in the RDD.</li>
</ul>
<p>This last <code>reduce</code> action causes the <code>parallelize</code>, the <code>filter</code>, and the <code>map</code> transformations to actually be executed, and return a result back to the driver machine.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sc.parallelize(Seq(1, 2, 3, 4))    // &lt;Ctrl+Enter&gt; will return Array(4, 8)
  .filter(x =&gt; x%2==0)             // (2, 4) is the filtered RDD
  .map(x =&gt; x*2)                   // (4, 8) is the mapped RDD
  .reduce(_+_)                     // 4+8=12 is the final result from reduce
</code></pre>
</div>
<div class="cell markdown">
<ol start="6">
<li>Transform the RDD by <code>distinct</code> to make another RDD</li>
</ol>
<hr />
<p>Let's declare another RDD named <code>rdd2</code> that has some repeated elements to apply the <code>distinct</code> transformation to it. That would give us a new RDD that only contains the distinct elements of the input RDD.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd2 = sc.parallelize(Seq(4, 1, 3, 2, 2, 2, 3, 4))    // &lt;Ctrl+Enter&gt; to declare rdd2
</code></pre>
</div>
<div class="cell markdown">
<p>Let's apply the <code>distinct</code> transformation to <code>rdd2</code> and have it return a new RDD named <code>rdd2Distinct</code> that contains the distinct elements of the source RDD <code>rdd2</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd2Distinct = rdd2.distinct() // &lt;Ctrl+Enter&gt; transformation: distinct gives distinct elements of rdd2
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rdd2Distinct.collect()    // &lt;Ctrl+Enter&gt; to collect (action) as Array(4, 2, 1, 3)
</code></pre>
</div>
<div class="cell markdown">
<ol start="7">
<li>more flatMap</li>
</ol>
<hr />
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd = sc. parallelize(Array(1,2,3)) // &lt;Shift+Enter&gt; to create an RDD of three Int elements 1,2,3
</code></pre>
</div>
<div class="cell markdown">
<p>Let us pass the <code>rdd</code> above to a map with a closure that will take in each element <code>x</code> and return <code>Array(x, x+5)</code>. So each element of the mapped RDD named <code>rddOfArrays</code> is an <code>Array[Int]</code>, an array of integers.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// &lt;Shift+Enter&gt; to make RDD of Arrays, i.e., RDD[Array[int]]
val rddOfArrays = rdd.map( x =&gt; Array(x, x+5) ) 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rddOfArrays.collect() // &lt;Ctrl+Enter&gt; to see it is RDD[Array[int]] = (Array(1, 6), Array(2, 7), Array(3, 8))
</code></pre>
</div>
<div class="cell markdown">
<p>Now let's observer what happens when we use <code>flatMap</code> to transform the same <code>rdd</code> and create another RDD called <code>rddfM</code>.</p>
<p>Interestingly, <code>flatMap</code> <em>flattens</em> our <code>rdd</code> by taking each <code>Array</code> (or sequence in general) and truning it into individual elements.</p>
<p>Thus, we end up with the RDD <code>rddfM</code> consisting of the elements (1, 6, 2, 7, 3, 8) as shown from the output of <code>rddfM.collect</code> below.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rddfM = rdd.flatMap(x =&gt; Array(x, x+5))    // &lt;Shift+Enter&gt; to flatMap the rdd using closure (x =&gt; Array(x, x+5))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rddfM.collect    // &lt;Ctrl+Enter&gt; to collect rddfM = (1, 6, 2, 7, 3, 8)
</code></pre>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="piped-rdds-and-bayesian-ab-testing"><a class="header" href="#piped-rdds-and-bayesian-ab-testing">Piped RDDs and Bayesian AB Testing</a></h1>
</div>
<div class="cell markdown">
<p>Here we will first take excerpts with minor modifications from the end of <strong>Chapter 12. Resilient Distributed Datasets (RDDs)</strong> of <em>Spark: The Definitive Guide</em>:</p>
<ul>
<li>https://learning.oreilly.com/library/view/spark-the-definitive/9781491912201/ch12.html</li>
</ul>
<p>Next, we will do Bayesian AB Testing using PipedRDDs.</p>
</div>
<div class="cell markdown">
<p>First, we create the toy RDDs as in <em>The Definitive Guide</em>:</p>
<blockquote>
<p><strong>From a Local Collection</strong></p>
</blockquote>
<blockquote>
<p>To create an RDD from a collection, you will need to use the parallelize method on a SparkContext (within a SparkSession). This turns a single node collection into a parallel collection. When creating this parallel collection, you can also explicitly state the number of partitions into which you would like to distribute this array. In this case, we are creating two partitions:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// in Scala
val myCollection = &quot;Spark The Definitive Guide : Big Data Processing Made Simple&quot;  .split(&quot; &quot;)
val words = spark.sparkContext.parallelize(myCollection, 2)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># in Python
myCollection = &quot;Spark The Definitive Guide : Big Data Processing Made Simple&quot;\
  .split(&quot; &quot;)
words = spark.sparkContext.parallelize(myCollection, 2)
words
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p><strong>glom</strong> from <em>The Definitive Guide</em></p>
</blockquote>
<blockquote>
<p><code>glom</code> is an interesting function that takes every partition in your dataset and converts them to arrays. This can be useful if you’re going to collect the data to the driver and want to have an array for each partition. However, this can cause serious stability issues because if you have large partitions or a large number of partitions, it’s simple to crash the driver.</p>
</blockquote>
</div>
<div class="cell markdown">
<p>Let's use <code>glom</code> to see how our <code>words</code> are distributed among the two partitions we used explicitly.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">words.glom.collect 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">words.glom().collect()
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p><strong>Checkpointing</strong> from <em>The Definitive Guide</em></p>
</blockquote>
<blockquote>
<p>One feature not available in the DataFrame API is the concept of checkpointing. Checkpointing is the act of saving an RDD to disk so that future references to this RDD point to those intermediate partitions on disk rather than recomputing the RDD from its original source. This is similar to caching except that it’s not stored in memory, only disk. This can be helpful when performing iterative computation, similar to the use cases for caching:</p>
</blockquote>
<p>Let's create a directory in <code>dbfs:///</code> for checkpointing of RDDs in the sequel. The following <code>%fs mkdirs /path_to_dir</code> is a shortcut to create a directory in <code>dbfs:///</code></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">mkdirs /datasets/ScaDaMaLe/checkpointing/
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">spark.sparkContext.setCheckpointDir(&quot;dbfs:///datasets/ScaDaMaLe/checkpointing&quot;)
words.checkpoint()
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p>Now, when we reference this RDD, it will derive from the checkpoint instead of the source data. This can be a helpful optimization.</p>
</blockquote>
</div>
<div class="cell markdown">
<h2 id="youtry"><a class="header" href="#youtry">YouTry</a></h2>
<p>Just some more words in <code>haha_words</code> with <code>\n</code>, the End-Of-Line (EOL) characters, in-place.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val haha_words = sc.parallelize(Seq(&quot;ha\nha&quot;, &quot;he\nhe\nhe&quot;, &quot;ho\nho\nho\nho&quot;),3)
</code></pre>
</div>
<div class="cell markdown">
<p>Let's use <code>glom</code> to see how our <code>haha_words</code> are distributed among the partitions</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">haha_words.glom.collect
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="pipe-rdds-to-system-commands"><a class="header" href="#pipe-rdds-to-system-commands">Pipe RDDs to System Commands</a></h1>
</blockquote>
<blockquote>
<p>The pipe method is probably one of Spark’s more interesting methods. With pipe, you can return an RDD created by piping elements to a forked external process. The resulting RDD is computed by executing the given process once per partition. All elements of each input partition are written to a process’s stdin as lines of input separated by a newline. The resulting partition consists of the process’s stdout output, with each line of stdout resulting in one element of the output partition. A process is invoked even for empty partitions.</p>
</blockquote>
<blockquote>
<p>The print behavior can be customized by providing two functions.</p>
</blockquote>
<p>We can use a simple example and pipe each partition to the command wc. Each row will be passed in as a new line, so if we perform a line count, we will get the number of lines, one per partition:</p>
</div>
<div class="cell markdown">
<p>The following produces a <code>PipedRDD</code>:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wc_l_PipedRDD = words.pipe(&quot;wc -l&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">wc_l_PipedRDD = words.pipe(&quot;wc -l&quot;)
wc_l_PipedRDD
</code></pre>
</div>
<div class="cell markdown">
<p>Now, we take an action via <code>collect</code> to bring the results to the Driver.</p>
<p>NOTE: Be careful what you collect! You can always write the output to parquet of binary files in <code>dbfs:///</code> if the returned output is large.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">wc_l_PipedRDD.collect
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">wc_l_PipedRDD.collect()
</code></pre>
</div>
<div class="cell markdown">
<p>In this case, we got the number of lines returned by <code>wc -l</code> per partition.</p>
</div>
<div class="cell markdown">
<h2 id="youtry-1"><a class="header" href="#youtry-1">YouTry</a></h2>
<p>Try to make sense of the next few cells where we do NOT specifiy the number of partitions explicitly and let Spark decide on the number of partitions automatically.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val haha_words = sc.parallelize(Seq(&quot;ha\nha&quot;, &quot;he\nhe\nhe&quot;, &quot;ho\nho\nho\nho&quot;),3)
haha_words.glom.collect
val wc_l_PipedRDD_haha_words = haha_words.pipe(&quot;wc -l&quot;)
wc_l_PipedRDD_haha_words.collect()
</code></pre>
</div>
<div class="cell markdown">
<p>Do you understand why the above <code>collect</code> statement returns what it does?</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val haha_words_again = sc.parallelize(Seq(&quot;ha\nha&quot;, &quot;he\nhe\nhe&quot;, &quot;ho\nho\nho\nho&quot;))
haha_words_again.glom.collect
val wc_l_PipedRDD_haha_words_again = haha_words_again.pipe(&quot;wc -l&quot;)
wc_l_PipedRDD_haha_words_again.collect()
</code></pre>
</div>
<div class="cell markdown">
<p>Did you understand why some of the results are <code>0</code> in the last <code>collect</code> statement?</p>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="mappartitions"><a class="header" href="#mappartitions">mapPartitions</a></h1>
</blockquote>
<blockquote>
<p>The previous command revealed that Spark operates on a per-partition basis when it comes to actually executing code. You also might have noticed earlier that the return signature of a map function on an RDD is actually <code>MapPartitionsRDD</code>.</p>
</blockquote>
<p>Or <code>ParallelCollectionRDD</code> in our case.</p>
<blockquote>
<p>This is because map is just a row-wise alias for <code>mapPartitions</code>, which makes it possible for you to map an individual partition (represented as an iterator). That’s because physically on the cluster we operate on each partition individually (and not a specific row). A simple example creates the value “1” for every partition in our data, and the sum of the following expression will count the number of partitions we have:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// in Scala
words.mapPartitions(part =&gt; Iterator[Int](1)).sum() // 2.0
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># in Python
words.mapPartitions(lambda part: [1]).sum() # 2
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p>Naturally, this means that we operate on a per-partition basis and therefore it allows us to perform an operation on that <em>entire</em> partition. This is valuable for performing something on an entire subdataset of your RDD. You can gather all values of a partition class or group into one partition and then operate on that entire group using arbitrary functions and controls. An example use case of this would be that you could pipe this through some custom machine learning algorithm and train an individual model for that company’s portion of the dataset. A Facebook engineer has an interesting demonstration of their particular implementation of the pipe operator with a similar use case demonstrated at <a href="https://spark-summit.org/east-2017/events/experiences-with-sparks-rdd-apis-for-complex-custom-applications/">Spark Summit East 2017</a>.</p>
</blockquote>
<blockquote>
<p>Other functions similar to <code>mapPartitions</code> include <code>mapPartitionsWithIndex</code>. With this you specify a function that accepts an index (within the partition) and an iterator that goes through all items within the partition. The partition index is the partition number in your RDD, which identifies where each record in our dataset sits (and potentially allows you to debug). You might use this to test whether your map functions are behaving correctly:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// in Scala
def indexedFunc(partitionIndex:Int, withinPartIterator: Iterator[String]) = {  withinPartIterator.toList.map(    
  value =&gt; s&quot;Partition: $partitionIndex =&gt; $value&quot;).iterator
                                                                            }
words.mapPartitionsWithIndex(indexedFunc).collect()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># in Python
def indexedFunc(partitionIndex, withinPartIterator):  
  return [&quot;partition: {} =&gt; {}&quot;.format(partitionIndex,    x) for x in withinPartIterator]
words.mapPartitionsWithIndex(indexedFunc).collect()
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="foreachpartition"><a class="header" href="#foreachpartition">foreachPartition</a></h1>
</blockquote>
<blockquote>
<p>Although <code>mapPartitions</code> needs a return value to work properly, this next function does not. <code>foreachPartition</code> simply iterates over all the partitions of the data. The difference is that the function has no return value. This makes it great for doing something with each partition like writing it out to a database. In fact, this is how many data source connectors are written. You can create</p>
</blockquote>
<p>your</p>
<blockquote>
<p>own text file source if you want by specifying outputs to the temp directory with a random ID:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">words.foreachPartition { iter =&gt;  
  import java.io._  
  import scala.util.Random  
  val randomFileName = new Random().nextInt()  
  val pw = new PrintWriter(new File(s&quot;/tmp/random-file-${randomFileName}.txt&quot;))  
  while (iter.hasNext) {
    pw.write(iter.next())  
  }  
  pw.close()
}
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p>You’ll find these two files if you scan your /tmp directory.</p>
</blockquote>
<p>You need to scan for the file across all the nodes. As the file may not be in the Driver node's <code>/tmp/</code> directory but in those of the executors that hosted the partition.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">pwd
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">ls /tmp/random-file-*.txt
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell markdown">
<h1 id="numerically-rigorous-bayesian-ab-testing"><a class="header" href="#numerically-rigorous-bayesian-ab-testing">Numerically Rigorous Bayesian AB Testing</a></h1>
<p>This is an example of Bayesian AB Testing with computer-aided proofs for the posterior samples.</p>
<p>The main learning goal for you is to use pipedRDDs to distribute, in an embarassingly paralle way, across all the worker nodes in the Spark cluster an executible <code>IsIt1or2Coins</code>.</p>
<h3 id="what-does-isit1or2coins-do"><a class="header" href="#what-does-isit1or2coins-do">What does <code>IsIt1or2Coins</code> do?</a></h3>
<p>At a very high-level, to understand what <code>IsIt1or2Coins</code> does, imagine the following simple experiment.</p>
<p>We are given</p>
<ul>
<li>the number of heads that result from a first sequence of independent and identical tosses of a coin and then</li>
<li>we are given the number of heads that result from a second sequence of independent and identical tosses of a coin</li>
</ul>
<p>Our decision problem is to do help shed light on whether both sequence of tosses came from the same coin or not (whatever the bias may be).</p>
<p><code>IsIt1or2Coins</code> tries to help us decide if the two sequence of coin-tosses are based on one coin with an unknown bias or two coins with different biases.</p>
<p>If you are curious about details feel free to see:</p>
<ul>
<li>Exact Bayesian A/B testing using distributed fault-tolerant Moore rejection sampler, Benny Avelin and Raazesh Sainudiin, Extended Abstract, 2 pages, 2018 <a href="http://lamastex.org/preprints/20180507_ABTestingViaDistributedMRS.pdf">(PDF 104KB)</a>.</li>
<li>which builds on: An auto-validating, trans-dimensional, universal rejection sampler for locally Lipschitz arithmetical expressions, Raazesh Sainudiin and Thomas York, <a href="http://interval.louisiana.edu/reliable-computing-journal/volume-18/reliable-computing-18-pp-015-054.pdf">Reliable Computing, vol.18, pp.15-54, 2013</a> (<a href="http://lamastex.org/preprints/avs_rc_2013.pdf">preprint: PDF 2612KB</a>)</li>
</ul>
<p><strong>See first about <code>PipedRDDs</code> excerpt from <em>Spark The Definitive Guide</em> earlier.</strong></p>
<h3 id="getting-the-executible-isit1or2coins-into-our-spark-cluster"><a class="header" href="#getting-the-executible-isit1or2coins-into-our-spark-cluster">Getting the executible <code>IsIt1or2Coins</code> into our Spark Cluster</a></h3>
<p><strong>This has already been done in the project-shard. You need not do it again for this executible!</strong></p>
<p>You need to upload the C++ executible <code>IsIt1or2Coins</code> from: - https://github.com/lamastex/mrs2</p>
<p>Here, suppose you have an executible for linux x86 64 bit processor with all dependencies pre-compiled into one executibe.</p>
<p>Say this executible is <code>IsIt10r2Coins</code>.</p>
<p>This executible comes from the following dockerised build:</p>
<ul>
<li>https://github.com/lamastex/mrs2/tree/master/docker</li>
<li>by statically compiling inside the docerised environment for mrs2:
<ul>
<li>https://github.com/lamastex/mrs2/tree/master/mrs-2.0/examples/MooreRejSam/IsIt1or2Coins</li>
</ul>
</li>
</ul>
<p>You can replace the executible with any other executible with appropriate I/O to it.</p>
<p>Then you upload the executible to databricks' <code>FileStore</code>.</p>
</div>
<div class="cell markdown">
<p>Just note the path to the file and DO NOT click <code>Create Table</code> or other buttons!</p>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/images/2020/ScaDaMaLe/screenShotOfUploadingStaticExecutibleIsIt1or2CoinsViaFileStore.png" alt="creenShotOfUploadingStaticExecutibleIsIt1or2CoinsViaFileStore" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">ls &quot;/FileStore/tables/IsIt1or2Coins&quot;
</code></pre>
</div>
<div class="cell markdown">
<p>Now copy the file from <code>dbfs://FileStore</code> that you just uploaded into the local file system of the Driver.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.cp(&quot;dbfs:/FileStore/tables/IsIt1or2Coins&quot;, &quot;file:/tmp/IsIt1or2Coins&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">ls -al /tmp/IsIt1or2Coins
</code></pre>
</div>
<div class="cell markdown">
<p>Note it is a big static executible with all dependencies inbuilt (it uses GNU Scientific Library and a specialized C++ Library called C-XSC or C Extended for Scientific Computing to do hard-ware optimized rigorous numerical proofs using Interval-Extended Hessian Differentiation Arithmetics over Rounding-Controlled Hardware-Specified Machine Intervals).</p>
<p>Just note it is over 6.5MB. Also we need to change the permissions so it is indeed executible.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">chmod +x /tmp/IsIt1or2Coins
</code></pre>
</div>
<div class="cell markdown">
<h1 id="usage-instructions-for-isit1or2coins"><a class="header" href="#usage-instructions-for-isit1or2coins">Usage instructions for IsIt1or2Coins</a></h1>
<p><code>./IsIt1or2Coins numboxes numiter seed numtosses1 heads1 numtosses2 heads2 logScale</code> - numboxes = Number of boxes for Moore Rejection Sampling (Rigorous von Neumann Rejection Sampler) - numiter = Number of samples drawn from posterior distribution to estimate the model probabilities - seed = a random number seed - numtosses1 = number of tosses for the first coin - heads1 = number of heads shown up on the first coin - numtosses2 = number of tosses for the second coin - heads2 = number of heads shown up on the second coin - logscale = True/False as Int</p>
<p>Don't worry about the details of what the executible <code>IsIt1or2Coins</code> is doing for now. Just realise that this executible takes some input on command-line and gives some output.</p>
</div>
<div class="cell markdown">
<p>Let's make sure the executible takes input and returns output string on the Driver node.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">/tmp/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh"># You can also do it like this

/dbfs/FileStore/tables/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1
</code></pre>
</div>
<div class="cell markdown">
<h2 id="moving-the-executables-to-the-worker-nodes"><a class="header" href="#moving-the-executables-to-the-worker-nodes">Moving the executables to the worker nodes</a></h2>
</div>
<div class="cell markdown">
<p>To copy the executible from <code>dbfs</code> to the local drive of each executor you can use the following helper function.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import scala.sys.process._
import scala.concurrent.duration._
// from Ivan Sadikov

def copyFile(): Unit = {
  &quot;mkdir -p /tmp/executor/bin&quot;.!!
  &quot;cp /dbfs/FileStore/tables/IsIt1or2Coins /tmp/executor/bin/&quot;.!!
}

sc.runOnEachExecutor(copyFile, new FiniteDuration(1, HOURS))
</code></pre>
</div>
<div class="cell markdown">
<p>Now, let us use piped RDDs via <code>bash</code> to execute the given command in each partition as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val input = Seq(&quot;/tmp/executor/bin/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1&quot;, &quot;/tmp/executor/bin/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1&quot;)

val output = sc
  .parallelize(input)
  .repartition(2)
  .pipe(&quot;bash&quot;)
  .collect()
</code></pre>
</div>
<div class="cell markdown">
<p>In fact, you can just use <code>DBFS FUSE</code> to run the commands without any file copy in databricks-provisioned Spark clusters we are on here:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val isIt1or2StaticExecutible = &quot;/dbfs/FileStore/tables/IsIt1or2Coins&quot;
val same_input = Seq(s&quot;$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1&quot;, 
                     s&quot;$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1&quot;)

val same_output = sc
  .parallelize(same_input)
  .repartition(2)
  .pipe(&quot;bash&quot;)
  .collect()
</code></pre>
</div>
<div class="cell markdown">
<p>Thus by mixing several different executibles that are statically compiled for linux 64 bit machine, we can mix and match multiple executibles with appropriate inputs.</p>
<p>The resulting outputs can themselves be re-processed in Spark to feed into toher pipedRDDs or normal RDDs or DataFrames and DataSets.</p>
</div>
<div class="cell markdown">
<p>Finally, we can have more than one command per partition and then use <code>mapPartitions</code> to send all the executible commands within the input partition that is to be run by the executor in which that partition resides as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val isIt1or2StaticExecutible = &quot;/dbfs/FileStore/tables/IsIt1or2Coins&quot;

// let us make 2 commands in each of the 2 input partitions
val same_input_mp = Seq(s&quot;$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1&quot;, 
                        s&quot;$isIt1or2StaticExecutible 1000 100 123456789 1000 500 1200 600 1&quot;,
                        s&quot;$isIt1or2StaticExecutible 1000 100 123456789 1000 500 1200 600 1&quot;,
                        s&quot;$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1&quot;)

val same_output_mp = sc
  .parallelize(same_input)
  .repartition(2)
  .pipe(&quot;bash&quot;)
  .mapPartitions(x =&gt; Seq(x.mkString(&quot;\n&quot;)).iterator)
  .collect()
</code></pre>
</div>
<div class="cell markdown">
<p>allCatch is a useful tool to use as a filtering function when testing if a command will work without error.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import scala.util.control.Exception.allCatch
(allCatch opt &quot; 12 &quot;.trim.toLong).isDefined
</code></pre>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="word-count-on-us-state-of-the-union-sou-addresses"><a class="header" href="#word-count-on-us-state-of-the-union-sou-addresses">Word Count on US State of the Union (SoU) Addresses</a></h1>
<ul>
<li>Word Count in big data is the equivalent of <code>Hello World</code> in programming</li>
<li>We count the number of occurences of each word in the first and last (2016) SoU addresses.</li>
</ul>
<p><strong>prerequisite</strong> see <strong>DO NOW</strong> below. You should have loaded data as instructed in <code>scalable-data-science/xtraResources/sdsDatasets</code>.</p>
<h4 id="do-now-if-not-done-already"><a class="header" href="#do-now-if-not-done-already">DO NOW (if not done already)</a></h4>
<p>In your databricks community edition:</p>
<ol>
<li>In your <code>WorkSpace</code> create a Folder named <code>scalable-data-science</code></li>
<li><code>Import</code> the databricks archive file at the following URL:
<ul>
<li><a href="https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/2017/parts/xtraResources.dbc">https://github.com/lamastex/scalable-data-science/raw/master/dbcArchives/2017/parts/xtraResources.dbc</a></li>
</ul>
</li>
<li>This should open a structure of directories in with path: <code>/Workspace/scalable-data-science/xtraResources/</code></li>
</ol>
</div>
<div class="cell markdown">
<p>An interesting analysis of the textual content of the <em>State of the Union (SoU)</em> addresses by all US presidents was done in:</p>
<ul>
<li><a href="http://www.pnas.org/content/112/35/10837.full">Alix Rule, Jean-Philippe Cointet, and Peter S. Bearman, Lexical shifts, substantive changes, and continuity in State of the Union discourse, 1790–2014, PNAS 2015 112 (35) 10837-10844; doi:10.1073/pnas.1512221112</a>.</li>
</ul>
<p><img src="http://www.pnas.org/content/112/35/10837/F5.large.jpg" alt="" /></p>
<p><a href="http://www.pnas.org/content/112/35/10837.full">Fig. 5</a>. A river network captures the flow across history of US political discourse, as perceived by contemporaries. Time moves along the x axis. Clusters on semantic networks of 300 most frequent terms for each of 10 historical periods are displayed as vertical bars. Relations between clusters of adjacent periods are indexed by gray flows, whose density reflects their degree of connection. Streams that connect at any point in history may be considered to be part of the same system, indicated with a single color.</p>
<h2 id="let-us-investigate-this-dataset-ourselves"><a class="header" href="#let-us-investigate-this-dataset-ourselves">Let us investigate this dataset ourselves!</a></h2>
<ol>
<li>We first get the source text data by scraping and parsing from <a href="http://stateoftheunion.onetwothree.net/texts/index.html">http://stateoftheunion.onetwothree.net/texts/index.html</a> as explained in <a href="contents/000_1-sds-3-x//#workspace/scalable-data-science/xtraResources/sdsDatasets/scraperUSStateofUnionAddresses">scraping and parsing SoU addresses</a>.</li>
</ol>
<ul>
<li>This data is already made available in DBFS, our distributed file system.</li>
<li>We only do the simplest word count with this data in this notebook and will do more sophisticated analyses in the sequel (including topic modeling, etc).</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="key-data-management-concepts"><a class="header" href="#key-data-management-concepts">Key Data Management Concepts</a></h2>
<h3 id="the-structure-spectrum"><a class="header" href="#the-structure-spectrum">The Structure Spectrum</a></h3>
<p><strong>(watch now 1:10)</strong>:</p>
<p><a href="https://www.youtube.com/watch?v=pMSGGZVSwqo?rel=0&amp;autoplay=1&amp;modestbranding=1&amp;start=1&amp;end=70"><img src="http://img.youtube.com/vi/pMSGGZVSwqo/0.jpg" alt="Structure Spectrum by Anthony Joseph in BerkeleyX/CS100.1x" /></a></p>
<p>Here we will be working with <strong>unstructured</strong> or <strong>schema-never</strong> data (plain text files). ***</p>
<h3 id="files"><a class="header" href="#files">Files</a></h3>
<p><strong>(watch later 1:43)</strong>:</p>
<p><a href="https://www.youtube.com/watch?v=NJyBQ-cQ3Ac?rel=0&amp;autoplay=1&amp;modestbranding=1&amp;start=1"><img src="http://img.youtube.com/vi/NJyBQ-cQ3Ac/0.jpg" alt="Files by Anthony Joseph in BerkeleyX/CS100.1x" /></a></p>
</div>
<div class="cell markdown">
<h3 id="dbfs-and-dbutils---where-is-this-dataset-in-our-distributed-file-system"><a class="header" href="#dbfs-and-dbutils---where-is-this-dataset-in-our-distributed-file-system">DBFS and dbutils - where is this dataset in our distributed file system?</a></h3>
<ul>
<li>Since we are on the databricks cloud, it has a file system called DBFS</li>
<li>DBFS is similar to HDFS, the Hadoop distributed file system</li>
<li>dbutils allows us to interact with dbfs.</li>
<li>The 'display' command displays the list of files in a given directory in the file system.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/datasets/sou&quot;)) 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/datasets/sou&quot;)) // Cntrl+Enter to display the files in dbfs:/datasets/sou
</code></pre>
</div>
<div class="cell markdown">
<p>Let us display the <em>head</em> or the first few lines of the file <code>dbfs:/datasets/sou/17900108.txt</code> to see what it contains using <code>dbutils.fs.head</code> method.
<code>head(file: String, maxBytes: int = 65536): String</code> -&gt; Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8 as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.head(&quot;dbfs:/datasets/sou/17900108.txt&quot;,673) // Cntrl+Enter to get the first 673 bytes of the file (which corresponds to the first five lines)
</code></pre>
</div>
<div class="cell markdown">
<h5 id="you-try-3"><a class="header" href="#you-try-3">You Try!</a></h5>
<p>Uncomment and modify <code>xxxx</code> in the cell below to read the first 1000 bytes from the file.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//dbutils.fs.head(&quot;dbfs:/datasets/sou/17900108.txt&quot;, xxxx) // Cntrl+Enter to get the first 1000 bytes of the file
</code></pre>
</div>
<div class="cell markdown">
<h3 id="read-the-file-into-spark-context-as-an-rdd-of-strings"><a class="header" href="#read-the-file-into-spark-context-as-an-rdd-of-strings">Read the file into Spark Context as an RDD of Strings</a></h3>
<ul>
<li>The <code>textFile</code> method on the available <code>SparkContext</code> <code>sc</code> can read the text file <code>dbfs:/datasets/sou/17900108.txt</code> into Spark and create an RDD of Strings
<ul>
<li>but this is done lazily until an action is taken on the RDD <code>sou17900108</code>!</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val sou17900108 = sc.textFile(&quot;dbfs:/datasets/sou/17900108.txt&quot;) // Cntrl+Enter to read in the textfile as RDD[String]
</code></pre>
</div>
<div class="cell markdown">
<h3 id="perform-some-actions-on-the-rdd"><a class="header" href="#perform-some-actions-on-the-rdd">Perform some actions on the RDD</a></h3>
<ul>
<li>Each String in the RDD <code>sou17900108</code> represents one line of data from the file and can be made to perform one of the following actions:
<ul>
<li>count the number of elements in the RDD <code>sou17900108</code> (i.e., the number of lines in the text file <code>dbfs:/datasets/sou/17900108.txt</code>) using <code>sou17900108.count()</code></li>
<li>display the contents of the RDD using <code>take</code> or <code>collect</code>.</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108.count() // &lt;Shift+Enter&gt; to count the number of elements in the RDD
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108.take(5) // &lt;Shift+Enter&gt; to display the first 5 elements of RDD
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108.take(5).foreach(println) // &lt;Shift+Enter&gt; to display the first 5 elements of RDD line by line
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108.collect // &lt;Cntrl+Enter&gt; to display all the elements of RDD
</code></pre>
</div>
<div class="cell markdown">
<h3 id="cache-the-rdd-in-distributed-memory-to-avoid-recreating-it-for-each-action"><a class="header" href="#cache-the-rdd-in-distributed-memory-to-avoid-recreating-it-for-each-action">Cache the RDD in (distributed) memory to avoid recreating it for each action</a></h3>
<ul>
<li>Above, every time we took an action on the same RDD, the RDD was reconstructed from the textfile.
<ul>
<li>Spark's advantage compared to Hadoop MapReduce is the ability to cache or store the RDD in distributed memory across the nodes.</li>
</ul>
</li>
<li>Let's use <code>.cache()</code> after creating an RDD so that it is in memory after the first action (and thus avoid reconstruction for subsequent actions).
<ul>
<li>count the number of elements in the RDD <code>sou17900108</code> (i.e., the number of lines in the text file <code>dbfs:/datasets/sou/17900108.txt</code>) using <code>sou17900108.count()</code></li>
<li>display the contents of the RDD using <code>take</code> or <code>collect</code>.</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Shift+Enter to read in the textfile as RDD[String] and cache it in distributed memory
val sou17900108 = sc.textFile(&quot;dbfs:/datasets/sou/17900108.txt&quot;)
sou17900108.cache() // cache the RDD in memory
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108.count() // Shift+Enter during this count action the RDD is constructed from texfile and cached
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108.count() // Shift+Enter during this count action the cached RDD is used (notice less time taken by the same command)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108.take(5) // &lt;Cntrl+Enter&gt; to display the first 5 elements of the cached RDD
</code></pre>
</div>
<div class="cell markdown">
<h4 id="lifecycle-of-a-spark-program"><a class="header" href="#lifecycle-of-a-spark-program">Lifecycle of a Spark Program</a></h4>
<p><strong>(watch now 0:23)</strong>:</p>
<p><a href="https://www.youtube.com/watch?v=HWZUqNYAJj4?rel=0&amp;autoplay=1&amp;modestbranding=1&amp;start=1"><img src="http://img.youtube.com/vi/HWZUqNYAJj4/0.jpg" alt="Spark Program Lifecycle by Anthony Joseph in BerkeleyX/CS100.1x" /></a></p>
<h5 id="summary-1"><a class="header" href="#summary-1">Summary</a></h5>
<ul>
<li>create RDDs from:
<ul>
<li>some external data source (such as a distributed file system)</li>
<li>parallelized collection in your driver program</li>
</ul>
</li>
<li>lazily transform these RDDs into new RDDs</li>
<li>cache some of those RDDs for future reuse</li>
<li>you perform actions to execute parallel computation to produce results</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="transform-lines-to-words"><a class="header" href="#transform-lines-to-words">Transform lines to words</a></h3>
<ul>
<li>We need to loop through each line and split the line into words</li>
<li>For now, let us split using whitespace</li>
<li>More sophisticated regular expressions can be used to split the line (as we will see soon)</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108
.flatMap(line =&gt; line.split(&quot; &quot;))
.take(100)
</code></pre>
</div>
<div class="cell markdown">
<h3 id="naive-word-count"><a class="header" href="#naive-word-count">Naive word count</a></h3>
<p>At a first glace, to do a word count of George Washingtons SoU address, we are templed to do the following:</p>
<ul>
<li>just break each line by the whitespace character &quot; &quot; and find the words using a <code>flatMap</code></li>
<li>then do the <code>map</code> with the closure <code>word =&gt; (word, 1)</code> to initialize each <code>word</code> with a integer count of <code>1</code>
<ul>
<li>ie., transform each word to a <em>(key, value)</em> pair or <code>Tuple</code> such as <code>(word, 1)</code></li>
</ul>
</li>
<li>then count all <em>value</em>s with the same <em>key</em> (<code>word</code> is the Key in our case) by doing a
<ul>
<li><code>reduceByKey(_+_)</code></li>
</ul>
</li>
<li>and finally <code>collect()</code> to display the results.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sou17900108
.flatMap( line =&gt; line.split(&quot; &quot;) )
.map( word =&gt; (word, 1) )
.reduceByKey(_+_)
.collect()
</code></pre>
</div>
<div class="cell markdown">
<p>Unfortunately, as you can see from the <code>collect</code> above:</p>
<ul>
<li>the words have punctuations at the end which means that the same words are being counted as different words. Eg: importance</li>
<li>empty words are being counted</li>
</ul>
<p>So we need a bit of <code>regex</code>'ing or regular-expression matching (all readily available from Scala via Java String types).</p>
<p>We will cover the three things we want to do with a simple example from Middle Earth!</p>
<ul>
<li>replace all multiple whitespace characters with one white space character &quot; &quot;</li>
<li>replace all punction characters we specify within <code>[</code> and <code>]</code> such as <code>[,?.!:;]</code> by the empty string <code>&quot;&quot;</code> (i.e., remove these punctuation characters)</li>
<li>convert everything to lower-case.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val example = &quot;Master, Master!   It's me, Sméagol... mhrhm*%* But they took away our precious, they wronged us. Gollum will protect us..., Master, it's me Sméagol.&quot;
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">example
  .replaceAll(&quot;\\s+&quot;, &quot; &quot;) //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace &quot; &quot;
  .replaceAll(&quot;&quot;&quot;([,?.!:;])&quot;&quot;&quot;, &quot;&quot;) // replace the following punctions characters: , ? . ! : ; . with the empty string &quot;&quot;
  .toLowerCase() // converting to lower-case
</code></pre>
</div>
<div class="cell markdown">
<h3 id="more-sophisticated-word-count"><a class="header" href="#more-sophisticated-word-count">More sophisticated word count</a></h3>
<p>We are now ready to do a word count of George Washington's SoU on January 8th 1790 as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wordCount_sou17900108 = 
 sou17900108
    .flatMap(line =&gt; 
         line.replaceAll(&quot;\\s+&quot;, &quot; &quot;) //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace &quot; &quot;
             .replaceAll(&quot;&quot;&quot;([,?.!:;])&quot;&quot;&quot;, &quot;&quot;) // replace the following punctions characters: , ? . ! : ; . with the empty string &quot;&quot;
             .toLowerCase() // converting to lower-case
             .split(&quot; &quot;))
    .map(x =&gt; (x, 1))
    .reduceByKey(_+_)
    
wordCount_sou17900108.collect()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val top10 = wordCount_sou17900108.sortBy(_._2, false).collect()
</code></pre>
</div>
<div class="cell markdown">
<h3 id="doing-it-all-together-for-george-washington-and-barrack-obama"><a class="header" href="#doing-it-all-together-for-george-washington-and-barrack-obama">Doing it all together for George Washington and Barrack Obama</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//sc.textFile(&quot;dbfs:/datasets/sou/17900108.txt&quot;) // George Washington's first SoU
sc.textFile(&quot;dbfs:/datasets/sou/20160112.txt&quot;)   // Barrack Obama's second SoU
    .flatMap(line =&gt; 
         line.replaceAll(&quot;\\s+&quot;, &quot; &quot;) //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace &quot; &quot;
             .replaceAll(&quot;&quot;&quot;([,?.!:;])&quot;&quot;&quot;, &quot;&quot;) // replace the following punctions characters: , ? . ! : ; . with the empty string &quot;&quot;
             .toLowerCase() // converting to lower-case
             .split(&quot; &quot;))
    .map(x =&gt; (x,1))
    .reduceByKey(_+_)
    .sortBy(_._2, false)
    .collect()
</code></pre>
</div>
<div class="cell markdown">
<h3 id="reading-all-sous-at-once-using-wholetextfiles"><a class="header" href="#reading-all-sous-at-once-using-wholetextfiles">Reading all SoUs at once using <code>wholetextFiles</code></a></h3>
<p>Let us next read all text files (ending with <code>.txt</code>) in the directory <code>dbfs:/datasets/sou/</code> at once!</p>
<p><code>SparkContext.wholeTextFiles</code> lets you read a directory containing multiple small text files, and returns each of them as <code>(filename, content)</code> pairs of strings.</p>
<p>This is in contrast with <code>textFile</code>, which would return one record per line in each file.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val souAll = sc.wholeTextFiles(&quot;dbfs:/datasets/sou/*.txt&quot;) // Shift+Enter to read all text files in dbfs:/datasets/sou/
souAll.cache() // let's cache this RDD for efficient reuse
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">souAll.count() // Shift+enter to count the number of entries in RDD[(String,String)]
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">souAll.count() // Cntrl+Enter to count the number of entries in cached RDD[(String,String)] again (much faster!)
</code></pre>
</div>
<div class="cell markdown">
<p>Let's examine the first two elements of the RDD <code>souAll</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">souAll.take(2) // Cntr+Enter to see the first two elements of souAll
</code></pre>
</div>
<div class="cell markdown">
<p>Clearly, the elements are a pair of Strings, where the first String gives the filename and the second String gives the contents in the file.</p>
<p>this can be very helpful to simply loop through the files and take an action, such as counting the number of words per address, as folows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// this just collects the file names which is the first element of the tuple given by &quot;._1&quot; 
souAll.map( fileContentsPair =&gt; fileContentsPair._1).collect()
</code></pre>
</div>
<div class="cell markdown">
<p>Let us find the number of words in each of the SoU addresses next (we need to work with Strings inside the closure!).</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wcs = souAll.map( fileContentsPair =&gt; 
  {
    val wc = fileContentsPair._2
                             .replaceAll(&quot;\\s+&quot;, &quot; &quot;) //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace &quot; &quot;
                             .replaceAll(&quot;&quot;&quot;([,?.!:;])&quot;&quot;&quot;, &quot;&quot;) // replace the following punctions characters: , ? . ! : ; . with the empty string &quot;&quot;
                             .toLowerCase() // converting to lower-case
                             .split(&quot; &quot;) // split each word separated by white space
                             .size // find the length of array
    wc
  }    
)      
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">wcs.collect()
</code></pre>
</div>
<div class="cell markdown">
<h2 id="youtry-homework"><a class="header" href="#youtry-homework">YouTry: HOMEWORK</a></h2>
<ul>
<li>HOWEWORK WordCount 1: <code>sortBy</code></li>
<li>HOMEWROK WordCount 2: <code>dbutils.fs</code></li>
</ul>
</div>
<div class="cell markdown">
<h5 id="homework-wordcount-1-sortby"><a class="header" href="#homework-wordcount-1-sortby">HOMEWORK WordCount 1. <code>sortBy</code></a></h5>
<p>Let's understand <code>sortBy</code> a bit more carefully.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val example = &quot;Master, Master!   It's me, Sméagol... mhrhm*%* But they took away our precious, they wronged us. Gollum will protect us..., Master, it's me Sméagol.&quot;
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val words = example.replaceAll(&quot;\\s+&quot;, &quot; &quot;) //replace multiple whitespace characters (including space, tab, new line, etc.) with one whitespace &quot; &quot;
       .replaceAll(&quot;&quot;&quot;([,?.!:;])&quot;&quot;&quot;, &quot;&quot;) // replace the following punctions characters: , ? . ! : ; . with the empty string &quot;&quot;
       .toLowerCase() // converting to lower-case
       .split(&quot; &quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rddWords = sc.parallelize(words)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rddWords.take(10)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wordCounts = rddWords
                  .map(x =&gt; (x,1))
                  .reduceByKey(_+_)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val top10 = wordCounts.sortBy(_._2, false).take(10)
</code></pre>
</div>
<div class="cell markdown">
<p>Make your code easy to read for other developers ;)
Use 'case classes' with well defined variable names that everyone can understand</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val top10 = wordCounts.sortBy({
  case (word, count) =&gt; count
}, false).take(10)
</code></pre>
</div>
<div class="cell markdown">
<p>If you just want a total count of all words in the file</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rddWords.count
</code></pre>
</div>
<div class="cell markdown">
<h5 id="youttry-homework-wordcount-2-dbutilsfs"><a class="header" href="#youttry-homework-wordcount-2-dbutilsfs">YoutTry: HOMEWORK WordCount 2: <code>dbutils.fs</code></a></h5>
<p>Have a brief look at what other commands dbutils.fs supports. We will introduce them as needed.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.help // some of these were used to ETL this data into dbfs:/datasets/sou 
</code></pre>
</div>
<div class="cell markdown">
<h2 id="exercise-2-souwordcount"><a class="header" href="#exercise-2-souwordcount">Exercise 2: SouWordCount</a></h2>
<p>Count the number of each word across all the &quot;dbfs:/datasets/sou/*.txt&quot; files and output the result as an Array of (word,count) tuples from the most frequent to the least frequent word.</p>
<p>This is the same as <a href="https://github.com/lamastex/scalable-data-science/tree/master/_sds/basics/infrastructure/onpremise/dockerCompose/programs/exercises/sparkSubmit#exercise-2-souwordcount">Exercise 2 in the local environment</a>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// code in this cell the solution to the above exercise in the notebook environment
//
//
</code></pre>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<p>This and the next sequence of notebooks are an elaboration of the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">http://spark.apache.org/docs/latest/sql-programming-guide.html</a> by Ivan Sadikov and Raazesh Sainudiin.</p>
<h1 id="spark-sql-programming-guide"><a class="header" href="#spark-sql-programming-guide">Spark Sql Programming Guide</a></h1>
<ul>
<li>Overview
<ul>
<li>SQL</li>
<li>DataFrames</li>
<li>Datasets</li>
</ul>
</li>
<li>Getting Started
<ul>
<li>Starting Point: SQLContext</li>
<li>Creating DataFrames</li>
<li>DataFrame Operations</li>
<li>Running SQL Queries Programmatically</li>
<li>Creating Datasets</li>
<li>Interoperating with RDDs
<ul>
<li>Inferring the Schema Using Reflection</li>
<li>Programmatically Specifying the Schema</li>
</ul>
</li>
</ul>
</li>
<li>Data Sources
<ul>
<li>Generic Load/Save Functions
<ul>
<li>Manually Specifying Options</li>
<li>Run SQL on files directly</li>
<li>Save Modes</li>
<li>Saving to Persistent Tables</li>
</ul>
</li>
<li>Parquet Files
<ul>
<li>Loading Data Programmatically</li>
<li>Partition Discovery</li>
<li>Schema Merging</li>
<li>Hive metastore Parquet table conversion
<ul>
<li>Hive/Parquet Schema Reconciliation</li>
<li>Metadata Refreshing</li>
</ul>
</li>
<li>Configuration</li>
</ul>
</li>
<li>JSON Datasets</li>
<li>Hive Tables
<ul>
<li>Interacting with Different Versions of Hive Metastore</li>
</ul>
</li>
<li>JDBC To Other Databases</li>
<li>Troubleshooting</li>
</ul>
</li>
<li>Performance Tuning
<ul>
<li>Caching Data In Memory</li>
<li>Other Configuration Options</li>
</ul>
</li>
<li>Distributed SQL Engine
<ul>
<li>Running the Thrift JDBC/ODBC server</li>
<li>Running the Spark SQL CLI</li>
</ul>
</li>
<li>SQL Reference</li>
</ul>
<h3 id="what-could-one-do-with-these-notebooks"><a class="header" href="#what-could-one-do-with-these-notebooks">What could one do with these notebooks?</a></h3>
<p>One could read the Spark SQL Programming Guide that is embedded below and also linked above while going through the cells and doing the YouTrys in the following notebooks.</p>
<h3 id="why-might-one-do-it"><a class="header" href="#why-might-one-do-it">Why might one do it?</a></h3>
<p>This homework/self-study will help you solve the assigned lab and theory exercises in the sequel, much faster by introducing you to some basic knowledge you need about Spark SQL.</p>
<h4 id="note-on-intra-iframe-html-navigation-within-a-notebook"><a class="header" href="#note-on-intra-iframe-html-navigation-within-a-notebook">NOTE on intra-iframe html navigation within a notebook:</a></h4>
<ul>
<li>When navigating in the html-page embedded as an iframe, as in the cell below, you can:
<ul>
<li>click on a link in the displayed html page to see the content of the clicked link and</li>
<li>then right-click on the page and click on the arrow keys <code>&lt;-</code> and <code>-&gt;</code> to go back or forward.</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//This allows easy embedding of publicly available information into any other notebook
//Example usage:
// displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;,250))
def frameIt( u:String, h:Int ) : String = {
      &quot;&quot;&quot;&lt;iframe 
 src=&quot;&quot;&quot;&quot;+ u+&quot;&quot;&quot;&quot;
 width=&quot;95%&quot; height=&quot;&quot;&quot;&quot; + h + &quot;&quot;&quot;&quot;
 sandbox&gt;
  &lt;p&gt;
    &lt;a href=&quot;http://spark.apache.org/docs/latest/index.html&quot;&gt;
      Fallback link for browsers that, unlikely, don't support frames
    &lt;/a&gt;
  &lt;/p&gt;
&lt;/iframe&gt;&quot;&quot;&quot;
   }
displayHTML(frameIt(&quot;https://spark.apache.org/docs/latest/sql-programming-guide.html&quot;,750))
</code></pre>
</div>
<div class="cell markdown">
<h3 id="lets-go-through-the-programming-guide-in-databricks-now"><a class="header" href="#lets-go-through-the-programming-guide-in-databricks-now">Let's go through the programming guide in databricks now</a></h3>
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">http://spark.apache.org/docs/latest/sql-programming-guide.html</a> by Ivan Sadikov and Raazesh Sainudiin.</p>
<h2 id="spark-sql-dataframes-and-datasets-guide"><a class="header" href="#spark-sql-dataframes-and-datasets-guide">Spark SQL, DataFrames and Datasets Guide</a></h2>
<p>Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations. There are several ways to interact with Spark SQL including SQL and the Dataset API. When computing a result, the same execution engine is used, independent of which API/language you are using to express the computation. This unification means that developers can easily switch back and forth between different APIs based on which provides the most natural way to express a given transformation.</p>
<p>All of the examples on this page use sample data included in the Spark distribution and can be run in the <code>spark-shell</code>, <code>pyspark</code> shell, or <code>sparkR</code> shell.</p>
<h2 id="sql"><a class="header" href="#sql">SQL</a></h2>
<p>One use of Spark SQL is to execute SQL queries. Spark SQL can also be used to read data from an existing Hive installation. For more on how to configure this feature, please refer to the <a href="https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html">Hive Tables</a> section. When running SQL from within another programming language the results will be returned as a <a href="contents/000_1-sds-3-x/sql-programming-guide.html#datasets-and-dataframes">Dataset/DataFrame</a>. You can also interact with the SQL interface using the <a href="https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html#running-the-spark-sql-cli">command-line</a> or over <a href="https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html#running-the-thrift-jdbcodbc-server">JDBC/ODBC</a>.</p>
<h2 id="datasets-and-dataframes"><a class="header" href="#datasets-and-dataframes">Datasets and DataFrames</a></h2>
<p>A Dataset is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be <a href="https://spark.apache.org/docs/latest/sql-getting-started.html#creating-datasets">constructed</a> from JVM objects and then manipulated using functional transformations (<code>map</code>, <code>flatMap</code>, <code>filter</code>, etc.). The Dataset API is available in <a href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html">Scala</a> and <a href="https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html">Java</a>. Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally <code>row.columnName</code>). The case for R is similar.</p>
<p>A DataFrame is a <em>Dataset</em> organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of <a href="https://spark.apache.org/docs/latest/sql-data-sources.html">sources</a> such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, <a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame">Python</a>, and <a href="https://spark.apache.org/docs/latest/api/R/index.html">R</a>. In Scala and Java, a DataFrame is represented by a Dataset of <code>Row</code>s. In <a href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html">the Scala API</a>, <code>DataFrame</code> is simply a type alias of <code>Dataset[Row]</code>. While, in <a href="https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html">Java API</a>, users need to use <code>Dataset&lt;Row&gt;</code> to represent a <code>DataFrame</code>.</p>
<p>Throughout this document, we will often refer to Scala/Java Datasets of <code>Row</code>s as DataFrames.</p>
</div>
<div class="cell markdown">
<h2 id="background-and-preparation"><a class="header" href="#background-and-preparation">Background and Preparation</a></h2>
<ul>
<li>If you are unfamiliar with SQL please brush-up from the basic links below.</li>
<li>SQL allows one to systematically explore any structured data (i.e., tables) using queries. This is necessary part of the data science process.</li>
</ul>
<p>One can use the <strong>SQL Reference</strong> at <a href="https://spark.apache.org/docs/latest/sql-ref.html">https://spark.apache.org/docs/latest/sql-ref.html</a> to learn SQL quickly.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/SQL&quot;,500))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Apache_Hive#HiveQL&quot;,175))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://spark.apache.org/docs/latest/sql-ref.html&quot;,700))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">http://spark.apache.org/docs/latest/sql-programming-guide.html</a> by Ivan Sadikov and Raazesh Sainudiin.</p>
<h1 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h1>
<h2 id="spark-sql-programming-guide-1"><a class="header" href="#spark-sql-programming-guide-1">Spark Sql Programming Guide</a></h2>
<ul>
<li>Starting Point: SparkSession</li>
<li>Creating DataFrames</li>
<li>Untyped Dataset Operations (aka DataFrame Operations)</li>
<li>Running SQL Queries Programmatically</li>
<li>Global Temporary View</li>
<li>Creating Datasets</li>
<li>Interoperating with RDDs
<ul>
<li>Inferring the Schema Using Reflection</li>
<li>Programmatically Specifying the Schema</li>
</ul>
</li>
<li>Scalar Functions</li>
<li>Aggregate Functions</li>
</ul>
</div>
<div class="cell markdown">
<h1 id="getting-started-1"><a class="header" href="#getting-started-1">Getting Started</a></h1>
<h2 id="starting-point-sparksession"><a class="header" href="#starting-point-sparksession">Starting Point: SparkSession</a></h2>
<p>The entry point into all functionality in Spark is the <a href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/SparkSession.html"><code>SparkSession</code></a> class and/or <code>SQLContext</code>/<code>HiveContext</code>. <code>SparkSession</code> is created for you as <code>spark</code> when you start <strong>spark-shell</strong> on command-line REPL or through a notebook server (databricks, zeppelin, jupyter, etc.). You will need to create <code>SparkSession</code> usually when building an application for submission to a Spark cluster. To create a basic <code>SparkSession</code>, just use <code>SparkSession.builder()</code>:</p>
<p>``` import org.apache.spark.sql.SparkSession</p>
<p>val spark = SparkSession .builder() .appName(&quot;Spark SQL basic example&quot;) .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) .getOrCreate()</p>
<p>// For implicit conversions like converting RDDs to DataFrames import spark.implicits._ ```</p>
<p>Find full example code in the Spark repo at:</p>
<ul>
<li><a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala">https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala</a></li>
</ul>
<p><code>SparkSession</code> in Spark 2.0 provides builtin support for Hive features including the ability to write queries using HiveQL, access to Hive UDFs, and the ability to read data from Hive tables. To use these features, you do not need to have an existing Hive setup.</p>
<p><code>// You could get SparkContext and SQLContext from SparkSession val sc = spark.sparkContext val sqlContext = spark.sqlContext</code></p>
<p>But in Databricks notebook (similar to <code>spark-shell</code>) <code>SparkSession</code> is already created for you and is available as <code>spark</code> (similarly, <code>sc</code> and <code>sqlContext</code> are also available).</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Evaluation of the cell by Ctrl+Enter will print spark session available in notebook
spark
</code></pre>
</div>
<div class="cell markdown">
<p>After evaluation you should see something like this, i.e., a reference to the <code>SparkSession</code> you just created:</p>
<p><code>res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5a289bf5</code></p>
</div>
<div class="cell markdown">
<h2 id="creating-dataframes"><a class="header" href="#creating-dataframes">Creating DataFrames</a></h2>
<p>With a <code>SparkSessions</code>, applications can create <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset">Dataset</a> or <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame"><code>DataFrame</code></a> from an <a href="https://spark.apache.org/docs/latest/sql-getting-started.html#interoperating-with-rdds">existing <code>RDD</code></a>, from a Hive table, or from various <a href="https://spark.apache.org/docs/latest/sql-data-sources.html">datasources</a>.</p>
<p>Just to recap, a DataFrame is a distributed collection of data organized into named columns. You can think of it as an organized into table RDD of case class <code>Row</code> (which is not exactly true). DataFrames, in comparison to RDDs, are backed by rich optimizations, including tracking their own schema, adaptive query execution, code generation including whole stage codegen, extensible Catalyst optimizer, and project <a href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html">Tungsten</a>.</p>
<p>Dataset provides type-safety when working with SQL, since <code>Row</code> is mapped to a case class, so that each column can be referenced by property of that class.</p>
<blockquote>
<p>Note that performance for Dataset/DataFrames is the same across languages Scala, Java, Python, and R. This is due to the fact that the planning phase is just language-specific, only logical plan is constructed in Python, and all the physical execution is compiled and executed as JVM bytecode.</p>
</blockquote>
</div>
<div class="cell markdown">
<p>As an example, the following creates a DataFrame based on the content of a JSON file:</p>
<p>``` val df = spark.read.json(&quot;examples/src/main/resources/people.json&quot;)</p>
<p>// Displays the content of the DataFrame to stdout df.show() // +----+-------+ // | age| name| // +----+-------+ // |null|Michael| // | 30| Andy| // | 19| Justin| // +----+-------+ ```</p>
<p>Find full example code at - https://raw.githubusercontent.com/apache/spark/master/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala in the Spark repo.</p>
<p>To be able to try this example in databricks we need to load the <code>people.json</code> file into <code>dbfs</code>. Let us do this programmatically next.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// the following lines merely fetch the file from the URL and load it into the dbfs for us to try in databricks
// getLines from the file at the URL
val peopleJsonLinesFromURL = scala.io.Source.fromURL(&quot;https://raw.githubusercontent.com/apache/spark/master/examples/src/main/resources/people.json&quot;).getLines
// remove any pre-existing file at the dbfs location
dbutils.fs.rm(&quot;dbfs:///datasets/spark-examples/people.json&quot;,recurse=true)
// convert the lines fetched from the URL to a Seq, then make it a RDD of String and finally save it as textfile to dbfs
sc.parallelize(peopleJsonLinesFromURL.toSeq).saveAsTextFile(&quot;dbfs:///datasets/spark-examples/people.json&quot;)
// read the text file we just saved and see what it has
sc.textFile(&quot;dbfs:///datasets/spark-examples/people.json&quot;).collect.mkString(&quot;\n&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val df = spark.read.json(&quot;dbfs:///datasets/spark-examples/people.json&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// you can also read into df like this
val df = spark.read.format(&quot;json&quot;).load(&quot;dbfs:///datasets/spark-examples/people.json&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">df.show()
</code></pre>
</div>
<div class="cell markdown">
<h2 id="untyped-dataset-operations-aka-dataframe-operations"><a class="header" href="#untyped-dataset-operations-aka-dataframe-operations">Untyped Dataset Operations (aka DataFrame Operations)</a></h2>
<p>DataFrames provide a domain-specific language for structured data manipulation in <a href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html">Scala</a>, <a href="https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html">Java</a>, <a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame">Python</a> and <a href="https://spark.apache.org/docs/latest/api/R/SparkDataFrame.html">R</a>.</p>
<p>As mentioned above, in Spark 2.0, DataFrames are just Dataset of <code>Row</code>s in Scala and Java API. These operations are also referred as “untyped transformations” in contrast to “typed transformations” come with strongly typed Scala/Java Datasets.</p>
<p>Here we include some basic examples of structured data processing using Datasets:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// This import is needed to use the $-notation
import spark.implicits._
// Print the schema in a tree format
df.printSchema()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Select only the &quot;name&quot; column
df.select(&quot;name&quot;).show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Select everybody, but increment the age by 1
df.select($&quot;name&quot;, $&quot;age&quot; + 1).show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Select people older than 21
df.filter($&quot;age&quot; &gt; 21).show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Count people by age
df.groupBy(&quot;age&quot;).count().show()
</code></pre>
</div>
<div class="cell markdown">
<p>Find full example code at - https://raw.githubusercontent.com/apache/spark/master/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala in the Spark repo.</p>
<p>For a complete list of the types of operations that can be performed on a Dataset, refer to the <a href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html">API Documentation</a>.</p>
<p>In addition to simple column references and expressions, Datasets also have a rich library of functions including string manipulation, date arithmetic, common math operations and more. The complete list is available in the <a href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html">DataFrame Function Reference</a>.</p>
</div>
<div class="cell markdown">
<h2 id="running-sql-queries-programmatically"><a class="header" href="#running-sql-queries-programmatically">Running SQL Queries Programmatically</a></h2>
<p>The <code>sql</code> function on a <code>SparkSession</code> enables applications to run SQL queries programmatically and returns the result as a <code>DataFrame</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Register the DataFrame as a SQL temporary view
df.createOrReplaceTempView(&quot;people&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val sqlDF = spark.sql(&quot;SELECT * FROM people&quot;)
sqlDF.show()
</code></pre>
</div>
<div class="cell markdown">
<h2 id="global-temporary-view"><a class="header" href="#global-temporary-view">Global Temporary View</a></h2>
<p>Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it terminates. If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a global temporary view. Global temporary view is tied to a system preserved database <code>global_temp</code>, and we must use the qualified name to refer it, e.g. <code>SELECT * FROM global_temp.view1</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Register the DataFrame as a global temporary view
df.createGlobalTempView(&quot;people&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Global temporary view is tied to a system preserved database `global_temp`
spark.sql(&quot;SELECT * FROM global_temp.people&quot;).show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Global temporary view is cross-session
spark.newSession().sql(&quot;SELECT * FROM global_temp.people&quot;).show()
</code></pre>
</div>
<div class="cell markdown">
<h2 id="creating-datasets"><a class="header" href="#creating-datasets">Creating Datasets</a></h2>
<p>Datasets are similar to RDDs, however, instead of using Java serialization or Kryo they use a specialized <a href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Encoder.html">Encoder</a> to serialize the objects for processing or transmitting over the network. While both encoders and standard serialization are responsible for turning an object into bytes, encoders are code generated dynamically and use a format that allows Spark to perform many operations like filtering, sorting and hashing without deserializing the bytes back into an object.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">case class Person(name: String, age: Long)

// Encoders are created for case classes
val caseClassDS = Seq(Person(&quot;Andy&quot;, 32)).toDS()
caseClassDS.show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Encoders for most common types are automatically provided by importing spark.implicits._
val primitiveDS = Seq(1, 2, 3).toDS()
primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name
val path = &quot;dbfs:///datasets/spark-examples/people.json&quot;
val peopleDS = spark.read.json(path).as[Person]
peopleDS.show()
</code></pre>
</div>
<div class="cell markdown">
<p><strong>Dataset is not available directly in PySpark or SparkR</strong>.</p>
<h2 id="interoperating-with-rdds"><a class="header" href="#interoperating-with-rdds">Interoperating with RDDs</a></h2>
<p>Spark SQL supports two different methods for converting existing RDDs into Datasets. The first method uses reflection to infer the schema of an RDD that contains specific types of objects. This reflection-based approach leads to more concise code and works well when you already know the schema while writing your Spark application.</p>
<p>The second method for creating Datasets is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. While this method is more verbose, it allows you to construct Datasets when the columns and their types are not known until runtime.</p>
<h3 id="inferring-the-schema-using-reflection"><a class="header" href="#inferring-the-schema-using-reflection">Inferring the Schema Using Reflection</a></h3>
<p>The Scala interface for Spark SQL supports automatically converting an RDD containing case classes to a DataFrame. The case class defines the schema of the table. The names of the arguments to the case class are read using reflection and become the names of the columns. Case classes can also be nested or contain complex types such as <code>Seq</code>s or <code>Array</code>s. This RDD can be implicitly converted to a DataFrame and then be registered as a table. Tables can be used in subsequent SQL statements.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// the following lines merely fetch the file from the URL and load it into the dbfs for us to try in databricks
// getLines from the file at the URL
val peopleTxtLinesFromURL = scala.io.Source.fromURL(&quot;https://raw.githubusercontent.com/apache/spark/master/examples/src/main/resources/people.txt&quot;).getLines
// remove any pre-existing file at the dbfs location
dbutils.fs.rm(&quot;dbfs:///datasets/spark-examples/people.txt&quot;,recurse=true)
// convert the lines fetched from the URL to a Seq, then make it a RDD of String and finally save it as textfile to dbfs
sc.parallelize(peopleTxtLinesFromURL.toSeq).saveAsTextFile(&quot;dbfs:///datasets/spark-examples/people.txt&quot;)
// read the text file we just saved and see what it has
sc.textFile(&quot;dbfs:///datasets/spark-examples/people.txt&quot;).collect.mkString(&quot;\n&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sc.textFile(&quot;dbfs:///datasets/spark-examples/people.txt&quot;).collect.mkString(&quot;\n&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// For implicit conversions from RDDs to DataFrames
import spark.implicits._

// make a case class
case class Person(name: String, age: Long)

// Create an RDD of Person objects from a text file, convert it to a Dataframe
val peopleDF = spark.sparkContext
  .textFile(&quot;dbfs:///datasets/spark-examples/people.txt&quot;)
  .map(_.split(&quot;,&quot;))
  .map(attributes =&gt; Person(attributes(0), attributes(1).trim.toLong))
  //.map(attributes =&gt; Person(attributes(0), attributes(1).trim.toLong))
  .toDF()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">peopleDF.show
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Register the DataFrame as a temporary view
peopleDF.createOrReplaceTempView(&quot;people&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// SQL statements can be run by using the sql methods provided by Spark
val teenagersDF = spark.sql(&quot;SELECT name, age FROM people WHERE age BETWEEN 13 AND 19&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">teenagersDF.show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// The columns of a row in the result can be accessed by field index
teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager(0)).show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// or by field name
teenagersDF.map(teenager =&gt; &quot;Name: &quot; + teenager.getAs[String](&quot;name&quot;)).show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// No pre-defined encoders for Dataset[Map[K,V]], define explicitly
implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Primitive types and case classes can be also defined as
// import more classes here...
//implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]
teenagersDF.map(teenager =&gt; teenager.getValuesMap[Any](List(&quot;name&quot;, &quot;age&quot;))).collect()
</code></pre>
</div>
<div class="cell markdown">
<h3 id="programmatically-specifying-the-schema"><a class="header" href="#programmatically-specifying-the-schema">Programmatically Specifying the Schema</a></h3>
<p>When case classes cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a <code>DataFrame</code> can be created programmatically with three steps.</p>
<ol>
<li>Create an RDD of <code>Row</code>s from the original RDD;</li>
<li>Create the schema represented by a <code>StructType</code> matching the structure of <code>Row</code>s in the RDD created in Step 1.</li>
<li>Apply the schema to the RDD of <code>Row</code>s via <code>createDataFrame</code> method provided by <code>SparkSession</code>.</li>
</ol>
<p>For example:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.sql.Row

import org.apache.spark.sql.types._

// Create an RDD
val peopleRDD = spark.sparkContext.textFile(&quot;dbfs:///datasets/spark-examples/people.txt&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// The schema is encoded in a string
val schemaString = &quot;name age&quot;
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Generate the schema based on the string of schema
val fields = schemaString.split(&quot; &quot;)
  .map(fieldName =&gt; StructField(fieldName, StringType, nullable = true))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val schema = StructType(fields)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Convert records of the RDD (people) to Rows
val rowRDD = peopleRDD
  .map(_.split(&quot;,&quot;))
  .map(attributes =&gt; Row(attributes(0), attributes(1).trim))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Apply the schema to the RDD
val peopleDF = spark.createDataFrame(rowRDD, schema)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">peopleDF.show
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Creates a temporary view using the DataFrame
peopleDF.createOrReplaceTempView(&quot;people&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// SQL can be run over a temporary view created using DataFrames
val results = spark.sql(&quot;SELECT name FROM people&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">results.show
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// The results of SQL queries are DataFrames and support all the normal RDD operations
// The columns of a row in the result can be accessed by field index or by field name
results.map(attributes =&gt; &quot;Name: &quot; + attributes(0)).show()
</code></pre>
</div>
<div class="cell markdown">
<p>Find full example code at - https://raw.githubusercontent.com/apache/spark/master/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala in the Spark repo.</p>
</div>
<div class="cell markdown">
<h2 id="scalar-functions"><a class="header" href="#scalar-functions">Scalar Functions</a></h2>
<p>Scalar functions are functions that return a single value per row, as opposed to aggregation functions, which return a value for a group of rows. Spark SQL supports a variety of <a href="https://spark.apache.org/docs/latest/sql-ref-functions.html#scalar-functions">Built-in Scalar Functions</a>. It also supports <a href="https://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html">User Defined Scalar Functions</a>.</p>
</div>
<div class="cell markdown">
<h2 id="aggregate-functions"><a class="header" href="#aggregate-functions">Aggregate Functions</a></h2>
<p>Aggregate functions are functions that return a single value on a group of rows. The <a href="https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#aggregate-functions">Built-in Aggregation Functions</a> provide common aggregations such as <code>count()</code>, <code>countDistinct()</code>, <code>avg()</code>, <code>max()</code>, <code>min()</code>, etc. Users are not limited to the predefined aggregate functions and can create their own. For more details about user defined aggregate functions, please refer to the documentation of <a href="https://spark.apache.org/docs/latest/sql-ref-functions-udf-aggregate.html">User Defined Aggregate Functions</a>.</p>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">http://spark.apache.org/docs/latest/sql-programming-guide.html</a> by Ivan Sadikov and Raazesh Sainudiin.</p>
<h1 id="getting-started---exercise"><a class="header" href="#getting-started---exercise">Getting Started - Exercise</a></h1>
<p>After having gone through the simple example dataset in the programming guide, let's try a slightly larger dataset next.</p>
</div>
<div class="cell markdown">
<p>Let us first create a table of social media usage from NYC</p>
<blockquote>
<p>See the <strong>Load Data</strong> section to create this <code>social_media_usage</code> table from raw data.</p>
</blockquote>
<p>First let's make sure this table is available for us. If you don't see <code>social_media_usage</code> as a <code>name</code>d table in the output of the next cell then we first need to ingest this dataset. Let's do it using the databricks' GUI for creating <code>Data</code> as done next.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Let's find out what tables are already available for loading
spark.catalog.listTables.show(50)
</code></pre>
</div>
<div class="cell markdown">
<h2 id="nyc-social-media-usage-data"><a class="header" href="#nyc-social-media-usage-data">NYC Social Media Usage Data</a></h2>
<p>This dataset is from <a href="https://datahub.io/JohnSnowLabs/nyc-social-media-usage#readme">https://datahub.io/JohnSnowLabs/nyc-social-media-usage#readme</a></p>
<p>The Demographic Reports are produced by the Economic, Demographic and Statistical Research unit within the Countywide Service Integration and Planning Management (CSIPM) Division of the Fairfax County Department of Neighborhood and Community Services. Information produced by the Economic, Demographic and Statistical Research unit is used by every county department, board, authority and the Fairfax County Public Schools. In addition to the small area estimates and forecasts, state and federal data on Fairfax County are collected and summarized, and special studies and Quantitative research are conducted by the unit.</p>
<p>We are going to fetch this data, with slightly simplified column names, from the following URL:</p>
<ul>
<li>http://lamastex.org/datasets/public/NYCUSA/social-media-usage.csv</li>
</ul>
<p>To turn the dataset into a registered table we will load it using the GUI as follows:</p>
<ul>
<li>Download it to your local machine / laptop and then use the 'Data' button on the left to upload it (we will try this method now).
<ul>
<li>This will put your data in the <code>Filestore</code> in databricks' distributed file system.</li>
</ul>
</li>
</ul>
<h3 id="overview"><a class="header" href="#overview">Overview</a></h3>
<p>Below we will show you how to create and query a table or DataFrame that you uploaded to DBFS. <a href="https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html">DBFS</a> is a Databricks File System (their distributed file system) that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.</p>
<p>In other setups, you can have the data in s3 (say in AWS) or in hdfs in your hadoop cluster, etc.</p>
<p>Alternatively, you can use <code>curl</code> or <code>wget</code> to download it to the local file system in <code>/databricks/driver</code> and then load it into <code>dbfs</code>, after this you can use read it via <code>spark</code> session into a dataframe and register it as a hive table.</p>
<p>You can also get the data directly from here (but in this case you need to change the column names in the databricks Data upload GUI or programmatically to follow this notebook):</p>
<ul>
<li>http://datahub.io/JohnSnowLabs/nyc-social-media-usage</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="load-data"><a class="header" href="#load-data">Load Data</a></h2>
<h3 id="how-to-uoload-csv-file-and-make-a-table-in-databricks"><a class="header" href="#how-to-uoload-csv-file-and-make-a-table-in-databricks">How to uoload csv file and make a table in databricks</a></h3>
<p>Okay, so how did we actually make table <code>social_media_usage</code>? Databricks allows us to upload/link external data and make it available as registerd SQL table. It involves several steps:</p>
<ol>
<li>Dowload this <code>social-media-usage.csv</code> file from the following URL to your laptop:</li>
</ol>
<ul>
<li>
<p>http://lamastex.org/datasets/public/NYCUSA/social-media-usage.csv</p>
</li>
<li>
<p>Go to Databricks cloud (where you log in to use Databricks notebooks) and open tab <strong>Data</strong> on the left panel</p>
</li>
<li>
<p>On the very top of the left sub-menu you will see button <strong>+Add Data</strong>, click on it</p>
</li>
<li>
<p>Choose <strong>Upload File</strong> for Data Sources by <strong>Browse</strong> or <strong>Drag and Drop</strong>, where <strong>File</strong> means any file (Parquet, Avro, CSV), but it works the best with CSV format</p>
</li>
<li>
<p>Upload <code>social-media-usage.csv</code> file you just downloaded to databricks</p>
</li>
<li>
<p>Just note the path to the uploaded file, for example in my case:</p>
<blockquote>
<p>File uploaded to <code>/FileStore/tables/social_media_usage.csv</code></p>
</blockquote>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// File location and type
// You may need to change the file_location &quot;social_media_usage-5dbee.csv&quot; depending on your location given by
// File uploaded to /FileStore/tables/social_media_usage.csv
val file_location = &quot;/FileStore/tables/social_media_usage.csv&quot;
val file_type = &quot;csv&quot;

// CSV options
val infer_schema = &quot;true&quot;
val first_row_is_header = &quot;true&quot;
val delimiter = &quot;,&quot;

// The applied options are for CSV files. For other file types, these will be ignored.
val socialMediaDF = spark.read.format(file_type) 
  .option(&quot;inferSchema&quot;, infer_schema) 
  .option(&quot;header&quot;, first_row_is_header) 
  .option(&quot;sep&quot;, delimiter) 
  .load(file_location)

socialMediaDF.show(10)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Let's create a view or table

val temp_table_name = &quot;social_media_usage&quot;

socialMediaDF.createOrReplaceTempView(temp_table_name)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Let's find out what tables are already available for loading
spark.catalog.listTables.show(100)
</code></pre>
</div>
<div class="cell markdown">
<p>With this registered as a temporary view, <code>social_media_usage</code> will only be available to this particular notebook.</p>
<p>If you'd like other users to be able to query this table (in the databricks professional shard - not the free community edition; or in a managed on-premise cluster), you can also create a table from the DataFrame.</p>
<p>Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data. To do so, choose your table name and use <code>saveAsTable</code> as done in the next cell.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val permanent_table_name = &quot;social_media_usage&quot;
socialMediaDF.write.format(&quot;parquet&quot;).saveAsTable(permanent_table_name)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Let's find out what tables are already available for loading
// spark.catalog.listTables.show(100)
</code></pre>
</div>
<div class="cell markdown">
<p>It looks like the table <code>social_media_usage</code> is available as a permanent table (<code>isTemporary</code> set as <code>false</code>), if you have not uncommented the last line in the previous cell (otherwise it will be available from a parquet file as a permanent table - we will see more about parquet in the sequel).</p>
<p>Next let us do the following:</p>
<ul>
<li>load this table as a DataFrame (yes, the dataframe already exists as <code>socialMediaDF</code>, but we want to make a new DataFrame directly from the table)</li>
<li>print its schema and</li>
<li>show the first 20 rows.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">spark.catalog.listTables.show(100)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val df = spark.table(&quot;social_media_usage&quot;) // Ctrl+Enter
</code></pre>
</div>
<div class="cell markdown">
<p>As you can see the immutable value <code>df</code> is a DataFrame and more specifically it is:</p>
<blockquote>
<p><code>org.apache.spark.sql.DataFrame = [agency: string, platform: string, url: string, date: timestamp, visits: integer]</code>.</p>
</blockquote>
</div>
<div class="cell markdown">
<p>Now let us print schema of the DataFrame <code>df</code> and have a look at the actual data:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Ctrl+Enter
df.printSchema() // prints schema of the DataFrame
df.show() // shows first n (default is 20) rows
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p>Note that <code>(nullable = true)</code> simply means if the value is allowed to be <code>null</code>.</p>
</blockquote>
<p>Let us count the number of rows in <code>df</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">df.count() // Ctrl+Enter to get 5898
</code></pre>
</div>
<div class="cell markdown">
<p>So there are 5899 records or rows in the DataFrame <code>df</code>. Pretty good! You can also select individual columns using so-called DataFrame API, as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val platforms = df.select(&quot;platform&quot;) // Shift+Enter
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">platforms.count() // Shift+Enter to count the number of rows
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">platforms.show(5) // Ctrl+Enter to show top 5 rows
</code></pre>
</div>
<div class="cell markdown">
<p>You can also apply <code>.distinct()</code> to extract only unique entries as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val uniquePlatforms = df.select(&quot;platform&quot;).distinct() // Shift+Enter
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">uniquePlatforms.count() // Ctrl+Enter to count the number of distinct platforms
</code></pre>
</div>
<div class="cell markdown">
<p>Let's see all the rows of the DataFrame <code>uniquePlatforms</code>.</p>
<blockquote>
<p>Note that <code>display(uniquePlatforms)</code> unlike <code>uniquePlatforms.show()</code> displays all rows of the DataFrame + gives you ability to select different view, e.g. charts.</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(uniquePlatforms) // Ctrl+Enter to show all rows; use the scroll-bar on the right of the display to see all platforms
</code></pre>
</div>
<div class="cell markdown">
<h3 id="spark-sql-and-dataframe-api"><a class="header" href="#spark-sql-and-dataframe-api">Spark SQL and DataFrame API</a></h3>
<p>Spark SQL provides DataFrame API that can perform relational operations on both external data sources and internal collections, which is similar to widely used data frame concept in R, but evaluates operations support lazily (remember RDDs?), so that it can perform relational optimizations. This API is also available in Java, Python and R, but some functionality may not be available, although with every release of Spark people minimize this gap.</p>
<p>So we give some examples how to query data in Python and R, but continue with Scala. You can do all DataFrame operations in this notebook using Python or R.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># Ctrl+Enter to evaluate this python cell, recall '#' is the pre-comment character in python
# Using Python to query our &quot;social_media_usage&quot; table
pythonDF = spark.table(&quot;social_media_usage&quot;).select(&quot;platform&quot;).distinct()
pythonDF.show(3)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">-- Ctrl+Enter to achieve the same result using standard SQL syntax!
select distinct platform from social_media_usage
</code></pre>
</div>
<div class="cell markdown">
<p>Now it is time for some tips around how you use <code>select</code> and what the difference is between <code>$&quot;a&quot;</code>, <code>col(&quot;a&quot;)</code>, <code>df(&quot;a&quot;)</code>.</p>
<p>As you probably have noticed by now, you can specify individual columns to select by providing String values in select statement. But sometimes you need to: - distinguish between columns with the same name - use it to filter (actually you can still filter using full String expression) - do some &quot;magic&quot; with joins and user-defined functions (this will be shown later)</p>
<p>So Spark gives you ability to actually specify columns when you select. Now the difference between all those three notations is ... none, those things are just aliases for a <code>Column</code> in Spark SQL, which means following expressions yield the same result:</p>
<p>``` // Using string expressions df.select(&quot;agency&quot;, &quot;visits&quot;)</p>
<p>// Using &quot;\(&quot; alias for column df.select(\)&quot;agency&quot;, $&quot;visits&quot;)</p>
<p>// Using &quot;col&quot; alias for column df.select(col(&quot;agency&quot;), col(&quot;visits&quot;))</p>
<p>// Using DataFrame name for column df.select(df(&quot;agency&quot;), df(&quot;visits&quot;)) ```</p>
<p>This &quot;same-difference&quot; applies to filtering, i.e. you can either use full expression to filter, or column as shown in the following example:</p>
<p>``` // Using column to filter df.select(&quot;visits&quot;).filter($&quot;visits&quot; &gt; 100)</p>
<p>// Or you can use full expression as string df.select(&quot;visits&quot;).filter(&quot;visits &gt; 100&quot;) ```</p>
<blockquote>
<p>Note that <code>$&quot;visits&quot; &gt; 100</code> expression looks amazing, but under the hood it is just another column, and it equals to <code>df(&quot;visits&quot;).&gt;(100)</code>, where, thanks to Scala paradigm <code>&gt;</code> is just another function that you can define.</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val sms = df.select($&quot;agency&quot;, $&quot;platform&quot;, $&quot;visits&quot;).filter($&quot;platform&quot; === &quot;SMS&quot;)
sms.show() // Ctrl+Enter
</code></pre>
</div>
<div class="cell markdown">
<p>Again you could have written the query above using any column aliases or String names or even writing the query directly.</p>
<p>For example, we can do it using String names, as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Ctrl+Enter Note that we are using &quot;platform = 'SMS'&quot; since it will be evaluated as actual SQL
val sms = df.select(df(&quot;agency&quot;), df(&quot;platform&quot;), df(&quot;visits&quot;)).filter(&quot;platform = 'SMS'&quot;)
sms.show(5)
</code></pre>
</div>
<div class="cell markdown">
<p>Refer to the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame">DataFrame API</a> for more detailed API. In addition to simple column references and expressions, DataFrames also have a rich library of functions including string manipulation, date arithmetic, common math operations and more. The complete list is available in the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$">DataFrame Function Reference</a>.</p>
</div>
<div class="cell markdown">
<p>Let's next explore some of the functionality that is available by transforming this DataFrame <code>df</code> into a new DataFrame called <code>fixedDF</code>.</p>
<ul>
<li>First, note that some columns are not exactly what we want them to be.
<ul>
<li>visits should not contain null values, but <code>0</code>s instead.</li>
</ul>
</li>
<li>Let us fix it using some code that is briefly explained here (don't worry if you don't get it completely now, you will get the hang of it by playing more)
<ul>
<li>The <code>coalesce</code> function is similar to <code>if-else</code> statement, i.e., if first column in expression is <code>null</code>, then the value of the second column is used and so on.</li>
<li><code>lit</code> just means column of constant value (<code>lit</code>erally speaking!).</li>
<li>we also remove <code>TOTAL</code> value from <code>platform</code> column.</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Ctrl+Enter to make fixedDF

// import the needed sql functions
import org.apache.spark.sql.functions.{coalesce, lit}

// make the fixedDF DataFrame
val fixedDF = df.
   select(
     $&quot;agency&quot;, 
     $&quot;platform&quot;, 
     $&quot;url&quot;, 
     $&quot;date&quot;, 
     coalesce($&quot;visits&quot;, lit(0)).as(&quot;visits&quot;))
    .filter($&quot;platform&quot; =!= &quot;TOTAL&quot;)

fixedDF.printSchema() // print its schema 
// and show the top 20 records fully
fixedDF.show(false) // the false argument does not truncate the rows, so you will not see something like this &quot;anot...&quot;
</code></pre>
</div>
<div class="cell markdown">
<p>Okay, this is better, but <code>url</code>s are still inconsistent.</p>
<p>Let's fix this by writing our own UDF (user-defined function) to deal with special cases.</p>
<p>Note that if you <strong>CAN USE Spark functions library</strong>, do it. But for the sake of the example, custom UDF is shown below.</p>
<p>We take value of a column as String type and return the same String type, but ignore values that do not start with <code>http</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Ctrl+Enter to evaluate this UDF which takes a input String called &quot;value&quot;
// and converts it into lower-case if it begins with http and otherwise leaves it as null, so we sort of remove non valid web-urls
val cleanUrl = udf((value: String) =&gt; if (value != null &amp;&amp; value.startsWith(&quot;http&quot;)) value.toLowerCase() else null)
</code></pre>
</div>
<div class="cell markdown">
<p>Let us apply our UDF on <code>fixedDF</code> to create a new DataFrame called <code>cleanedDF</code> as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Ctrl+Enter
val cleanedDF = fixedDF.select($&quot;agency&quot;, $&quot;platform&quot;, cleanUrl($&quot;url&quot;).as(&quot;url&quot;), $&quot;date&quot;, $&quot;visits&quot;)
</code></pre>
</div>
<div class="cell markdown">
<p>Now, let's check that it actually worked by seeing the first 5 rows of the <code>cleanedDF</code> whose <code>url</code> <code>isNull</code> and <code>isNotNull</code>, as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Shift+Enter
cleanedDF.filter($&quot;url&quot;.isNull).show(5)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Ctrl+Enter
cleanedDF.filter($&quot;url&quot;.isNotNull).show(5, false) // false in .show(5, false) shows rows untruncated
</code></pre>
</div>
<div class="cell markdown">
<p>Now there is a suggestion from you manager's manager's manager that due to some perceived privacy concerns we want to replace <code>agency</code> with some unique identifier.</p>
<p>So we need to do the following:</p>
<ul>
<li>create unique list of agencies with ids and</li>
<li>join them with main DataFrame.</li>
</ul>
<p>Sounds easy, right? Let's do it.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Crtl+Enter
// Import Spark SQL function that will give us unique id across all the records in this DataFrame
import org.apache.spark.sql.functions.monotonically_increasing_id

// We append column as SQL function that creates unique ids across all records in DataFrames 
val agencies = cleanedDF.select(cleanedDF(&quot;agency&quot;))
                        .distinct()
                        .withColumn(&quot;id&quot;, monotonically_increasing_id())
agencies.show(5)
</code></pre>
</div>
<div class="cell markdown">
<p>Those who want to understand left/right inner/outer joins can see the video lectures in Module 3 of Anthony Joseph's Introduction to Big data edX course.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Ctrl+Enter
// And join with the rest of the data, note how join condition is specified 
val anonym = cleanedDF.join(agencies, cleanedDF(&quot;agency&quot;) === agencies(&quot;agency&quot;), &quot;inner&quot;).select(&quot;id&quot;, &quot;platform&quot;, &quot;url&quot;, &quot;date&quot;, &quot;visits&quot;)

// We also cache DataFrame since it can be quite expensive to recompute join
anonym.cache()

// Display result
anonym.show(5)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">spark.catalog.listTables().show() // look at the available tables
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">-- to remove a TempTable if it exists already
drop table if exists anonym
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Register table for Spark SQL, we also import &quot;month&quot; function 
import org.apache.spark.sql.functions.month

anonym.createOrReplaceTempView(&quot;anonym&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">-- Interesting. Now let's do some aggregation. Display platform, month, visits
-- Date column allows us to extract month directly

select platform, month(date) as month, sum(visits) as visits from anonym group by platform, month(date)
</code></pre>
</div>
<div class="cell markdown">
<p>Note, that we could have done aggregation using DataFrame API instead of Spark SQL.</p>
</div>
<div class="cell markdown">
<p>Alright, now let's see some <em>cool</em> operations with window functions.</p>
<p>Our next task is to compute <code>(daily visits / monthly average)</code> for all platforms.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.sql.functions.{dayofmonth, month, row_number, sum}
import org.apache.spark.sql.expressions.Window

val coolDF = anonym.select($&quot;id&quot;, $&quot;platform&quot;, dayofmonth($&quot;date&quot;).as(&quot;day&quot;), month($&quot;date&quot;).as(&quot;month&quot;), $&quot;visits&quot;).
  groupBy($&quot;id&quot;, $&quot;platform&quot;, $&quot;day&quot;, $&quot;month&quot;).agg(sum(&quot;visits&quot;).as(&quot;visits&quot;))

// Run window aggregation on visits per month and platform
val window = coolDF.select($&quot;id&quot;, $&quot;day&quot;, $&quot;visits&quot;, sum($&quot;visits&quot;).over(Window.partitionBy(&quot;platform&quot;, &quot;month&quot;)).as(&quot;monthly_visits&quot;))

// Create and register percent table
val percent = window.select($&quot;id&quot;, $&quot;day&quot;, ($&quot;visits&quot; / $&quot;monthly_visits&quot;).as(&quot;percent&quot;))

percent.createOrReplaceTempView(&quot;percent&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">-- A little bit of visualization as result of our efforts
select id, day, `percent` from percent where `percent` &gt; 0.3 and day = 2
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">-- You also could just use plain SQL to write query above, note that you might need to group by id and day as well.
with aggr as (
  select id, dayofmonth(date) as day, visits / sum(visits) over (partition by (platform, month(date))) as percent
  from anonym
)
select * from aggr where day = 2 and percent &gt; 0.3
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell markdown">
<h2 id="interoperating-with-rdds-1"><a class="header" href="#interoperating-with-rdds-1">Interoperating with RDDs</a></h2>
<p>Spark SQL supports two different methods for converting existing RDDs into DataFrames. The first method uses reflection to infer the schema of an RDD that contains specific types of objects. This reflection based approach leads to more concise code and works well when you already know the schema.</p>
<p>The second method for creating DataFrames is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. While this method is more verbose, it allows you to construct DataFrames when the columns and their types are not known until runtime.</p>
<h3 id="inferring-the-schema-using-reflection-1"><a class="header" href="#inferring-the-schema-using-reflection-1">Inferring the Schema Using Reflection</a></h3>
<p>The Scala interface for Spark SQL supports automatically converting an RDD containing case classes to a DataFrame. The case class defines the schema of the table. The names of the arguments to the case class are read using reflection and become the names of the columns. Case classes can also be nested or contain complex types such as Sequences or Arrays. This RDD can be implicitly converted to a DataFrame and then be registered as a table.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Define case class that will be our schema for DataFrame
case class Hubot(name: String, year: Int, manufacturer: String, version: Array[Int], details: Map[String, String])

// You can process a text file, for example, to convert every row to our Hubot, but we will create RDD manually
val rdd = sc.parallelize(
  Array(
    Hubot(&quot;Jerry&quot;, 2015, &quot;LCorp&quot;, Array(1, 2, 3), Map(&quot;eat&quot; -&gt; &quot;yes&quot;, &quot;sleep&quot; -&gt; &quot;yes&quot;, &quot;drink&quot; -&gt; &quot;yes&quot;)),
    Hubot(&quot;Mozart&quot;, 2010, &quot;LCorp&quot;, Array(1, 2), Map(&quot;eat&quot; -&gt; &quot;no&quot;, &quot;sleep&quot; -&gt; &quot;no&quot;, &quot;drink&quot; -&gt; &quot;no&quot;)),
    Hubot(&quot;Einstein&quot;, 2012, &quot;LCorp&quot;, Array(1, 2, 3), Map(&quot;eat&quot; -&gt; &quot;yes&quot;, &quot;sleep&quot; -&gt; &quot;yes&quot;, &quot;drink&quot; -&gt; &quot;no&quot;))
  )
)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// In order to convert RDD into DataFrame you need to do this:
val hubots = rdd.toDF()

// Display DataFrame, note how array and map fields are displayed
hubots.printSchema()
hubots.show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// You can query complex type the same as you query any other column
// By the way you can use `sql` function to invoke Spark SQL to create DataFrame
hubots.createOrReplaceTempView(&quot;hubots&quot;)

val onesThatEat = sqlContext.sql(&quot;select name, details.eat from hubots where details.eat = 'yes'&quot;)

onesThatEat.show()
</code></pre>
</div>
<div class="cell markdown">
<h3 id="programmatically-specifying-the-schema-1"><a class="header" href="#programmatically-specifying-the-schema-1">Programmatically Specifying the Schema</a></h3>
<p>When case classes cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a <code>DataFrame</code> can be created programmatically with three steps.</p>
<ol>
<li>Create an RDD of <code>Row</code>s from the original RDD</li>
<li>Create the schema represented by a <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.types.StructType">StructType</a> and <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.types.StructField">StructField</a> classes matching the structure of <code>Row</code>s in the RDD created in Step 1.</li>
<li>Apply the schema to the RDD of <code>Row</code>s via <code>createDataFrame</code> method provided by <code>SQLContext</code>.</li>
</ol>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.sql.types._

// Let's say we have an RDD of String and we need to convert it into a DataFrame with schema &quot;name&quot;, &quot;year&quot;, and &quot;manufacturer&quot;
// As you can see every record is space-separated.
val rdd = sc.parallelize(
  Array(
    &quot;Jerry 2015 LCorp&quot;,
    &quot;Mozart 2010 LCorp&quot;,
    &quot;Einstein 2012 LCorp&quot;
  )
)

// Create schema as StructType //
val schema = StructType(
  StructField(&quot;name&quot;, StringType, false) :: 
  StructField(&quot;year&quot;, IntegerType, false) :: 
  StructField(&quot;manufacturer&quot;, StringType, false) :: 
  Nil
)

// Prepare RDD[Row]
val rows = rdd.map { entry =&gt; 
  val arr = entry.split(&quot;\\s+&quot;)
  val name = arr(0)
  val year = arr(1).toInt
  val manufacturer = arr(2)
  
  Row(name, year, manufacturer)
}

// Create DataFrame
val bots = sqlContext.createDataFrame(rows, schema)
bots.printSchema()
bots.show()
</code></pre>
</div>
<div class="cell markdown">
<h2 id="creating-datasets-1"><a class="header" href="#creating-datasets-1">Creating Datasets</a></h2>
<p>A <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset">Dataset</a> is a strongly-typed, immutable collection of objects that are mapped to a relational schema. At the core of the Dataset API is a new concept called an <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder">encoder</a>, which is responsible for converting between JVM objects and tabular representation. The tabular representation is stored using Spark’s internal Tungsten binary format, allowing for operations on serialized data and improved memory utilization. Spark 2.2 comes with support for automatically generating encoders for a wide variety of types, including primitive types (e.g. String, Integer, Long), and Scala case classes.</p>
<blockquote>
<p>Simply put, you will get all the benefits of DataFrames with fair amount of flexibility of RDD API.</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// We can start working with Datasets by using our &quot;hubots&quot; table

// To create Dataset from DataFrame do this (assuming that case class Hubot exists):
val ds = hubots.as[Hubot]
ds.show()
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p><strong>Side-note:</strong> Dataset API is first-class citizen in Spark, and DataFrame is an alias for Dataset[Row]. Note that Python and R use DataFrames (since they are dynamically typed), but it is essentially a Dataset.</p>
</blockquote>
</div>
<div class="cell markdown">
<h2 id="finally"><a class="header" href="#finally">Finally</a></h2>
<p>DataFrames and Datasets can simplify and improve most of the applications pipelines by bringing concise syntax and performance optimizations. We would highly recommend you to check out the official API documentation, specifically around</p>
<ul>
<li><a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame">DataFrame API</a>,</li>
<li><a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$">Spark SQL functions library</a>,</li>
<li><a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.GroupedData">GroupBy clause and aggregated functions</a>.</li>
</ul>
<p>Unfortunately, this is just <em>a getting started quickly</em> course, and we skip features like custom aggregations, types, pivoting, etc., but if you are keen to know then start from the links above and this notebook and others in this directory.</p>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">http://spark.apache.org/docs/latest/sql-programming-guide.html</a> by Ivan Sadikov and Raazesh Sainudiin.</p>
<h1 id="data-sources"><a class="header" href="#data-sources">Data Sources</a></h1>
<h2 id="spark-sql-programming-guide-2"><a class="header" href="#spark-sql-programming-guide-2">Spark Sql Programming Guide</a></h2>
<ul>
<li>Data Sources
<ul>
<li>Generic Load/Save Functions
<ul>
<li>Manually Specifying Options</li>
<li>Run SQL on files directly</li>
<li>Save Modes</li>
<li>Saving to Persistent Tables</li>
</ul>
</li>
<li>Parquet Files
<ul>
<li>Loading Data Programmatically</li>
<li>Partition Discovery</li>
<li>Schema Merging</li>
<li>Hive metastore Parquet table conversion
<ul>
<li>Hive/Parquet Schema Reconciliation</li>
<li>Metadata Refreshing</li>
</ul>
</li>
<li>Configuration</li>
</ul>
</li>
<li>JSON Datasets</li>
<li>Hive Tables
<ul>
<li>Interacting with Different Versions of Hive Metastore</li>
</ul>
</li>
<li>JDBC To Other Databases</li>
<li>Troubleshooting</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h1 id="data-sources-1"><a class="header" href="#data-sources-1">Data Sources</a></h1>
<p>Spark SQL supports operating on a variety of data sources through the <code>DataFrame</code> or <code>DataFrame</code> interfaces. A Dataset can be operated on as normal RDDs and can also be registered as a temporary table. Registering a Dataset as a table allows you to run SQL queries over its data. But from time to time you would need to either load or save Dataset. Spark SQL provides built-in data sources as well as Data Source API to define your own data source and use it read / write data into Spark.</p>
</div>
<div class="cell markdown">
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>Spark provides some built-in datasources that you can use straight out of the box, such as <a href="https://parquet.apache.org/">Parquet</a>, <a href="http://www.json.org/">JSON</a>, <a href="https://en.wikipedia.org/wiki/Java_Database_Connectivity">JDBC</a>, <a href="https://orc.apache.org/">ORC</a> (available with enabled Hive Support, but this is changing, and ORC will not require Hive support and will work with default Spark session starting from next release), and Text (since Spark 1.6) and CSV (since Spark 2.0, before that it is accessible as a package).</p>
<h2 id="third-party-datasource-packages"><a class="header" href="#third-party-datasource-packages">Third-party datasource packages</a></h2>
<p>Community also have built quite a few datasource packages to provide easy access to the data from other formats. You can find list of those packages on http://spark-packages.org/, e.g. <a href="http://spark-packages.org/package/databricks/spark-avro">Avro</a>, <a href="http://spark-packages.org/package/databricks/spark-csv">CSV</a>, <a href="http://spark-packages.org/package/databricks/spark-redshift">Amazon Redshit</a> (for Spark &lt; 2.0), <a href="http://spark-packages.org/package/HyukjinKwon/spark-xml">XML</a>, <a href="http://spark-packages.org/package/sadikovi/spark-netflow">NetFlow</a> and many others.</p>
</div>
<div class="cell markdown">
<h2 id="generic-loadsave-functions"><a class="header" href="#generic-loadsave-functions">Generic Load/Save functions</a></h2>
<p>In order to load or save DataFrame you have to call either <code>read</code> or <code>write</code>. This will return <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader">DataFrameReader</a> or <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter">DataFrameWriter</a> depending on what you are trying to achieve. Essentially these classes are entry points to the reading / writing actions. They allow you to specify writing mode or provide additional options to read data source.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// This will return DataFrameReader to read data source
println(spark.read)

val df = spark.range(0, 10)

// This will return DataFrameWriter to save DataFrame
println(df.write)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Saving Parquet table in Scala
// DataFrames and tables can be saved as Parquet files, maintaining the schema information
val df_save = spark.table(&quot;social_media_usage&quot;).select(&quot;platform&quot;, &quot;visits&quot;) // assuming you made the social_media_usage table permanent in previous notebook
df_save.write.mode(&quot;overwrite&quot;).parquet(&quot;/tmp/platforms.parquet&quot;)

// Read in the parquet file created above
// Parquet files are self-describing so the schema is preserved
// The result of loading a Parquet file is also a DataFrame
val df = spark.read.parquet(&quot;/tmp/platforms.parquet&quot;)
df.show(5)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// in databricks '/tmp/...' is the same as 'dbfs:///tmp/...'
display(dbutils.fs.ls(&quot;/tmp/&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;/tmp/platforms.parquet/&quot;)) // note this is a directory with many files in it... files beginning with part have content in possibly many partitions
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># Loading Parquet table in Python
dfPy = spark.read.parquet(&quot;/tmp/platforms.parquet&quot;)
dfPy.show(5)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Saving JSON dataset in Scala
val df_save = spark.table(&quot;social_media_usage&quot;).select(&quot;platform&quot;, &quot;visits&quot;)
df_save.write.mode(&quot;overwrite&quot;).json(&quot;/tmp/platforms.json&quot;)

// Loading JSON dataset in Scala
val df = spark.read.json(&quot;/tmp/platforms.json&quot;)
df.show(5)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># Loading JSON dataset in Python
dfPy = spark.read.json(&quot;/tmp/platforms.json&quot;)
dfPy.show(5)
</code></pre>
</div>
<div class="cell markdown">
<h3 id="manually-specifying-options"><a class="header" href="#manually-specifying-options">Manually Specifying Options</a></h3>
<p>You can also manually specify the data source that will be used along with any extra options that you would like to pass to the data source. Data sources are specified by their fully qualified name (i.e., <code>org.apache.spark.sql.parquet</code>), but for built-in sources you can also use their short names (<code>json</code>, <code>parquet</code>, <code>jdbc</code>). DataFrames of any type can be converted into other types using this syntax.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val json = sqlContext.read.format(&quot;json&quot;).load(&quot;/tmp/platforms.json&quot;)
json.select(&quot;platform&quot;).show(10)

val parquet = sqlContext.read.format(&quot;parquet&quot;).load(&quot;/tmp/platforms.parquet&quot;)
parquet.select(&quot;platform&quot;).show(10)
</code></pre>
</div>
<div class="cell markdown">
<h3 id="run-sql-on-files-directly"><a class="header" href="#run-sql-on-files-directly">Run SQL on files directly</a></h3>
<p>Instead of using read API to load a file into DataFrame and query it, you can also query that file directly with SQL.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val df = sqlContext.sql(&quot;SELECT * FROM parquet.`/tmp/platforms.parquet`&quot;)
df.printSchema()
</code></pre>
</div>
<div class="cell markdown">
<h3 id="save-modes"><a class="header" href="#save-modes">Save Modes</a></h3>
<p>Save operations can optionally take a <code>SaveMode</code>, that specifies how to handle existing data if present. It is important to realize that these save modes do not utilize any locking and are not atomic. Additionally, when performing a <code>Overwrite</code>, the data will be deleted before writing out the new data.</p>
<p>| Scala/Java | Any language | Meaning | | --- | --- | --- | | <code>SaveMode.ErrorIfExists</code> (default) | <code>&quot;error&quot;</code> (default) | When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown. | | <code>SaveMode.Append</code> | <code>&quot;append&quot;</code> | When saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data. | | <code>SaveMode.Overwrite</code> | <code>&quot;overwrite&quot;</code> | Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame. | | <code>SaveMode.Ignore</code> | <code>&quot;ignore&quot;</code> | Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data. This is similar to a <code>CREATE TABLE IF NOT EXISTS</code> in SQL. |</p>
</div>
<div class="cell markdown">
<h3 id="saving-to-persistent-tables"><a class="header" href="#saving-to-persistent-tables">Saving to Persistent Tables</a></h3>
<p><code>DataFrame</code> and <code>Dataset</code> can also be saved as persistent tables using the <code>saveAsTable</code> command. Unlike the <code>createOrReplaceTempView</code> command, <code>saveAsTable</code> will materialize the contents of the dataframe and create a pointer to the data in the metastore. Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. A DataFrame for a persistent table can be created by calling the <code>table</code> method on a <code>SparkSession</code> with the name of the table.</p>
<p>By default <code>saveAsTable</code> will create a “managed table”, meaning that the location of the data will be controlled by the metastore. Managed tables will also have their data deleted automatically when a table is dropped.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// First of all list tables to see that table we are about to create does not exist
spark.catalog.listTables.show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">drop table if exists simple_range
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val df = spark.range(0, 100)
df.write.saveAsTable(&quot;simple_range&quot;)

// Verify that table is saved and it is marked as persistent (&quot;isTemporary&quot; value should be &quot;false&quot;)
spark.catalog.listTables.show()
</code></pre>
</div>
<div class="cell markdown">
<h2 id="parquet-files"><a class="header" href="#parquet-files">Parquet Files</a></h2>
<p><a href="http://parquet.io">Parquet</a> is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. When writing Parquet files, all columns are automatically converted to be nullable for compatibility reasons.</p>
</div>
<div class="cell markdown">
<h3 id="more-on-parquet"><a class="header" href="#more-on-parquet">More on Parquet</a></h3>
<p><a href="https://parquet.apache.org/">Apache Parquet</a> is a <a href="http://en.wikipedia.org/wiki/Column-oriented_DBMS">columnar storage</a> format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language. It is a more efficient way to store data frames.</p>
<ul>
<li>To understand the ideas read <a href="http://research.google.com/pubs/pub36632.html">Dremel: Interactive Analysis of Web-Scale Datasets, Sergey Melnik, Andrey Gubarev, Jing Jing Long, Geoffrey Romer, Shiva Shivakumar, Matt Tolton and Theo Vassilakis,Proc. of the 36th Int'l Conf on Very Large Data Bases (2010), pp. 330-339</a>, whose Abstract is as follows:
<ul>
<li>Dremel is a scalable, interactive ad-hoc query system for analysis of read-only nested data. By combining multi-level execution trees and columnar data layouts it is <strong>capable of running aggregation queries over trillion-row tables in seconds</strong>. The system <strong>scales to thousands of CPUs and petabytes of data, and has thousands of users at Google</strong>. In this paper, we describe the architecture and implementation of Dremel, and explain how it complements MapReduce-based computing. We present a novel columnar storage representation for nested records and discuss experiments on few-thousand node instances of the system.</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//This allows easy embedding of publicly available information into any other notebook
//when viewing in git-book just ignore this block - you may have to manually chase the URL in frameIt(&quot;URL&quot;).
//Example usage:
// displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;,250))
def frameIt( u:String, h:Int ) : String = {
      &quot;&quot;&quot;&lt;iframe 
 src=&quot;&quot;&quot;&quot;+ u+&quot;&quot;&quot;&quot;
 width=&quot;95%&quot; height=&quot;&quot;&quot;&quot; + h + &quot;&quot;&quot;&quot;
 sandbox&gt;
  &lt;p&gt;
    &lt;a href=&quot;http://spark.apache.org/docs/latest/index.html&quot;&gt;
      Fallback link for browsers that, unlikely, don't support frames
    &lt;/a&gt;
  &lt;/p&gt;
&lt;/iframe&gt;&quot;&quot;&quot;
   }
displayHTML(frameIt(&quot;https://parquet.apache.org/documentation/latest/&quot;,500))
</code></pre>
</div>
<div class="cell markdown">
<h3 id="loading-data-programmatically"><a class="header" href="#loading-data-programmatically">Loading Data Programmatically</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Read in the parquet file created above. Parquet files are self-describing so the schema is preserved.
// The result of loading a Parquet file is also a DataFrame.
val parquetFile = sqlContext.read.parquet(&quot;/tmp/platforms.parquet&quot;)

// Parquet files can also be registered as tables and then used in SQL statements.
parquetFile.createOrReplaceTempView(&quot;parquetFile&quot;)
val platforms = sqlContext.sql(&quot;SELECT platform FROM parquetFile WHERE visits &gt; 0&quot;)
platforms.distinct.map(t =&gt; &quot;Name: &quot; + t(0)).collect().foreach(println)
</code></pre>
</div>
<div class="cell markdown">
<h2 id="bucketing-sorting-and-partitioning"><a class="header" href="#bucketing-sorting-and-partitioning">Bucketing, Sorting and Partitioning</a></h2>
<p>For file-based data source, it is also possible to bucket and sort or partition the output. Bucketing and sorting are applicable only to persistent tables:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val social_media_usage_DF = spark.table(&quot;social_media_usage&quot;)
</code></pre>
</div>
<div class="cell markdown">
<p>Find full example code at - https://raw.githubusercontent.com/apache/spark/master/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala in the Spark repo.</p>
<p>Note that partitioning can be used with both save and saveAsTable when using the Dataset APIs.</p>
<p><code>partitionBy</code> creates a directory structure as described in the Partition Discovery section. Thus, it has limited applicability to columns with high cardinality. In contrast <code>bucketBy</code> distributes data across a fixed number of buckets and can be used when the number of unique values is unbounded. One can use <code>partitionBy</code> by itself or along with `bucketBy.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">social_media_usage_DF.write.mode(&quot;overwrite&quot;).parquet(&quot;/tmp/social_media_usage.parquet&quot;) // write to parquet
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;/tmp/social_media_usage.parquet&quot;)) // there is one part-00000 file inside the parquet folder
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val social_media_usage_readFromParquet_DF = spark.read.parquet(&quot;/tmp/social_media_usage.parquet&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">social_media_usage_readFromParquet_DF.count
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">social_media_usage_readFromParquet_DF.rdd.getNumPartitions
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">social_media_usage_readFromParquet_DF.printSchema
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">social_media_usage_readFromParquet_DF.select(&quot;platform&quot;).distinct.count
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">social_media_usage_readFromParquet_DF
  .write
  .partitionBy(&quot;platform&quot;)
  .mode(&quot;overwrite&quot;).parquet(&quot;/tmp/social_media_usage_partitionedByPlatform.parquet&quot;) 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;/tmp/social_media_usage_partitionedByPlatform.parquet&quot;)) // there are many platform=* folders inside the parquet folder
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;/tmp/social_media_usage_partitionedByPlatform.parquet/platform=Android&quot;)) // threre are part-00000- files with contents inside each platform=* folder in the parquet folder
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">spark.read.parquet(&quot;/tmp/social_media_usage_partitionedByPlatform.parquet&quot;).rdd.getNumPartitions
</code></pre>
</div>
<div class="cell markdown">
<p>We can also use a fixed number of buckets and sort by a column within each partition. Such finer control of the dataframe written as a parquet file can help with optimizing downstream operations on the dataframe.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">social_media_usage_readFromParquet_DF
  .write
  .partitionBy(&quot;platform&quot;)
  .bucketBy(10, &quot;date&quot;)
  .sortBy(&quot;date&quot;)
  .mode(&quot;overwrite&quot;)
  .saveAsTable(&quot;social_media_usage_table_partitionedByPlatformBucketedByDate&quot;) 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">spark.catalog.listTables.show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val df = spark.table(&quot;social_media_usage_table_partitionedByPlatformBucketedByDate&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">df.rdd.getNumPartitions
</code></pre>
</div>
<div class="cell markdown">
<h3 id="partition-discovery"><a class="header" href="#partition-discovery">Partition Discovery</a></h3>
<p>Table partitioning is a common optimization approach used in systems like Hive. In a partitioned table, data are usually stored in different directories, with partitioning column values encoded in the path of each partition directory. The Parquet data source is now able to discover and infer partitioning information automatically. For example, we can store all our previously used population data (from the programming guide example!) into a partitioned table using the following directory structure, with two extra columns, <code>gender</code> and <code>country</code> as partitioning columns: <code>path     └── to         └── table             ├── gender=male             │   ├── ...             │   │             │   ├── country=US             │   │   └── data.parquet             │   ├── country=CN             │   │   └── data.parquet             │   └── ...             └── gender=female                 ├── ...                 │                 ├── country=US                 │   └── data.parquet                 ├── country=CN                 │   └── data.parquet                 └── ...</code> By passing <code>path/to/table</code> to either <code>SparkSession.read.parquet</code> or <code>SparkSession.read.load</code>, Spark SQL will automatically extract the partitioning information from the paths. Now the schema of the returned DataFrame becomes: <code>root     |-- name: string (nullable = true)     |-- age: long (nullable = true)     |-- gender: string (nullable = true)     |-- country: string (nullable = true)</code> Notice that the data types of the partitioning columns are automatically inferred. Currently, numeric data types and string type are supported. Sometimes users may not want to automatically infer the data types of the partitioning columns. For these use cases, the automatic type inference can be configured by <code>spark.sql.sources.partitionColumnTypeInference.enabled</code>, which is default to <code>true</code>. When type inference is disabled, string type will be used for the partitioning columns.</p>
<p>Starting from Spark 1.6.0, partition discovery only finds partitions under the given paths by default. For the above example, if users pass <code>path/to/table/gender=male</code> to either <code>SparkSession.read.parquet</code> or <code>SparkSession.read.load</code>, <code>gender</code> will not be considered as a partitioning column. If users need to specify the base path that partition discovery should start with, they can set <code>basePath</code> in the data source options. For example, when <code>path/to/table/gender=male</code> is the path of the data and users set <code>basePath</code> to <code>path/to/table/</code>, <code>gender</code> will be a partitioning column.</p>
</div>
<div class="cell markdown">
<h3 id="schema-merging"><a class="header" href="#schema-merging">Schema Merging</a></h3>
<p>Like ProtocolBuffer, Avro, and Thrift, Parquet also supports schema evolution. Users can start with a simple schema, and gradually add more columns to the schema as needed. In this way, users may end up with multiple Parquet files with different but mutually compatible schemas. The Parquet data source is now able to automatically detect this case and merge schemas of all these files.</p>
<p>Since schema merging is a relatively expensive operation, and is not a necessity in most cases, we turned it off by default starting from 1.5.0. You may enable it by:</p>
<ol>
<li>setting data source option <code>mergeSchema</code> to <code>true</code> when reading Parquet files (as shown in the examples below), or</li>
<li>setting the global SQL option <code>spark.sql.parquet.mergeSchema</code> to <code>true</code>.</li>
</ol>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create a simple DataFrame, stored into a partition directory
val df1 = sc.parallelize(1 to 5).map(i =&gt; (i, i * 2)).toDF(&quot;single&quot;, &quot;double&quot;)
df1.write.mode(&quot;overwrite&quot;).parquet(&quot;/tmp/data/test_table/key=1&quot;)

// Create another DataFrame in a new partition directory, adding a new column and dropping an existing column
val df2 = sc.parallelize(6 to 10).map(i =&gt; (i, i * 3)).toDF(&quot;single&quot;, &quot;triple&quot;)
df2.write.mode(&quot;overwrite&quot;).parquet(&quot;/tmp/data/test_table/key=2&quot;)

// Read the partitioned table
val df3 = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;/tmp/data/test_table&quot;)
df3.printSchema()

// The final schema consists of all 3 columns in the Parquet files together
// with the partitioning column appeared in the partition directory paths.
// root
//  |-- single: integer (nullable = true)
//  |-- double: integer (nullable = true)
//  |-- triple: integer (nullable = true)
//  |-- key: integer (nullable = true))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">df3.show
</code></pre>
</div>
<div class="cell markdown">
<h3 id="hive-metastore-parquet-table-conversion"><a class="header" href="#hive-metastore-parquet-table-conversion">Hive metastore Parquet table conversion</a></h3>
<p>When reading from and writing to Hive metastore Parquet tables, Spark SQL will try to use its own Parquet support instead of Hive SerDe for better performance. This behavior is controlled by the <code>spark.sql.hive.convertMetastoreParquet</code> configuration, and is turned on by default.</p>
<h4 id="hiveparquet-schema-reconciliation"><a class="header" href="#hiveparquet-schema-reconciliation">Hive/Parquet Schema Reconciliation</a></h4>
<p>There are two key differences between Hive and Parquet from the perspective of table schema processing.</p>
<ol>
<li>Hive is case insensitive, while Parquet is not</li>
<li>Hive considers all columns nullable, while nullability in Parquet is significant</li>
</ol>
<p>Due to this reason, we must reconcile Hive metastore schema with Parquet schema when converting a Hive metastore Parquet table to a Spark SQL Parquet table. The reconciliation rules are:</p>
<ol>
<li>Fields that have the same name in both schema must have the same data type regardless of nullability. The reconciled field should have the data type of the Parquet side, so that nullability is respected.</li>
<li>The reconciled schema contains exactly those fields defined in Hive metastore schema.</li>
</ol>
<ul>
<li>Any fields that only appear in the Parquet schema are dropped in the reconciled schema.</li>
<li>Any fileds that only appear in the Hive metastore schema are added as nullable field in the reconciled schema.</li>
</ul>
<h4 id="metadata-refreshing"><a class="header" href="#metadata-refreshing">Metadata Refreshing</a></h4>
<p>Spark SQL caches Parquet metadata for better performance. When Hive metastore Parquet table conversion is enabled, metadata of those converted tables are also cached. If these tables are updated by Hive or other external tools, you need to refresh them manually to ensure consistent metadata.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// should refresh table metadata
spark.catalog.refreshTable(&quot;simple_range&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">-- Or you can use SQL to refresh table
REFRESH TABLE simple_range;
</code></pre>
</div>
<div class="cell markdown">
<h3 id="configuration"><a class="header" href="#configuration">Configuration</a></h3>
<p>Configuration of Parquet can be done using the <code>setConf</code> method on <code>SQLContext</code> or by running <code>SET key=value</code> commands using SQL.</p>
<p>| Property Name | Default | Meaning | | --- | --- | --- | --- | | <code>spark.sql.parquet.binaryAsString</code> | false | Some other Parquet-producing systems, in particular Impala, Hive, and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems. | | <code>spark.sql.parquet.int96AsTimestamp</code> | true | Some Parquet-producing systems, in particular Impala and Hive, store Timestamp into INT96. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems. | | <code>spark.sql.parquet.cacheMetadata</code> | true | Turns on caching of Parquet schema metadata. Can speed up querying of static data. | | <code>spark.sql.parquet.compression.codec</code> | gzip | Sets the compression codec use when writing Parquet files. Acceptable values include: uncompressed, snappy, gzip, lzo. | | <code>spark.sql.parquet.filterPushdown</code> | true | Enables Parquet filter push-down optimization when set to true. | | <code>spark.sql.hive.convertMetastoreParquet</code> | true | When set to false, Spark SQL will use the Hive SerDe for parquet tables instead of the built in support. | | <code>spark.sql.parquet.output.committer.class</code> | <code>org.apache.parquet.hadoop.ParquetOutputCommitter</code> | The output committer class used by Parquet. The specified class needs to be a subclass of <code>org.apache.hadoop.mapreduce.OutputCommitter</code>. Typically, it's also a subclass of <code>org.apache.parquet.hadoop.ParquetOutputCommitter</code>. Spark SQL comes with a builtin <code>org.apache.spark.sql.parquet.DirectParquetOutputCommitter</code>, which can be more efficient then the default Parquet output committer when writing data to S3. | | <code>spark.sql.parquet.mergeSchema</code> | <code>false</code> | When true, the Parquet data source merges schemas collected from all data files, otherwise the schema is picked from the summary file or a random data file if no summary file is available. |</p>
</div>
<div class="cell markdown">
<h2 id="json-datasets"><a class="header" href="#json-datasets">JSON Datasets</a></h2>
<p>Spark SQL can automatically infer the schema of a JSON dataset and load it as a DataFrame. This conversion can be done using <code>SparkSession.read.json()</code> on either an RDD of String, or a JSON file.</p>
<p>Note that the file that is offered as <em>a json file</em> is not a typical JSON file. Each line must contain a separate, self-contained valid JSON object. As a consequence, a regular multi-line JSON file will most often fail.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// A JSON dataset is pointed to by path.
// The path can be either a single text file or a directory storing text files.
val path = &quot;/tmp/platforms.json&quot;
val platforms = spark.read.json(path)

// The inferred schema can be visualized using the printSchema() method.
platforms.printSchema()
// root
//  |-- platform: string (nullable = true)
//  |-- visits: long (nullable = true)

// Register this DataFrame as a table.
platforms.createOrReplaceTempView(&quot;platforms&quot;)

// SQL statements can be run by using the sql methods provided by sqlContext.
val facebook = spark.sql(&quot;SELECT platform, visits FROM platforms WHERE platform like 'Face%k'&quot;)
facebook.show()

// Alternatively, a DataFrame can be created for a JSON dataset represented by
// an RDD[String] storing one JSON object per string.
val rdd = sc.parallelize(&quot;&quot;&quot;{&quot;name&quot;:&quot;IWyn&quot;,&quot;address&quot;:{&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;}}&quot;&quot;&quot; :: Nil)
val anotherPlatforms = spark.read.json(rdd)
anotherPlatforms.show()
</code></pre>
</div>
<div class="cell markdown">
<h2 id="hive-tables"><a class="header" href="#hive-tables">Hive Tables</a></h2>
<p>Spark SQL also supports reading and writing data stored in <a href="http://hive.apache.org/">Apache Hive</a>. However, since Hive has a large number of dependencies, it is not included in the default Spark assembly. Hive support is enabled by adding the <code>-Phive</code> and <code>-Phive-thriftserver</code> flags to Spark’s build. This command builds a new assembly jar that includes Hive. Note that this Hive assembly jar must also be present on all of the worker nodes, as they will need access to the Hive serialization and deserialization libraries (SerDes) in order to access data stored in Hive.</p>
<p>Configuration of Hive is done by placing your <code>hive-site.xml</code>, <code>core-site.xml</code> (for security configuration), <code>hdfs-site.xml</code> (for HDFS configuration) file in <code>conf/</code>. Please note when running the query on a YARN cluster (<code>cluster</code> mode), the <code>datanucleus</code> jars under the <code>lib_managed/jars</code> directory and <code>hive-site.xml</code> under <code>conf/</code> directory need to be available on the driver and all executors launched by the YARN cluster. The convenient way to do this is adding them through the <code>--jars</code> option and <code>--file</code> option of the <code>spark-submit</code> command.</p>
<p>When working with Hive one must construct a <code>HiveContext</code>, which inherits from <code>SQLContext</code>, and adds support for finding tables in the MetaStore and writing queries using HiveQL. Users who do not have an existing Hive deployment can still create a <code>HiveContext</code>. When not configured by the hive-site.xml, the context automatically creates <code>metastore_db</code> in the current directory and creates <code>warehouse</code> directory indicated by HiveConf, which defaults to <code>/user/hive/warehouse</code>. Note that you may need to grant write privilege on <code>/user/hive/warehouse</code> to the user who starts the spark application.</p>
<p>```scala val spark = SparkSession.builder.enableHiveSupport().getOrCreate()</p>
<p>spark.sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;) spark.sql(&quot;LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src&quot;)</p>
<p>// Queries are expressed in HiveQL spark.sql(&quot;FROM src SELECT key, value&quot;).collect().foreach(println) ```</p>
<h3 id="interacting-with-different-versions-of-hive-metastore"><a class="header" href="#interacting-with-different-versions-of-hive-metastore">Interacting with Different Versions of Hive Metastore</a></h3>
<p>One of the most important pieces of Spark SQL’s Hive support is interaction with Hive metastore, which enables Spark SQL to access metadata of Hive tables. Starting from Spark 1.4.0, a single binary build of Spark SQL can be used to query different versions of Hive metastores, using the configuration described below. Note that independent of the version of Hive that is being used to talk to the metastore, internally Spark SQL will compile against Hive 1.2.1 and use those classes for internal execution (serdes, UDFs, UDAFs, etc).</p>
<p>The following options can be used to configure the version of Hive that is used to retrieve metadata:</p>
<p>| Property Name | Default | Meaning | | --- | --- | --- | | <code>spark.sql.hive.metastore.version</code> | <code>1.2.1</code> | Version of the Hive metastore. Available options are <code>0.12.0</code> through <code>1.2.1</code>. | | <code>spark.sql.hive.metastore.jars</code> | <code>builtin</code> | Location of the jars that should be used to instantiate the HiveMetastoreClient. This property can be one of three options: <code>builtin</code>, <code>maven</code>, a classpath in the standard format for the JVM. This classpath must include all of Hive and its dependencies, including the correct version of Hadoop. These jars only need to be present on the driver, but if you are running in yarn cluster mode then you must ensure they are packaged with you application. | | <code>spark.sql.hive.metastore.sharedPrefixes</code> | <code>com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc</code> | A comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log4j. | | <code>spark.sql.hive.metastore.barrierPrefixes</code> | <code>(empty)</code> | A comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e. <code>org.apache.spark.*</code>). |</p>
</div>
<div class="cell markdown">
<h2 id="jdbc-to-other-databases"><a class="header" href="#jdbc-to-other-databases">JDBC To Other Databases</a></h2>
<p>Spark SQL also includes a data source that can read data from other databases using JDBC. This functionality should be preferred over using <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.JdbcRDD">JdbcRDD</a>. This is because the results are returned as a DataFrame and they can easily be processed in Spark SQL or joined with other data sources. The JDBC data source is also easier to use from Java or Python as it does not require the user to provide a ClassTag. (Note that this is different than the Spark SQL JDBC server, which allows other applications to run queries using Spark SQL).</p>
<p>To get started you will need to include the JDBC driver for you particular database on the spark classpath. For example, to connect to postgres from the Spark Shell you would run the following command:</p>
<p><code>SPARK_CLASSPATH=postgresql-9.3-1102-jdbc41.jar bin/spark-shell</code></p>
<p>Tables from the remote database can be loaded as a DataFrame or Spark SQL Temporary table using the Data Sources API. The following options are supported:</p>
<p>| Property Name | Meaning | | --- | --- | --- | | <code>url</code> | The JDBC URL to connect to. | | <code>dbtable</code> | The JDBC table that should be read. Note that anything that is valid in a <code>FROM</code> clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses. | | <code>driver</code> | The class name of the JDBC driver needed to connect to this URL. This class will be loaded on the master and workers before running an JDBC commands to allow the driver to register itself with the JDBC subsystem. | | <code>partitionColumn, lowerBound, upperBound, numPartitions</code> | These options must all be specified if any of them is specified. They describe how to partition the table when reading in parallel from multiple workers. <code>partitionColumn</code> must be a numeric column from the table in question. Notice that <code>lowerBound</code> and <code>upperBound</code> are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned. | | <code>fetchSize</code> | The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows). |</p>
<p><code>// Example of using JDBC datasource val jdbcDF = spark.read.format(&quot;jdbc&quot;).options(Map(&quot;url&quot; -&gt; &quot;jdbc:postgresql:dbserver&quot;, &quot;dbtable&quot; -&gt; &quot;schema.tablename&quot;)).load()</code></p>
<p><code>-- Or using JDBC datasource in SQL CREATE TEMPORARY TABLE jdbcTable USING org.apache.spark.sql.jdbc OPTIONS (   url &quot;jdbc:postgresql:dbserver&quot;,   dbtable &quot;schema.tablename&quot; )</code></p>
</div>
<div class="cell markdown">
<h3 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h3>
<ul>
<li>The JDBC driver class must be visible to the primordial class loader on the client session and on all executors. This is because Java’s DriverManager class does a security check that results in it ignoring all drivers not visible to the primordial class loader when one goes to open a connection. One convenient way to do this is to modify compute_classpath.sh on all worker nodes to include your driver JARs.</li>
<li>Some databases, such as H2, convert all names to upper case. You’ll need to use upper case to refer to those names in Spark SQL.</li>
</ul>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="performance-tuning"><a class="header" href="#performance-tuning">Performance Tuning</a></h1>
<h2 id="spark-sql-programming-guide-3"><a class="header" href="#spark-sql-programming-guide-3">Spark Sql Programming Guide</a></h2>
<p>If you have read the spark-SQL paper and have some idea of how distributed sorting and joining work then you will need to know the following part of the programming guide to tune the performance of Spark SQL queries:</p>
<ul>
<li>https://spark.apache.org/docs/latest/sql-performance-tuning.html</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//This allows easy embedding of publicly available information into any other notebook
//Example usage:
// displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;,250))
def frameIt( u:String, h:Int ) : String = {
      &quot;&quot;&quot;&lt;iframe 
 src=&quot;&quot;&quot;&quot;+ u+&quot;&quot;&quot;&quot;
 width=&quot;95%&quot; height=&quot;&quot;&quot;&quot; + h + &quot;&quot;&quot;&quot;&gt;
  &lt;p&gt;
    &lt;a href=&quot;http://spark.apache.org/docs/latest/index.html&quot;&gt;
      Fallback link for browsers that, unlikely, don't support frames
    &lt;/a&gt;
  &lt;/p&gt;
&lt;/iframe&gt;&quot;&quot;&quot;
   }
displayHTML(frameIt(&quot;https://spark.apache.org/docs/latest/sql-performance-tuning.html&quot;,700))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">http://spark.apache.org/docs/latest/sql-programming-guide.html</a> by Ivan Sadikov and Raazesh Sainudiin.</p>
<h1 id="distributed-sql-engine"><a class="header" href="#distributed-sql-engine">Distributed SQL Engine</a></h1>
<h2 id="spark-sql-programming-guide-4"><a class="header" href="#spark-sql-programming-guide-4">Spark Sql Programming Guide</a></h2>
<ul>
<li>Distributed SQL Engine
<ul>
<li>Running the Thrift JDBC/ODBC server</li>
<li>Running the Spark SQL CLI</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h1 id="distributed-sql-engine-1"><a class="header" href="#distributed-sql-engine-1">Distributed SQL Engine</a></h1>
<p>Spark SQL can also act as a distributed query engine using its JDBC/ODBC or command-line interface. In this mode, end-users or applications can interact with Spark SQL directly to run SQL queries, without the need to write any code.</p>
<h2 id="running-the-thrift-jdbcodbc-server"><a class="header" href="#running-the-thrift-jdbcodbc-server">Running the Thrift JDBC/ODBC server</a></h2>
<p>The Thrift JDBC/ODBC server implemented here corresponds to the <a href="https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2"><code>HiveServer2</code></a> in Hive 1.2.1 You can test the JDBC server with the beeline script that comes with either Spark or Hive 1.2.1.</p>
<p>To start the JDBC/ODBC server, run the following in the Spark directory:</p>
<pre><code>./sbin/start-thriftserver.sh
</code></pre>
<p>This script accepts all <code>bin/spark-submit</code> command line options, plus a <code>--hiveconf</code> option to specify Hive properties. You may run <code>./sbin/start-thriftserver.sh --help</code> for a complete list of all available options. By default, the server listens on localhost:10000. You may override this behaviour via either environment variables, i.e.:</p>
<pre><code>export HIVE_SERVER2_THRIFT_PORT=&lt;listening-port&gt;
export HIVE_SERVER2_THRIFT_BIND_HOST=&lt;listening-host&gt;
./sbin/start-thriftserver.sh \
  --master &lt;master-uri&gt; \
  ...
</code></pre>
<p>or system properties:</p>
<pre><code>./sbin/start-thriftserver.sh \
  --hiveconf hive.server2.thrift.port=&lt;listening-port&gt; \
  --hiveconf hive.server2.thrift.bind.host=&lt;listening-host&gt; \
  --master &lt;master-uri&gt;
  ...
</code></pre>
<p>Now you can use beeline to test the Thrift JDBC/ODBC server:</p>
<pre><code>./bin/beeline
</code></pre>
<p>Connect to the JDBC/ODBC server in beeline with:</p>
<pre><code>beeline&gt; !connect jdbc:hive2://localhost:10000
</code></pre>
<p>Beeline will ask you for a username and password. In non-secure mode, simply enter the username on your machine and a blank password. For secure mode, please follow the instructions given in the <a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients">beeline documentation</a>.</p>
<p>Configuration of Hive is done by placing your <code>hive-site.xml</code>, <code>core-site.xml</code> and <code>hdfs-site.xml</code> files in <code>conf/</code>.</p>
<p>You may also use the beeline script that comes with Hive.</p>
<p>Thrift JDBC server also supports sending thrift RPC messages over HTTP transport. Use the following setting to enable HTTP mode as system property or in <code>hive-site.xml</code> file in <code>conf/</code>:</p>
<pre><code>hive.server2.transport.mode - Set this to value: http
hive.server2.thrift.http.port - HTTP port number fo listen on; default is 10001
hive.server2.http.endpoint - HTTP endpoint; default is cliservice
</code></pre>
<p>To test, use beeline to connect to the JDBC/ODBC server in http mode with:</p>
<pre><code>beeline&gt; !connect jdbc:hive2://&lt;host&gt;:&lt;port&gt;/&lt;database&gt;?hive.server2.transport.mode=http;hive.server2.thrift.http.path=&lt;http_endpoint&gt;
</code></pre>
<h2 id="running-the-spark-sql-cli"><a class="header" href="#running-the-spark-sql-cli">Running the Spark SQL CLI</a></h2>
<p>The Spark SQL CLI is a convenient tool to run the Hive metastore service in local mode and execute queries input from the command line. Note that the Spark SQL CLI cannot talk to the Thrift JDBC server.</p>
<p>To start the Spark SQL CLI, run the following in the Spark directory:</p>
<pre><code>./bin/spark-sql
</code></pre>
<p>Configuration of Hive is done by placing your <code>hive-site.xml</code>, <code>core-site.xml</code> and <code>hdfs-site.xml</code> files in <code>conf/</code>. You may run <code>./bin/spark-sql --help</code> for a complete list of all available options.</p>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="sql-pivoting-since-spark-24"><a class="header" href="#sql-pivoting-since-spark-24">SQL Pivoting since Spark 2.4</a></h1>
<h2 id="sql-pivot-converting-rows-to-columns"><a class="header" href="#sql-pivot-converting-rows-to-columns">SQL Pivot: Converting Rows to Columns</a></h2>
<p>This is from the following blogpost: - https://databricks.com/blog/2018/11/01/sql-pivot-converting-rows-to-columns.html</p>
<p>This is a useful trick to know when having to do ETL before exploring datasets that need row to column conversions.</p>
</div>
<div class="cell markdown">
<h1 id="load-data-1"><a class="header" href="#load-data-1">Load Data</a></h1>
<p>Create tables and load temperature data</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">CREATE OR REPLACE TEMPORARY VIEW high_temps
  USING csv
  OPTIONS (path &quot;/databricks-datasets/weather/high_temps&quot;, header &quot;true&quot;, mode &quot;FAILFAST&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">CREATE OR REPLACE TEMPORARY VIEW low_temps
  USING csv
  OPTIONS (path &quot;/databricks-datasets/weather/low_temps&quot;, header &quot;true&quot;, mode &quot;FAILFAST&quot;)
</code></pre>
</div>
<div class="cell markdown">
<h1 id="pivoting-in-sql"><a class="header" href="#pivoting-in-sql">Pivoting in SQL</a></h1>
<p>Getting the monthly average high temperatures with month as columns and year as rows.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">SELECT * FROM (
  SELECT year(date) year, month(date) month, temp
  FROM high_temps
  WHERE date BETWEEN DATE '2015-01-01' AND DATE '2018-08-31'
)
PIVOT (
  CAST(avg(temp) AS DECIMAL(4, 1))
  FOR month in (
    1 JAN, 2 FEB, 3 MAR, 4 APR, 5 MAY, 6 JUN,
    7 JUL, 8 AUG, 9 SEP, 10 OCT, 11 NOV, 12 DEC
  )
)
ORDER BY year DESC
</code></pre>
</div>
<div class="cell markdown">
<h1 id="pivoting-with-multiple-aggregate-expressions"><a class="header" href="#pivoting-with-multiple-aggregate-expressions">Pivoting with Multiple Aggregate Expressions</a></h1>
<p>Getting monthly average and maximum high temperatures with month as columns and year as rows.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">SELECT * FROM (
  SELECT year(date) year, month(date) month, temp
  FROM high_temps
  WHERE date BETWEEN DATE '2015-01-01' AND DATE '2018-08-31'
)
PIVOT (
  CAST(avg(temp) AS DECIMAL(4, 1)) avg, max(temp) max
  FOR month in (6 JUN, 7 JUL, 8 AUG, 9 SEP)
)
ORDER BY year DESC
</code></pre>
</div>
<div class="cell markdown">
<h1 id="pivoting-with-multiple-grouping-columns"><a class="header" href="#pivoting-with-multiple-grouping-columns">Pivoting with Multiple Grouping Columns</a></h1>
<p>Getting monthly average high and average low temperatures with month as columns and (year, hi/lo) as rows.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">SELECT * FROM (
  SELECT year(date) year, month(date) month, temp, flag `H/L`
  FROM (
    SELECT date, temp, 'H' as flag
    FROM high_temps
    UNION ALL
    SELECT date, temp, 'L' as flag
    FROM low_temps
  )
  WHERE date BETWEEN DATE '2015-01-01' AND DATE '2018-08-31'
)
PIVOT (
  CAST(avg(temp) AS DECIMAL(4, 1))
  FOR month in (6 JUN, 7 JUL, 8 AUG, 9 SEP)
)
ORDER BY year DESC, `H/L` ASC
</code></pre>
</div>
<div class="cell markdown">
<h1 id="pivoting-with-multiple-pivot-columns"><a class="header" href="#pivoting-with-multiple-pivot-columns">Pivoting with Multiple Pivot Columns</a></h1>
<p>Getting monthly average high and average low temperatures with (month, hi/lo) as columns and year as rows.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">SELECT * FROM (
  SELECT year(date) year, month(date) month, temp, flag
  FROM (
    SELECT date, temp, 'H' as flag
    FROM high_temps
    UNION ALL
    SELECT date, temp, 'L' as flag
    FROM low_temps
  )
  WHERE date BETWEEN DATE '2015-01-01' AND DATE '2018-08-31'
)
PIVOT (
  CAST(avg(temp) AS DECIMAL(4, 1))
  FOR (month, flag) in (
    (6, 'H') JUN_hi, (6, 'L') JUN_lo,
    (7, 'H') JUL_hi, (7, 'L') JUL_lo,
    (8, 'H') AUG_hi, (8, 'L') AUG_lo,
    (9, 'H') SEP_hi, (9, 'L') SEP_lo
  )
)
ORDER BY year DESC
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="introduction-to-spark-sql"><a class="header" href="#introduction-to-spark-sql">Introduction to Spark SQL</a></h1>
<ul>
<li>This notebook explains the motivation behind Spark SQL</li>
<li>It introduces interactive SparkSQL queries and visualizations</li>
<li>This notebook uses content from Databricks SparkSQL notebook and <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">SparkSQL programming guide</a></li>
</ul>
</div>
<div class="cell markdown">
<p><strong>Some resources on SQL</strong></p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/SQL">https://en.wikipedia.org/wiki/SQL</a></li>
<li><a href="https://en.wikipedia.org/wiki/Apache_Hive">https://en.wikipedia.org/wiki/Apache_Hive</a></li>
<li><a href="http://www.infoq.com/articles/apache-spark-sql">http://www.infoq.com/articles/apache-spark-sql</a></li>
<li><a href="https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html">https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html</a></li>
<li><a href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html">https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html</a></li>
<li><strong>READ</strong>: <a href="https://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf">https://people.csail.mit.edu/matei/papers/2015/sigmod<em>spark</em>sql.pdf</a></li>
</ul>
<p>Some of them are embedded below in-place for your convenience.</p>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://en.wikipedia.org/wiki/SQL"
 width="95%" height="500"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://en.wikipedia.org/wiki/Apache_Hive#HiveQL"
 width="95%" height="175"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<p>READ - <a href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html">https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html</a></p>
</div>
<div class="cell markdown">
<p>This is an elaboration of the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Apache Spark 2.2 sql-progamming-guide</a>.</p>
<h2 id="overview-2"><a class="header" href="#overview-2">Overview</a></h2>
<p>Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations. There are several ways to interact with Spark SQL including SQL and the Dataset API. When computing a result the same execution engine is used, independent of which API/language you are using to express the computation. This unification means that developers can easily switch back and forth between different APIs based on which provides the most natural way to express a given transformation.</p>
<p>All of the examples on this page use sample data included in the Spark distribution and can be run in the spark-shell, pyspark shell, or sparkR shell.</p>
<h2 id="datasets-and-dataframes-1"><a class="header" href="#datasets-and-dataframes-1">Datasets and DataFrames</a></h2>
<p>A Dataset is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#creating-datasets">constructed</a> from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). The Dataset API is available in <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset">Scala</a> and <a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html">Java</a>. Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally <code>row.columnName</code>). The case for R is similar.</p>
<p>A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources">sources</a> such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame">Python</a>, and <a href="http://spark.apache.org/docs/latest/api/R/index.html">R</a>. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset">Scala API</a>, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use <code>Dataset&lt;Row&gt;</code> to represent a DataFrame.</p>
<p>Throughout this document, we will often refer to Scala/Java Datasets of <code>Rows</code> as DataFrames.</p>
</div>
<div class="cell markdown">
<h1 id="getting-started-in-spark-2x"><a class="header" href="#getting-started-in-spark-2x">Getting Started in Spark 2.x</a></h1>
<h2 id="starting-point-sparksession-1"><a class="header" href="#starting-point-sparksession-1">Starting Point: SparkSession</a></h2>
<p>The entry point into all functionality in Spark is the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession">SparkSession</a>. To create a basic SparkSession in your scala Spark code, just use <code>SparkSession.builder()</code>:</p>
<p>``` import org.apache.spark.sql.SparkSession</p>
<p>val spark = SparkSession .builder() .appName(&quot;Spark SQL basic example&quot;) .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) .getOrCreate()</p>
<p>// For implicit conversions like converting RDDs to DataFrames import spark.implicits._ ```</p>
<p>Conveniently, in Databricks notebook (similar to <code>spark-shell</code>) <code>SparkSession</code> is already created for you and is available as <code>spark</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">spark // ready-made Spark-Session
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res2: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@393e6c25
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="creating-dataframes-1"><a class="header" href="#creating-dataframes-1">Creating DataFrames</a></h2>
<p>With a <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession"><code>SparkSession</code></a> or <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext"><code>SQLContext</code></a>, applications can create <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame"><code>DataFrame</code></a></p>
<ul>
<li>from an existing <code>RDD</code>,</li>
<li>from a Hive table, or</li>
<li>from various other data sources.</li>
</ul>
<h3 id="just-to-recap"><a class="header" href="#just-to-recap">Just to recap:</a></h3>
<ul>
<li>A DataFrame is a distributed collection of data organized into named columns (it is not strogly typed).</li>
<li>You can think of it as being organized into table RDD of case class <code>Row</code> (which is not exactly true).</li>
<li>DataFrames, in comparison to RDDs, are backed by rich optimizations, including:
<ul>
<li>tracking their own schema,</li>
<li>adaptive query execution,</li>
<li>code generation including whole stage codegen,</li>
<li>extensible Catalyst optimizer, and</li>
<li>project <a href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html">Tungsten</a> for optimized storage.</li>
</ul>
</li>
</ul>
<blockquote>
<p>Note that performance for DataFrames is the same across languages Scala, Java, Python, and R. This is due to the fact that the only planning phase is language-specific (logical + physical SQL plan), not the actual execution of the SQL plan.</p>
</blockquote>
</div>
<div class="cell markdown">
<h2 id="dataframe-basics"><a class="header" href="#dataframe-basics">DataFrame Basics</a></h2>
<h4 id="1-an-empty-dataframe"><a class="header" href="#1-an-empty-dataframe">1. An empty DataFrame</a></h4>
<h4 id="2-dataframe-from-a-range"><a class="header" href="#2-dataframe-from-a-range">2. DataFrame from a range</a></h4>
<h4 id="3-dataframe-from-an-rdd"><a class="header" href="#3-dataframe-from-an-rdd">3. DataFrame from an RDD</a></h4>
<h4 id="4-dataframe-operations-aka-untyped-dataset-operations"><a class="header" href="#4-dataframe-operations-aka-untyped-dataset-operations">4. DataFrame Operations (aka Untyped Dataset Operations)</a></h4>
<h4 id="5-running-sql-queries-programmatically"><a class="header" href="#5-running-sql-queries-programmatically">5. Running SQL Queries Programmatically</a></h4>
<h4 id="6-creating-datasets"><a class="header" href="#6-creating-datasets">6. Creating Datasets</a></h4>
</div>
<div class="cell markdown">
<h3 id="1-making-an-empty-dataframe"><a class="header" href="#1-making-an-empty-dataframe">1. Making an empty DataFrame</a></h3>
<p>Spark has some of the pre-built methods to create simple DataFrames</p>
<ul>
<li>let us make an Empty DataFrame</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val emptyDF = spark.emptyDataFrame // Ctrl+Enter to make an empty DataFrame
</code></pre>
</div>
<div class="cell markdown">
<p>Not really interesting, or is it?</p>
<p><strong>You Try!</strong></p>
<p>Uncomment the following cell, put your cursor after <code>emptyDF.</code> below and hit Tab to see what can be done with <code>emptyDF</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//emptyDF.
</code></pre>
</div>
<div class="cell markdown">
<h3 id="2-making-a-dataframe-from-a-range"><a class="header" href="#2-making-a-dataframe-from-a-range">2. Making a DataFrame from a range</a></h3>
<p>Let us make a DataFrame next</p>
<ul>
<li>from a range of numbers, as follows:</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rangeDF = spark.range(0, 3).toDF() // Ctrl+Enter to make DataFrame with 0,1,2
// sc.parallelize(1 to 3).toDF() 
</code></pre>
</div>
<div class="cell markdown">
<p>Note that Spark automatically names column as <code>id</code> and casts integers to type <code>bigint</code> for big integer or Long.</p>
<p>In order to get a preview of data in DataFrame use <code>show()</code> as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rangeDF.show() // Ctrl+Enter
</code></pre>
</div>
<div class="cell markdown">
<h3 id="3-making-a-dataframe-from-an-rdd"><a class="header" href="#3-making-a-dataframe-from-an-rdd">3. Making a DataFrame from an RDD</a></h3>
<ul>
<li>Make an RDD</li>
<li>Conver the RDD into a DataFrame using the defualt <code>.toDF()</code> method</li>
<li>Conver the RDD into a DataFrame using the non-default <code>.toDF(...)</code> method</li>
<li>Do it all in one line</li>
</ul>
</div>
<div class="cell markdown">
<p>Let's first make an RDD using the <code>sc.parallelize</code> method, transform it by a <code>map</code> and perform the <code>collect</code> action to display it, as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rdd1 = sc.parallelize(1 to 5).map(i =&gt; (i, i*2))
rdd1.collect() // Ctrl+Enter
</code></pre>
</div>
<div class="cell markdown">
<p>Next, let us convert the RDD into DataFrame using the <code>.toDF()</code> method, as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val df1 = rdd1.toDF() // Ctrl+Enter 
</code></pre>
</div>
<div class="cell markdown">
<p>As it is clear, the DataFrame has columns named <code>_1</code> and <code>_2</code>, each of type <code>int</code>. Let us see its content using the <code>.show()</code> method next.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">df1.show() // Ctrl+Enter
</code></pre>
</div>
<div class="cell markdown">
<p>Note that by default, i.e. without specifying any options as in <code>toDF()</code>, the column names are given by <code>_1</code> and <code>_2</code>.</p>
<p>We can easily specify column names as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val df1 = rdd1.toDF(&quot;once&quot;, &quot;twice&quot;) // Ctrl+Enter
df1.show()
</code></pre>
</div>
<div class="cell markdown">
<p>Of course, we can do all of the above steps to make the DataFrame <code>df1</code> in one line and then show it, as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val df1 = sc.parallelize(1 to 5)
            .map(i =&gt; (i, i*2))
            .toDF(&quot;once&quot;, &quot;twice&quot;) //Ctrl+enter
df1.show()
</code></pre>
</div>
<div class="cell markdown">
<h3 id="4-dataframe-operations-aka-untyped-dataset-operations-1"><a class="header" href="#4-dataframe-operations-aka-untyped-dataset-operations-1">4. DataFrame Operations (aka Untyped Dataset Operations)</a></h3>
<p>DataFrames provide a domain-specific language for structured data manipulation in <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset">Scala</a>, <a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html">Java</a>, <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame">Python</a> and <a href="http://spark.apache.org/docs/latest/api/R/SparkDataFrame.html">R</a>.</p>
<p>As mentioned above, in Spark 2.0, DataFrames are just Dataset of Rows in Scala and Java API. These operations are also referred as “untyped transformations” in contrast to “typed transformations” come with strongly typed Scala/Java Datasets.</p>
<p>Here we include some basic examples of structured data processing using Datasets:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// This import is needed to use the $-notation
import spark.implicits._
// Print the schema in a tree format
df1.printSchema()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Select only the &quot;name&quot; column
df1.select(&quot;once&quot;).show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Select both columns, but increment the double column by 1
df1.select($&quot;once&quot;, $&quot;once&quot; + 1).show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Select both columns, but increment the double column by 1 and rename it as &quot;oncemore&quot;
df1.select($&quot;once&quot;, ($&quot;once&quot; * 1).as(&quot;oncemore&quot;)).show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">df1.filter($&quot;once&quot; &gt; 2).show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Count the number of distinct singles -  a bit boring
df1.groupBy(&quot;once&quot;).count().show()
</code></pre>
</div>
<div class="cell markdown">
<p>Let's make a more interesting DataFrame for <code>groupBy</code> with repeated elements so that the <code>count</code> will be more than <code>1</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">df1.show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val df11 = sc.parallelize(3 to 5).map(i =&gt; (i, i*2)).toDF(&quot;once&quot;, &quot;twice&quot;) // just make a small one
df11.show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val df111 = df1.union(df11) // let's take the unionAll of df1 and df11 into df111
df111.show() // df111 is obtained by simply appending the rows of df11 to df1
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Count the number of distinct singles -  a bit less boring
df111.groupBy(&quot;once&quot;).count().show()
</code></pre>
</div>
<div class="cell markdown">
<p>For a complete list of the types of operations that can be performed on a Dataset refer to the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset">API Documentation</a>.</p>
<p>In addition to simple column references and expressions, Datasets also have a rich library of functions including string manipulation, date arithmetic, common math operations and more. The complete list is available in the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$">DataFrame Function Reference</a>.</p>
</div>
<div class="cell markdown">
<p><strong>You Try!</strong></p>
<p>Uncomment the two lines in the next cell, and then fill in the <code>???</code> below to get a DataFrame <code>df2</code> whose first two columns are the same as <code>df1</code> and whose third column named triple has values that are three times the values in the first column.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//val df2 = sc.parallelize(1 to 5).map(i =&gt; (i, i*2, i????)).toDF(&quot;single&quot;, &quot;double&quot;, &quot;triple&quot;) // Ctrl+enter after editing ???
//df2.show()
</code></pre>
</div>
<div class="cell markdown">
<h3 id="5-running-sql-queries-programmatically-1"><a class="header" href="#5-running-sql-queries-programmatically-1">5. Running SQL Queries Programmatically</a></h3>
<p>The <code>sql</code> function on a <code>SparkSession</code> enables applications to run SQL queries programmatically and returns the result as a <code>DataFrame</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">df1
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Register the DataFrame as a SQL temporary view
df1.createOrReplaceTempView(&quot;sdtable&quot;)

val sqlDF = spark.sql(&quot;SELECT * FROM sdtable&quot;)
sqlDF.show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">spark.sql(&quot;SELECT * FROM SDTable WHERE once&gt;2&quot;).show()
</code></pre>
</div>
<div class="cell markdown">
<h3 id="5-using-sql-for-interactively-querying-a-table-is-very-powerful"><a class="header" href="#5-using-sql-for-interactively-querying-a-table-is-very-powerful">5. Using SQL for interactively querying a table is very powerful!</a></h3>
<p>Note <code>-- comments</code> are how you add <code>comments</code> in SQL cells beginning with <code>%sql</code>.</p>
<ul>
<li>You can run SQL <code>select *</code> statement to see all columns of the table, as follows:
<ul>
<li>This is equivalent to the above `display(diamondsDF)' with the DataFrame</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">-- Ctrl+Enter to select all columns of the table
select * from SDTable
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">-- Ctrl+Enter to select all columns of the table
-- note table names of registered tables are case-insensitive
select * from sdtable
</code></pre>
</div>
<div class="cell markdown">
<h4 id="global-temporary-view-1"><a class="header" href="#global-temporary-view-1">Global Temporary View</a></h4>
<p>Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it terminates. If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a global temporary view. Global temporary view is tied to a system preserved database <code>global_temp</code>, and we must use the qualified name to refer it, e.g. <code>SELECT * FROM global_temp.view1</code>. See <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#global-temporary-view">http://spark.apache.org/docs/latest/sql-programming-guide.html#global-temporary-view</a> for details.</p>
</div>
<div class="cell markdown">
<ol start="6">
<li>Creating Datasets</li>
</ol>
<hr />
<p>Datasets are similar to RDDs, however, instead of using Java serialization or Kryo they use a specialized <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder">Encoder</a> to serialize the objects for processing or transmitting over the network. While both encoders and standard serialization are responsible for turning an object into bytes, encoders are code generated dynamically and use a format that allows Spark to perform many operations like filtering, sorting and hashing without deserializing the bytes back into an object.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rangeDS = spark.range(0, 3) // Ctrl+Enter to make DataSet with 0,1,2; Note we added '.toDF()' to this to create a DataFrame
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">rangeDS.show() // the column name 'id' is made by default here
</code></pre>
</div>
<div class="cell markdown">
<p>We can have more complicated objects in a <code>DataSet</code> too.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,
// you can use custom classes that implement the Product interface
case class Person(name: String, age: Long)

// Encoders are created for case classes
val caseClassDS = Seq(Person(&quot;Andy&quot;, 32), Person(&quot;Erik&quot;,44), Person(&quot;Anna&quot;, 15)).toDS()
caseClassDS.show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Encoders for most common types are automatically provided by importing spark.implicits._
val primitiveDS = Seq(1, 2, 3).toDS()
primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">df1
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">df1.show
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// let's make a case class for our DF so we can convert it to Dataset
case class singleAndDoubleIntegers(once: Integer, twice: Integer)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val ds1 = df1.as[singleAndDoubleIntegers]
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">ds1.show()
</code></pre>
</div>
<div class="cell markdown">
<hr />
<hr />
<h2 id="next-we-will-play-with-data"><a class="header" href="#next-we-will-play-with-data">Next we will play with data</a></h2>
<p>The data here is <strong>semi-structured tabular data</strong> (Tab-delimited text file in dbfs). Let us see what Anthony Joseph in BerkeleyX/CS100.1x had to say about such data.</p>
<h3 id="key-data-management-concepts-semi-structured-tabular-data"><a class="header" href="#key-data-management-concepts-semi-structured-tabular-data">Key Data Management Concepts: Semi-Structured Tabular Data</a></h3>
<p><strong>(watch now 1:26)</strong>:</p>
<p><a href="https://www.youtube.com/watch?v=G_67yUxdDbU?rel=0&amp;autoplay=1&amp;modestbranding=1&amp;start=1"><img src="http://img.youtube.com/vi/G_67yUxdDbU/0.jpg" alt="Semi-Structured Tabular Data by Anthony Joseph in BerkeleyX/CS100.1x" /></a></p>
<hr />
</div>
<div class="cell markdown">
<h2 id="go-through-the-databricks-introductions-now"><a class="header" href="#go-through-the-databricks-introductions-now">Go through the databricks Introductions Now</a></h2>
<ul>
<li>
<p><a href="https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-scala.html">https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-scala.html</a></p>
</li>
<li>
<p><a href="https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-datasets.html">https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-datasets.html</a></p>
</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="recommended-homework"><a class="header" href="#recommended-homework">Recommended Homework</a></h3>
<p>This week's recommended homework is a deep dive into the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">SparkSQL programming guide</a>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h3 id="diamonds-ml-pipeline-workflow---dataframe-etl-and-eda-part"><a class="header" href="#diamonds-ml-pipeline-workflow---dataframe-etl-and-eda-part">Diamonds ML Pipeline Workflow - DataFrame ETL and EDA Part</a></h3>
<p>This is the Spark SQL parts that are focussed on extract-transform-Load (ETL) and exploratory-data-analysis (EDA) parts of an end-to-end example of a Machine Learning (ML) workflow.</p>
<p><strong>Why are we using DataFrames?</strong> <em>This is because of the <strong>Announcement</strong> in the Spark MLlib Main Guide for Spark 2.2</em> <a href="https://spark.apache.org/docs/latest/ml-guide.html">https://spark.apache.org/docs/latest/ml-guide.html</a> that <em>&quot;DataFrame-based API is primary API&quot;</em>.</p>
<p>This notebook is a scala<em>rific</em> break-down of the python<em>ic</em> 'Diamonds ML Pipeline Workflow' from the Databricks Guide.</p>
<p><strong>We will see this example again in the sequel</strong></p>
<p>For this example, we analyze the Diamonds dataset from the R Datasets hosted on DBC.</p>
<p>Later on, we will use the <a href="http://spark.apache.org/docs/latest/ml-classification-regression.html#decision-trees">DecisionTree algorithm</a> to predict the price of a diamond from its characteristics.</p>
<p>Here is an outline of our pipeline:</p>
<ul>
<li><strong>Step 1. <em>Load data</em>: Load data as DataFrame</strong></li>
<li><strong>Step 2. <em>Understand the data</em>: Compute statistics and create visualizations to get a better understanding of the data.</strong></li>
<li>Step 3. <em>Hold out data</em>: Split the data randomly into training and test sets. We will not look at the test data until <em>after</em> learning.</li>
<li>Step 4. On the training dataset:
<ul>
<li><em>Extract features</em>: We will index categorical (String-valued) features so that DecisionTree can handle them.</li>
<li><em>Learn a model</em>: Run DecisionTree to learn how to predict a diamond's price from a description of the diamond.</li>
<li><em>Tune the model</em>: Tune the tree depth (complexity) using the training data. (This process is also called <em>model selection</em>.)</li>
</ul>
</li>
<li>Step 5. <em>Evaluate the model</em>: Now look at the test dataset. Compare the initial model with the tuned model to see the benefit of tuning parameters.</li>
<li>Step 6. <em>Understand the model</em>: We will examine the learned model and results to gain further insight.</li>
</ul>
<p>In this notebook, we will only cover <strong>Step 1</strong> and <strong>Step 2.</strong> above. The other Steps will be revisited in the sequel.</p>
</div>
<div class="cell markdown">
<h3 id="step-1-load-data-as-dataframe"><a class="header" href="#step-1-load-data-as-dataframe">Step 1. Load data as DataFrame</a></h3>
<p>This section loads a dataset as a DataFrame and examines a few rows of it to understand the schema.</p>
<p>For more info, see the DB guide on <a href="https://docs.databricks.com/user-guide/importing-data.html">importing data</a>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// We'll use the Diamonds dataset from the R datasets hosted on DBC.
val diamondsFilePath = &quot;dbfs:/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv&quot;
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sc.textFile(diamondsFilePath).take(2) // looks like a csv file as it should
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val diamondsRawDF = sqlContext.read    // we can use sqlContext instead of SparkSession for backwards compatibility to 1.x
    .format(&quot;com.databricks.spark.csv&quot;) // use spark.csv package
    .option(&quot;header&quot;, &quot;true&quot;) // Use first line of all files as header
    .option(&quot;inferSchema&quot;, &quot;true&quot;) // Automatically infer data types
    //.option(&quot;delimiter&quot;, &quot;,&quot;) // Specify the delimiter as comma or ',' DEFAULT
    .load(diamondsFilePath)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//There are 10 columns.  We will try to predict the price of diamonds, treating the other 9 columns as features.
diamondsRawDF.printSchema()
</code></pre>
</div>
<div class="cell markdown">
<p><em>Note:</em> <code>(nullable = true)</code> simply means if the value is allowed to be <code>null</code>.</p>
<p>Let us count the number of rows in <code>diamondsDF</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">diamondsRawDF.count() // Ctrl+Enter
</code></pre>
</div>
<div class="cell markdown">
<p>So there are 53940 records or rows in the DataFrame.</p>
<p>Use the <code>show(n)</code> method to see the first <code>n</code> (default is 20) rows of the DataFrame, as folows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">diamondsRawDF.show(10)
</code></pre>
</div>
<div class="cell markdown">
<p>If you notice the schema of <code>diamondsRawDF</code> you will see that the automatic schema inference of <code>SqlContext.read</code> method has cast the values in the column <code>price</code> as <code>integer</code>.</p>
<p>To cleanup:</p>
<ul>
<li>let's recast the column <code>price</code> as <code>double</code> for downstream ML tasks later and</li>
<li>let's also get rid of the first column of row indices.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.sql.types.DoubleType
//we will convert price column from int to double for being able to model, fit and predict in downstream ML task
val diamondsDF = diamondsRawDF.select($&quot;carat&quot;, $&quot;cut&quot;, $&quot;color&quot;, $&quot;clarity&quot;, $&quot;depth&quot;, $&quot;table&quot;,$&quot;price&quot;.cast(DoubleType).as(&quot;price&quot;), $&quot;x&quot;, $&quot;y&quot;, $&quot;z&quot;)
diamondsDF.cache() // let's cache it for reuse
diamondsDF.printSchema // print schema
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">diamondsDF.show(10,false) // notice that price column has Double values that end in '.0' now
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//View DataFrame in databricks
// note this 'display' is a databricks notebook specific command that is quite powerful for visual interaction with the data
// other notebooks like zeppelin have similar commands for interactive visualisation
display(diamondsDF) 
</code></pre>
</div>
<div class="cell markdown">
<h3 id="step-2-understand-the-data"><a class="header" href="#step-2-understand-the-data">Step 2. Understand the data</a></h3>
<p>Let's examine the data to get a better understanding of what is there. We only examine a couple of features (columns), but it gives an idea of the type of exploration you might do to understand a new dataset.</p>
<p>For more examples of using Databricks's visualization (even across languages) see <a href="https://docs.databricks.com/user-guide/visualizations/index.html">https://docs.databricks.com/user-guide/visualizations/index.html</a> NOW.</p>
</div>
<div class="cell markdown">
<p>We can see that we have a mix of</p>
<ul>
<li>categorical features (<code>cut</code>, <code>color</code>, <code>clarity</code>) and</li>
<li>continuous features (<code>depth</code>, <code>x</code>, <code>y</code>, <code>z</code>).</li>
</ul>
<h5 id="lets-first-look-at-the-categorical-features"><a class="header" href="#lets-first-look-at-the-categorical-features">Let's first look at the categorical features.</a></h5>
</div>
<div class="cell markdown">
<p>You can also select one or more individual columns using so-called DataFrame API.</p>
<p>Let us <code>select</code> the column <code>cut</code> from <code>diamondsDF</code> and create a new DataFrame called <code>cutsDF</code> and then display it as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val cutsDF = diamondsDF.select(&quot;cut&quot;) // Shift+Enter
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">cutsDF.show(10) // Ctrl+Enter
</code></pre>
</div>
<div class="cell markdown">
<p>Let us use <code>distinct</code> to find the distinct types of <code>cut</code>'s in the dataset.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// View distinct diamond cuts in dataset
val cutsDistinctDF = diamondsDF.select(&quot;cut&quot;).distinct()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">cutsDistinctDF.show()
</code></pre>
</div>
<div class="cell markdown">
<p>Clearly, there are just 5 kinds of cuts.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// View distinct diamond colors in dataset
val colorsDistinctDF = diamondsDF.select(&quot;color&quot;).distinct() //.collect()
colorsDistinctDF.show()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// View distinct diamond clarities in dataset
val claritiesDistinctDF = diamondsDF.select(&quot;clarity&quot;).distinct() // .collect()
claritiesDistinctDF.show()
</code></pre>
</div>
<div class="cell markdown">
<p>We can examine the distribution of a particular feature by using display(),</p>
<p><strong>You Try!</strong></p>
<ol>
<li>Click on the chart icon and Plot Options, and setting:</li>
</ol>
<ul>
<li>Value=<code>&lt;id&gt;</code></li>
<li>Series groupings='cut'</li>
<li>and Aggregation=<code>COUNT</code>.</li>
</ul>
<ol>
<li>You can also try this using columns &quot;color&quot; and &quot;clarity&quot;</li>
</ol>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(diamondsDF.select(&quot;cut&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// come on do the same for color NOW!
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// and clarity too...
</code></pre>
</div>
<div class="cell markdown">
<p>** You Try!**</p>
<p>Now play around with display of the entire DF and choosing what you want in the GUI as opposed to a <code>.select(...)</code> statement earlier.</p>
<p>For instance, the following <code>display(diamondsDF)</code> shows the counts of the colors by choosing in the <code>Plot Options</code> a <code>bar-chart</code> that is <code>grouped</code> with <code>Series Grouping</code> as <code>color</code>, <code>values</code> as <code>&lt;id&gt;</code> and <code>Aggregation</code> as <code>COUNT</code>. You can click on <code>Plot Options</code> to see these settings and can change them as you wish by dragging and dropping.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala"> display(diamondsDF)
</code></pre>
</div>
<div class="cell markdown">
<p>Now let's examine one of the continuous features as an example.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Select: &quot;Plot Options...&quot; --&gt; &quot;Display type&quot; --&gt; &quot;histogram plot&quot; and choose to &quot;Plot over all results&quot; OTHERWISE you get the image from first 1000 rows only
display(diamondsDF.select(&quot;carat&quot;))
</code></pre>
</div>
<div class="cell markdown">
<p>The above histogram of the diamonds' carat ratings shows that carats have a skewed distribution: Many diamonds are small, but there are a number of diamonds in the dataset which are much larger.</p>
<ul>
<li>Extremely skewed distributions can cause problems for some algorithms (e.g., Linear Regression).</li>
<li>However, Decision Trees handle skewed distributions very naturally.</li>
</ul>
<p>Note: When you call <code>display</code> to create a histogram like that above, <strong>it will plot using a subsample from the dataset</strong> (for efficiency), but you can plot using the full dataset by selecting &quot;Plot over all results&quot;. For our dataset, the two plots can actually look very different due to the long-tailed distribution.</p>
<p>We will not examine the label distribution for now. It can be helpful to examine the label distribution, but it is best to do so only on the training set, not on the test set which we will hold out for evaluation. These will be seen in the sequel</p>
</div>
<div class="cell markdown">
<p><strong>You Try!</strong> Of course knock youself out visually exploring the dataset more...</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(diamondsDF.select(&quot;cut&quot;,&quot;carat&quot;))
</code></pre>
</div>
<div class="cell markdown">
<p>Try scatter plot to see pairwise scatter plots of continuous features.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(diamondsDF) //Ctrl+Enter 
</code></pre>
</div>
<div class="cell markdown">
<p>Note that columns of type string are not in the scatter plot!</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">diamondsDF.printSchema // Ctrl+Enter
</code></pre>
</div>
<div class="cell markdown">
<h3 id="let-us-run-through-some-basic-inteactive-sql-queries-next"><a class="header" href="#let-us-run-through-some-basic-inteactive-sql-queries-next">Let us run through some basic inteactive SQL queries next</a></h3>
<ul>
<li>HiveQL supports =, &lt;, &gt;, &lt;=, &gt;= and != operators. It also supports LIKE operator for fuzzy matching of Strings</li>
<li>Enclose Strings in single quotes</li>
<li>Multiple conditions can be combined using <code>and</code> and <code>or</code></li>
<li>Enclose conditions in <code>()</code> for precedence</li>
<li>...</li>
<li>...</li>
</ul>
<p><strong>Why do I need to learn interactive SQL queries?</strong></p>
<p>Such queries in the widely known declarative SQL language can help us explore the data and thereby inform the modeling process!!!</p>
</div>
<div class="cell markdown">
<p>Using DataFrame API, we can apply a <code>filter</code> after <code>select</code> to transform the DataFrame <code>diamondsDF</code> to the new DataFrame <code>diamondsDColoredDF</code>.</p>
<p>Below, <code>$</code> is an alias for column.</p>
<p>Let as select the columns named <code>carat</code>, <code>colour</code>, <code>price</code> where <code>color</code> value is equal to <code>D</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val diamondsDColoredDF = diamondsDF.select(&quot;carat&quot;, &quot;color&quot;, &quot;price&quot;).filter($&quot;color&quot; === &quot;D&quot;) // Shift+Enter
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">diamondsDColoredDF.show(10) // Ctrl+Enter
</code></pre>
</div>
<div class="cell markdown">
<p>As you can see all the colors are now 'D'. But to really confirm this we can do the following for fun:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">diamondsDColoredDF.select(&quot;color&quot;).distinct().show
</code></pre>
</div>
<div class="cell markdown">
<p>Let's try to do the same in SQL for those who know SQL from before.</p>
<p>First we need to see if the table is registerd (not just the DataFrame), and if not we ened to register our DataFrame as a temporary table.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sqlContext.tables.show() // Ctrl+Enter to see available tables
</code></pre>
</div>
<div class="cell markdown">
<p>Looks like diamonds is already there (if not just execute the following cell).</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">diamondsDF.createOrReplaceTempView(&quot;diamonds&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sqlContext.tables.show() // Ctrl+Enter to see available tables
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">-- Shift+Enter to do the same in SQL
select carat, color, price from diamonds where color='D'
</code></pre>
</div>
<div class="cell markdown">
<p>Alternatively, one could just write the SQL statement in scala to create a new DataFrame <code>diamondsDColoredDF_FromTable</code> from the table <code>diamonds</code> and display it, as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val diamondsDColoredDF_FromTable = sqlContext.sql(&quot;select carat, color, price from diamonds where color='D'&quot;) // Shift+Enter
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// or if you like use upper case for SQL then this is equivalent
val diamondsDColoredDF_FromTable = sqlContext.sql(&quot;SELECT carat, color, price FROM diamonds WHERE color='D'&quot;) // Shift+Enter
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// from version 2.x onwards you can call from SparkSession, the pre-made spark in spark-shell or databricks notebook
val diamondsDColoredDF_FromTable = spark.sql(&quot;SELECT carat, color, price FROM diamonds WHERE color='D'&quot;) // Shift+Enter
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(diamondsDColoredDF_FromTable) // Ctrl+Enter to see the same DF!
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// You can also use the familiar wildchard character '%' when matching Strings
display(spark.sql(&quot;SELECT * FROM diamonds WHERE clarity LIKE 'V%'&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Combining conditions
display(spark.sql(&quot;SELECT * FROM diamonds WHERE clarity LIKE 'V%' AND price &gt; 10000&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// selecting a subset of fields
display(spark.sql(&quot;SELECT carat, clarity, price FROM diamonds WHERE color = 'D'&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//renaming a field using as
display(spark.sql(&quot;SELECT carat AS carrot, clarity, price FROM diamonds&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//sorting
display(spark.sql(&quot;SELECT carat, clarity, price FROM diamonds ORDER BY price DESC&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">diamondsDF.printSchema // since price is double in the DF that was turned into table we can rely on the descenting sort on doubles
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// sort by multiple fields
display(spark.sql(&quot;SELECT carat, clarity, price FROM diamonds ORDER BY carat ASC, price DESC&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// use this to type cast strings into Int when the table is loaded with string-valued columns
//display(spark.sql(&quot;select cast(carat as Int) as carat, clarity, cast(price as Int) as price from diamond order by carat asc, price desc&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// sort by multiple fields and limit to first 5
// I prefer lowercase for SQL - and you can use either in this course - but in the field do what your Boss or your colleagues prefer :)
display(spark.sql(&quot;select carat, clarity, price from diamonds order by carat desc, price desc limit 5&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//aggregate functions
display(spark.sql(&quot;select avg(price) as avgprice from diamonds&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//average operator is doing an auto-type conversion from int to double
display(spark.sql(&quot;select avg(cast(price as Integer)) as avgprice from diamonds&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//aggregate function and grouping
display(spark.sql(&quot;select color, avg(price) as avgprice from diamonds group by color&quot;))
</code></pre>
</div>
<div class="cell markdown">
<h3 id="why-do-we-need-to-know-these-interactive-sql-queries"><a class="header" href="#why-do-we-need-to-know-these-interactive-sql-queries">Why do we need to know these interactive SQL queries?</a></h3>
<p>Such queries can help us explore the data and thereby inform the modeling process!!!</p>
<p>Of course, if you don't know SQL then don't worry, we will be doing these things in scala using DataFrames.</p>
<p>Finally, those who are planning to take the Spark Developer Exams online, then you can't escape from SQL questions there...</p>
</div>
<div class="cell markdown">
<h3 id="we-will-continue-later-with-ml-pipelines-to-do-prediction-with-a-fitted-model-from-this-dataset"><a class="header" href="#we-will-continue-later-with-ml-pipelines-to-do-prediction-with-a-fitted-model-from-this-dataset">We will continue later with ML pipelines to do prediction with a fitted model from this dataset</a></h3>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h3 id="power-plant-ml-pipeline-application---dataframe-part"><a class="header" href="#power-plant-ml-pipeline-application---dataframe-part">Power Plant ML Pipeline Application - DataFrame Part</a></h3>
<p>This is the Spark SQL parts of an end-to-end example of using a number of different machine learning algorithms to solve a supervised regression problem.</p>
<p>This is a break-down of <em>Power Plant ML Pipeline Application</em> from databricks.</p>
<p><strong>This will be a recurring example in the sequel</strong></p>
<h5 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h5>
<ul>
<li><strong>Step 1: Business Understanding</strong></li>
<li><strong>Step 2: Load Your Data</strong></li>
<li><strong>Step 3: Explore Your Data</strong></li>
<li><strong>Step 4: Visualize Your Data</strong></li>
<li><em>Step 5: Data Preparation</em></li>
<li><em>Step 6: Data Modeling</em></li>
<li><em>Step 7: Tuning and Evaluation</em></li>
<li><em>Step 8: Deployment</em></li>
</ul>
<p><em>We are trying to predict power output given a set of readings from various sensors in a gas-fired power generation plant. Power generation is a complex process, and understanding and predicting power output is an important element in managing a plant and its connection to the power grid.</em></p>
<ul>
<li>Given this business problem, we need to translate it to a Machine Learning task (actually a <em>Statistical</em> Machine Learning task).</li>
<li>The ML task here is <em>regression</em> since the label (or target) we will be trying to predict takes a <em>continuous numeric</em> value
<ul>
<li>Note: if the labels took values from a finite discrete set, such as, <code>Spam</code>/<code>Not-Spam</code> or <code>Good</code>/<code>Bad</code>/<code>Ugly</code>, then the ML task would be <em>classification</em>.</li>
</ul>
</li>
</ul>
<p><strong>Today, we will only cover Steps 1, 2, 3 and 4 above</strong>. You need introductions to linear algebra, stochastic gradient descent and decision trees before we can accomplish the <strong>applied ML task</strong> with some intuitive understanding. If you can't wait for ML then <strong>check out <a href="https://spark.apache.org/docs/latest/mllib-guide.html">Spark MLLib Programming Guide</a> for comming attractions!</strong></p>
<p>The example data is provided by UCI at <a href="https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant">UCI Machine Learning Repository Combined Cycle Power Plant Data Set</a></p>
<p>You can read the background on the UCI page, but in summary:</p>
<ul>
<li>we have collected a number of readings from sensors at a Gas Fired Power Plant (also called a Peaker Plant) and</li>
<li>want to use those sensor readings to predict how much power the plant will generate in a couple weeks from now.</li>
<li>Again, today we will just focus on Steps 1-4 above that pertain to DataFrames.</li>
</ul>
<p>More information about Peaker or Peaking Power Plants can be found on Wikipedia <a href="https://en.wikipedia.org/wiki/Peaking_power_plant">https://en.wikipedia.org/wiki/Peaking<em>power</em>plant</a>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//This allows easy embedding of publicly available information into any other notebook
//when viewing in git-book just ignore this block - you may have to manually chase the URL in frameIt(&quot;URL&quot;).
//Example usage:
// displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;,250))
def frameIt( u:String, h:Int ) : String = {
      &quot;&quot;&quot;&lt;iframe 
 src=&quot;&quot;&quot;&quot;+ u+&quot;&quot;&quot;&quot;
 width=&quot;95%&quot; height=&quot;&quot;&quot;&quot; + h + &quot;&quot;&quot;&quot;
 sandbox&gt;
  &lt;p&gt;
    &lt;a href=&quot;http://spark.apache.org/docs/latest/index.html&quot;&gt;
      Fallback link for browsers that, unlikely, don't support frames
    &lt;/a&gt;
  &lt;/p&gt;
&lt;/iframe&gt;&quot;&quot;&quot;
   }
displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Peaking_power_plant&quot;,300))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant&quot;,500))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sc.version.replace(&quot;.&quot;, &quot;&quot;).toInt
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// a good habit to ensure the code is being run on the appropriate version of Spark - we are using Spark 2.2 actually if we use SparkSession object spark down the road...
require(sc.version.replace(&quot;.&quot;, &quot;&quot;).toInt &gt;= 140, &quot;Spark 1.4.0+ is required to run this notebook. Please attach it to a Spark 1.4.0+ cluster.&quot;)
</code></pre>
</div>
<div class="cell markdown">
<h2 id="step-1-business-understanding"><a class="header" href="#step-1-business-understanding">Step 1: Business Understanding</a></h2>
<p>The first step in any machine learning task is to understand the business need.</p>
<p>As described in the overview we are trying to predict power output given a set of readings from various sensors in a gas-fired power generation plant.</p>
<p>The problem is a regression problem since the label (or target) we are trying to predict is numeric</p>
</div>
<div class="cell markdown">
<h2 id="step-2-load-your-data"><a class="header" href="#step-2-load-your-data">Step 2: Load Your Data</a></h2>
<p>Now that we understand what we are trying to do, we need to load our data and describe it, explore it and verify it.</p>
</div>
<div class="cell markdown">
<p>Data was downloaded already as these five Tab-separated-variable or tsv files.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;/databricks-datasets/power-plant/data&quot;)) // Ctrl+Enter
</code></pre>
</div>
<div class="cell markdown">
<p>Now let us load the data from the Tab-separated-variable or tsv text file into an <code>RDD[String]</code> using the familiar <code>textFile</code> method.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val powerPlantRDD = sc.textFile(&quot;/databricks-datasets/power-plant/data/Sheet1.tsv&quot;) // Ctrl+Enter
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">powerPlantRDD.take(5).foreach(println) // Ctrl+Enter to print first 5 lines
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// let us make sure we are using Spark version greater than 2.2 - we need a version closer to 2.0 if we want to use SparkSession and SQLContext 
require(sc.version.replace(&quot;.&quot;, &quot;&quot;).toInt &gt;= 220, &quot;Spark 2.2.0+ is required to run this notebook. Please attach it to a Spark 2.2.0+ cluster.&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// this reads the tsv file and turns it into a dataframe
val powerPlantDF = spark.read // use 'sqlContext.read' instead if you want to use older Spark version &gt; 1.3  see 008_ notebook
    .format(&quot;csv&quot;) // use spark.csv package
    .option(&quot;header&quot;, &quot;true&quot;) // Use first line of all files as header
    .option(&quot;inferSchema&quot;, &quot;true&quot;) // Automatically infer data types
    .option(&quot;delimiter&quot;, &quot;\t&quot;) // Specify the delimiter as Tab or '\t'
    .load(&quot;/databricks-datasets/power-plant/data/Sheet1.tsv&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">powerPlantDF.printSchema // print the schema of the DataFrame that was inferred
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">powerPlantDF.count
</code></pre>
</div>
<div class="cell markdown">
<h3 id="21-alternatively-load-data-via-the-upload-gui-feature-in-databricks"><a class="header" href="#21-alternatively-load-data-via-the-upload-gui-feature-in-databricks">2.1. Alternatively, load data via the upload GUI feature in databricks</a></h3>
<h2 id="use-this-for-other-smallish-datasets-you-want-to-import-to-your-ce"><a class="header" href="#use-this-for-other-smallish-datasets-you-want-to-import-to-your-ce">USE THIS FOR OTHER SMALLish DataSets you want to import to your CE</a></h2>
<p>Since the dataset is relatively small, we will use the upload feature in Databricks to upload the data as a table.</p>
<p>First download the Data Folder from <a href="https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant">UCI Machine Learning Repository Combined Cycle Power Plant Data Set</a></p>
<p>The file is a multi-tab Excel document so you will need to save each tab as a Text file export.</p>
<p>I prefer exporting as a Tab-Separated-Values (TSV) since it is more consistent than CSV.</p>
<p>Call each file Folds5x2_pp&lt;Sheet 1..5&gt;.tsv and save to your machine.</p>
<p>Go to the Databricks Menu &gt; Tables &gt; Create Table</p>
<p>Select Datasource as &quot;File&quot;</p>
<p>Upload <em>ALL</em> 5 files at once.</p>
<p>See screenshots below (but refer <a href="https://docs.databricks.com/user-guide/importing-data.html">https://docs.databricks.com/user-guide/importing-data.html</a> for latest methods to import data):</p>
<p><strong>2.1.1. Create Table</strong> _________________</p>
<p>When you import your data, name your table <code>power_plant</code>, specify all of the columns with the datatype <code>Double</code> and make sure you check the <code>First row is header</code> box.</p>
<p><img src="http://training.databricks.com/databricks_guide/1_4_ML_Power_Plant_Import_Table.png" alt="alt text" /></p>
<p><strong>2.1.2. Review Schema</strong> __________________</p>
<p>Your table schema and preview should look like this after you click <code>Create Table</code>:</p>
<p><img src="http://training.databricks.com/databricks_guide/1_4_ML_Power_Plant_Import_Table_Schema.png" alt="alt text" /></p>
</div>
<div class="cell markdown">
<p>Now that your data is loaded let's explore it.</p>
</div>
<div class="cell markdown">
<h2 id="step-3-explore-your-data"><a class="header" href="#step-3-explore-your-data">Step 3: Explore Your Data</a></h2>
<p>Now that we understand what we are trying to do, we need to load our data and describe it, explore it and verify it.</p>
</div>
<div class="cell markdown">
<h4 id="viewing-the-table-as-text"><a class="header" href="#viewing-the-table-as-text">Viewing the table as text</a></h4>
<p>By uisng <code>.show</code> method we can see some of the contents of the table in plain text.</p>
<p>This works in pure Apache Spark, say in <code>Spark-Shell</code> without any notebook layer on top of Spark like databricks, zeppelin or jupyter.</p>
<p>It is a good idea to use this method when possible.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">powerPlantDF.show(10) // try putting 1000 here instead of 10
</code></pre>
</div>
<div class="cell markdown">
<h4 id="viewing-as-dataframe"><a class="header" href="#viewing-as-dataframe">Viewing as DataFrame</a></h4>
<p>This is almost necessary for a data scientist to gain visual insights into all pair-wise relationships between the several (3 to 6 or so) variables in question.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(powerPlantDF) 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">powerPlantDF.count() // count the number of rows in DF
</code></pre>
</div>
<div class="cell markdown">
<h4 id="viewing-as-table-via-sql"><a class="header" href="#viewing-as-table-via-sql">Viewing as Table via SQL</a></h4>
<p>Let us look at what tables are already available, as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sqlContext.tables.show() // Ctrl+Enter to see available tables
</code></pre>
</div>
<div class="cell markdown">
<p>We can also access the list of tables and databases using <code>spark.catalog</code> methods as explained here:</p>
<ul>
<li><a href="https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html">https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html</a></li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">spark.catalog.listTables.show(false)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">spark.catalog.listDatabases.show(false)
</code></pre>
</div>
<div class="cell markdown">
<p>We need to create a temporary view of the DataFrame as a table before being able to access it via SQL.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">powerPlantDF.createOrReplaceTempView(&quot;power_plant_table&quot;) // Shift+Enter
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sqlContext.tables.show() 
</code></pre>
</div>
<div class="cell markdown">
<p>Note that table names are in lower-case only!</p>
</div>
<div class="cell markdown">
<p><strong>You Try!</strong></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//sqlContext // uncomment and put . after sqlContext and hit Tab to see what methods are available
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//sqlContext.dropTempTable(&quot;power_plant_table&quot;) // uncomment and Ctrl+Enter if you want to remove the table!
</code></pre>
</div>
<div class="cell markdown">
<p>The following SQL statement simply selects all the columns (due to <code>*</code>) from <code>powerPlantTable</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">-- Ctrl+Enter to query the rows via SQL
SELECT * FROM power_plant_table
</code></pre>
</div>
<div class="cell markdown">
<p>Note that the output of the above command is the same as <code>display(powerPlantDF)</code> we did earlier.</p>
</div>
<div class="cell markdown">
<p>We can use the SQL <code>desc</code> command to describe the schema. This is the SQL equivalent of <code>powerPlantDF.printSchema</code> we saw earlier.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">desc power_plant_table
</code></pre>
</div>
<div class="cell markdown">
<p><strong>Schema Definition</strong></p>
<p>Our schema definition from UCI appears below:</p>
<ul>
<li>AT = Atmospheric Temperature in C</li>
<li>V = Exhaust Vaccum Speed</li>
<li>AP = Atmospheric Pressure</li>
<li>RH = Relative Humidity</li>
<li>PE = Power Output</li>
</ul>
<p>PE is our label or target. This is the value we are trying to predict given the measurements.</p>
<p><em>Reference <a href="https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant">UCI Machine Learning Repository Combined Cycle Power Plant Data Set</a></em></p>
</div>
<div class="cell markdown">
<p>Let's do some basic statistical analysis of all the columns.</p>
<p>We can use the describe function with no parameters to get some basic stats for each column like count, mean, max, min and standard deviation. More information can be found in the <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame">Spark API docs</a></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(powerPlantDF.describe())
</code></pre>
</div>
<div class="cell markdown">
<h2 id="step-4-visualize-your-data"><a class="header" href="#step-4-visualize-your-data">Step 4: Visualize Your Data</a></h2>
<p>To understand our data, we will look for correlations between features and the label. This can be important when choosing a model. E.g., if features and a label are linearly correlated, a linear model like Linear Regression can do well; if the relationship is very non-linear, more complex models such as Decision Trees or neural networks can be better. We use the Databricks built in visualization to view each of our predictors in relation to the label column as a scatter plot to see the correlation between the predictors and the label.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">select AT as Temperature, PE as Power from power_plant_table
</code></pre>
</div>
<div class="cell markdown">
<p>From the above plot, it looks like there is strong linear correlation between temperature and Power Output!</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">select V as ExhaustVaccum, PE as Power from power_plant_table;
</code></pre>
</div>
<div class="cell markdown">
<p>The linear correlation is not as strong between Exhaust Vacuum Speed and Power Output but there is some semblance of a pattern.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">select AP as Pressure, PE as Power from power_plant_table;
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">select RH as Humidity, PE as Power from power_plant_table;
</code></pre>
</div>
<div class="cell markdown">
<p>...and atmospheric pressure and relative humidity seem to have little to no linear correlation.</p>
<p>These pairwise plots can also be done directly using <code>display</code> on <code>select</code>ed columns of the DataFrame <code>powerPlantDF</code>.</p>
<p>In general <strong>we will shy from SQL as much as possible</strong> to focus on ML pipelines written with DataFrames and DataSets with occassional in-and-out of RDDs.</p>
<p>The illustations in <code>%sql</code> above are to mainly reassure those with a RDBMS background and SQL that their SQL expressibility can be directly used in Apache Spark and in databricks notebooks.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(powerPlantDF.select($&quot;RH&quot;, $&quot;PE&quot;))
</code></pre>
</div>
<div class="cell markdown">
<p>Furthermore, you can interactively start playing with <code>display</code> on the full DataFrame!</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(powerPlantDF) // just as we did for the diamonds dataset
</code></pre>
</div>
<div class="cell markdown">
<p>We will do the following steps in the sequel.</p>
<ul>
<li><em>Step 5: Data Preparation</em></li>
<li><em>Step 6: Data Modeling</em></li>
<li><em>Step 7: Tuning and Evaluation</em></li>
<li><em>Step 8: Deployment</em></li>
</ul>
</div>
<div class="cell markdown">
<p>Datasource References:</p>
<ul>
<li>Pinar Tüfekci, Prediction of full load electrical power output of a base load operated combined cycle power plant using machine learning methods, International Journal of Electrical Power &amp; Energy Systems, Volume 60, September 2014, Pages 126-140, ISSN 0142-0615, <a href="http://www.journals.elsevier.com/international-journal-of-electrical-power-and-energy-systems/">Web Link</a></li>
<li>Heysem Kaya, Pinar Tüfekci , Sadik Fikret Gürgen: Local and Global Learning Methods for Predicting Power of a Combined Gas &amp; Steam Turbine, Proceedings of the International Conference on Emerging Trends in Computer and Electronics Engineering ICETCEE 2012, pp. 13-18 (Mar. 2012, Dubai) <a href="http://www.cmpe.boun.edu.tr/%7Ekaya/kaya2012gasturbine.pdf">Web Link</a></li>
</ul>
</div>
<div class="cell markdown">
<h3 id="we-will-continue-later-with-ml-pipelines-to-do-prediction-with-a-fitted-model-from-this-dataset-1"><a class="header" href="#we-will-continue-later-with-ml-pipelines-to-do-prediction-with-a-fitted-model-from-this-dataset-1">We will continue later with ML pipelines to do prediction with a fitted model from this dataset</a></h3>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="wiki-clickstream-analysis"><a class="header" href="#wiki-clickstream-analysis">Wiki Clickstream Analysis</a></h1>
<p>** Dataset: 3.2 billion requests collected during the month of February 2015 grouped by (src, dest) **</p>
<p>** Source: https://datahub.io/dataset/wikipedia-clickstream/ **</p>
<p><img src="https://databricks-prod-cloudfront.s3.amazonaws.com/docs/images/ny.clickstream.png" alt="NY clickstream image" title="NY clickstream image" /></p>
<p><em>This notebook requires Spark 1.6+.</em></p>
</div>
<div class="cell markdown">
<p>This notebook was originally a data analysis workflow developed with <a href="https://databricks.com/blog/2016/02/17/introducing-databricks-community-edition-apache-spark-for-all.html">Databricks Community Edition</a>, a free version of Databricks designed for learning <a href="https://spark.apache.org/">Apache Spark</a>.</p>
<p>Here we elucidate the original python notebook (<a href="contents/000_1-sds-3-x//#workspace/scalable-data-science/xtraResources/sparkSummitEast2016/Wikipedia%20Clickstream%20Data">also linked here</a>) used in the talk by Michael Armbrust at Spark Summit East February 2016 shared from <a href="https://twitter.com/michaelarmbrust/status/699969850475737088">https://twitter.com/michaelarmbrust/status/699969850475737088</a> (watch later)</p>
<p><a href="https://www.youtube.com/v/35Y-rqSMCCA"><img src="http://img.youtube.com/vi/35Y-rqSMCCA/0.jpg" alt="Michael Armbrust Spark Summit East" /></a></p>
</div>
<div class="cell markdown">
<h3 id="data-set"><a class="header" href="#data-set">Data set</a></h3>
<h1 id="img-srchttpsameerf-dbc-labss3-website-us-west-2amazonawscomdatawikipediaimagesw_logo_for_labspng-altwikipedia-logo-"><a class="header" href="#img-srchttpsameerf-dbc-labss3-website-us-west-2amazonawscomdatawikipediaimagesw_logo_for_labspng-altwikipedia-logo-"><img src="http://sameerf-dbc-labs.s3-website-us-west-2.amazonaws.com/data/wikipedia/images/w_logo_for_labs.png" alt="Wikipedia Logo" /></a></h1>
<p>The data we are exploring in this lab is the February 2015 English Wikipedia Clickstream data, and it is available here: http://datahub.io/dataset/wikipedia-clickstream/resource/be85cc68-d1e6-4134-804a-fd36b94dbb82.</p>
<p>According to Wikimedia:</p>
<blockquote>
<p>&quot;The data contains counts of (referer, resource) pairs extracted from the request logs of English Wikipedia. When a client requests a resource by following a link or performing a search, the URI of the webpage that linked to the resource is included with the request in an HTTP header called the &quot;referer&quot;. This data captures 22 million (referer, resource) pairs from a total of 3.2 billion requests collected during the month of February 2015.&quot;</p>
</blockquote>
<p>The data is approximately 1.2GB and it is hosted in the following Databricks file: <code>/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed</code></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;/databricks-datasets/wikipedia-datasets/&quot;))
</code></pre>
</div>
<div class="cell markdown">
<h3 id="let-us-first-understand-this-wikimedia-data-set-a-bit-more"><a class="header" href="#let-us-first-understand-this-wikimedia-data-set-a-bit-more">Let us first understand this Wikimedia data set a bit more</a></h3>
<p>Let's read the datahub-hosted link <a href="https://datahub.io/dataset/wikipedia-clickstream">https://datahub.io/dataset/wikipedia-clickstream</a> in the embedding below. Also click the <a href="http://ewulczyn.github.io/Wikipedia_Clickstream_Getting_Started/">blog</a> by Ellery Wulczyn, Data Scientist at The Wikimedia Foundation, to better understand how the data was generated (remember to Right-Click and use -&gt; and &lt;- if navigating within the embedded html frame below).</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//This allows easy embedding of publicly available information into any other notebook
//when viewing in git-book just ignore this block - you may have to manually chase the URL in frameIt(&quot;URL&quot;).
//Example usage:
// displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;,250))
def frameIt( u:String, h:Int ) : String = {
      &quot;&quot;&quot;&lt;iframe 
 src=&quot;&quot;&quot;&quot;+ u+&quot;&quot;&quot;&quot;
 width=&quot;95%&quot; height=&quot;&quot;&quot;&quot; + h + &quot;&quot;&quot;&quot;
 sandbox&gt;
  &lt;p&gt;
    &lt;a href=&quot;http://spark.apache.org/docs/latest/index.html&quot;&gt;
      Fallback link for browsers that, unlikely, don't support frames
    &lt;/a&gt;
  &lt;/p&gt;
&lt;/iframe&gt;&quot;&quot;&quot;
   }
displayHTML(frameIt(&quot;https://datahub.io/dataset/wikipedia-clickstream&quot;,500))
</code></pre>
</div>
<div class="cell markdown">
<p>Run the next two cells for some housekeeping.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">if (org.apache.spark.BuildInfo.sparkBranch &lt; &quot;1.6&quot;) sys.error(&quot;Attach this notebook to a cluster running Spark 1.6+&quot;)
</code></pre>
</div>
<div class="cell markdown">
<h3 id="loading-and-exploring-the-data"><a class="header" href="#loading-and-exploring-the-data">Loading and Exploring the data</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val data = sc.textFile(&quot;dbfs:///databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed&quot;)
</code></pre>
</div>
<div class="cell markdown">
<h5 id="looking-at-the-first-few-lines-of-the-data"><a class="header" href="#looking-at-the-first-few-lines-of-the-data">Looking at the first few lines of the data</a></h5>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">data.take(5).foreach(println) 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">data.take(2)
</code></pre>
</div>
<div class="cell markdown">
<ul>
<li>The first line looks like a header</li>
<li>The second line (separated from the first by &quot;,&quot;) contains data organized according to the header, i.e., <code>prev_id</code> = 3632887, <code>curr_id</code> = 121&quot;, and so on.</li>
</ul>
<p>Actually, here is the meaning of each column:</p>
<ul>
<li>
<p><code>prev_id</code>: if the referer does not correspond to an article in the main namespace of English Wikipedia, this value will be empty. Otherwise, it contains the unique MediaWiki page ID of the article corresponding to the referer i.e. the previous article the client was on</p>
</li>
<li>
<p><code>curr_id</code>: the MediaWiki unique page ID of the article the client requested</p>
</li>
<li>
<p><code>prev_title</code>: the result of mapping the referer URL to the fixed set of values described below</p>
</li>
<li>
<p><code>curr_title</code>: the title of the article the client requested</p>
</li>
<li>
<p><code>n</code>: the number of occurrences of the (referer, resource) pair</p>
</li>
<li>
<p><code>type</code></p>
<ul>
<li>&quot;link&quot; if the referer and request are both articles and the referer links to the request</li>
<li>&quot;redlink&quot; if the referer is an article and links to the request, but the request is not in the production enwiki.page table</li>
<li>&quot;other&quot; if the <em>referer</em> and request are both articles but the referer does not link to the request. This can happen when clients search or spoof their refer</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<p>Referers were mapped to a fixed set of values corresponding to internal traffic or external traffic from one of the top 5 global traffic sources to English Wikipedia, based on this scheme:</p>
<blockquote>
<ul>
<li>an article in the main namespace of English Wikipedia -&gt; the article title</li>
<li>any Wikipedia page that is not in the main namespace of English Wikipedia -&gt; <code>other-wikipedia</code></li>
<li>an empty referer -&gt; <code>other-empty</code></li>
<li>a page from any other Wikimedia project -&gt; <code>other-internal</code></li>
<li>Google -&gt; <code>other-google</code></li>
<li>Yahoo -&gt; <code>other-yahoo</code></li>
<li>Bing -&gt; <code>other-bing</code></li>
<li>Facebook -&gt; <code>other-facebook</code></li>
<li>Twitter -&gt; <code>other-twitter</code></li>
<li>anything else -&gt; <code>other-other</code></li>
</ul>
</blockquote>
</div>
<div class="cell markdown">
<p>In the second line of the file above, we can see there were 121 clicks from Google to the Wikipedia page on &quot;!!&quot; (double exclamation marks). People search for everything!</p>
<ul>
<li>prev_id = <em>(nothing)</em></li>
<li>curr_id = 3632887 <em>--&gt; (Wikipedia page ID)</em></li>
<li>n = 121 <em>(People clicked from Google to this page 121 times in this month.)</em></li>
<li>prev_title = other-google <em>(This data record is for referals from Google.)</em></li>
<li>curr_title = !! <em>(This Wikipedia page is about a double exclamation mark.)</em></li>
<li>type = other</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="create-a-dataframe-from-this-csv"><a class="header" href="#create-a-dataframe-from-this-csv">Create a DataFrame from this CSV</a></h3>
<ul>
<li>From the next Spark release - 2.0, CSV as a datasource will be part of Spark's standard release. But, we are using Spark 1.6</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Load the raw dataset stored as a CSV file
val clickstream = sqlContext
    .read
    .format(&quot;com.databricks.spark.csv&quot;)
    .options(Map(&quot;header&quot; -&gt; &quot;true&quot;, &quot;delimiter&quot; -&gt; &quot;\t&quot;, &quot;mode&quot; -&gt; &quot;PERMISSIVE&quot;, &quot;inferSchema&quot; -&gt; &quot;true&quot;))
    .load(&quot;dbfs:///databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed&quot;)
  
</code></pre>
</div>
<div class="cell markdown">
<h5 id="print-the-schema"><a class="header" href="#print-the-schema">Print the schema</a></h5>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">clickstream.printSchema
</code></pre>
</div>
<div class="cell markdown">
<h4 id="display-some-sample-data"><a class="header" href="#display-some-sample-data">Display some sample data</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(clickstream)
</code></pre>
</div>
<div class="cell markdown">
<p>Display is a utility provided by Databricks. If you are programming directly in Spark, use the show(numRows: Int) function of DataFrame</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">clickstream.show(5)
</code></pre>
</div>
<div class="cell markdown">
<h3 id="reading-from-disk-vs-memory"><a class="header" href="#reading-from-disk-vs-memory">Reading from disk vs memory</a></h3>
<p>The 1.2 GB Clickstream file is currently on S3, which means each time you scan through it, your Spark cluster has to read the 1.2 GB of data remotely over the network.</p>
</div>
<div class="cell markdown">
<p>Call the <code>count()</code> action to check how many rows are in the DataFrame and to see how long it takes to read the DataFrame from S3.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">clickstream.cache().count()
</code></pre>
</div>
<div class="cell markdown">
<ul>
<li>It took about several minutes to read the 1.2 GB file into your Spark cluster. The file has 22.5 million rows/lines.</li>
<li>Although we have called cache, remember that it is evaluated (cached) only when an action(count) is called</li>
</ul>
</div>
<div class="cell markdown">
<p>Now call count again to see how much faster it is to read from memory</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">clickstream.count()
</code></pre>
</div>
<div class="cell markdown">
<ul>
<li>Orders of magnitude faster!</li>
<li>If you are going to be using the same data source multiple times, it is better to cache it in memory</li>
</ul>
</div>
<div class="cell markdown">
<h3 id="what-are-the-top-10-articles-requested"><a class="header" href="#what-are-the-top-10-articles-requested">What are the top 10 articles requested?</a></h3>
<p>To do this we also need to order by the sum of column <code>n</code>, in descending order.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Type in your answer here...
display(clickstream
  .select(clickstream(&quot;curr_title&quot;), clickstream(&quot;n&quot;))
  .groupBy(&quot;curr_title&quot;)
  .sum()
  .orderBy($&quot;sum(n)&quot;.desc)
  .limit(10))
</code></pre>
</div>
<div class="cell markdown">
<h3 id="who-sent-the-most-traffic-to-wikipedia-in-feb-2015"><a class="header" href="#who-sent-the-most-traffic-to-wikipedia-in-feb-2015">Who sent the most traffic to Wikipedia in Feb 2015?</a></h3>
<p>In other words, who were the top referers to Wikipedia?</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(clickstream
  .select(clickstream(&quot;prev_title&quot;), clickstream(&quot;n&quot;))
  .groupBy(&quot;prev_title&quot;)
  .sum()
  .orderBy($&quot;sum(n)&quot;.desc)
  .limit(10))
</code></pre>
</div>
<div class="cell markdown">
<p>As expected, the top referer by a large margin is Google. Next comes refererless traffic (usually clients using HTTPS). The third largest sender of traffic to English Wikipedia are Wikipedia pages that are not in the main namespace (ns = 0) of English Wikipedia. Learn about the Wikipedia namespaces here: https://en.wikipedia.org/wiki/Wikipedia:Project_namespace</p>
<p>Also, note that Twitter sends 10x more requests to Wikipedia than Facebook.</p>
</div>
<div class="cell markdown">
<h3 id="what-were-the-top-5-trending-articles-people-from-twitter-were-looking-up-in-wikipedia"><a class="header" href="#what-were-the-top-5-trending-articles-people-from-twitter-were-looking-up-in-wikipedia">What were the top 5 trending articles people from Twitter were looking up in Wikipedia?</a></h3>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//Type in your answer here...
display(clickstream
  .select(clickstream(&quot;curr_title&quot;), clickstream(&quot;prev_title&quot;), clickstream(&quot;n&quot;))
  .filter(&quot;prev_title = 'other-twitter'&quot;)
  .groupBy(&quot;curr_title&quot;)
  .sum()
  .orderBy($&quot;sum(n)&quot;.desc)
  .limit(5))
</code></pre>
</div>
<div class="cell markdown">
<h4 id="what-percentage-of-page-visits-in-wikipedia-are-from-other-pages-in-wikipedia-itself"><a class="header" href="#what-percentage-of-page-visits-in-wikipedia-are-from-other-pages-in-wikipedia-itself">What percentage of page visits in Wikipedia are from other pages in Wikipedia itself?</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val allClicks = clickstream.selectExpr(&quot;sum(n)&quot;).first.getLong(0)
val referals = clickstream.
                filter(clickstream(&quot;prev_id&quot;).isNotNull).
                selectExpr(&quot;sum(n)&quot;).first.getLong(0)
(referals * 100.0) / allClicks
</code></pre>
</div>
<div class="cell markdown">
<h4 id="register-the-dataframe-to-perform-more-complex-queries"><a class="header" href="#register-the-dataframe-to-perform-more-complex-queries">Register the DataFrame to perform more complex queries</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">clickstream.createOrReplaceTempView(&quot;clicks&quot;)
</code></pre>
</div>
<div class="cell markdown">
<h4 id="which-wikipedia-pages-have-the-most-referrals-to-the-donald-trump-page"><a class="header" href="#which-wikipedia-pages-have-the-most-referrals-to-the-donald-trump-page">Which Wikipedia pages have the most referrals to the Donald Trump page?</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">SELECT *
FROM clicks
WHERE 
  curr_title = 'Donald_Trump' AND
  prev_id IS NOT NULL AND prev_title != 'Main_Page'
ORDER BY n DESC
LIMIT 20
</code></pre>
</div>
<div class="cell markdown">
<h4 id="youtry-top-referrers-to-other-2016-us-presidential-candidate-pages"><a class="header" href="#youtry-top-referrers-to-other-2016-us-presidential-candidate-pages">YouTry: Top referrers to other 2016 US presidential candidate pages</a></h4>
<p><code>'Donald_Trump', 'Bernie_Sanders', 'Hillary_Rodham_Clinton', 'Ted_Cruz'</code></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sql">-- YouTry 
---
-- fill in the right sql query here
</code></pre>
</div>
<div class="cell markdown">
<h4 id="load-a-visualization-library"><a class="header" href="#load-a-visualization-library">Load a visualization library</a></h4>
<p>This code is copied after doing a live google search (by Michael Armbrust at Spark Summit East February 2016 shared from <a href="https://twitter.com/michaelarmbrust/status/699969850475737088">https://twitter.com/michaelarmbrust/status/699969850475737088</a>). The <code>d3ivan</code> package is an updated version of the original package used by Michael Armbrust as it needed some TLC for Spark 2.2 on newer databricks notebook. These changes were kindly made by Ivan Sadikov from Middle Earth.</p>
<p>You need to hit the Play Button in next cell and 'Run Cell' exactly once.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">package d3ivan
// We use a package object so that we can define top level classes like Edge that need to be used in other cells

import org.apache.spark.sql._
import com.databricks.backend.daemon.driver.EnhancedRDDFunctions.displayHTML

case class Edge(src: String, dest: String, count: Long)

case class Node(name: String)
case class Link(source: Int, target: Int, value: Long)
case class Graph(nodes: Seq[Node], links: Seq[Link])

object graphs {
// val sqlContext = SQLContext.getOrCreate(org.apache.spark.SparkContext.getOrCreate())  /// fix
val sqlContext = SparkSession.builder().getOrCreate().sqlContext
import sqlContext.implicits._
  
def force(clicks: Dataset[Edge], height: Int = 100, width: Int = 960): Unit = {
  val data = clicks.collect()
  val nodes = (data.map(_.src) ++ data.map(_.dest)).map(_.replaceAll(&quot;_&quot;, &quot; &quot;)).toSet.toSeq.map(Node)
  val links = data.map { t =&gt;
    Link(nodes.indexWhere(_.name == t.src.replaceAll(&quot;_&quot;, &quot; &quot;)), nodes.indexWhere(_.name == t.dest.replaceAll(&quot;_&quot;, &quot; &quot;)), t.count / 20 + 1)
  }
  showGraph(height, width, Seq(Graph(nodes, links)).toDF().toJSON.first())
}

/**
 * Displays a force directed graph using d3
 * input: {&quot;nodes&quot;: [{&quot;name&quot;: &quot;...&quot;}], &quot;links&quot;: [{&quot;source&quot;: 1, &quot;target&quot;: 2, &quot;value&quot;: 0}]}
 */
def showGraph(height: Int, width: Int, graph: String): Unit = {

displayHTML(s&quot;&quot;&quot;
&lt;style&gt;

.node_circle {
  stroke: #777;
  stroke-width: 1.3px;
}

.node_label {
  pointer-events: none;
}

.link {
  stroke: #777;
  stroke-opacity: .2;
}

.node_count {
  stroke: #777;
  stroke-width: 1.0px;
  fill: #999;
}

text.legend {
  font-family: Verdana;
  font-size: 13px;
  fill: #000;
}

.node text {
  font-family: &quot;Helvetica Neue&quot;,&quot;Helvetica&quot;,&quot;Arial&quot;,sans-serif;
  font-size: 17px;
  font-weight: 200;
}

&lt;/style&gt;

&lt;div id=&quot;clicks-graph&quot;&gt;
&lt;script src=&quot;//d3js.org/d3.v3.min.js&quot;&gt;&lt;/script&gt;
&lt;script&gt;

var graph = $graph;

var width = $width,
    height = $height;

var color = d3.scale.category20();

var force = d3.layout.force()
    .charge(-700)
    .linkDistance(180)
    .size([width, height]);

var svg = d3.select(&quot;#clicks-graph&quot;).append(&quot;svg&quot;)
    .attr(&quot;width&quot;, width)
    .attr(&quot;height&quot;, height);
    
force
    .nodes(graph.nodes)
    .links(graph.links)
    .start();

var link = svg.selectAll(&quot;.link&quot;)
    .data(graph.links)
    .enter().append(&quot;line&quot;)
    .attr(&quot;class&quot;, &quot;link&quot;)
    .style(&quot;stroke-width&quot;, function(d) { return Math.sqrt(d.value); });

var node = svg.selectAll(&quot;.node&quot;)
    .data(graph.nodes)
    .enter().append(&quot;g&quot;)
    .attr(&quot;class&quot;, &quot;node&quot;)
    .call(force.drag);

node.append(&quot;circle&quot;)
    .attr(&quot;r&quot;, 10)
    .style(&quot;fill&quot;, function (d) {
    if (d.name.startsWith(&quot;other&quot;)) { return color(1); } else { return color(2); };
})

node.append(&quot;text&quot;)
      .attr(&quot;dx&quot;, 10)
      .attr(&quot;dy&quot;, &quot;.35em&quot;)
      .text(function(d) { return d.name });
      
//Now we are giving the SVGs co-ordinates - the force layout is generating the co-ordinates which this code is using to update the attributes of the SVG elements
force.on(&quot;tick&quot;, function () {
    link.attr(&quot;x1&quot;, function (d) {
        return d.source.x;
    })
        .attr(&quot;y1&quot;, function (d) {
        return d.source.y;
    })
        .attr(&quot;x2&quot;, function (d) {
        return d.target.x;
    })
        .attr(&quot;y2&quot;, function (d) {
        return d.target.y;
    });
    d3.selectAll(&quot;circle&quot;).attr(&quot;cx&quot;, function (d) {
        return d.x;
    })
        .attr(&quot;cy&quot;, function (d) {
        return d.y;
    });
    d3.selectAll(&quot;text&quot;).attr(&quot;x&quot;, function (d) {
        return d.x;
    })
        .attr(&quot;y&quot;, function (d) {
        return d.y;
    });
});
&lt;/script&gt;
&lt;/div&gt;
&quot;&quot;&quot;)
}
  
  def help() = {
displayHTML(&quot;&quot;&quot;
&lt;p&gt;
Produces a force-directed graph given a collection of edges of the following form:&lt;/br&gt;
&lt;tt&gt;&lt;font color=&quot;#a71d5d&quot;&gt;case class&lt;/font&gt; &lt;font color=&quot;#795da3&quot;&gt;Edge&lt;/font&gt;(&lt;font color=&quot;#ed6a43&quot;&gt;src&lt;/font&gt;: &lt;font color=&quot;#a71d5d&quot;&gt;String&lt;/font&gt;, &lt;font color=&quot;#ed6a43&quot;&gt;dest&lt;/font&gt;: &lt;font color=&quot;#a71d5d&quot;&gt;String&lt;/font&gt;, &lt;font color=&quot;#ed6a43&quot;&gt;count&lt;/font&gt;: &lt;font color=&quot;#a71d5d&quot;&gt;Long&lt;/font&gt;)&lt;/tt&gt;
&lt;/p&gt;
&lt;p&gt;Usage:&lt;br/&gt;
&lt;tt&gt;&lt;font color=&quot;#a71d5d&quot;&gt;import&lt;/font&gt; &lt;font color=&quot;#ed6a43&quot;&gt;d3._&lt;/font&gt;&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;font color=&quot;#795da3&quot;&gt;graphs.force&lt;/font&gt;(&lt;/br&gt;
&amp;nbsp;&amp;nbsp;&lt;font color=&quot;#ed6a43&quot;&gt;height&lt;/font&gt; = &lt;font color=&quot;#795da3&quot;&gt;500&lt;/font&gt;,&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&lt;font color=&quot;#ed6a43&quot;&gt;width&lt;/font&gt; = &lt;font color=&quot;#795da3&quot;&gt;500&lt;/font&gt;,&lt;br/&gt;
&amp;nbsp;&amp;nbsp;&lt;font color=&quot;#ed6a43&quot;&gt;clicks&lt;/font&gt;: &lt;font color=&quot;#795da3&quot;&gt;Dataset&lt;/font&gt;[&lt;font color=&quot;#795da3&quot;&gt;Edge&lt;/font&gt;])&lt;/tt&gt;
&lt;/p&gt;&quot;&quot;&quot;)
  }
}
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">d3ivan.graphs.help()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">d3ivan.graphs.force(
  height = 800,
  width = 800,
  clicks = sql(&quot;&quot;&quot;
    SELECT 
      prev_title AS src,
      curr_title AS dest,
      n AS count FROM clicks
    WHERE 
      curr_title IN ('Donald_Trump', 'Bernie_Sanders', 'Hillary_Rodham_Clinton', 'Ted_Cruz') AND
      prev_id IS NOT NULL AND prev_title != 'Main_Page'
    ORDER BY n DESC
    LIMIT 20&quot;&quot;&quot;).as[d3ivan.Edge])
</code></pre>
</div>
<div class="cell markdown">
<p>What we have done above is essentially pass the output of an SQL query into a D3 visualizer via javascript. Don't worry about all the details. The main idea here is that SQL and interactive visualizations usually come together in a proper data exploratory tool and the above steps are minimal excursions into how to do it in a simple way from within a notebook environment like databricks. Python and R have many plotting libraries and we can always write the dataframe to parquet and load it into pySpark or SparkR to leverage those languages. But D3 is a nice solutions also especially if you want somethinf customized for your queries.</p>
</div>
<div class="cell markdown">
<h3 id="convert-raw-data-to-parquet"><a class="header" href="#convert-raw-data-to-parquet">Convert raw data to parquet</a></h3>
<p><strong>Recall:</strong></p>
<p><a href="https://parquet.apache.org/">Apache Parquet</a> is a <a href="http://en.wikipedia.org/wiki/Column-oriented_DBMS">columnar storage</a> format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language. It is a more efficient way to store data frames.</p>
<ul>
<li>To understand the ideas read <a href="http://research.google.com/pubs/pub36632.html">Dremel: Interactive Analysis of Web-Scale Datasets, Sergey Melnik, Andrey Gubarev, Jing Jing Long, Geoffrey Romer, Shiva Shivakumar, Matt Tolton and Theo Vassilakis,Proc. of the 36th Int'l Conf on Very Large Data Bases (2010), pp. 330-339</a>, whose Abstract is as follows:
<ul>
<li>Dremel is a scalable, interactive ad-hoc query system for analysis of read-only nested data. By combining multi-level execution trees and columnar data layouts it is <strong>capable of running aggregation queries over trillion-row tables in seconds</strong>. The system <strong>scales to thousands of CPUs and petabytes of data, and has thousands of users at Google</strong>. In this paper, we describe the architecture and implementation of Dremel, and explain how it complements MapReduce-based computing. We present a novel columnar storage representation for nested records and discuss experiments on few-thousand node instances of the system.</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://parquet.apache.org/documentation/latest/&quot;,350))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Convert the DatFrame to a more efficent format to speed up our analysis
clickstream.
  write.
  mode(SaveMode.Overwrite).
  parquet(&quot;/datasets/wiki-clickstream&quot;) 
</code></pre>
</div>
<div class="cell markdown">
<h4 id="load-parquet-file-efficiently-and-quickly-into-a-dataframe"><a class="header" href="#load-parquet-file-efficiently-and-quickly-into-a-dataframe">Load parquet file efficiently and quickly into a DataFrame</a></h4>
<p>Now we can simply load from this parquet file next time instead of creating the RDD from the text file (much slower).</p>
<p>Also using parquet files to store DataFrames allows us to go between languages quickly in a a scalable manner.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val clicks = sqlContext.read.parquet(&quot;/datasets/wiki-clickstream&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">clicks.printSchema
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(clicks)  // let's display this DataFrame
</code></pre>
</div>
<div class="cell markdown">
<h5 id="dataframe-in-python"><a class="header" href="#dataframe-in-python">DataFrame in python</a></h5>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">clicksPy = sqlContext.read.parquet(&quot;/datasets/wiki-clickstream&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># in Python you need to put the object int its own line like this to get the type information
clicksPy 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">clicksPy.show()
</code></pre>
</div>
<div class="cell markdown">
<p>Now you can continue from the original python notebook tweeted by Michael.</p>
<p>Recall from the beginning of this notebook that this python databricks notebook was used in the talk by Michael Armbrust at Spark Summit East February 2016 shared from <a href="https://twitter.com/michaelarmbrust/status/699969850475737088">https://twitter.com/michaelarmbrust/status/699969850475737088</a></p>
<p>(watch now, if you haven't already!)</p>
<p><a href="https://www.youtube.com/watch?v=35Y-rqSMCCA"><img src="http://img.youtube.com/vi/35Y-rqSMCCA/0.jpg" alt="Michael Armbrust Spark Summit East" /></a></p>
</div>
<div class="cell markdown">
<p><strong>You Try!</strong></p>
<p>Try to laoad a DataFrame in R from the parquet file just as we did for python. Read the docs in databricks guide first:</p>
<ul>
<li><a href="https://docs.databricks.com/spark/latest/sparkr/overview.html">https://docs.databricks.com/spark/latest/sparkr/overview.html</a></li>
</ul>
<p>And see the <code>R</code> example in the Programming Guide:</p>
<ul>
<li><a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files">https://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files</a>.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-r">library(SparkR)

# just a quick test
df &lt;- createDataFrame(faithful)
head(df)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-r"># Read in the Parquet file created above. Parquet files are self-describing so the schema is preserved.
# The result of loading a parquet file is also a DataFrame.
clicksR &lt;- read.df(&quot;/datasets/wiki-clickstream&quot;, source = &quot;parquet&quot;)
clicksR # in R you need to put the object int its own line like this to get the type information
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-r">head(clicksR)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-r">display(clicksR)
</code></pre>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="old-bailey-online-data-analysis-in-apache-spark"><a class="header" href="#old-bailey-online-data-analysis-in-apache-spark">Old Bailey Online Data Analysis in Apache Spark</a></h1>
<p>2016, by Raaz Sainudiin and James Smithies is licensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.</p>
<p><strong>Old Bailey, London's Central Criminal Court, 1674 to 1913</strong></p>
<ul>
<li>with Full XML Data for another great project. This is a starting point for ETL of Old Bailey Online Data from <a href="http://lamastex.org/datasets/public/OldBailey/index.html">http://lamastex.org/datasets/public/OldBailey/index.html</a>.</li>
</ul>
<p>This work merely builds on <a href="https://www.oldbaileyonline.org/">Old Bailey Online by Clive Emsley, Tim Hitchcock and Robert Shoemaker</a> that is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. Permissions beyond the scope of this license may be available at https://www.oldbaileyonline.org/static/Legal-info.jsp.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//This allows easy embedding of publicly available information into any other notebook
//when viewing in git-book just ignore this block - you may have to manually chase the URL in frameIt(&quot;URL&quot;).
//Example usage:
// displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;,250))
def frameIt( u:String, h:Int ) : String = {
      &quot;&quot;&quot;&lt;iframe 
 src=&quot;&quot;&quot;&quot;+ u+&quot;&quot;&quot;&quot;
 width=&quot;95%&quot; height=&quot;&quot;&quot;&quot; + h + &quot;&quot;&quot;&quot;
 sandbox&gt;
  &lt;p&gt;
    &lt;a href=&quot;http://spark.apache.org/docs/latest/index.html&quot;&gt;
      Fallback link for browsers that, unlikely, don't support frames
    &lt;/a&gt;
  &lt;/p&gt;
&lt;/iframe&gt;&quot;&quot;&quot;
   }
displayHTML(frameIt(&quot;https://www.oldbaileyonline.org/&quot;, 450))
</code></pre>
</div>
<div class="cell markdown">
<h3 id="this-exciting-dataset-is-here-for-a-course-project-in-digital-humanities"><a class="header" href="#this-exciting-dataset-is-here-for-a-course-project-in-digital-humanities">This exciting dataset is here for a course project in digital humanities</a></h3>
<h4 id="to-understand-the-extraction-job-we-are-about-to-do-here"><a class="header" href="#to-understand-the-extraction-job-we-are-about-to-do-here">To understand the extraction job we are about to do here:</a></h4>
<ul>
<li>see <a href="http://lamastex.org/preprints/20150828_civilizingProcOBO.pdf">Jasper Mackenzie, Raazesh Sainudiin, James Smithies and Heather Wolffram, A nonparametric view of the civilizing process in London's Old Bailey, Research Report UCDMS2015/1, 32 pages, 2015</a>.</li>
</ul>
</div>
<div class="cell markdown">
<p>The data is already loaded in dbfs (see dowloading and loading section below for these details).</p>
</div>
<div class="cell markdown">
<h1 id="analysing-the-full-old-bailey-online-sessions-papers-dataset"><a class="header" href="#analysing-the-full-old-bailey-online-sessions-papers-dataset">Analysing the Full Old Bailey Online Sessions Papers Dataset</a></h1>
<p>First <strong>Step 0: Dowloading and Loading Data (The Full Dataset)</strong> below should have been done on the shard.
This currently cannot be done in Community Edition as the dataset is not loaded into the dbfs available in CE yet. But the datset is in the academic shard and this is a walkthorugh of the Old Bailey Online data in the academic shard.</p>
<p>Let's first check that the datasets are there in the distributed file system.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/datasets/obo/tei/&quot;)) // full data if you have it - not in CE!!
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/datasets/obo/tei/ordinarysAccounts&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/datasets/obo/tei/sessionsPapers&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/XML&quot;, 450))
</code></pre>
</div>
<div class="cell markdown">
<h2 id="step-1-exploring-data-first-xml-parsing-in-scala"><a class="header" href="#step-1-exploring-data-first-xml-parsing-in-scala">Step 1: Exploring data first: xml parsing in scala</a></h2>
<p>But, first let's understand the data and its structure.</p>
<p><strong>Step 0: Dowloading and Loading Data (The Full Dataset)</strong> should have been done already with data in dbfs alread.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val raw = sc.wholeTextFiles(&quot;dbfs:/datasets/obo/tei/ordinarysAccounts/OA17070912.xml&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val raw = sc.wholeTextFiles(&quot;dbfs:/datasets/obo/tei/sessionsPapers/17280717.xml&quot;) // has data on crimes and punishments
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//val oboTest = sc.wholeTextFiles(&quot;dbfs:/datasets/obo/tei/ordinaryAccounts/OA1693072*.xml&quot;)
val xml = raw.map( x =&gt; x._2 )
val x = xml.take(1)(0) // getting content of xml file as a string
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val elem = scala.xml.XML.loadString(x)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">elem
</code></pre>
</div>
<div class="cell markdown">
<h2 id="quick-preparation"><a class="header" href="#quick-preparation">Quick Preparation</a></h2>
<h4 id="some-examples-to-learn-xml-and-scala-in-a-hurry"><a class="header" href="#some-examples-to-learn-xml-and-scala-in-a-hurry">Some examples to learn xml and scala in a hurry</a></h4>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val p = new scala.xml.PrettyPrinter(80, 2)

p.format(elem)
</code></pre>
</div>
<div class="cell markdown">
<h3 id="better-examples"><a class="header" href="#better-examples">Better examples:</a></h3>
<p>http://alvinalexander.com/scala/how-to-extract-data-from-xml-nodes-in-scala</p>
<p>http://alvinalexander.com/scala/scala-xml-xpath-example</p>
<h4 id="more-advanced-topics"><a class="header" href="#more-advanced-topics">More advanced topics:</a></h4>
<p>https://alvinalexander.com/scala/serializing-deserializing-xml-scala-classes</p>
<h4 id="xml-to-json-if-you-want-to-go-this-route"><a class="header" href="#xml-to-json-if-you-want-to-go-this-route">XML to JSON, if you want to go this route:</a></h4>
<p>https://stackoverflow.com/questions/9516973/xml-to-json-with-scala</p>
</div>
<div class="cell markdown">
<h2 id="our-parsing-problem"><a class="header" href="#our-parsing-problem">Our Parsing Problem</a></h2>
<p>Let's dive deep on this data right away. See links above to learn xml more systematically to be able to parse other subsets of the data for your own project.</p>
<p>For now, we will jump in to parse the input data of counts used in <a href="http://lamastex.org/preprints/20150828_civilizingProcOBO.pdf">Jasper Mackenzie, Raazesh Sainudiin, James Smithies and Heather Wolffram, A nonparametric view of the civilizing process in London's Old Bailey, Research Report UCDMS2015/1, 32 pages, 2015</a>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">(elem \\ &quot;div0&quot;).map(Node =&gt; (Node \ &quot;@type&quot;).text) // types of div0 node, the singleton root node for the file
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">(elem \\ &quot;div1&quot;).map(Node =&gt; (Node \ &quot;@type&quot;).text) // types of div1 node
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">(elem \\ &quot;div1&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">(elem \\ &quot;div1&quot;).filter(Node =&gt; ((Node \ &quot;@type&quot;).text == &quot;trialAccount&quot;))
                 .map(Node =&gt; (Node \ &quot;@type&quot;, Node \ &quot;@id&quot; ))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val trials = (elem \\ &quot;div1&quot;).filter(Node =&gt; ((Node \ &quot;@type&quot;).text == &quot;trialAccount&quot;))
                 .map(Node =&gt; (Node \ &quot;@type&quot;, Node \ &quot;@id&quot;, (Node \\ &quot;rs&quot; \\ &quot;interp&quot;).map( n =&gt; ((n \\ &quot;@type&quot;).text, (n \\ &quot;@value&quot;).text ))))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wantedFields = Seq(&quot;verdictCategory&quot;,&quot;punishmentCategory&quot;,&quot;offenceCategory&quot;).toSet
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val trials = (elem \\ &quot;div1&quot;).filter(Node =&gt; ((Node \ &quot;@type&quot;).text == &quot;trialAccount&quot;))
                 .map(Node =&gt; ((Node \ &quot;@type&quot;).text, (Node \ &quot;@id&quot;).text, (Node \\ &quot;rs&quot; \\ &quot;interp&quot;)
                                                               .filter(n =&gt; wantedFields.contains( (n \\ &quot;@type&quot;).text))
                                                               .map( n =&gt; ((n \\ &quot;@type&quot;).text, (n \\ &quot;@value&quot;).text ))))
</code></pre>
</div>
<div class="cell markdown">
<p>Since there can be more than one defendant in a trial, we need to reduce by key as follows.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def reduceByKey(collection: Traversable[Tuple2[String, Int]]) = {    
    collection
      .groupBy(_._1)
      .map { case (group: String, traversable) =&gt; traversable.reduce{(a,b) =&gt; (a._1, a._2 + b._2)} }
  }
</code></pre>
</div>
<div class="cell markdown">
<p>Let's process the coarsest data on the trial as json strings.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val trials = (elem \\ &quot;div1&quot;).filter(Node =&gt; ((Node \ &quot;@type&quot;).text == &quot;trialAccount&quot;))
                 .map(Node =&gt; {val trialId = (Node \ &quot;@id&quot;).text;
                               val trialInterps = (Node \\ &quot;rs&quot; \\ &quot;interp&quot;)
                                                                 .filter(n =&gt; wantedFields.contains( (n \\ &quot;@type&quot;).text))
                                                                 //.map( n =&gt; ((n \\ &quot;@type&quot;).text, (n \\ &quot;@value&quot;).text ));
                                                                 .map( n =&gt; ((n \\ &quot;@value&quot;).text , 1 ));
                               val trialCounts = reduceByKey(trialInterps).toMap;
                               //(trialId, trialInterps, trialCounts)
                               scala.util.parsing.json.JSONObject(trialCounts updated (&quot;id&quot;, trialId))
                              })
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">trials.foreach(println)
</code></pre>
</div>
<div class="cell markdown">
<h2 id="step-2-extract-transform-and-load-xml-files-to-get-dataframe-of-counts"><a class="header" href="#step-2-extract-transform-and-load-xml-files-to-get-dataframe-of-counts">Step 2: Extract, Transform and Load XML files to get DataFrame of counts</a></h2>
<p>We have played enough (see <strong>Step 1: Exploring data first: xml parsing in scala</strong> above first) to understand what to do now with our xml data in order to get it converted to counts of crimes, verdicts and punishments.</p>
<p>Let's parse the xml files and turn into Dataframe in one block.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val rawWTF = sc.wholeTextFiles(&quot;dbfs:/datasets/obo/tei/sessionsPapers/*.xml&quot;) // has all data on crimes and punishments
val raw = rawWTF.map( x =&gt; x._2 )
val trials = raw.flatMap( x =&gt; { 
                       val elem = scala.xml.XML.loadString(x);
                       val outJson = (elem \\ &quot;div1&quot;).filter(Node =&gt; ((Node \ &quot;@type&quot;).text == &quot;trialAccount&quot;))
                           .map(Node =&gt; {val trialId = (Node \ &quot;@id&quot;).text;
                               val trialInterps = (Node \\ &quot;rs&quot; \\ &quot;interp&quot;)
                                                                 .filter(n =&gt; wantedFields.contains( (n \\ &quot;@type&quot;).text))
                                                                 //.map( n =&gt; ((n \\ &quot;@type&quot;).text, (n \\ &quot;@value&quot;).text ));
                                                                 .map( n =&gt; ((n \\ &quot;@value&quot;).text , 1 ));
                               val trialCounts = reduceByKey(trialInterps).toMap;
                               //(trialId, trialInterps, trialCounts)
                               scala.util.parsing.json.JSONObject(trialCounts updated (&quot;id&quot;, trialId)).toString()
                              })
  outJson
})
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.rm(&quot;dbfs:/datasets/obo/processed/trialCounts&quot;,recurse=true) // let's remove the files from the previous analysis
trials.saveAsTextFile(&quot;dbfs:/datasets/obo/processed/trialCounts&quot;) // now let's save the trial counts - aboout 220 seconds to pars all data and get counts
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/datasets/obo/processed/trialCounts&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val trialCountsDF = sqlContext.read.json(&quot;dbfs:/datasets/obo/processed/trialCounts&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">trialCountsDF.printSchema
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">trialCountsDF.count // total number of trials = 197751
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(trialCountsDF)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val trDF = trialCountsDF.na.fill(0) // filling nulls with 0
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(trDF)
</code></pre>
</div>
<div class="cell markdown">
<p>This is already available as the following csv file:</p>
<ul>
<li><a href="http://lamastex.org/datasets/public/OldBailey/oboOffencePunnishmentCountsFrom-sds-2-2-ApacheSparkScalaProcessingOfOBOXMLDoneByRaazOn20180405.csv">http://lamastex.org/datasets/public/OldBailey/oboOffencePunnishmentCountsFrom-sds-2-2-ApacheSparkScalaProcessingOfOBOXMLDoneByRaazOn20180405.csv</a></li>
</ul>
<p>Please cite this URL if you use this data or the Apache licensed codes in the databricks notebook above for your own non-commerical analysis:</p>
<ul>
<li><a href="http://lamastex.org/datasets/public/OldBailey/">http://lamastex.org/datasets/public/OldBailey/</a></li>
</ul>
<p>Raazesh Sainudiin generated this header <strong>Old bailey Processing in Apache Spark</strong> on Thu Apr 5 18:22:43 CEST 2018 in Uppsala, Sweden.</p>
</div>
<div class="cell markdown">
<h2 id="step-0-dowloading-and-loading-data-the-full-dataset"><a class="header" href="#step-0-dowloading-and-loading-data-the-full-dataset">Step 0: Dowloading and Loading Data (The Full Dataset)</a></h2>
<p>First we will be downloading data from <a href="http://lamastex.org/datasets/public/OldBailey/index.html">http://lamastex.org/datasets/public/OldBailey/index.html</a>.</p>
<p>The steps below need to be done once for a give shard!</p>
<p>You can download the tiny dataset <code>obo-tiny/OB-tiny_tei_7-2_CC-BY-NC.zip</code> <strong>to save time and space in db CE</strong></p>
<p><strong>Optional TODOs:</strong></p>
<ul>
<li>one could just read the zip files directly (see week 10 on Beijing taxi trajectories example from the scalable-data-science course in 2016 or read 'importing zip files' in the Guide).</li>
<li>one could just download from s3 directly</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh"># if you want to download the tiny dataset
wget https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/datasets/obo-tiny/OB-tiny_tei_7-2_CC-BY-NC.zip
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh"># this is the full dataset - necessary for a project on this dataset
wget http://lamastex.org/datasets/public/OldBailey/OB_tei_7-2_CC-BY-NC.zip
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">pwd &amp;&amp; ls -al
</code></pre>
</div>
<div class="cell markdown">
<p>Make sure you comment/uncomment the right files depending on wheter you have downloaded the tiny dataset or the big one.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">unzip OB-tiny_tei_7-2_CC-BY-NC.zip
#unzip OB_tei_7-2_CC-BY-NC.zip
</code></pre>
</div>
<div class="cell markdown">
<p>Let's put the files in dbfs.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.mkdirs(&quot;dbfs:/datasets/obo/tei&quot;) //need not be done again!
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//dbutils.fs.rm(&quot;dbfs:/datasets/obo/tei&quot;,true)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">ls 
#ls obo-tiny/tei
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala"> dbutils.fs.cp(&quot;file:/databricks/driver/obo-tiny/tei&quot;, &quot;dbfs:/datasets/obo/tei/&quot;,recurse=true) // already done and it takes 1500 seconds - a while!
 //dbutils.fs.cp(&quot;file:/databricks/driver/tei&quot;, &quot;dbfs:/datasets/obo/tei/&quot;,recurse=true) // already done and it takes 19 minutes - a while!
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//dbutils.fs.rm(&quot;dbfs:/datasets/tweets&quot;,true) // remove files to make room for the OBO dataset
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/datasets/&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/datasets/obo/tei/&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">util.Properties.versionString // check scala version
</code></pre>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="piped-rdds-and-bayesian-ab-testing-1"><a class="header" href="#piped-rdds-and-bayesian-ab-testing-1">Piped RDDs and Bayesian AB Testing</a></h1>
<p><strong>Continued with Application to Old Bailey Online Data</strong></p>
</div>
<div class="cell markdown">
<p>This is a recall/repeat of <code>006a_PipedRDD</code>. After the recall, we continue with applying Bayesian A/B Testing to the data extracted from Old Bailey Online counts of crimes and punishments.</p>
</div>
<div class="cell markdown">
<p>Here we will first take excerpts with minor modifications from the end of <strong>Chapter 12. Resilient Distributed Datasets (RDDs)</strong> of <em>Spark: The Definitive Guide</em>:</p>
<ul>
<li>https://learning.oreilly.com/library/view/spark-the-definitive/9781491912201/ch12.html</li>
</ul>
<p>Next, we will do Bayesian AB Testing using PipedRDDs.</p>
</div>
<div class="cell markdown">
<p>First, we create the toy RDDs as in <em>The Definitive Guide</em>:</p>
<blockquote>
<h1 id="from-a-local-collection"><a class="header" href="#from-a-local-collection">From a Local Collection</a></h1>
</blockquote>
<p>To create an RDD from a collection, you will need to use the parallelize method on a SparkContext (within a SparkSession). This turns a single node collection into a parallel collection. When creating this parallel collection, you can also explicitly state the number of partitions into which you would like to distribute this array. In this case, we are creating two partitions:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// in Scala
val myCollection = &quot;Spark The Definitive Guide : Big Data Processing Made Simple&quot;  .split(&quot; &quot;)
val words = spark.sparkContext.parallelize(myCollection, 2)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># in Python
myCollection = &quot;Spark The Definitive Guide : Big Data Processing Made Simple&quot;\
  .split(&quot; &quot;)
words = spark.sparkContext.parallelize(myCollection, 2)
words
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="glom"><a class="header" href="#glom">glom</a></h1>
</blockquote>
<blockquote>
<p><code>glom</code> is an interesting function that takes every partition in your dataset and converts them to arrays. This can be useful if you’re going to collect the data to the driver and want to have an array for each partition. However, this can cause serious stability issues because if you have large partitions or a large number of partitions, it’s simple to crash the driver.</p>
</blockquote>
</div>
<div class="cell markdown">
<p>Let's use <code>glom</code> to see how our <code>words</code> are distributed among the two partitions we used explicitly.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">words.glom.collect 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">words.glom().collect()
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="checkpointing"><a class="header" href="#checkpointing">Checkpointing</a></h1>
<p>One feature not available in the DataFrame API is the concept of checkpointing. Checkpointing is the act of saving an RDD to disk so that future references to this RDD point to those intermediate partitions on disk rather than recomputing the RDD from its original source. This is similar to caching except that it’s not stored in memory, only disk. This can be helpful when performing iterative computation, similar to the use cases for caching:</p>
</blockquote>
<p>Let's create a directory in <code>dbfs:///</code> for checkpointing of RDDs in the sequel. The following <code>%fs mkdirs /path_to_dir</code> is a shortcut to create a directory in <code>dbfs:///</code></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">mkdirs /datasets/ScaDaMaLe/checkpointing/
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">spark.sparkContext.setCheckpointDir(&quot;dbfs:///datasets/ScaDaMaLe/checkpointing&quot;)
words.checkpoint()
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p>Now, when we reference this RDD, it will derive from the checkpoint instead of the source data. This can be a helpful optimization.</p>
</blockquote>
</div>
<div class="cell markdown">
<h2 id="youtry-2"><a class="header" href="#youtry-2">YouTry</a></h2>
<p>Just some more words in <code>haha_words</code> with <code>\n</code>, the End-Of-Line (EOL) characters, in-place.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val haha_words = sc.parallelize(Seq(&quot;ha\nha&quot;, &quot;he\nhe\nhe&quot;, &quot;ho\nho\nho\nho&quot;),3)
</code></pre>
</div>
<div class="cell markdown">
<p>Let's use <code>glom</code> to see how our <code>haha_words</code> are distributed among the partitions</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">haha_words.glom.collect
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="pipe-rdds-to-system-commands-1"><a class="header" href="#pipe-rdds-to-system-commands-1">Pipe RDDs to System Commands</a></h1>
</blockquote>
<blockquote>
<p>The pipe method is probably one of Spark’s more interesting methods. With pipe, you can return an RDD created by piping elements to a forked external process. The resulting RDD is computed by executing the given process once per partition. All elements of each input partition are written to a process’s stdin as lines of input separated by a newline. The resulting partition consists of the process’s stdout output, with each line of stdout resulting in one element of the output partition. A process is invoked even for empty partitions.</p>
</blockquote>
<blockquote>
<p>The print behavior can be customized by providing two functions.</p>
</blockquote>
<p>We can use a simple example and pipe each partition to the command wc. Each row will be passed in as a new line, so if we perform a line count, we will get the number of lines, one per partition:</p>
</div>
<div class="cell markdown">
<p>The following produces a <code>PipedRDD</code>:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val wc_l_PipedRDD = words.pipe(&quot;wc -l&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">wc_l_PipedRDD = words.pipe(&quot;wc -l&quot;)
wc_l_PipedRDD
</code></pre>
</div>
<div class="cell markdown">
<p>Now, we take an action via <code>collect</code> to bring the results to the Driver.</p>
<p>NOTE: Be careful what you collect! You can always write the output to parquet of binary files in <code>dbfs:///</code> if the returned output is large.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">wc_l_PipedRDD.collect
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">wc_l_PipedRDD.collect()
</code></pre>
</div>
<div class="cell markdown">
<p>In this case, we got the number of lines returned by <code>wc -l</code> per partition.</p>
</div>
<div class="cell markdown">
<h2 id="youtry-3"><a class="header" href="#youtry-3">YouTry</a></h2>
<p>Try to make sense of the next few cells where we do NOT specifiy the number of partitions explicitly and let Spark decide on the number of partitions automatically.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val haha_words = sc.parallelize(Seq(&quot;ha\nha&quot;, &quot;he\nhe\nhe&quot;, &quot;ho\nho\nho\nho&quot;),3)
haha_words.glom.collect
val wc_l_PipedRDD_haha_words = haha_words.pipe(&quot;wc -l&quot;)
wc_l_PipedRDD_haha_words.collect()
</code></pre>
</div>
<div class="cell markdown">
<p>Do you understand why the above <code>collect</code> statement returns what it does?</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val haha_words_again = sc.parallelize(Seq(&quot;ha\nha&quot;, &quot;he\nhe\nhe&quot;, &quot;ho\nho\nho\nho&quot;))
haha_words_again.glom.collect
val wc_l_PipedRDD_haha_words_again = haha_words_again.pipe(&quot;wc -l&quot;)
wc_l_PipedRDD_haha_words_again.collect()
</code></pre>
</div>
<div class="cell markdown">
<p>Did you understand why some of the results are <code>0</code> in the last <code>collect</code> statement?</p>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="mappartitions-1"><a class="header" href="#mappartitions-1">mapPartitions</a></h1>
</blockquote>
<blockquote>
<p>The previous command revealed that Spark operates on a per-partition basis when it comes to actually executing code. You also might have noticed earlier that the return signature of a map function on an RDD is actually <code>MapPartitionsRDD</code>.</p>
</blockquote>
<p>Or <code>ParallelCollectionRDD</code> in our case.</p>
<blockquote>
<p>This is because map is just a row-wise alias for <code>mapPartitions</code>, which makes it possible for you to map an individual partition (represented as an iterator). That’s because physically on the cluster we operate on each partition individually (and not a specific row). A simple example creates the value “1” for every partition in our data, and the sum of the following expression will count the number of partitions we have:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// in Scala
words.mapPartitions(part =&gt; Iterator[Int](1)).sum() // 2.0
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># in Python
words.mapPartitions(lambda part: [1]).sum() # 2
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p>Naturally, this means that we operate on a per-partition basis and therefore it allows us to perform an operation on that <em>entire</em> partition. This is valuable for performing something on an entire subdataset of your RDD. You can gather all values of a partition class or group into one partition and then operate on that entire group using arbitrary functions and controls. An example use case of this would be that you could pipe this through some custom machine learning algorithm and train an individual model for that company’s portion of the dataset. A Facebook engineer has an interesting demonstration of their particular implementation of the pipe operator with a similar use case demonstrated at <a href="https://spark-summit.org/east-2017/events/experiences-with-sparks-rdd-apis-for-complex-custom-applications/">Spark Summit East 2017</a>.</p>
</blockquote>
<blockquote>
<p>Other functions similar to <code>mapPartitions</code> include <code>mapPartitionsWithIndex</code>. With this you specify a function that accepts an index (within the partition) and an iterator that goes through all items within the partition. The partition index is the partition number in your RDD, which identifies where each record in our dataset sits (and potentially allows you to debug). You might use this to test whether your map functions are behaving correctly:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// in Scala
def indexedFunc(partitionIndex:Int, withinPartIterator: Iterator[String]) = {  withinPartIterator.toList.map(    
  value =&gt; s&quot;Partition: $partitionIndex =&gt; $value&quot;).iterator
                                                                            }
words.mapPartitionsWithIndex(indexedFunc).collect()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># in Python
def indexedFunc(partitionIndex, withinPartIterator):  
  return [&quot;partition: {} =&gt; {}&quot;.format(partitionIndex,    x) for x in withinPartIterator]
words.mapPartitionsWithIndex(indexedFunc).collect()
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<h1 id="foreachpartition-1"><a class="header" href="#foreachpartition-1">foreachPartition</a></h1>
</blockquote>
<blockquote>
<p>Although <code>mapPartitions</code> needs a return value to work properly, this next function does not. <code>foreachPartition</code> simply iterates over all the partitions of the data. The difference is that the function has no return value. This makes it great for doing something with each partition like writing it out to a database. In fact, this is how many data source connectors are written. You can create</p>
</blockquote>
<p>your</p>
<blockquote>
<p>own text file source if you want by specifying outputs to the temp directory with a random ID:</p>
</blockquote>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">words.foreachPartition { iter =&gt;  
  import java.io._  
  import scala.util.Random  
  val randomFileName = new Random().nextInt()  
  val pw = new PrintWriter(new File(s&quot;/tmp/random-file-${randomFileName}.txt&quot;))  
  while (iter.hasNext) {
    pw.write(iter.next())  
  }  
  pw.close()
}
</code></pre>
</div>
<div class="cell markdown">
<blockquote>
<p>You’ll find these two files if you scan your /tmp directory.</p>
</blockquote>
<p>You need to scan for the file across all the nodes. As the file may not be in the Driver node's <code>/tmp/</code> directory but in those of the executors that hosted the partition.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">pwd
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">ls /tmp/random-file-*.txt
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div class="cell markdown">
<h1 id="numerically-rigorous-bayesian-ab-testing-1"><a class="header" href="#numerically-rigorous-bayesian-ab-testing-1">Numerically Rigorous Bayesian AB Testing</a></h1>
<p>This is an example of Bayesian AB Testing with computer-aided proofs for the posterior samples.</p>
<p>The main learning goal for you is to use pipedRDDs to distribute, in an embarassingly paralle way, across all the worker nodes in the Spark cluster an executible <code>IsIt1or2Coins</code>.</p>
<h3 id="what-does-isit1or2coins-do-1"><a class="header" href="#what-does-isit1or2coins-do-1">What does <code>IsIt1or2Coins</code> do?</a></h3>
<p>At a very high-level, to understand what <code>IsIt1or2Coins</code> does, imagine the following simple experiment.</p>
<p>We are given</p>
<ul>
<li>the number of heads that result from a first sequence of independent and identical tosses of a coin and then</li>
<li>we are given the number of heads that result from a second sequence of independent and identical tosses of a coin</li>
</ul>
<p>Our decision problem is to do help shed light on whether both sequence of tosses came from the same coin or not (whatever the bias may be).</p>
<p><code>IsIt1or2Coins</code> tries to help us decide if the two sequence of coin-tosses are based on one coin with an unknown bias or two coins with different biases.</p>
<p>If you are curious about details feel free to see:</p>
<ul>
<li>Exact Bayesian A/B testing using distributed fault-tolerant Moore rejection sampler, Benny Avelin and Raazesh Sainudiin, Extended Abstract, 2 pages, 2018 <a href="http://lamastex.org/preprints/20180507_ABTestingViaDistributedMRS.pdf">(PDF 104KB)</a>.</li>
<li>which builds on: An auto-validating, trans-dimensional, universal rejection sampler for locally Lipschitz arithmetical expressions, Raazesh Sainudiin and Thomas York, <a href="http://interval.louisiana.edu/reliable-computing-journal/volume-18/reliable-computing-18-pp-015-054.pdf">Reliable Computing, vol.18, pp.15-54, 2013</a> (<a href="http://lamastex.org/preprints/avs_rc_2013.pdf">preprint: PDF 2612KB</a>)</li>
</ul>
<p><strong>See first about <code>PipedRDDs</code> excerpt from <em>Spark The Definitive Guide</em> earlier.</strong></p>
<h3 id="getting-the-executible-isit1or2coins-into-our-spark-cluster-1"><a class="header" href="#getting-the-executible-isit1or2coins-into-our-spark-cluster-1">Getting the executible <code>IsIt1or2Coins</code> into our Spark Cluster</a></h3>
<p><strong>This has already been done in the project-shard. You need not do it again for this executible!</strong></p>
<p>You need to upload the C++ executible <code>IsIt1or2Coins</code> from: - https://github.com/lamastex/mrs2</p>
<p>Here, suppose you have an executible for linux x86 64 bit processor with all dependencies pre-compiled into one executibe.</p>
<p>Say this executible is <code>IsIt10r2Coins</code>.</p>
<p>This executible comes from the following dockerised build:</p>
<ul>
<li>https://github.com/lamastex/mrs2/tree/master/docker</li>
<li>by statically compiling inside the docerised environment for mrs2:
<ul>
<li>https://github.com/lamastex/mrs2/tree/master/mrs-2.0/examples/MooreRejSam/IsIt1or2Coins</li>
</ul>
</li>
</ul>
<p>You can replace the executible with any other executible with appropriate I/O to it.</p>
<p>Then you upload the executible to databricks' <code>FileStore</code>.</p>
</div>
<div class="cell markdown">
<p>Just note the path to the file and DO NOT click <code>Create Table</code> or other buttons!</p>
<p><img src="https://raw.githubusercontent.com/lamastex/scalable-data-science/master/images/2020/ScaDaMaLe/screenShotOfUploadingStaticExecutibleIsIt1or2CoinsViaFileStore.png" alt="creenShotOfUploadingStaticExecutibleIsIt1or2CoinsViaFileStore" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">ls &quot;/FileStore/tables/IsIt1or2Coins&quot;
</code></pre>
</div>
<div class="cell markdown">
<p>Now copy the file from <code>dbfs://FileStore</code> that you just uploaded into the local file system of the Driver.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.cp(&quot;dbfs:/FileStore/tables/IsIt1or2Coins&quot;, &quot;file:/tmp/IsIt1or2Coins&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">ls -al /tmp/IsIt1or2Coins
</code></pre>
</div>
<div class="cell markdown">
<p>Note it is a big static executible with all dependencies inbuilt (it uses GNU Scientific Library and a specialized C++ Library called C-XSC or C Extended for Scientific Computing to do hard-ware optimized rigorous numerical proofs using Interval-Extended Hessian Differentiation Arithmetics over Rounding-Controlled Hardware-Specified Machine Intervals).</p>
<p>Just note it is over 6.5MB. Also we need to change the permissions so it is indeed executible.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">chmod +x /tmp/IsIt1or2Coins
</code></pre>
</div>
<div class="cell markdown">
<h1 id="usage-instructions-for-isit1or2coins-1"><a class="header" href="#usage-instructions-for-isit1or2coins-1">Usage instructions for IsIt1or2Coins</a></h1>
<p><code>./IsIt1or2Coins numboxes numiter seed numtosses1 heads1 numtosses2 heads2 logScale</code> - numboxes = Number of boxes for Moore Rejection Sampling (Rigorous von Neumann Rejection Sampler) - numiter = Number of samples drawn from posterior distribution to estimate the model probabilities - seed = a random number seed - numtosses1 = number of tosses for the first coin - heads1 = number of heads shown up on the first coin - numtosses2 = number of tosses for the second coin - heads2 = number of heads shown up on the second coin - logscale = True/False as Int</p>
<p>Don't worry about the details of what the executible <code>IsIt1or2Coins</code> is doing for now. Just realise that this executible takes some input on command-line and gives some output.</p>
</div>
<div class="cell markdown">
<p>Let's make sure the executible takes input and returns output string on the Driver node.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">/tmp/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh"># You can also do it like this

/dbfs/FileStore/tables/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1
</code></pre>
</div>
<div class="cell markdown">
<h2 id="moving-the-executables-to-the-worker-nodes-1"><a class="header" href="#moving-the-executables-to-the-worker-nodes-1">Moving the executables to the worker nodes</a></h2>
</div>
<div class="cell markdown">
<p>To copy the executible from <code>dbfs</code> to the local drive of each executor you can use the following helper function.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import scala.sys.process._
import scala.concurrent.duration._
// from Ivan Sadikov

def copyFile(): Unit = {
  &quot;mkdir -p /tmp/executor/bin&quot;.!!
  &quot;cp /dbfs/FileStore/tables/IsIt1or2Coins /tmp/executor/bin/&quot;.!!
}

sc.runOnEachExecutor(copyFile, new FiniteDuration(1, HOURS))
</code></pre>
</div>
<div class="cell markdown">
<p>Now, let us use piped RDDs via <code>bash</code> to execute the given command in each partition as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val input = Seq(&quot;/tmp/executor/bin/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1&quot;, &quot;/tmp/executor/bin/IsIt1or2Coins 1000 100 234565432 1000 500 1200 600 1&quot;)

val output = sc
  .parallelize(input)
  .repartition(2)
  .pipe(&quot;bash&quot;)
  .collect()
</code></pre>
</div>
<div class="cell markdown">
<p>In fact, you can just use <code>DBFS FUSE</code> to run the commands without any file copy in databricks-provisioned Spark clusters we are on here:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val isIt1or2StaticExecutible = &quot;/dbfs/FileStore/tables/IsIt1or2Coins&quot;
val same_input = Seq(s&quot;$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1&quot;, 
                     s&quot;$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1&quot;)

val same_output = sc
  .parallelize(same_input)
  .repartition(2)
  .pipe(&quot;bash&quot;)
  .collect()
</code></pre>
</div>
<div class="cell markdown">
<p>Thus by mixing several different executibles that are statically compiled for linux 64 bit machine, we can mix and match multiple executibles with appropriate inputs.</p>
<p>The resulting outputs can themselves be re-processed in Spark to feed into toher pipedRDDs or normal RDDs or DataFrames and DataSets.</p>
</div>
<div class="cell markdown">
<p>Finally, we can have more than one command per partition and then use <code>mapPartitions</code> to send all the executible commands within the input partition that is to be run by the executor in which that partition resides as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val isIt1or2StaticExecutible = &quot;/dbfs/FileStore/tables/IsIt1or2Coins&quot;

// let us make 2 commands in each of the 2 input partitions
val same_input_mp = Seq(s&quot;$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1&quot;, 
                        s&quot;$isIt1or2StaticExecutible 1000 100 123456789 1000 500 1200 600 1&quot;,
                        s&quot;$isIt1or2StaticExecutible 1000 100 123456789 1000 500 1200 600 1&quot;,
                        s&quot;$isIt1or2StaticExecutible 1000 100 234565432 1000 500 1200 600 1&quot;)

val same_output_mp = sc
  .parallelize(same_input)
  .repartition(2)
  .pipe(&quot;bash&quot;)
  .mapPartitions(x =&gt; Seq(x.mkString(&quot;\n&quot;)).iterator)
  .collect()
</code></pre>
</div>
<div class="cell markdown">
<p>allCatch is a useful tool to use as a filtering function when testing if a command will work without error.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import scala.util.control.Exception.allCatch
(allCatch opt &quot; 12 &quot;.trim.toLong).isDefined
</code></pre>
</div>
<div class="cell markdown">
<p>The following should only be done after you have been introduced to Notebook: <code>033_OBO_PipedRDD_RigorousBayesianABTesting</code> and the Old baile Online Data.</p>
<p><strong>TODO</strong>: The below needs redo with DBFS FUSE.</p>
</div>
<div class="cell markdown">
<h1 id="parsing-the-output-from-isit1or2coins"><a class="header" href="#parsing-the-output-from-isit1or2coins">Parsing the output from <code>IsIt1or2Coins</code></a></h1>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">/**
 * Returns the label proportions from the output of IsIt1or2Coins
 *
 * This function takes an array of Strings, where each element
 * contains the whole output of one execution of IsIt1or2Coins.
 * It returns an Array[Array[Double]], where the first index denotes which
 * execution it belonged to, the second index is whether it is label0 or label1
 * i.e. it is a numExec x 2 array.
 */

def getLabelProps(input : Array[String]):Array[Array[Double]] = {
  input
  .map(out =&gt; out
       .split(&quot;\n&quot;)
       .filter(line =&gt; line.contains(&quot;label:&quot;))
       .map(filtLine =&gt; filtLine
            .split(&quot; &quot;)
            .filter(line =&gt; (allCatch opt line.trim.toDouble).isDefined)
            .map(filtFiltLine =&gt; filtFiltLine.toDouble)))
  .map(trial =&gt; trial.map(labels =&gt; labels(1)))
}

/**
 * Returns the label means from the output of IsIt1or2Coins
 *
 * This function takes an array of Strings, where each element
 * contains the whole output of one execution of IsIt1or2Coins.
 * It returns an Array[Array[Double]], where the first index denotes which
 * execution it belonged to, the second index is whether it is label0 Mean 
 * or label1 Mean1 or label1 Mean2, i.e. it is a numExec x 3 array.
 */

def getLabelMeans(input : Array[String]):Array[Array[Double]] = {
  val output_pre = input
  .map(out =&gt; out.split(&quot;\n&quot;)
       .filter(line =&gt; (allCatch opt line.trim.toDouble).isDefined))
  .map(arr =&gt; arr.map(num =&gt; num.toDouble))
  // Some runs have such a low probability for a label that some end up being only 2 in length instead of three
  // That means we should pad with a 0 to fix this
  output_pre.map(trial =&gt; if (trial.length == 2) Array(0,trial(0),trial(1)) else trial)
}
</code></pre>
</div>
<div class="cell markdown">
<h1 id="providing-case-classes-for-input-and-output-for-easy-spark-communication"><a class="header" href="#providing-case-classes-for-input-and-output-for-easy-spark-communication">Providing case classes for input and output for easy spark communication</a></h1>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.sql.DataFrame

case class OutputRow(ID:Long, 
                     NumTosses1:Int, NumHeads1:Int,
                     NumTosses2:Int, NumHeads2:Int,
                     Label0Prob:Double,
                     Label1Prob:Double,
                     Label0Mean0:Double, Label0Mean1:Double, 
                     Label1Mean0:Double, Label1Mean1:Double)

case class InputOpts(ID:Long,NumBoxes:Int, NumIter:Int, Seed:Long, 
                     NumTosses1:Int, NumHeads1:Int,
                     NumTosses2:Int, NumHeads2:Int, 
                     LogScaling:Int) {
  def toExecutableString:String = {
    &quot;/tmp/IsIt1or2Coins &quot;+Array(NumBoxes, NumIter, Seed, NumTosses1, NumHeads1, NumTosses2, NumHeads2, LogScaling).mkString(&quot; &quot;)
  }
}

/**
 * Returns the result of running all trials in the array of InputOpts
 *
 * This function takes an Array[InputOpts], creates executable strings
 * via suckAs and runs all in a pipedRDD, after it creates this it will parse the output
 * and assemble it into the case class OutputRow, i.e. it returns an Array of OutputRow
 */

def execute(trials : Array[InputOpts]) = {
  sc.parallelize(trials.map(trial =&gt; trial.toExecutableString))
    .repartition(trials.length)
    .pipe(&quot;/tmp/suckAs.sh&quot;)
    .mapPartitions(x =&gt; Seq(x.mkString(&quot;\n&quot;).split(&quot;theSeed&quot;)).iterator) // We know that theSeed is included once in every output
    .collect
    .flatMap(x =&gt; x.filter(y =&gt; y.length &gt; 0)) // Since each collection of outputs are split at theSeed we need to remove all empty strings and flatmap
}

def parseOutput(trials: Array[InputOpts], res:Array[String]) = {
  val labProp = getLabelProps(res)
  val labMean = getLabelMeans(res)
  (trials zip labProp zip labMean)
  .map(trial =&gt; (trial._1._1,trial._1._2,trial._2))
  .map(trial =&gt; OutputRow(trial._1.ID,
                          trial._1.NumTosses1,trial._1.NumHeads1,
                          trial._1.NumTosses2,trial._1.NumHeads2,
                          trial._2(0),trial._2(1),
                          trial._3(0),trial._3(0),
                          trial._3(1),trial._3(2)
                         ))
}

/* Returns a DataFrame of the Array[OutputRow] for ease of displaying in Databricks */

def resultAsDF(result:Array[OutputRow]):DataFrame = {
  sc.parallelize(result).toDF
}
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val inputOpts = Array(InputOpts(1680,1000,1000,100,792,245,151,63,1), InputOpts(1690,1000,1000,100,1805,526,215,68,1), InputOpts(1700,1000,1000,100,1060,242,57,26,1),InputOpts(1710,1000,1000,100,1060,243,57,26,1),InputOpts(1720,1000,1000,100,1060,245,57,26,1),InputOpts(1730,1000,1000,100,1060,245,57,26,1))
val res = execute(inputOpts)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(resultAsDF(parseOutput(inputOpts,res)))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="topic-modeling-of-movie-dialogs-with-latent-dirichlet-allocation"><a class="header" href="#topic-modeling-of-movie-dialogs-with-latent-dirichlet-allocation">Topic Modeling of Movie Dialogs with Latent Dirichlet Allocation</a></h1>
<p><strong>Let us cluster the conversations from different movies!</strong></p>
<p>This notebook will provide a brief algorithm summary, links for further reading, and an example of how to use LDA for Topic Modeling.</p>
<p><strong>not tested in Spark 2.2+ yet (see 034 notebook for syntactic issues, if any)</strong></p>
</div>
<div class="cell markdown">
<h2 id="algorithm-summary"><a class="header" href="#algorithm-summary">Algorithm Summary</a></h2>
<ul>
<li><strong>Task</strong>: Identify topics from a collection of text documents</li>
<li><strong>Input</strong>: Vectors of word counts</li>
<li><strong>Optimizers</strong>:
<ul>
<li>EMLDAOptimizer using <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation Maximization</a></li>
<li>OnlineLDAOptimizer using Iterative Mini-Batch Sampling for <a href="https://www.cs.princeton.edu/%7Eblei/papers/HoffmanBleiBach2010b.pdf">Online Variational Bayes</a></li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="links"><a class="header" href="#links">Links</a></h2>
<ul>
<li>Spark API docs
<ul>
<li>Scala: <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.LDA">LDA</a></li>
<li>Python: <a href="https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.clustering.LDA">LDA</a></li>
</ul>
</li>
<li><a href="http://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda">MLlib Programming Guide</a></li>
<li><a href="http://spark.apache.org/docs/latest/ml-features.html">ML Feature Extractors &amp; Transformers</a></li>
<li><a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Wikipedia: Latent Dirichlet Allocation</a></li>
</ul>
</div>
<div class="cell markdown">
<h2 id="readings-for-lda"><a class="header" href="#readings-for-lda">Readings for LDA</a></h2>
<ul>
<li>A high-level introduction to the topic from Communications of the ACM
<ul>
<li><a href="http://www.cs.columbia.edu/%7Eblei/papers/Blei2012.pdf">http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf</a></li>
</ul>
</li>
<li>A very good high-level humanities introduction to the topic (recommended by Chris Thomson in English Department at UC, Ilam):
<ul>
<li><a href="http://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/">http://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/</a></li>
</ul>
</li>
</ul>
<p>Also read the methodological and more formal papers cited in the above links if you want to know more.</p>
</div>
<div class="cell markdown">
<p>Let's get a bird's eye view of LDA from http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf next.</p>
<ul>
<li>See pictures (hopefully you read the paper last night!)</li>
<li>Algorithm of the generative model (this is unsupervised clustering)</li>
<li>For a careful introduction to the topic see Section 27.3 and 27.4 (pages 950-970) pf Murphy's <em>Machine Learning: A Probabilistic Perspective, MIT Press, 2012</em>.</li>
<li>We will be quite application focussed or applied here!</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//This allows easy embedding of publicly available information into any other notebook
//when viewing in git-book just ignore this block - you may have to manually chase the URL in frameIt(&quot;URL&quot;).
//Example usage:
// displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;,250))
def frameIt( u:String, h:Int ) : String = {
      &quot;&quot;&quot;&lt;iframe 
 src=&quot;&quot;&quot;&quot;+ u+&quot;&quot;&quot;&quot;
 width=&quot;95%&quot; height=&quot;&quot;&quot;&quot; + h + &quot;&quot;&quot;&quot;
 sandbox&gt;
  &lt;p&gt;
    &lt;a href=&quot;http://spark.apache.org/docs/latest/index.html&quot;&gt;
      Fallback link for browsers that, unlikely, don't support frames
    &lt;/a&gt;
  &lt;/p&gt;
&lt;/iframe&gt;&quot;&quot;&quot;
   }
displayHTML(frameIt(&quot;http://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/&quot;,900))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Topics_in_LDA&quot;,250))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Model&quot;,600))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">displayHTML(frameIt(&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Mathematical_definition&quot;,910))
</code></pre>
</div>
<div class="cell markdown">
<h2 id="probabilistic-topic-modeling-example"><a class="header" href="#probabilistic-topic-modeling-example">Probabilistic Topic Modeling Example</a></h2>
<p>This is an outline of our Topic Modeling workflow. Feel free to jump to any subtopic to find out more.</p>
<ul>
<li>Step 0. Dataset Review</li>
<li>Step 1. Downloading and Loading Data into DBFS
<ul>
<li>(Step 1. only needs to be done once per shard - see details at the end of the notebook for Step 1.)</li>
</ul>
</li>
<li>Step 2. Loading the Data and Data Cleaning</li>
<li>Step 3. Text Tokenization</li>
<li>Step 4. Remove Stopwords</li>
<li>Step 5. Vector of Token Counts</li>
<li>Step 6. Create LDA model with Online Variational Bayes</li>
<li>Step 7. Review Topics</li>
<li>Step 8. Model Tuning - Refilter Stopwords</li>
<li>Step 9. Create LDA model with Expectation Maximization</li>
<li>Step 10. Visualize Results</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="step-0-dataset-review"><a class="header" href="#step-0-dataset-review">Step 0. Dataset Review</a></h2>
<p>In this example, we will use the <a href="https://people.mpi-sws.org/%7Ecristian/Cornell_Movie-Dialogs_Corpus.html">Cornell Movie Dialogs Corpus</a>.</p>
<p>Here is the <code>README.txt</code>:</p>
<hr />
<hr />
<p>Cornell Movie-Dialogs Corpus</p>
<p>Distributed together with:</p>
<p>&quot;Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs&quot; Cristian Danescu-Niculescu-Mizil and Lillian Lee Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, ACL 2011.</p>
<p>(this paper is included in this zip file)</p>
<p>NOTE: If you have results to report on these corpora, please send email to cristian@cs.cornell.edu or llee@cs.cornell.edu so we can add you to our list of people using this data. Thanks!</p>
<p>Contents of this README:</p>
<pre><code>    A) Brief description
    B) Files description
    C) Details on the collection procedure
    D) Contact
</code></pre>
<p>A) Brief description:</p>
<p>This corpus contains a metadata-rich collection of fictional conversations extracted from raw movie scripts:</p>
<ul>
<li>220,579 conversational exchanges between 10,292 pairs of movie characters</li>
<li>involves 9,035 characters from 617 movies</li>
<li>in total 304,713 utterances</li>
<li>movie metadata included: - genres - release year - IMDB rating - number of IMDB votes - IMDB rating</li>
<li>character metadata included: - gender (for 3,774 characters) - position on movie credits (3,321 characters)</li>
</ul>
<p>B) Files description:</p>
<p>In all files the field separator is &quot; +++$+++ &quot;</p>
<ul>
<li>
<p>movie<em>titles</em>metadata.txt - contains information about each movie title - fields: - movieID, - movie title, - movie year, - IMDB rating, - no. IMDB votes, - genres in the format ['genre1','genre2',...,'genreN']</p>
</li>
<li>
<p>movie<em>characters</em>metadata.txt - contains information about each movie character - fields: - characterID - character name - movieID - movie title - gender (&quot;?&quot; for unlabeled cases) - position in credits (&quot;?&quot; for unlabeled cases)</p>
</li>
<li>
<p>movie_lines.txt - contains the actual text of each utterance - fields: - lineID - characterID (who uttered this phrase) - movieID - character name - text of the utterance</p>
</li>
<li>
<p>movie<em>conversations.txt - the structure of the conversations - fields - characterID of the first character involved in the conversation - characterID of the second character involved in the conversation - movieID of the movie in which the conversation occurred - list of the utterances that make the conversation, in chronological order: ['lineID1','lineID2',...,'lineIDN'] has to be matched with movie</em>lines.txt to reconstruct the actual content</p>
</li>
<li>
<p>raw<em>script</em>urls.txt - the urls from which the raw sources were retrieved</p>
</li>
</ul>
<p>C) Details on the collection procedure:</p>
<p>We started from raw publicly available movie scripts (sources acknowledged in raw<em>script</em>urls.txt). In order to collect the metadata necessary for this study and to distinguish between two script versions of the same movie, we automatically matched each script with an entry in movie database provided by IMDB (The Internet Movie Database; data interfaces available at http://www.imdb.com/interfaces). Some amount of manual correction was also involved. When more than one movie with the same title was found in IMBD, the match was made with the most popular title (the one that received most IMDB votes)</p>
<p>After discarding all movies that could not be matched or that had less than 5 IMDB votes, we were left with 617 unique titles with metadata including genre, release year, IMDB rating and no. of IMDB votes and cast distribution. We then identified the pairs of characters that interact and separated their conversations automatically using simple data processing heuristics. After discarding all pairs that exchanged less than 5 conversational exchanges there were 10,292 left, exchanging 220,579 conversational exchanges (304,713 utterances). After automatically matching the names of the 9,035 involved characters to the list of cast distribution, we used the gender of each interpreting actor to infer the fictional gender of a subset of 3,321 movie characters (we raised the number of gendered 3,774 characters through manual annotation). Similarly, we collected the end credit position of a subset of 3,321 characters as a proxy for their status.</p>
<p>D) Contact:</p>
<p>Please email any questions to: cristian@cs.cornell.edu (Cristian Danescu-Niculescu-Mizil)</p>
<hr />
<hr />
</div>
<div class="cell markdown">
<h2 id="step-2-loading-the-data-and-data-cleaning"><a class="header" href="#step-2-loading-the-data-and-data-cleaning">Step 2. Loading the Data and Data Cleaning</a></h2>
<p>We have already used the wget command to download the file, and put it in our distributed file system (this process takes about 1 minute). To repeat these steps or to download data from another source follow the steps at the bottom of this worksheet on <strong>Step 1. Downloading and Loading Data into DBFS</strong>.</p>
<p>Let's make sure these files are in dbfs now:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// this is where the data resides in dbfs (see below to download it first, if you go to a new shard!)
display(dbutils.fs.ls(&quot;dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/&quot;)) 
</code></pre>
</div>
<div class="cell markdown">
<h2 id="conversations-data"><a class="header" href="#conversations-data">Conversations Data</a></h2>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sc.textFile(&quot;dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_conversations.txt&quot;).top(5).foreach(println)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Load text file, leave out file paths, convert all strings to lowercase
val conversationsRaw = sc.textFile(&quot;dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_conversations.txt&quot;).zipWithIndex()
</code></pre>
</div>
<div class="cell markdown">
<p>Review first 5 lines to get a sense for the data format.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">conversationsRaw.top(5).foreach(println) // the first five Strings in the RDD
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">conversationsRaw.count // there are over 83,000 conversations in total
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import scala.util.{Failure, Success}

val regexConversation = &quot;&quot;&quot;\s*(\w+)\s+(\+{3}\$\+{3})\s*(\w+)\s+(\2)\s*(\w+)\s+(\2)\s*(\[.*\]\s*$)&quot;&quot;&quot;.r

case class conversationLine(a: String, b: String, c: String, d: String)

val conversationsRaw = sc.textFile(&quot;dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_conversations.txt&quot;)
 .zipWithIndex()
  .map(x =&gt; 
          {
            val id:Long = x._2
            val line = x._1
            val pLine = regexConversation.findFirstMatchIn(line)
                               .map(m =&gt; conversationLine(m.group(1), m.group(3), m.group(5), m.group(7))) 
                                  match {
                                    case Some(l) =&gt; Success(l)
                                    case None =&gt; Failure(new Exception(s&quot;Non matching input: $line&quot;))
                                  }
              (id,pLine)
           }
  )
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">conversationsRaw.filter(x =&gt; x._2.isSuccess).count()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">conversationsRaw.filter(x =&gt; x._2.isFailure).count()
</code></pre>
</div>
<div class="cell markdown">
<p>The conversation number and line numbers of each conversation are in one line in <code>conversationsRaw</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">conversationsRaw.filter(x =&gt; x._2.isSuccess).take(5).foreach(println)
</code></pre>
</div>
<div class="cell markdown">
<p>Let's create <code>conversations</code> that have just the coversation id and line-number with order information.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val conversations 
    = conversationsRaw
      .filter(x =&gt; x._2.isSuccess)
      .flatMap { 
        case (id,Success(l))  
                  =&gt; { val conv = l.d.replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot;'&quot;,&quot;&quot;).replace(&quot; &quot;,&quot;&quot;)
                       val convLinesIndexed = conv.split(&quot;,&quot;).zipWithIndex
                       convLinesIndexed.map( cLI =&gt; (id, cLI._2, cLI._1))
                      }
       }.toDF(&quot;conversationID&quot;,&quot;intraConversationID&quot;,&quot;lineID&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">conversations.show(15)
</code></pre>
</div>
<div class="cell markdown">
<h2 id="movie-titles"><a class="header" href="#movie-titles">Movie Titles</a></h2>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val moviesMetaDataRaw = sc.textFile(&quot;dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_titles_metadata.txt&quot;)
moviesMetaDataRaw.top(5).foreach(println)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">moviesMetaDataRaw.count() // number of movies
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import scala.util.{Failure, Success}

/*  - contains information about each movie title
  - fields:
          - movieID,
          - movie title,
          - movie year,
          - IMDB rating,
          - no. IMDB votes,
          - genres in the format ['genre1','genre2',...,'genreN']
          */
val regexMovieMetaData = &quot;&quot;&quot;\s*(\w+)\s+(\+{3}\$\+{3})\s*(.+)\s+(\2)\s+(.+)\s+(\2)\s+(.+)\s+(\2)\s+(.+)\s+(\2)\s+(\[.*\]\s*$)&quot;&quot;&quot;.r

case class lineInMovieMetaData(movieID: String, movieTitle: String, movieYear: String, IMDBRating: String, NumIMDBVotes: String, genres: String)

val moviesMetaDataRaw = sc.textFile(&quot;dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_titles_metadata.txt&quot;)
  .map(line =&gt; 
          {
            val pLine = regexMovieMetaData.findFirstMatchIn(line)
                               .map(m =&gt; lineInMovieMetaData(m.group(1), m.group(3), m.group(5), m.group(7), m.group(9), m.group(11))) 
                                  match {
                                    case Some(l) =&gt; Success(l)
                                    case None =&gt; Failure(new Exception(s&quot;Non matching input: $line&quot;))
                                  }
              pLine
           }
  )
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">moviesMetaDataRaw.count
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">moviesMetaDataRaw.filter(x =&gt; x.isSuccess).count()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">moviesMetaDataRaw.filter(x =&gt; x.isSuccess).take(10).foreach(println)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//moviesMetaDataRaw.filter(x =&gt; x.isFailure).take(10).foreach(println) // to regex refine for casting
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val moviesMetaData 
    = moviesMetaDataRaw
      .filter(x =&gt; x.isSuccess)
      .map { case Success(l) =&gt; l }
      .toDF().select(&quot;movieID&quot;,&quot;movieTitle&quot;,&quot;movieYear&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">moviesMetaData.show(10,false)
</code></pre>
</div>
<div class="cell markdown">
<h2 id="lines-data"><a class="header" href="#lines-data">Lines Data</a></h2>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val linesRaw = sc.textFile(&quot;dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_lines.txt&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">linesRaw.count() // number of lines making up the conversations
</code></pre>
</div>
<div class="cell markdown">
<p>Review first 5 lines to get a sense for the data format.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">linesRaw.top(5).foreach(println)
</code></pre>
</div>
<div class="cell markdown">
<p>To see 5 random lines in the <code>lines.txt</code> evaluate the following cell.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">linesRaw.takeSample(false, 5).foreach(println)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import scala.util.{Failure, Success}

/*  field in line.txt are:
          - lineID
          - characterID (who uttered this phrase)
          - movieID
          - character name
          - text of the utterance
          */
val regexLine = &quot;&quot;&quot;\s*(\w+)\s+(\+{3}\$\+{3})\s*(\w+)\s+(\2)\s*(\w+)\s+(\2)\s*(.+)\s+(\2)\s*(.*$)&quot;&quot;&quot;.r

case class lineInMovie(lineID: String, characterID: String, movieID: String, characterName: String, text: String)

val linesRaw = sc.textFile(&quot;dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_lines.txt&quot;)
  .map(line =&gt; 
          {
            val pLine = regexLine.findFirstMatchIn(line)
                               .map(m =&gt; lineInMovie(m.group(1), m.group(3), m.group(5), m.group(7), m.group(9))) 
                                  match {
                                    case Some(l) =&gt; Success(l)
                                    case None =&gt; Failure(new Exception(s&quot;Non matching input: $line&quot;))
                                  }
              pLine
           }
  )
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">linesRaw.filter(x =&gt; x.isSuccess).count()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">linesRaw.filter(x =&gt; x.isFailure).count()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">linesRaw.filter(x =&gt; x.isSuccess).take(5).foreach(println)
</code></pre>
</div>
<div class="cell markdown">
<p>Let's make a DataFrame out of the successfully parsed line.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val lines 
    = linesRaw
      .filter(x =&gt; x.isSuccess)
      .map { case Success(l) =&gt; l }
      .toDF()
      .join(moviesMetaData, &quot;movieID&quot;) // and join it to get movie meta data
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">lines.show(5)
</code></pre>
</div>
<div class="cell markdown">
<h2 id="dialogs-with-lines"><a class="header" href="#dialogs-with-lines">Dialogs with Lines</a></h2>
<p>Let's join ght two DataFrames on <code>lineID</code> next.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val convLines = conversations.join(lines, &quot;lineID&quot;).sort($&quot;conversationID&quot;, $&quot;intraConversationID&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">convLines.count
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">conversations.count
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(convLines)
</code></pre>
</div>
<div class="cell markdown">
<p>Let's amalgamate the texts utered in the same conversations together.</p>
<p>By doing this we loose all the information in the order of utterance.</p>
<p>But this is fine as we are going to do LDA with just the <em>first-order information of words uttered in each conversation</em> by anyone involved in the dialogue.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.sql.functions.{collect_list, udf, lit, concat_ws}

val corpusDF = convLines.groupBy($&quot;conversationID&quot;,$&quot;movieID&quot;)
  .agg(concat_ws(&quot; :-()-: &quot;,collect_list($&quot;text&quot;)).alias(&quot;corpus&quot;))
  .join(moviesMetaData, &quot;movieID&quot;) // and join it to get movie meta data
  .select($&quot;conversationID&quot;.as(&quot;id&quot;),$&quot;corpus&quot;,$&quot;movieTitle&quot;,$&quot;movieYear&quot;)
  .cache()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">corpusDF.count()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">corpusDF.take(5).foreach(println)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(corpusDF)
</code></pre>
</div>
<div class="cell markdown">
<h2 id="feature-extraction-and-transformation-apis"><a class="header" href="#feature-extraction-and-transformation-apis">Feature extraction and transformation APIs</a></h2>
</div>
<div class="cell markdown">
<p>We will use the convenient <a href="http://spark.apache.org/docs/latest/ml-features.html">Feature extraction and transformation APIs</a>.</p>
</div>
<div class="cell markdown">
<h2 id="step-3-text-tokenization"><a class="header" href="#step-3-text-tokenization">Step 3. Text Tokenization</a></h2>
<p>We will use the RegexTokenizer to split each document into tokens. We can setMinTokenLength() here to indicate a minimum token length, and filter away all tokens that fall below the minimum. See:</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/ml-features.html#tokenizer">http://spark.apache.org/docs/latest/ml-features.html#tokenizer</a>.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.ml.feature.RegexTokenizer

// Set params for RegexTokenizer
val tokenizer = new RegexTokenizer()
.setPattern(&quot;[\\W_]+&quot;) // break by white space character(s)
.setMinTokenLength(4) // Filter away tokens with length &lt; 4
.setInputCol(&quot;corpus&quot;) // name of the input column
.setOutputCol(&quot;tokens&quot;) // name of the output column

// Tokenize document
val tokenized_df = tokenizer.transform(corpusDF)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(tokenized_df.sample(false,0.001,1234L)) 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(tokenized_df.sample(false,0.001,123L).select(&quot;tokens&quot;))
</code></pre>
</div>
<div class="cell markdown">
<h2 id="step-4-remove-stopwords"><a class="header" href="#step-4-remove-stopwords">Step 4. Remove Stopwords</a></h2>
<p>We can easily remove stopwords using the StopWordsRemover(). See:</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/ml-features.html#stopwordsremover">http://spark.apache.org/docs/latest/ml-features.html#stopwordsremover</a>.</li>
</ul>
</div>
<div class="cell markdown">
<p>If a list of stopwords is not provided, the StopWordsRemover() will use <a href="http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words">this list of stopwords</a>, also shown below, by default.</p>
<p><code>a,about,above,across,after,afterwards,again,against,all,almost,alone,along,already,also,although,always,am,among,amongst,amoungst,amount,an,and,another,any,anyhow,anyone,anything,anyway,anywhere, are,around,as,at,back,be,became,because,become,becomes,becoming,been,before,beforehand,behind,being,below,beside,besides,between,beyond,bill,both,bottom,but,by,call,can,cannot,cant,co,computer,con,could, couldnt,cry,de,describe,detail,do,done,down,due,during,each,eg,eight,either,eleven,else,elsewhere,empty,enough,etc,even,ever,every,everyone,everything,everywhere,except,few,fifteen,fify,fill,find,fire,first, five,for,former,formerly,forty,found,four,from,front,full,further,get,give,go,had,has,hasnt,have,he,hence,her,here,hereafter,hereby,herein,hereupon,hers,herself,him,himself,his,how,however,hundred,i,ie,if, in,inc,indeed,interest,into,is,it,its,itself,keep,last,latter,latterly,least,less,ltd,made,many,may,me,meanwhile,might,mill,mine,more,moreover,most,mostly,move,much,must,my,myself,name,namely,neither,never, nevertheless,next,nine,no,nobody,none,noone,nor,not,nothing,now,nowhere,of,off,often,on,once,one,only,onto,or,other,others,otherwise,our,ours,ourselves,out,over,own,part,per,perhaps,please,put,rather,re,same, see,seem,seemed,seeming,seems,serious,several,she,should,show,side,since,sincere,six,sixty,so,some,somehow,someone,something,sometime,sometimes,somewhere,still,such,system,take,ten,than,that,the,their,them, themselves,then,thence,there,thereafter,thereby,therefore,therein,thereupon,these,they,thick,thin,third,this,those,though,three,through,throughout,thru,thus,to,together,too,top,toward,towards,twelve,twenty,two, un,under,until,up,upon,us,very,via,was,we,well,were,what,whatever,when,whence,whenever,where,whereafter,whereas,whereby,wherein,whereupon,wherever,whether,which,while,whither,who,whoever,whole,whom,whose,why,will, with,within,without,would,yet,you,your,yours,yourself,yourselves</code></p>
<p>You can use <code>getStopWords()</code> to see the list of stopwords that will be used.</p>
<p>In this example, we will specify a list of stopwords for the StopWordsRemover() to use. We do this so that we can add on to the list later on.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/tmp/stopwords&quot;)) // check if the file already exists from earlier wget and dbfs-load
</code></pre>
</div>
<div class="cell markdown">
<p>If the file <code>dbfs:/tmp/stopwords</code> already exists then skip the next two cells, otherwise download and load it into DBFS by uncommenting and evaluating the next two cells.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">wget http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words -O /tmp/stopwords # uncomment '//' at the beginning and repeat only if needed again
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-fs">cp file:/tmp/stopwords dbfs:/tmp/stopwords 
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// List of stopwords
val stopwords = sc.textFile(&quot;/tmp/stopwords&quot;).collect()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">stopwords.length // find the number of stopwords in the scala Array[String]
</code></pre>
</div>
<div class="cell markdown">
<p>Finally, we can just remove the stopwords using the <code>StopWordsRemover</code> as follows:</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.ml.feature.StopWordsRemover

// Set params for StopWordsRemover
val remover = new StopWordsRemover()
.setStopWords(stopwords) // This parameter is optional
.setInputCol(&quot;tokens&quot;)
.setOutputCol(&quot;filtered&quot;)

// Create new DF with Stopwords removed
val filtered_df = remover.transform(tokenized_df)
</code></pre>
</div>
<div class="cell markdown">
<h2 id="step-5-vector-of-token-counts"><a class="header" href="#step-5-vector-of-token-counts">Step 5. Vector of Token Counts</a></h2>
<p>LDA takes in a vector of token counts as input. We can use the <code>CountVectorizer()</code> to easily convert our text documents into vectors of token counts.</p>
<p>The <code>CountVectorizer</code> will return <code>(VocabSize, Array(Indexed Tokens), Array(Token Frequency))</code>.</p>
<p>Two handy parameters to note:</p>
<ul>
<li><code>setMinDF</code>: Specifies the minimum number of different documents a term must appear in to be included in the vocabulary.</li>
<li><code>setMinTF</code>: Specifies the minimum number of times a term has to appear in a document to be included in the vocabulary.</li>
</ul>
<p>See:</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/ml-features.html#countvectorizer">http://spark.apache.org/docs/latest/ml-features.html#countvectorizer</a>.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.ml.feature.CountVectorizer

// Set params for CountVectorizer
val vectorizer = new CountVectorizer()
.setInputCol(&quot;filtered&quot;)
.setOutputCol(&quot;features&quot;)
.setVocabSize(10000) 
.setMinDF(5) // the minimum number of different documents a term must appear in to be included in the vocabulary.
.fit(filtered_df)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create vector of token counts
val countVectors = vectorizer.transform(filtered_df).select(&quot;id&quot;, &quot;features&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// see the first countVectors
countVectors.take(1)
</code></pre>
</div>
<div class="cell markdown">
<p>To use the LDA algorithm in the MLlib library, we have to convert the DataFrame back into an RDD.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Convert DF to RDD - ideally we should use ml for everything an not ml and mllib ; DAN
import org.apache.spark.ml.feature.{CountVectorizer, RegexTokenizer, StopWordsRemover}
import org.apache.spark.ml.linalg.{Vector =&gt; MLVector}
import org.apache.spark.mllib.clustering.{LDA, OnlineLDAOptimizer}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.sql.{Row, SparkSession}

val lda_countVector = countVectors.map { case Row(id: Long, countVector: MLVector) =&gt; (id, Vectors.fromML(countVector)) }.rdd
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// format: Array(id, (VocabSize, Array(indexedTokens), Array(Token Frequency)))
lda_countVector.take(1)
</code></pre>
</div>
<div class="cell markdown">
<h2 id="lets-get-an-overview-of-lda-in-sparks-mllib"><a class="header" href="#lets-get-an-overview-of-lda-in-sparks-mllib">Let's get an overview of LDA in Spark's MLLIB</a></h2>
<p>See:</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda">http://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda</a>.</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="create-lda-model-with-online-variational-bayes"><a class="header" href="#create-lda-model-with-online-variational-bayes">Create LDA model with Online Variational Bayes</a></h2>
<p>We will now set the parameters for LDA. We will use the OnlineLDAOptimizer() here, which implements Online Variational Bayes.</p>
<p>Choosing the number of topics for your LDA model requires a bit of domain knowledge. As we do not know the number of &quot;topics&quot;, we will set numTopics to be 20.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val numTopics = 20
</code></pre>
</div>
<div class="cell markdown">
<p>We will set the parameters needed to build our LDA model. We can also setMiniBatchFraction for the OnlineLDAOptimizer, which sets the fraction of corpus sampled and used at each iteration. In this example, we will set this to 0.8.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.mllib.clustering.{LDA, OnlineLDAOptimizer}

// Set LDA params
val lda = new LDA()
.setOptimizer(new OnlineLDAOptimizer().setMiniBatchFraction(0.8))
.setK(numTopics)
.setMaxIterations(3)
.setDocConcentration(-1) // use default values
.setTopicConcentration(-1) // use default values
</code></pre>
</div>
<div class="cell markdown">
<p>Create the LDA model with Online Variational Bayes.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val ldaModel = lda.run(lda_countVector)
</code></pre>
</div>
<div class="cell markdown">
<p>Watch <strong>Online Learning for Latent Dirichlet Allocation</strong> in NIPS2010 by Matt Hoffman (right click and open in new tab)</p>
<p><a href="http://videolectures.net/nips2010_hoffman_oll/thumb.jpg">![Matt Hoffman's NIPS 2010 Talk Online LDA]</a>](http://videolectures.net/nips2010<em>hoffman</em>oll/)</p>
<p>Also see the paper on <em>Online varioational Bayes</em> by Matt linked for more details (from the above URL): <a href="http://videolectures.net/site/normal_dl/tag=83534/nips2010_1291.pdf">http://videolectures.net/site/normal<em>dl/tag=83534/nips2010</em>1291.pdf</a></p>
</div>
<div class="cell markdown">
<p>Note that using the OnlineLDAOptimizer returns us a <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.LocalLDAModel">LocalLDAModel</a>, which stores the inferred topics of your corpus.</p>
</div>
<div class="cell markdown">
<h2 id="review-topics"><a class="header" href="#review-topics">Review Topics</a></h2>
<p>We can now review the results of our LDA model. We will print out all 20 topics with their corresponding term probabilities.</p>
<p>Note that you will get slightly different results every time you run an LDA model since LDA includes some randomization.</p>
<p>Let us review results of LDA model with Online Variational Bayes, step by step.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 5)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val vocabList = vectorizer.vocabulary
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val topics = topicIndices.map { case (terms, termWeights) =&gt;
  terms.map(vocabList(_)).zip(termWeights)
}
</code></pre>
</div>
<div class="cell markdown">
<p>Feel free to take things apart to understand!</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">topicIndices(0)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">topicIndices(0)._1
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">topicIndices(0)._1(0)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">vocabList(topicIndices(0)._1(0))
</code></pre>
</div>
<div class="cell markdown">
<p>Review Results of LDA model with Online Variational Bayes - Doing all four steps earlier at once.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 5)
val vocabList = vectorizer.vocabulary
val topics = topicIndices.map { case (terms, termWeights) =&gt;
  terms.map(vocabList(_)).zip(termWeights)
}
println(s&quot;$numTopics topics:&quot;)
topics.zipWithIndex.foreach { case (topic, i) =&gt;
  println(s&quot;TOPIC $i&quot;)
  topic.foreach { case (term, weight) =&gt; println(s&quot;$term\t$weight&quot;) }
  println(s&quot;==========&quot;)
}
</code></pre>
</div>
<div class="cell markdown">
<p>Going through the results, you may notice that some of the topic words returned are actually stopwords that are specific to our dataset (for eg: &quot;writes&quot;, &quot;article&quot;...). Let's try improving our model.</p>
</div>
<div class="cell markdown">
<h2 id="step-8-model-tuning---refilter-stopwords"><a class="header" href="#step-8-model-tuning---refilter-stopwords">Step 8. Model Tuning - Refilter Stopwords</a></h2>
<p>We will try to improve the results of our model by identifying some stopwords that are specific to our dataset. We will filter these stopwords out and rerun our LDA model to see if we get better results.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val add_stopwords = Array(&quot;whatever&quot;) // add  more stop-words like the name of your company!
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Combine newly identified stopwords to our exising list of stopwords
val new_stopwords = stopwords.union(add_stopwords)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.ml.feature.StopWordsRemover

// Set Params for StopWordsRemover with new_stopwords
val remover = new StopWordsRemover()
.setStopWords(new_stopwords)
.setInputCol(&quot;tokens&quot;)
.setOutputCol(&quot;filtered&quot;)

// Create new df with new list of stopwords removed
val new_filtered_df = remover.transform(tokenized_df)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Set Params for CountVectorizer
val vectorizer = new CountVectorizer()
.setInputCol(&quot;filtered&quot;)
.setOutputCol(&quot;features&quot;)
.setVocabSize(10000)
.setMinDF(5)
.fit(new_filtered_df)

// Create new df of countVectors
val new_countVectors = vectorizer.transform(new_filtered_df).select(&quot;id&quot;, &quot;features&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Convert DF to RDD
val new_lda_countVector = new_countVectors.map { case Row(id: Long, countVector: MLVector) =&gt; (id, Vectors.fromML(countVector)) }.rdd
</code></pre>
</div>
<div class="cell markdown">
<p>We will also increase MaxIterations to 10 to see if we get better results.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Set LDA parameters
val new_lda = new LDA()
.setOptimizer(new OnlineLDAOptimizer().setMiniBatchFraction(0.8))
.setK(numTopics)
.setMaxIterations(10)
.setDocConcentration(-1) // use default values
.setTopicConcentration(-1) // use default values
</code></pre>
</div>
<div class="cell markdown">
<h4 id="how-to-find-what-the-default-values-are"><a class="header" href="#how-to-find-what-the-default-values-are">How to find what the default values are?</a></h4>
<p>Dive into the source!!!</p>
<ol>
<li>Let's find the default value for <code>docConcentration</code> now.</li>
<li>Got to Apache Spark package Root: <a href="https://spark.apache.org/docs/latest/api/scala/#package">https://spark.apache.org/docs/latest/api/scala/#package</a></li>
</ol>
<ul>
<li>
<p>search for 'ml' in the search box on the top left (ml is for ml library)</p>
</li>
<li>
<p>Then find the <code>LDA</code> by scrolling below on the left to mllib's <code>clustering</code> methods and click on <code>LDA</code></p>
</li>
<li>
<p>Then click on the source code link which should take you here:</p>
<ul>
<li><a href="https://github.com/apache/spark/blob/v1.6.1/mllib/src/main/scala/org/apache/spark/ml/clustering/LDA.scala">https://github.com/apache/spark/blob/v1.6.1/mllib/src/main/scala/org/apache/spark/ml/clustering/LDA.scala</a></li>
<li>Now, simply go to the right function and see the following comment block:</li>
</ul>
<p>``` /**</p>
<ul>
<li>
<p>Concentration parameter (commonly named &quot;alpha&quot;) for the prior placed on documents'</p>
</li>
<li>
<p>distributions over topics (&quot;theta&quot;).</p>
</li>
<li></li>
<li>
<p>This is the parameter to a Dirichlet distribution, where larger values mean more smoothing</p>
</li>
<li>
<p>(more regularization).</p>
</li>
<li></li>
<li>
<p>If not set by the user, then docConcentration is set automatically. If set to</p>
</li>
<li>
<p>singleton vector [alpha], then alpha is replicated to a vector of length k in fitting.</p>
</li>
<li>
<p>Otherwise, the [[docConcentration]] vector must be length k.</p>
</li>
<li>
<p>(default = automatic)</p>
</li>
<li></li>
<li>
<p>Optimizer-specific parameter settings:</p>
</li>
<li>
<ul>
<li>EM</li>
</ul>
</li>
<li>
<ul>
<li>Currently only supports symmetric distributions, so all values in the vector should be</li>
</ul>
</li>
<li>
<pre><code>the same.
</code></pre>
</li>
<li>
<ul>
<li>Values should be &gt; 1.0</li>
</ul>
</li>
<li>
<ul>
<li>default = uniformly (50 / k) + 1, where 50/k is common in LDA libraries and +1 follows</li>
</ul>
</li>
<li>
<pre><code>from Asuncion et al. (2009), who recommend a +1 adjustment for EM.
</code></pre>
</li>
<li>
<ul>
<li>Online</li>
</ul>
</li>
<li>
<ul>
<li>Values should be &gt;= 0</li>
</ul>
</li>
<li>
<ul>
<li>default = uniformly (1.0 / k), following the implementation from</li>
</ul>
</li>
<li>
<pre><code>[[https://github.com/Blei-Lab/onlineldavb]].
</code></pre>
</li>
<li>
<p>@group param */ ```</p>
</li>
</ul>
</li>
</ul>
</div>
<div class="cell markdown">
<p><strong>HOMEWORK:</strong> Try to find the default value for <code>TopicConcentration</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create LDA model with stopwords refiltered
val new_ldaModel = new_lda.run(new_lda_countVector)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val topicIndices = new_ldaModel.describeTopics(maxTermsPerTopic = 5)
val vocabList = vectorizer.vocabulary
val topics = topicIndices.map { case (terms, termWeights) =&gt;
  terms.map(vocabList(_)).zip(termWeights)
}
println(s&quot;$numTopics topics:&quot;)
topics.zipWithIndex.foreach { case (topic, i) =&gt;
  println(s&quot;TOPIC $i&quot;)
  topic.foreach { case (term, weight) =&gt; println(s&quot;$term\t$weight&quot;) }
  println(s&quot;==========&quot;)
}
</code></pre>
</div>
<div class="cell markdown">
<h2 id="step-9-create-lda-model-with-expectation-maximization"><a class="header" href="#step-9-create-lda-model-with-expectation-maximization">Step 9. Create LDA model with Expectation Maximization</a></h2>
<p>Let's try creating an LDA model with Expectation Maximization on the data that has been refiltered for additional stopwords. We will also increase MaxIterations here to 100 to see if that improves results. See:</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda">http://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda</a>.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.mllib.clustering.EMLDAOptimizer

// Set LDA parameters
val em_lda = new LDA()
.setOptimizer(new EMLDAOptimizer())
.setK(numTopics)
.setMaxIterations(100)
.setDocConcentration(-1) // use default values
.setTopicConcentration(-1) // use default values
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val em_ldaModel = em_lda.run(new_lda_countVector) // takes a long long time 22 minutes
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.mllib.clustering.DistributedLDAModel;
val em_DldaModel = em_ldaModel.asInstanceOf[DistributedLDAModel]
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val top10ConversationsPerTopic = em_DldaModel.topDocumentsPerTopic(10)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">top10ConversationsPerTopic.length // number of topics
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//em_DldaModel.topicDistributions.take(10).foreach(println)
</code></pre>
</div>
<div class="cell markdown">
<p>Note that the EMLDAOptimizer produces a DistributedLDAModel, which stores not only the inferred topics but also the full training corpus and topic distributions for each document in the training corpus.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val topicIndices = em_ldaModel.describeTopics(maxTermsPerTopic = 5)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val vocabList = vectorizer.vocabulary
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">vocabList.size
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val topics = topicIndices.map { case (terms, termWeights) =&gt;
  terms.map(vocabList(_)).zip(termWeights)
}
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">vocabList(47) // 47 is the index of the term 'university' or the first term in topics - this may change due to randomness in algorithm
</code></pre>
</div>
<div class="cell markdown">
<p>This is just doing it all at once.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val topicIndices = em_ldaModel.describeTopics(maxTermsPerTopic = 5)
val vocabList = vectorizer.vocabulary
val topics = topicIndices.map { case (terms, termWeights) =&gt;
  terms.map(vocabList(_)).zip(termWeights)
}
println(s&quot;$numTopics topics:&quot;)
topics.zipWithIndex.foreach { case (topic, i) =&gt;
  println(s&quot;TOPIC $i&quot;)
  topic.foreach { case (term, weight) =&gt; println(s&quot;$term\t$weight&quot;) }
  println(s&quot;==========&quot;)
}
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">top10ConversationsPerTopic(2)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">top10ConversationsPerTopic(2)._1
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val scenesForTopic2 = sc.parallelize(top10ConversationsPerTopic(2)._1).toDF(&quot;id&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(scenesForTopic2.join(corpusDF,&quot;id&quot;))
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sc.parallelize(top10ConversationsPerTopic(2)._1).toDF(&quot;id&quot;).join(corpusDF,&quot;id&quot;).show(10,false)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sc.parallelize(top10ConversationsPerTopic(5)._1).toDF(&quot;id&quot;).join(corpusDF,&quot;id&quot;).show(10,false)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">corpusDF.show(5)
</code></pre>
</div>
<div class="cell markdown">
<p>We've managed to get some good results here. For example, we can easily infer that Topic 2 is about space, Topic 3 is about israel, etc.</p>
<p>We still get some ambiguous results like Topic 0.</p>
</div>
<div class="cell markdown">
<p>To improve our results further, we could employ some of the below methods:</p>
<ul>
<li>Refilter data for additional data-specific stopwords</li>
<li>Use Stemming or Lemmatization to preprocess data</li>
<li>Experiment with a smaller number of topics, since some of these topics in the 20 Newsgroups are pretty similar</li>
<li>Increase model's MaxIterations</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="visualize-results"><a class="header" href="#visualize-results">Visualize Results</a></h2>
<p>We will try visualizing the results obtained from the EM LDA model with a d3 bubble chart.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Zip topic terms with topic IDs
val termArray = topics.zipWithIndex
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Transform data into the form (term, probability, topicId)
val termRDD = sc.parallelize(termArray)
val termRDD2 =termRDD.flatMap( (x: (Array[(String, Double)], Int)) =&gt; {
  val arrayOfTuple = x._1
  val topicId = x._2
  arrayOfTuple.map(el =&gt; (el._1, el._2, topicId))
})
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create DF with proper column names
val termDF = termRDD2.toDF.withColumnRenamed(&quot;_1&quot;, &quot;term&quot;).withColumnRenamed(&quot;_2&quot;, &quot;probability&quot;).withColumnRenamed(&quot;_3&quot;, &quot;topicId&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(termDF)
</code></pre>
</div>
<div class="cell markdown">
<p>We will convert the DataFrame into a JSON format, which will be passed into d3.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create JSON data
val rawJson = termDF.toJSON.collect().mkString(&quot;,\n&quot;)
</code></pre>
</div>
<div class="cell markdown">
<p>We are now ready to use D3 on the rawJson data.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">displayHTML(s&quot;&quot;&quot;
&lt;!DOCTYPE html&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;style&gt;

circle {
  fill: rgb(31, 119, 180);
  fill-opacity: 0.5;
  stroke: rgb(31, 119, 180);
  stroke-width: 1px;
}

.leaf circle {
  fill: #ff7f0e;
  fill-opacity: 1;
}

text {
  font: 14px sans-serif;
}

&lt;/style&gt;
&lt;body&gt;
&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js&quot;&gt;&lt;/script&gt;
&lt;script&gt;

var json = {
 &quot;name&quot;: &quot;data&quot;,
 &quot;children&quot;: [
  {
     &quot;name&quot;: &quot;topics&quot;,
     &quot;children&quot;: [
      ${rawJson}
     ]
    }
   ]
};

var r = 1500,
    format = d3.format(&quot;,d&quot;),
    fill = d3.scale.category20c();

var bubble = d3.layout.pack()
    .sort(null)
    .size([r, r])
    .padding(1.5);

var vis = d3.select(&quot;body&quot;).append(&quot;svg&quot;)
    .attr(&quot;width&quot;, r)
    .attr(&quot;height&quot;, r)
    .attr(&quot;class&quot;, &quot;bubble&quot;);

  
var node = vis.selectAll(&quot;g.node&quot;)
    .data(bubble.nodes(classes(json))
    .filter(function(d) { return !d.children; }))
    .enter().append(&quot;g&quot;)
    .attr(&quot;class&quot;, &quot;node&quot;)
    .attr(&quot;transform&quot;, function(d) { return &quot;translate(&quot; + d.x + &quot;,&quot; + d.y + &quot;)&quot;; })
    color = d3.scale.category20();
  
  node.append(&quot;title&quot;)
      .text(function(d) { return d.className + &quot;: &quot; + format(d.value); });

  node.append(&quot;circle&quot;)
      .attr(&quot;r&quot;, function(d) { return d.r; })
      .style(&quot;fill&quot;, function(d) {return color(d.topicName);});

var text = node.append(&quot;text&quot;)
    .attr(&quot;text-anchor&quot;, &quot;middle&quot;)
    .attr(&quot;dy&quot;, &quot;.3em&quot;)
    .text(function(d) { return d.className.substring(0, d.r / 3)});
  
  text.append(&quot;tspan&quot;)
      .attr(&quot;dy&quot;, &quot;1.2em&quot;)
      .attr(&quot;x&quot;, 0)
      .text(function(d) {return Math.ceil(d.value * 10000) /10000; });

// Returns a flattened hierarchy containing all leaf nodes under the root.
function classes(root) {
  var classes = [];

  function recurse(term, node) {
    if (node.children) node.children.forEach(function(child) { recurse(node.term, child); });
    else classes.push({topicName: node.topicId, className: node.term, value: node.probability});
  }

  recurse(null, root);
  return {children: classes};
}
&lt;/script&gt;
&quot;&quot;&quot;)
</code></pre>
</div>
<div class="cell markdown">
<h2 id="step-1-downloading-and-loading-data-into-dbfs"><a class="header" href="#step-1-downloading-and-loading-data-into-dbfs">Step 1. Downloading and Loading Data into DBFS</a></h2>
<p>Here are the steps taken for downloading and saving data to the distributed file system. Uncomment them for repeating this process on your databricks cluster or for downloading a new source of data.</p>
<p>Unfortunately, the original data at:</p>
<ul>
<li><a href="http://www.mpi-sws.org/%7Ecristian/data/cornell_movie_dialogs_corpus.zip">http://www.mpi-sws.org/~cristian/data/cornell<em>movie</em>dialogs_corpus.zip</a></li>
</ul>
<p>is not suited for manipulation and loading into dbfs easily. So the data has been downloaded, directory renamed without white spaces, superfluous OS-specific files removed, <code>dos2unix</code>'d, <code>tar -zcvf</code>'d and uploaded to the following URL for an easily dbfs-loadable download:</p>
<ul>
<li><a href="http://lamastex.org/datasets/public/nlp/cornell_movie_dialogs_corpus.tgz">http://lamastex.org/datasets/public/nlp/cornell<em>movie</em>dialogs_corpus.tgz</a></li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">wget http://lamastex.org/datasets/public/nlp/cornell_movie_dialogs_corpus.tgz
</code></pre>
</div>
<div class="cell markdown">
<p>Untar the file.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">tar zxvf cornell_movie_dialogs_corpus.tgz
</code></pre>
</div>
<div class="cell markdown">
<p>Let us list and load all the files into dbfs after <code>dbfs.fs.mkdirs(...)</code> to create the directory <code>dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/</code>.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-sh">pwd &amp;&amp; ls -al cornell_movie_dialogs_corpus
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.rm(&quot;dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/&quot;,true)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.mkdirs(&quot;dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">dbutils.fs.cp(&quot;file:///databricks/driver/cornell_movie_dialogs_corpus/movie_characters_metadata.txt&quot;,&quot;dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_characters_metadata.txt&quot;)
dbutils.fs.cp(&quot;file:///databricks/driver/cornell_movie_dialogs_corpus/movie_conversations.txt&quot;,&quot;dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_conversations.txt&quot;)
dbutils.fs.cp(&quot;file:///databricks/driver/cornell_movie_dialogs_corpus/movie_lines.txt&quot;,&quot;dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_lines.txt&quot;)
dbutils.fs.cp(&quot;file:///databricks/driver/cornell_movie_dialogs_corpus/movie_titles_metadata.txt&quot;,&quot;dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/movie_titles_metadata.txt&quot;)
dbutils.fs.cp(&quot;file:///databricks/driver/cornell_movie_dialogs_corpus/raw_script_urls.txt&quot;,&quot;dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/raw_script_urls.txt&quot;)
dbutils.fs.cp(&quot;file:///databricks/driver/cornell_movie_dialogs_corpus/README.txt&quot;,&quot;dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/README.txt&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;dbfs:/datasets/sds/nlp/cornell_movie_dialogs_corpus/&quot;))
</code></pre>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
        
        

    </body>
</html>
